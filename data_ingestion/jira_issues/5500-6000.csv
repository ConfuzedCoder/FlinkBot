Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Dependent),Outward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Issue split),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Outward issue link (Supercedes),Inward issue link (Testing),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Add data type compatibility check in SchemaChange.updateColumnType,FLINK-30013,13502080,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,14/Nov/22 07:13,15/Nov/22 02:01,04/Jun/24 20:41,15/Nov/22 02:01,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Add LogicalTypeCasts.supportsImplicitCast to check operation in SchemaChange.updateColumnType to avoid data type conversion failures when reading data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 02:01:07 UTC 2022,,,,,,,,,,"0|z1c2hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 02:01;lzljs3620320;master: 13d2ce91ff2bd81fd474363d16c9cf8f2cbb5fb1;;;",,,,,,,,,,,,,,,,,,,,,,
A typo in official Table Store document.,FLINK-30012,13502079,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,StarBoy1005,StarBoy1005,StarBoy1005,14/Nov/22 07:05,14/Nov/22 11:22,04/Jun/24 20:41,14/Nov/22 11:22,1.16.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Found a typo in Rescale Bucket document which is ""exiting"".
[Rescale Bucket|https://nightlies.apache.org/flink/flink-table-store-docs-release-0.2/docs/development/rescale-bucket/#rescale-bucket]",Flink 1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 11:22:47 UTC 2022,,,,,,,,,,"0|z1c2hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 11:22;TsReaper;master: 5205627e553ce17bd414b33354c2c3c2fe8c41a5;;;",,,,,,,,,,,,,,,,,,,,,,
HiveCatalogGenericMetadataTest azure CI failed due to catalog does not exist,FLINK-30011,13502033,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,14/Nov/22 04:07,17/Mar/23 03:48,04/Jun/24 20:41,17/Mar/23 03:48,1.16.1,,,,,,,,,,Connectors / Hive,,,,0,,,,,"
{noformat}

Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetPartitionStats:1212 » Catalog F...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetPartition_PartitionNotExist:1160 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetPartition_PartitionSpecInvalid_invalidPartitionSpec:1124 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetPartition_PartitionSpecInvalid_sizeNotEqual:1139 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetPartition_TableNotPartitioned:1110 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetTableStats_TableNotExistException:1201 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testGetTable_TableNotExistException:323 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest.testHiveStatistics:251 » Catalog Failed to create ...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testListFunctions:749 » Catalog Failed...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testListPartitionPartialSpec:1188 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testListTables:498 » Catalog Failed to...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testListView:620 » Catalog Failed to c...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testPartitionExists:1174 » Catalog Fai...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testRenameTable_TableAlreadyExistException:483 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testRenameTable_TableNotExistException:465 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testRenameTable_TableNotExistException_ignored:477 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testRenameTable_nonPartitionedTable:451 » Catalog
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testRenameView:637 » Catalog Failed to...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest>CatalogTest.testTableExists:510 » Catalog Failed t...
Nov 13 01:55:18 [ERROR]   HiveCatalogHiveMetadataTest.testViewCompatibility:115 » Catalog Failed to crea...
Nov 13 01:55:18 [INFO] 
Nov 13 01:55:18 [ERROR] Tests run: 361, Failures: 0, Errors: 132, Skipped: 0
{noformat}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43104&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30433,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 04:07:37 UTC 2022,,,,,,,,,,"0|z1c27k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 04:07;leonard;[~luoyuxia] Could you take a look this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,
flink-quickstart-test failed due to could not resolve dependencies ,FLINK-30010,13502003,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,leonard,leonard,14/Nov/22 03:54,14/Nov/22 07:15,04/Jun/24 20:41,14/Nov/22 07:15,1.17.0,,,,,,1.17.0,,,,Examples,Tests,,,0,,,,,"
{noformat}
Nov 13 02:10:37 [ERROR] Failed to execute goal on project flink-quickstart-test: Could not resolve dependencies for project org.apache.flink:flink-quickstart-test:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-quickstart-scala:jar:1.17-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -> [Help 1]
Nov 13 02:10:37 [ERROR] 
Nov 13 02:10:37 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Nov 13 02:10:37 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Nov 13 02:10:37 [ERROR] 
Nov 13 02:10:37 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Nov 13 02:10:37 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
Nov 13 02:10:37 [ERROR] 
Nov 13 02:10:37 [ERROR] After correcting the problems, you can resume the build with the command
Nov 13 02:10:37 [ERROR]   mvn <goals> -rf :flink-quickstart-test
Nov 13 02:10:38 Process exited with EXIT CODE: 1.
Nov 13 02:10:38 Trying to KILL watchdog (293).
/__w/1/s/tools/ci/watchdog.sh: line 100:   293 Terminated              watchdog
Nov 13 02:10:38 ==============================================================================
Nov 13 02:10:38 Compilation failure detected, skipping test execution.
Nov 13 02:10:38 ==============================================================================
{noformat}



https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43102&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=18363",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 07:15:46 UTC 2022,,,,,,,,,,"0|z1c20w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 07:15;chesnay;master: 7ebe31829c8cf9b4452514284ce0ca298ab746ff;;;",,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinator.start()'s JavaDoc mismatches its behavior,FLINK-30009,13501969,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,14/Nov/22 03:41,14/Nov/22 03:41,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Documentation,,,,0,,,,,"The following description lies in the JavaDoc of {{OperatorCoordinator.start()}}.

{{This method is called once at the beginning, before any other methods.}}

This description is incorrect because the method {{resetToCheckpoint()}} can be invoked before {{start()}}. For example, {{RecreateOnResetOperatorCoordinator.DeferrableCoordinator.resetAndStart()}} uses these methods in this way. Thus the JavaDoc of {{OperatorCoordinator}}'s methods should be modified to match this behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-14 03:41:04.0,,,,,,,,,,"0|z1c1tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Flink 1.16.0 Support,FLINK-30008,13501577,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,13/Nov/22 10:43,14/Nov/22 15:27,04/Jun/24 20:41,14/Nov/22 15:27,,,,,,,aws-connector-3.0.0,,,,Connectors / AWS,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 15:27:12 UTC 2022,,,,,,,,,,"0|z1bze8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 15:27;dannycranmer;Merged commit [{{dc09993}}|https://github.com/apache/flink-connector-aws/commit/dc099938054398045cfceb87baaa95f853e5f1c8] into main;;;",,,,,,,,,,,,,,,,,,,,,,
Document how users can request a Jira account / file a bug ,FLINK-30007,13501573,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Nov/22 08:38,15/Feb/23 08:13,04/Jun/24 20:41,15/Nov/22 10:52,,,,,,,,,,,Documentation,Project Website,,,0,pull-request-available,,,,Follow-up of https://lists.apache.org/thread/y8vx7qr32xny31qq00f1jzpnz4kw8hpg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31081,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 10:52:07 UTC 2022,,,,,,,,,,"0|z1bzdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 10:52;chesnay;asf-site: b5e211c376af6572c97735a0af376d62ab82b422;;;",,,,,,,,,,,,,,,,,,,,,,
Cannot remove columns that are incorrectly considered constants from an Aggregate In Streaming,FLINK-30006,13501570,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,13/Nov/22 07:41,06/Feb/23 12:25,04/Jun/24 20:41,06/Feb/23 12:25,1.16.0,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"In Streaming, columns generated by dynamic functions are incorrectly considered constants and removed from an Aggregate via optimization rule `CoreRules.AGGREGATE_PROJECT_PULL_UP_CONSTANTS` (inside the RelMdPredicates, it only considers the non-deterministic functions, but this doesn't applicable for streaming)

an example query:
{code}
  @Test
  def testReduceGroupKey(): Unit = {
    util.tableEnv.executeSql(""""""
                               |CREATE TABLE t1(
                               | a int,
                               | b varchar,
                               | cat VARCHAR,
                               | gmt_date DATE,
                               | cnt BIGINT,
                               | PRIMARY KEY (cat) NOT ENFORCED
                               |) WITH (
                               | 'connector' = 'values'
                               |)
                               |"""""".stripMargin)
    util.verifyExecPlan(s""""""
                           |SELECT
                           |     cat, gmt_date, SUM(cnt), count(*)
                           |FROM t1
                           |WHERE gmt_date = current_date
                           |GROUP BY cat, gmt_date
                           |"""""".stripMargin)
  }
{code}

the wrong plan:
{code}
Calc(select=[cat, CAST(CURRENT_DATE() AS DATE) AS gmt_date, EXPR$2, EXPR$3])
+- GroupAggregate(groupBy=[cat], select=[cat, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
   +- Exchange(distribution=[hash[cat]])
      +- Calc(select=[cat, cnt], where=[=(gmt_date, CURRENT_DATE())])
         +- TableSourceScan(table=[[default_catalog, default_database, t1, filter=[], project=[cat, cnt, gmt_date], metadata=[]]], fields=[cat, cnt, gmt_date])
{code}

expect plan:
{code}
GroupAggregate(groupBy=[cat, gmt_date], select=[cat, gmt_date, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
+- Exchange(distribution=[hash[cat, gmt_date]])
   +- Calc(select=[cat, gmt_date, cnt], where=[(gmt_date = CURRENT_DATE())])
      +- TableSourceScan(table=[[default_catalog, default_database, t1, filter=[], project=[cat, gmt_date, cnt], metadata=[]]], fields=[cat, gmt_date, cnt])
{code}

In addition to this issue, we need to check all optimization rules in streaming completely to avoid similar problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 12:25:13 UTC 2023,,,,,,,,,,"0|z1bzco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 12:25;lincoln.86xy;fixed in master: 01743000c10720598dfb6f27d849da2283772e50;;;",,,,,,,,,,,,,,,,,,,,,,
"Translate ""Schema Migration Limitations for State Schema Evolution"" into Chinese",FLINK-30005,13501569,13213877,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,wahno,wahno,13/Nov/22 07:40,02/Jan/23 11:36,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Documentation,,,,0,pull-request-available,,,,"Translate paragraph ""Schema Migration Limitations"" in [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/] page into Chinese.

This doc located in ""flink/docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution.md""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-13 07:40:17.0,,,,,,,,,,"0|z1bzcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot resume deployment after suspend with savepoint due to leftover configmaps,FLINK-30004,13501204,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,12/Nov/22 17:57,16/Nov/22 01:26,04/Jun/24 20:41,16/Nov/22 01:26,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Due to the possibility of incomplete cleanup of HA data in Flink 1.14, the deployment can get into a limbo state that requires manual intervention after suspend with savepoint. If the config maps are not cleaned up the resumed job will be considered finished and the operator recognize the JM deployment as missing. Due to check for HA data which are now cleaned up, the job fails to start and manual redeployment with initial savepoint is necessary.

This can be avoided by removing any leftover HA config maps after the job has successfully stopped with savepoint (upgrade mode savepoint).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-12 17:57:22.0,,,,,,,,,,"0|z1bx3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The ConcurrentModificationException occurred at ContextClassLoadingSettingTest,FLINK-30003,13501189,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,fanrui,fanrui,12/Nov/22 11:55,15/Nov/22 08:59,04/Jun/24 20:41,15/Nov/22 08:59,1.17.0,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,," 

 

CI fails due to main thread didn't wait the future is done. The contextClassLoaders may be empty when checking.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43092&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&s=ae4f8708-9994-57d3-c2d7-b892156e7812]

 

!image-2022-11-12-19-53-53-963.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29249,,,,,,,,,,,,"12/Nov/22 11:53;fanrui;image-2022-11-12-19-53-53-963.png;https://issues.apache.org/jira/secure/attachment/13052138/image-2022-11-12-19-53-53-963.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 08:59:30 UTC 2022,,,,,,,,,,"0|z1bx00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 08:59;chesnay;master: 4fb7eee670011609b926a9eb46823ab78e06aab1;;;",,,,,,,,,,,,,,,,,,,,,,
Change the alignmentTimeout to alignedCheckpointTimeout,FLINK-30002,13501180,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,12/Nov/22 04:05,14/Nov/22 09:26,04/Jun/24 20:41,14/Nov/22 09:26,1.16.0,,,,,,1.17.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"The alignmentTimeout has been changed to alignedCheckpointTimeout in FLINK-23041 .

But some fields or methods still use alignmentTimeout. They should be renamed to alignedCheckpointTimeout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 09:26:26 UTC 2022,,,,,,,,,,"0|z1bwy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 09:26;pnowojski;merged commit 0a3e711 into apache:master now;;;",,,,,,,,,,,,,,,,,,,,,,
sql-client.sh start failed,FLINK-30001,13501178,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xiaohang.li,xiaohang.li,12/Nov/22 03:05,14/Nov/22 14:41,04/Jun/24 20:41,14/Nov/22 14:41,1.15.2,1.16.0,,,,,,,,,Command Line Client,,,,0,,,,,"[hadoop@master flink-1.15.0]$ ./bin/sql-client.sh 
Setting HADOOP_CONF_DIR=/etc/hadoop/conf because no HADOOP_CONF_DIR or HADOOP_CLASSPATH was set.
Setting HBASE_CONF_DIR=/etc/hbase/conf because no HBASE_CONF_DIR was set.


Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
Caused by: org.apache.flink.table.api.TableException: Could not instantiate the executor. Make sure a planner module is on the classpath
        at org.apache.flink.table.client.gateway.context.ExecutionContext.lookupExecutor(ExecutionContext.java:163)
        at org.apache.flink.table.client.gateway.context.ExecutionContext.createTableEnvironment(ExecutionContext.java:111)
        at org.apache.flink.table.client.gateway.context.ExecutionContext.<init>(ExecutionContext.java:66)
        at org.apache.flink.table.client.gateway.context.SessionContext.create(SessionContext.java:247)
        at org.apache.flink.table.client.gateway.local.LocalContextUtils.buildSessionContext(LocalContextUtils.java:87)
        at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:87)
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:88)
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
        ... 1 more
Caused by: org.apache.flink.table.api.TableException: Unexpected error when trying to load service provider for factories.
        at org.apache.flink.table.factories.FactoryUtil.lambda$discoverFactories$19(FactoryUtil.java:813)
        at java.util.ArrayList.forEach(ArrayList.java:1259)
        at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:799)
        at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:517)
        at org.apache.flink.table.client.gateway.context.ExecutionContext.lookupExecutor(ExecutionContext.java:154)
        ... 8 more
Caused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.table.planner.loader.DelegateExecutorFactory could not be instantiated
        at java.util.ServiceLoader.fail(ServiceLoader.java:232)
        at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42)
        at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:798)
        ... 10 more
Caused by: java.lang.ExceptionInInitializerError
        at org.apache.flink.table.planner.loader.PlannerModule.getInstance(PlannerModule.java:135)
        at org.apache.flink.table.planner.loader.DelegateExecutorFactory.<init>(DelegateExecutorFactory.java:34)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at java.lang.Class.newInstance(Class.java:442)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
        ... 14 more
Caused by: org.apache.flink.table.api.TableException: Could not initialize the table planner components loader.
        at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:123)
        at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:52)
        at org.apache.flink.table.planner.loader.PlannerModule$PlannerComponentsHolder.<clinit>(PlannerModule.java:131)
        ... 22 more
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:727)
        at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:96)
        ... 24 more",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29728,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 04:40:20 UTC 2022,,,,,,,,,,"0|z1bwxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 04:40;xiaohang.li;经查询，
默认情况下 flink 中的 org.apache.flink.table.planner.loader.PlannerModule 模块使用 /tmp 目录来作为临时的工作路径，因此会尝试调用 jave 的 java.nio.file.Files 类来创建这个目录，但是如果  /tmp 目录是一个指向 /mnt/tmp 的符号软链接，这种情况 java.nio.file.Files 类无法处理，从而导致出现报错。需要在sql-client.sh添加临时路径的配置：
  export JVM_ARGS=""-Djava.io.tmpdir=/mnt/tmp;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce FileSystemFactory to create FileSystem from custom configuration,FLINK-30000,13501118,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,11/Nov/22 13:18,19/Mar/23 05:52,04/Jun/24 20:41,19/Mar/23 05:52,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Currently, table store uses static Flink FileSystem. This can not support:
1. Use another FileSystem different from checkpoint FileSystem.
2. Use FileSystem in Hive and Spark from custom configuration instead of using FileSystem.initialize.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-11 13:18:23.0,,,,,,,,,,"0|z1bwk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve documentation on how a user should migrate from FlinkKafkaConsumer to KafkaSource,FLINK-29999,13501112,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jingge,jingge,jingge,11/Nov/22 12:39,11/Nov/22 12:49,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Documentation,,,,0,,,,,"We have described how to migrate job from FlinkKafkaConsumer to KafkaSource at [https://nightlies.apache.org/flink/flink-docs-master/release-notes/flink-1.14/#flink-24055httpsissuesapacheorgjirabrowseflink-24055.] But there are more things to take care of beyond it, one example is the idleness handling. Related documentation should improved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28302,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-11 12:39:19.0,,,,,,,,,,"0|z1bwiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the backpressure tab could be sort by the busy percent,FLINK-29998,13501102,13501095,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,yunta,yunta,11/Nov/22 11:50,30/Dec/22 07:52,04/Jun/24 20:41,13/Dec/22 05:17,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"Currently, we cannot sort the backpressure tab to see which task is busiest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30468,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 05:17:40 UTC 2022,,,,,,,,,,"0|z1bwgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 05:17;yunta;merged in master: b6c5534efc8a4b59c3f0993777aa8119af205f0c;;;",,,,,,,,,,,,,,,,,,,,,,
Link to the taskmanager page in the expired sub-task of checkpoint tab,FLINK-29997,13501099,13501095,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,yunta,yunta,11/Nov/22 11:42,19/Dec/22 06:29,04/Jun/24 20:41,19/Dec/22 06:29,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"Currently, when we debug why some of the sub-tasks cannot complete the checkpoints in time, we have a complex steps to find which task manager containing such logs. This could be simplified via a direct link.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 06:29:44 UTC 2022,,,,,,,,,,"0|z1bwg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 06:29;yunta;merged in master: 1064b13a2f2e0bdae64bdaecce10ec22a162a5e1;;;",,,,,,,,,,,,,,,,,,,,,,
Link to the task manager's thread dump page in the backpressure tab,FLINK-29996,13501096,13501095,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,yunta,yunta,11/Nov/22 11:40,05/Jun/23 11:24,04/Jun/24 20:41,19/Dec/22 12:24,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"Currently, we have a complex steps to find the thread dump of backpressured tasks, however, this could be simplified with a link in the backpressure tab of web UI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32186,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 12:24:42 UTC 2022,,,,,,,,,,"0|z1bwfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 12:24;yunta;merged in master: 3c043cfd68fd52135175d4ae3d65a1e1909af133;;;",,,,,,,,,,,,,,,,,,,,,,
"Improve the usability of job analysis for backpressure, expired checkpoints and exceptions",FLINK-29995,13501095,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,11/Nov/22 11:36,14/Jan/23 10:14,04/Jun/24 20:41,14/Jan/23 10:12,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-11 11:36:27.0,,,,,,,,,,"0|z1bwf4:",9223372036854775807,"The web-ui has been improved for better debug experiences: one-click to the thread dump page of busiest operator, one-click to the taskmanager log containing uncompleted checkpoint. Make the thread dump depth deeper and add the readable datetime of watermark to know the processing progress.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update official document about Lookup Join.,FLINK-29994,13501084,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,StarBoy1005,StarBoy1005,StarBoy1005,11/Nov/22 10:41,11/Nov/22 13:37,04/Jun/24 20:41,11/Nov/22 13:37,1.16.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Missed a period in Description of ""lookup.cache-rows"".
[About rocksdb'configuration|https://nightlies.apache.org/flink/flink-table-store-docs-release-0.2/docs/development/lookup-join/#rocksdboptions]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 13:37:46 UTC 2022,,,,,,,,,,"0|z1bwco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 13:37;lzljs3620320;master: bfef114bfce78d0ffa2cc952606894d043d82874;;;",,,,,,,,,,,,,,,,,,,,,,
Provide more convenient way to programmatically configure reporters,FLINK-29993,13501074,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Nov/22 09:48,21/Nov/22 11:15,04/Jun/24 20:41,21/Nov/22 11:15,,,,,,,1.17.0,,,,Runtime / Metrics,Tests,,,0,pull-request-available,,,,"Configuring reporters programmatically is an uncommon task in tests, but is currently not convenient and error prone since you need to manually assemble the config key as it must contain the reporter name.

Add some factory methods to the `MetricOptions` to generate a config options at runtime given a reporter name.",,,,,,,,,,,,FLINK-30067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 11:15:53 UTC 2022,,,,,,,,,,"0|z1bwag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 11:15;chesnay;master: c814348d486a2e16644f084c841df4f1bb586fd3;;;",,,,,,,,,,,,,,,,,,,,,,
Hive lookupJoin execution plan parsing error,FLINK-29992,13501073,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,HunterHunter,HunterHunter,11/Nov/22 09:39,16/Nov/22 08:00,04/Jun/24 20:41,16/Nov/22 07:59,1.14.7,1.15.4,1.16.0,1.17.0,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"{code:java}
//
tableEnv.executeSql("" CREATE CATALOG hive WITH (\n""
        + ""  'type' = 'hive',\n""
        + "" 'default-database' = 'flinkdebug',\n""
        + "" 'hive-conf-dir' = '/programe/hadoop/hive-3.1.2/conf'\n""
        + "" )"");
tableEnv.executeSql(""create table datagen_tbl (\n""
        + ""id STRING\n""
        + "",name STRING\n""
        + "",age bigint\n""
        + "",ts bigint\n""
        + "",`par` STRING\n""
        + "",pro_time as PROCTIME()\n""
        + "") with (\n""
        + ""  'connector'='datagen'\n""
        + "",'rows-per-second'='10'\n""
        + "" \n""
        + "")"");
String dml1 = ""select * ""
        + "" from datagen_tbl as p ""
        + "" join hive.flinkdebug.default_hive_src_tbl ""
        + "" FOR SYSTEM_TIME AS OF p.pro_time AS c""
        + "" ON p.id = c.id"";
// Execution succeeded
  System.out.println(tableEnv.explainSql(dml1));
String dml2 = ""select p.id ""
        + "" from datagen_tbl as p ""
        + "" join hive.flinkdebug.default_hive_src_tbl ""
        + "" FOR SYSTEM_TIME AS OF p.pro_time AS c""
        + "" ON p.id = c.id"";
// Throw an exception
 System.out.println(tableEnv.explainSql(dml2)); {code}
{code:java}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: FlinkLogicalCalc(select=[id]) +- FlinkLogicalJoin(condition=[=($0, $1)], joinType=[inner])    :- FlinkLogicalCalc(select=[id])    :  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, datagen_tbl]], fields=[id, name, age, ts, par])    +- FlinkLogicalSnapshot(period=[$cor1.pro_time])       +- FlinkLogicalTableSourceScan(table=[[hive, flinkdebug, default_hive_src_tbl, project=[id]]], fields=[id])This exception indicates that the query uses an unsupported SQL feature. Please check the documentation for the set of currently supported SQL features.    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)     at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 02:05:42 UTC 2022,,,,,,,,,,"0|z1bwa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:45;HunterHunter;This use case is normal in 1.15.;;;","11/Nov/22 13:00;luoyuxia;[~HunterHunter] Thanks for raising it. I'll fix it asap;;;","12/Nov/22 03:14;luoyuxia;FLINK-29138 introduce project push down for lookup source, then the `PushProjectIntoTableSourceScanRule` will work which will then 
{code:java}
sourceTable.tableSource().copy(); {code}
 But the HiveLookupTableSource doesn't implement copy method which will delegate to it's parent method, and then copy a `HiveTableSource` which is not a lookup source.;;;","14/Nov/22 02:05;leonard;master:a4f9bfd1483ef64b0ed167bd29c98596e3bd5f49
release-1.16: c946b0b95b7b8396eac7f03019eb279becddd301
release-1.15: c257f98bf38668f5bd9d48ccd307f9f76e2463b2
release-1.14: 9cb7b338c90b8183aa7ab4c9e094e47221d182cb
;;;",,,,,,,,,,,,,,,,,,,
KinesisFirehoseSinkTest#firehoseSinkFailsWhenUnableToConnectToRemoteService failed ,FLINK-29991,13501066,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chalixar,martijnvisser,martijnvisser,11/Nov/22 09:03,15/Aug/23 09:01,04/Jun/24 20:41,,1.15.2,,,,,,,,,,Connectors / Kinesis,,,,0,stale-assigned,test-stability,,,"{code:java}
Nov 10 10:22:53 [ERROR] org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkTest.firehoseSinkFailsWhenUnableToConnectToRemoteService  Time elapsed: 7.394 s  <<< FAILURE!
Nov 10 10:22:53 java.lang.AssertionError: 
Nov 10 10:22:53 
Nov 10 10:22:53 Expecting throwable message:
Nov 10 10:22:53   ""An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: '[NoMoreSplitEvent]', targetTask: Source: Sequence Source -> Map -> Map -> Sink: Writer (15/32) - execution #0""
Nov 10 10:22:53 to contain:
Nov 10 10:22:53   ""Received an UnknownHostException when attempting to interact with a service.""
Nov 10 10:22:53 but did not.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43017&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=44513",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 09:01:17 UTC 2023,,,,,,,,,,"0|z1bw8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:17;chalixar;Hi [~martijnvisser] 
Can you assign this to me, I will have a look at it asap!;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","15/Aug/23 09:01;chalixar;Wasn't able to reproduce the issue, do we have other occurances?;;;",,,,,,,,,,,,,,,,,,,,
Unparsed SQL for SqlTableLike cannot be parsed correctly,FLINK-29990,13501044,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,icshuo,icshuo,icshuo,11/Nov/22 07:15,11/Nov/22 14:18,04/Jun/24 20:41,11/Nov/22 14:18,1.16.0,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Consider the following DDL sql (LIKE without any options):
{code:java}
create table source_table(
  a int,
  b bigint,
  c string
)
LIKE parent_table{code}
After unparsed by sql parser, we get the following result:
{code:java}
CREATE TABLE `SOURCE_TABLE` (
  `A` INTEGER,
  `B` BIGINT,
  `C` STRING
)
LIKE `PARENT_TABLE` (
) {code}
Exception will be thrown if you try to parse the above sql.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 14:18:59 UTC 2022,,,,,,,,,,"0|z1bw3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 14:18;libenchao;Fixed via [https://github.com/apache/flink/commit/dd76844342489a252f8b76417090f137028af0bc] 

[~icshuo] thanks for reporting and fixing this.;;;",,,,,,,,,,,,,,,,,,,,,,
Enable FlameGraph for arbitrary thread on TaskManager,FLINK-29989,13501038,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhuzh,zhuzh,11/Nov/22 06:56,11/Nov/22 06:59,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,,FlameGraph for arbitrary thread on TaskManager can be helpful for tasks which will spawn other worker threads. See FLINK-29629 for more details.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29629,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-11 06:56:06.0,,,,,,,,,,"0|z1bw2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve upper case fields for hive metastore,FLINK-29988,13501019,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,11/Nov/22 03:42,19/Mar/23 05:50,04/Jun/24 20:41,19/Mar/23 05:50,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"If the fields in the fts table are uppercase, there will be a mismatched exception when used in the Hive.

1. If it is not supported at the beginning, throw an exception when flink creates a table to the hive metastore.
2. If it is supported, so that no error is reported in the whole process, but save lower case in hive metastore. We can check columns with the same name when creating a table in Flink with hive metastore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-11 03:42:55.0,,,,,,,,,,"0|z1bvy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartialUpdateITCase.testForeignKeyJo is unstable,FLINK-29987,13501014,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Gerrrr,lzljs3620320,lzljs3620320,11/Nov/22 03:10,28/Nov/22 06:59,04/Jun/24 20:41,28/Nov/22 06:59,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 06:59:36 UTC 2022,,,,,,,,,,"0|z1bvx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 11:50;Gerrrr;As far I understand the flakiness is caused by unconditional {{Thread.sleep}} statement followed by assertion in the test. If the condition does not become true during the fixed sleep period, the assertion fails. I would like to convert this code to something like {{await().atMost(5, SECONDS).until(() -> assert(...));}}

To implement that we can use [https://github.com/awaitility/awaitility] as a test dependency. What do you think?;;;","18/Nov/22 06:20;lzljs3620320;[~Gerrrr] Sounds nice~;;;","18/Nov/22 09:16;Gerrrr;Thanks! I'll make a PR in the next couple of days. Can you please assign the issue to me (or give permission to do that myself)?;;;","28/Nov/22 06:59;lzljs3620320;master: cd0870bab446ad8e91dab3ddd3b3b6e7ef71612f;;;",,,,,,,,,,,,,,,,,,,
How to persist flink catalogs configuration?,FLINK-29986,13500999,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,ge.bugman,ge.bugman,11/Nov/22 01:46,14/Nov/22 14:43,04/Jun/24 20:41,14/Nov/22 14:43,1.16.0,,,,,,,,,,Table SQL / API,,,,0,,,,,"Hello every one.

 

When I closed sql terminal(./sql-client.sh), The Catalogs I created are all gone when I open sql terminal(./sql-client.sh) next time.

Do I have some other way to persist these catalogs?

 

Thanks.

 ","Cent OS

Flink 1.16

Hive 3.1.3

Hadoop 3.3.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 14:43:08 UTC 2022,,,,,,,,,,"0|z1bvts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 11:40;luoyuxia;The short answer is no. 

But as a work around, you can prepare a file which contains the catalogs you need to init the session, and then use `./bin/sql-client.sh -i` to init the session with the scrip file. Refer the doc [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sqlclient/#starting-the-sql-client-cli] for more details.;;;","14/Nov/22 14:43;martijnvisser;This is documented at https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/catalogs/;;;",,,,,,,,,,,,,,,,,,,,,
TaskManager might not close SlotTable on SIGTERM,FLINK-29985,13500947,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,roman,roman,10/Nov/22 18:00,21/Nov/22 07:29,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,Runtime / Coordination,Runtime / Task,,,0,,,,,"When TM is stopped by RM, its slot table is closed, causing all its slots to be released.
However, when TM is stopped by SIGTERM (i.e. external resource manager), its slot table is NOT closed.
 

When a slot is released, the associated resources are released as well, in particular, MemoryManager.
MemoryManager might hold not only memory, but also arbitrary shared resources (currently, PythonSharedResources and RocksDBSharedResources).
As of now, RocksDBSharedResources contains only ephemeral resources. Not sure about PythonSharedResources, but likely it is associated with a separate process.
That means that in standalone clusters, some resources might not be released.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 15:17:05 UTC 2022,,,,,,,,,,"0|z1bvig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 18:07;roman;cc: [~mapohl] , [~pnowojski] ;;;","11/Nov/22 15:17;roman;I found that TM actually does try to release the slot table, but there are two hard-coded timeouts:
1. 5s JvmShutdownSafeguard
2. 10s in flink-daemon.sh

[cluster.services.shutdown-timeout|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#cluster-services-shutdown-timeout] is not taken into account by TM.

Another issue is that even if the above timeouts don't fire, the logging system is stopped prematurely.
That's why SlotTable release can be is silent.
;;;",,,,,,,,,,,,,,,,,,,,,
Flink Histogram not emitting min and max when using Prometheus Reporter,FLINK-29984,13500941,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,qingwei91,qingwei91,qingwei91,10/Nov/22 17:08,14/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,1.16.0,,,,,,,,,Runtime / Metrics,,,,0,pull-request-available,stale-assigned,,,"Flink Histogram when using the Prometheus Metrics Reporter only produces
 * quantiles of 0.5, 0,75, 0.95, 0.98, 0.99, 0.999
 * count

 

I think it would be a good idea to also produce min and max, as they are already available in the state, we can model it as p0 and p1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:06 UTC 2023,,,,,,,,,,"0|z1bvh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 17:13;qingwei91;I am happy to work on this if we agree this is desirable, my workplace is using Prometheus Reporter and this will benefit us;;;","17/Nov/22 09:34;xtsong;Thanks for reporting and volunteering to fix this, [~qingwei91]. I think this is a valid issue, and have assigned you to the ticket.

Concerning the solution, I wonder if we can make the quantiles configurable for the prometheus reporter. Otherwise, I'd a little concerned that adding two more quantiles would increase the amount of metrics being reported, and users who don't want that many quantiles can do nothing about it.;;;","17/Nov/22 09:52;qingwei91;Hi [~xtsong] , thanks for the assignment.

 

I agree with the hardcoded quantiles, I think we can make it configurable. What is the standard way to configure such things? Can we expose the config in regular flink config?

The only configurable metrics I am aware of is controlled by `{*}{{taskmanager.net.detailed-metric`}}{*} {{which is a binary option.}};;;","17/Nov/22 10:05;xtsong;[~qingwei91],

You can add a config option in {{PrometheusPushGatewayReporterOptions}}.;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,
Table Store with Hive3 profile lacks hive-standalone-metastore dependency,FLINK-29983,13500928,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,10/Nov/22 16:05,29/Nov/22 03:37,04/Jun/24 20:41,29/Nov/22 03:37,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"For Hive3,  {{org.apache.hadoop.hive.metastore.api}} is moved to {{{}org.apache.hive:hive-standalone-metastore{}}}. We should shade this package as well to avoid ClassNotFoundException",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28157,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 03:37:34 UTC 2022,,,,,,,,,,"0|z1bve8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 03:37;lzljs3620320;master: 6152dbc07a4e802d77f0e2140c345f497306c948;;;",,,,,,,,,,,,,,,,,,,,,,
Move cassandra connector to dedicated repo,FLINK-29982,13500920,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,10/Nov/22 15:06,25/Nov/22 09:09,04/Jun/24 20:41,25/Nov/22 01:31,,,,,,,cassandra-3.0.0,,,,Connectors / Cassandra,,,,0,pull-request-available,,,,"[This repo|https://github.com/apache/flink-connector-cassandra] has just been created to host the new Cassandra Source. But before merging the new source, the whole connector-cassandra module needs to be migrated to this dedicated repo.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30055,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 01:31:01 UTC 2022,,,,,,,,,,"0|z1bvcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 01:31;chesnay;cassandra-main: 691e4605039b336736711a40deaf9bb4fc3715b6;;;",,,,,,,,,,,,,,,,,,,,,,
Improve WatermarkAssignerChangelogNormalizeTransposeRule,FLINK-29981,13500901,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,godfreyhe,godfreyhe,10/Nov/22 12:54,10/Nov/22 12:54,04/Jun/24 20:41,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,," 

WatermarkAssignerChangelogNormalizeTransposeRule is too complex to maintain. It's better we can do some improvement, such as splitting WatermarkAssignerChangelogNormalizeTransposeRule into two rules",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-10 12:54:12.0,,,,,,,,,,"0|z1bv88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrap the HiveSource's bulkFormat to handle the partition keys,FLINK-29980,13500898,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,10/Nov/22 12:42,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Hive,,,,0,auto-deprioritized-major,pull-request-available,,,"As described in https://issues.apache.org/jira/browse/FLINK-25113 to clean up the partition keys logic in the parquet and orc formats, hive source should leverage the {{FileInfoExtractorBulkFormat}} to handle the partition keys internally",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:16 UTC 2023,,,,,,,,,,"0|z1bv7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
connector_artifact should only work for stable releases,FLINK-29979,13500879,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Nov/22 11:04,10/Nov/22 13:53,04/Jun/24 20:41,10/Nov/22 13:53,1.17.0,,,,,,1.17.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 13:53:22 UTC 2022,,,,,,,,,,"0|z1bv3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 13:53;chesnay;master: bb6e01b181e45d2ea3944910eb06ef0fd95c0540;;;",,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaInternalProducer not compatible with kafka-clients-3.3.x,FLINK-29978,13500863,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,k8s,k8s,10/Nov/22 09:40,10/Nov/22 09:51,04/Jun/24 20:41,10/Nov/22 09:49,1.16.0,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"FlinkKafkaInternalProducer _resumeTransaction_ fetches _topicPartitionBookkeeper_ field from _TransactionManager_ which has been renamed to _TxnPartitionMap_ in 3.3.x. Failing to retrieve the field raises an exception (Incompatible KafkaProducer version)

 
{code:java}
public void resumeTransaction(long producerId, short epoch) {
  ...
  Object topicPartitionBookkeeper = getField(transactionManager, ""topicPartitionBookkeeper"");
...
}
{code}
 

[https://github.com/apache/kafka/blob/3.3.0/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java]
{code:java}
private final TxnPartitionMap txnPartitionMap; {code}
Users should be advised not to use version 3.3.x or the field name should be corrected accordingly.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29977,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 09:51:31 UTC 2022,,,,,,,,,,"0|z1buzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 09:47;Weijie Guo;Is this the same problem with FLINK-29977?;;;","10/Nov/22 09:49;martijnvisser;[~k8s] I'm not sure this is a bug or tech debt, because Flink is tested against Kafka 3.2.3 at the moment and not higher versions. I do agree that it makes sense to mention that Flink doesn't yet work with Kafka 3.3.;;;","10/Nov/22 09:50;k8s;Yes, I mentioned this issue at slack and looks like someone else reported it ahead.;;;","10/Nov/22 09:51;k8s;[~martijnvisser] Yes, I agree. This should be noted when upgrading to 3.3 and users should be advised to use 3.2 until then.;;;",,,,,,,,,,,,,,,,,,,
Kafka connector not compatible with kafka-clients 3.3x,FLINK-29977,13500860,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,chesnay,chesnay,chesnay,10/Nov/22 09:30,27/Jul/23 09:11,04/Jun/24 20:41,27/Jul/23 09:11,1.16.0,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"In https://github.com/apache/kafka/commit/3ea7b418fb3d7e9fc74c27751c1b02b04877f197 the TransactionManager was modified and no longer has a {{topicPartitionBookkeeper}} that we access via reflection.

The {{TopicPartitionBookkeeper}} was refactored to a {{TxnPartitionMap}} class.
The {{reset}} method we require is still present though; so this could potentially be fixed by just falling back to another field name if required.",,,,,,,,,,,,,,,,,,,,,,FLINK-31599,,,,,,FLINK-29978,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-10 09:30:56.0,,,,,,,,,,"0|z1buz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The cron connectors test run out of given timeout in Azure,FLINK-29976,13500858,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leonard,leonard,10/Nov/22 09:11,10/Dec/22 12:31,04/Jun/24 20:41,,1.15.0,1.17.0,,,,,,,,,Tests,,,,0,,,,,"The cron connector tests run out of the available time 222minutes.

1.15 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43000&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 10 12:30:48 UTC 2022,,,,,,,,,,"0|z1buyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/22 12:30;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43850&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0;;;",,,,,,,,,,,,,,,,,,,,,,
Let hybrid full spilling strategy supports partition reuse,FLINK-29975,13500854,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,10/Nov/22 08:55,17/Nov/22 10:19,04/Jun/24 20:41,17/Nov/22 10:19,1.17.0,,,,,,1.17.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"Partition reuse is a very useful optimization in some topologies. In essence, multiple downstream tasks consume the same subpartition's data. Therefore, hybrid shuffle should also enjoy the benefits it brings. After FLINK-28889, we are finally able to achieve repeated consumption at the subpartition level for hybrid full spilling strategy, so let's make it better.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 10:19:27 UTC 2022,,,,,,,,,,"0|z1buxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 10:19;xtsong;master (1.17): ba4b182955867fedfa9891bf0bf430e92eeab41a;;;",,,,,,,,,,,,,,,,,,,,,,
Session jobs in FINISHED/FAILED state cannot be suspended,FLINK-29974,13500852,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gyfora,gyfora,10/Nov/22 08:52,02/Dec/22 14:25,04/Jun/24 20:41,02/Dec/22 13:09,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The AbstractFlinkService#cancelSessionJob method currently does not take into consideration the current job state.

This means that if we call this on an already failed/canceld job we will get an exception from Flink:


ava.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 13:09:04 UTC 2022,,,,,,,,,,"0|z1buxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 18:34;sriramgr;[~gyfora] - [https://github.com/apache/flink-kubernetes-operator/pull/438.] Please check.

 ;;;","28/Nov/22 09:34;gyfora;cc [~gsomogyi] 
Could you please help [~sriramgr] finish this?;;;","02/Dec/22 13:09;mbalassi;[{{ea01e29}}|https://github.com/apache/flink-kubernetes-operator/commit/ea01e294cf1b68d597244d0a11b3c81822a163e7] in main;;;",,,,,,,,,,,,,,,,,,,,
connector_artifact should append Flink minor version,FLINK-29973,13500849,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Nov/22 08:39,10/Nov/22 10:40,04/Jun/24 20:41,10/Nov/22 10:40,,,,,,,1.16.1,1.17.0,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 10:40:36 UTC 2022,,,,,,,,,,"0|z1buwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 10:40;chesnay;master: 84f96c145796ada9cdd6d8e17ccaf21f49104288
1.16: b9ec6217b02be4b16908d779e7526134c6bdd5cb;;;",,,,,,,,,,,,,,,,,,,,,,
Pin Flink docs to Elasticsearch Connector 3.0.0,FLINK-29972,13500848,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Nov/22 08:30,10/Nov/22 13:57,04/Jun/24 20:41,10/Nov/22 13:57,,,,,,,1.16.1,1.17.0,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 13:57:00 UTC 2022,,,,,,,,,,"0|z1buwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 13:57;chesnay;master: b7d0b0f02c567c845cd94a9b5b585e859985af52
1.16: 956c249f3362b1dbcd4492f21d2cd0d67fc4a229;;;",,,,,,,,,,,,,,,,,,,,,,
Hbase sink will lose data at extreme case,FLINK-29971,13500845,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,Stephenwwc,Stephenwwc,Stephenwwc,10/Nov/22 08:24,14/Aug/23 22:35,04/Jun/24 20:41,,1.13.6,1.15.3,,,,,,,,,Connectors / HBase,,,,0,pull-request-available,stale-assigned,,,"h2. Situation:

When I use kafka as source and hbase as sink but the hbase table I didn't have the permission, I send data to kafka one message with a long time gap.

In this situation the normal result will be when trigger checkpoint the job will failed. But actually the jobs will continue to run and can trigger checkpoint successfully.
h2. Analysis

The hbase sink will throw exception in *checkErrorAndRethrow()* funciton. And this function will be called in two function, *invoke()* and {*}flush(){*}. Beside {*}invoke(){*}, *flush()* will be called at two place, one is {*}snapshot(){*}, one is in the scheduledThread as the follow snippet of code:

!image-2022-11-10-16-02-51-402.png!

We can see that in the  scheduledThread the exception throw by *flush()* will be catch and reset to failureThrowable.

So if there's no message come, the only way to throw the exception is in {*}snapshot(){*}. But the snapshot function call flush() is conditional as  the follow snippet of code:

!image-2022-11-10-16-08-23-325.png!

But the scheduledThread will called flush() periodically and set numPendingRequests as 0.

!image-2022-11-10-16-10-50-711.png!

So if no other message comes the snapshot will run successfully which means the checkpoint will be success but that message was not written to hbase, the message is loss.

 
h2. Solution

I think the reason is that when trigger checkpoint and call snapshot function, need to call *checkErrorAndRethrow()* first as follow: 

!image-2022-11-10-16-24-01-396.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/22 08:02;Stephenwwc;image-2022-11-10-16-02-51-402.png;https://issues.apache.org/jira/secure/attachment/13052049/image-2022-11-10-16-02-51-402.png","10/Nov/22 08:08;Stephenwwc;image-2022-11-10-16-08-23-325.png;https://issues.apache.org/jira/secure/attachment/13052048/image-2022-11-10-16-08-23-325.png","10/Nov/22 08:10;Stephenwwc;image-2022-11-10-16-10-50-711.png;https://issues.apache.org/jira/secure/attachment/13052047/image-2022-11-10-16-10-50-711.png","10/Nov/22 08:24;Stephenwwc;image-2022-11-10-16-24-01-396.png;https://issues.apache.org/jira/secure/attachment/13052046/image-2022-11-10-16-24-01-396.png",,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:06 UTC 2023,,,,,,,,,,"0|z1buvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 08:26;Stephenwwc;If the problem do exist, can this assign to me?;;;","16/Nov/22 13:02;Stephenwwc;[~fpaul] [~martijnvisser] Can you help to review the pull request?;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Prometheus cannot collect flink metrics,FLINK-29970,13500819,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,zhongyangyang,zhongyangyang,10/Nov/22 03:21,06/Apr/23 08:13,04/Jun/24 20:41,06/Apr/23 08:13,1.14.6,1.15.3,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"When I use the native k8s method to deploy my flink application cluster, if I do not manually deploy a Service to expose the 9249 port of my jobmanager and taskmanager, the prometheus I deployed in k8s cannot collect the metrics in my flink. Should flink generate these services itself when deploying？Or am I using it incorrectly?Thanks!

My deploy command is（Some content is omitted）
{code:sh}
flink run-application --target-application  \n
-Dmetrics.reporters=prom  \n
-Dmetric.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReport  \n      -Dkubernetes.jobmanager.annotaions=prometheus.io/scrape:true,prometheus.io/port:9249 \n
-Dkubernetes.jobmanager.annotaions=prometheus.io/scrape:true,prometheus.io/port:9249
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 06 08:13:04 UTC 2023,,,,,,,,,,"0|z1buq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 13:26;huwh;[~zhongyangyang], Hi, there are some typo in your configuration. such as: 'metric.reporter.prom.class' should be 'metrics.reporter.prom.class' and 'annotaions' shouldbe 'annotations'.


you can try with these configs:

 
{code:java}

metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
metrics.reporters: prom
kubernetes.jobmanager.annotations: prometheus.io/port:9249,prometheus.io/scrape:true
kubernetes.taskmanager.annotations: prometheus.io/scrape:true,prometheus.io/port:9249

{code}

And why do you have to use the service to expose port 9249? Maybe You can use the role: pod to discover flink jobmanager and taskmanager.
 

 ;;;","06/Apr/23 08:13;wanglijie;I think [~huwh] has pointed out the root cause. I'll close this ticket as ""Not A Problem"".  cc [~zhongyangyang];;;",,,,,,,,,,,,,,,,,,,,,
Show the root cause when exceeded checkpoint tolerable failure threshold,FLINK-29969,13500817,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,10/Nov/22 02:54,17/Nov/22 04:07,04/Jun/24 20:41,17/Nov/22 03:12,1.17.0,,,,,,1.17.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"Add the root cause when exceeded checkpoint tolerable failure threshold, it's helpful during troubleshooting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 04:07:16 UTC 2022,,,,,,,,,,"0|z1bupk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 03:12;yunta;merged in master: 5b8ea81f11df1d094b6331de6cb6f824e5401bcd;;;","17/Nov/22 04:07;fanrui;Thanks [~yunta] 's good suggestion and review.;;;",,,,,,,,,,,,,,,,,,,,,
Update streaming query document for Table Store to include full compaction changelog producer,FLINK-29968,13500815,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,10/Nov/22 02:36,02/Dec/22 09:55,04/Jun/24 20:41,02/Dec/22 09:55,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"As we've now implemented the full compaction changelog producer, we need to update the document so that user can understand when and how to use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 09:55:19 UTC 2022,,,,,,,,,,"0|z1bup4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 09:55;lzljs3620320;master: d03a9b6cc5f503ce846cbeed3a1b2c679bcf42f4;;;",,,,,,,,,,,,,,,,,,,,,,
Optimize determinism requirements from sink node with considering that SinkUpsertMaterializer already supports upsertKey ,FLINK-29967,13500814,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lincoln.86xy,lincoln.86xy,10/Nov/22 02:31,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,This is followup optimization for FLINK-28569,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-10 02:31:39.0,,,,,,,,,,"0|z1buow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace and redesign the Python api documentation base,FLINK-29966,13500812,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,10/Nov/22 02:24,16/Nov/22 06:17,04/Jun/24 20:41,16/Nov/22 06:17,1.15.3,1.16.0,,,,,1.15.4,1.16.1,,,API / Python,Documentation,,,0,pull-request-available,,,,The doc of the existing python api is difficult to read and use. I have a demo site for redesigning the Python api documentation base. See https://pyflink-api-docs-test.readthedocs.io/en/latest/ as an example.,,,,,,,,,,,,,,,,,FLINK-28957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 06:17:50 UTC 2022,,,,,,,,,,"0|z1buog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 06:17;hxbks2ks;Merged into master via b838a4e76e226a0567d39db1f9c29305cb6d913f
Merged into release-1.16 via 7d5585dd9ea4f26bf28c43cbb5207f2a5b74420b
Merged into release-1.15 via d6bf30a013569c4a8e0cd546f3105296c7b32efe;;;",,,,,,,,,,,,,,,,,,,,,,
Support Spark/Hive with S3,FLINK-29965,13500766,13500764,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,qingyue,qingyue,qingyue,09/Nov/22 19:00,09/Jan/23 03:11,04/Jun/24 20:41,09/Jan/23 03:11,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 19:01:25 UTC 2022,,,,,,,,,,"0|z1bue8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 19:01;qingyue;Please assign the ticket to me, thanks, cc [~lzljs3620320] ;;;",,,,,,,,,,,,,,,,,,,,,,
Support Spark/Hive with OSS,FLINK-29964,13500765,13500764,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,09/Nov/22 18:58,29/Nov/22 03:38,04/Jun/24 20:41,29/Nov/22 03:38,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 03:38:39 UTC 2022,,,,,,,,,,"0|z1bue0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 19:00;qingyue;Please assign the ticket to me, thanks [~lzljs3620320] ;;;","29/Nov/22 03:38;lzljs3620320;master: 18cdaee7cf6eb3fadeeb958ecb3b3c4a7bd57440;;;",,,,,,,,,,,,,,,,,,,,,
Flink Table Store supports pluggable filesystem,FLINK-29963,13500764,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,09/Nov/22 18:56,09/Jan/23 03:12,04/Jun/24 20:41,09/Jan/23 03:12,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,,,,,"Currently, users cannot query the FTS table from Spark/Hive if using OSS/S3 as the underlying filesystem. We need to support them to improve user experience.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 18:57:11 UTC 2022,,,,,,,,,,"0|z1buds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 18:57;qingyue;Hi [~lzljs3620320], I've tested spark/hive read oss/s3 on spark and hive. Please assign the ticket to me.;;;",,,,,,,,,,,,,,,,,,,,,,
Exclude Jamon 2.3.1,FLINK-29962,13500762,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,vvcephei,vvcephei,vvcephei,09/Nov/22 18:35,15/Nov/22 05:17,04/Jun/24 20:41,14/Nov/22 04:29,,,,,,,1.17.0,,,,Connectors / Hive,Table SQL / Gateway,,,0,pull-request-available,,,,"Hi all,

My Maven mirror is complaining that the pom for jamon-runtime:2.3.1 has a malformed pom. It looks like it's fixed in jamon-runtime:2.4.1. According to dependency:tree, Flink already has transitive dependencies on both versions, so I'm proposing to just exclude the transitive dependency from the problematic direct dependencies and pin the dependency to 2.4.1.

I'll send a PR shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 04:28:59 UTC 2022,,,,,,,,,,"0|z1budc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 13:39;lzljs3620320;Merged in Flink Table Store master: a91de34e9448abd559bfde28d6fdb8236ea9f0d5;;;","14/Nov/22 04:28;jgrier;Merged in Flink master: 9572cf6b287d71ee9c307546d8cd8f8898137bdd;;;",,,,,,,,,,,,,,,,,,,,,
Make referencing custom image clearer for Docker,FLINK-29961,13500755,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,liuml07,liuml07,liuml07,09/Nov/22 17:46,14/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Documentation,,,,0,pull-request-available,stale-assigned,,,"Make referencing custom image clearer for Docker.
- Rephrase the words how to reference custom image when building with Docker standalone mode.
- Also add example code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:06 UTC 2023,,,,,,,,,,"0|z1bubs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,
Update README to Reflect AWS Connectors Single Repo,FLINK-29960,13500738,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mrapcooper,dannycranmer,dannycranmer,09/Nov/22 15:48,09/Nov/22 16:12,04/Jun/24 20:41,09/Nov/22 16:12,,,,,,,aws-connector-3.0.0,,,,Connectors / AWS,,,,0,pull-request-available,,,,We recently renamed and pivoted the {{flink-connectors-dynamodb}} repository to be a more general {{{}flink-connectors-aws{}}}. Update the README to reflect these changes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 16:12:16 UTC 2022,,,,,,,,,,"0|z1bu80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 16:12;dannycranmer;Merged commit [{{7ec7a0b}}|https://github.com/apache/flink-connector-aws/commit/7ec7a0b6efb7dad60f8361de7222a8567ba69c76] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,
Use optimistic locking when patching resource status,FLINK-29959,13500731,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,09/Nov/22 15:00,16/Nov/22 15:49,04/Jun/24 20:41,16/Nov/22 15:49,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The operator currently does not use optimistic locking on the CR when patching status. This worked because we always wanted to overwrite the status.

With leader election and potentially two operators running at the same time, we are now exposed to some race conditions that were not previously present with the status update logic.

To ensure that the operator always sees the latest status we should change our logic to optimistic locking with retries. If we get a lock error (resource updated) we check if only the spec changed and then retry locking on the new version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 15:49:54 UTC 2022,,,,,,,,,,"0|z1bu6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 17:09;gyfora;There are 2 main problems that we target here:
 # Status updates by zombie operators who has last lost leadership but not realized/dead yet.
 # Stale status received when a new leader starts

*Why would these happen?*

Zombie operator:
It could in theory happen that an operator loses leadership in a middle of reconciliation due to a very long GC pause (or some network issue or whatever) and the current CR reconcile loop continues while the new leader already started to reconcile this resource. This is very unlikely but can happen with leader election and a standby operator. In these cases we don't want to allow the old operator who lost leadership to be able to make any status updates. The new logic guarantees that if the new leader made any status update the old would never be able to do so again.

Stale status:
When the new leader starts processing (if it was on standby) there is no guarantee that the status/spec reconciled at the first time is up to date. This can happen because due to some unlucky cache update timing or even a zombie operator submitting late status updates. The current operator logic very much relies on seeing the last status otherwise we can have some very weird cornercases that would definitely cause problems for the resources.

*How the new logic tackles this in a safe way*

What the new logic does is that it basically only allows status updates to go through when the operator has the latest status information. So it's sort of a locking on the current status. If anyone else changed the status in the meantime, we simply throw an error and retrigger the reconciliation. This is actually safe to do as the operator reconcile logic already runs with the assumption that the operator can fail at any time before status update, and we always use the status as a ""write-ahead-log"" of the actions we are taking. In these cases zombie operators who have already lost leadership would never reconcile again (the leader election guarantees that), and in other cases this would give us the latest version of the resource.;;;","16/Nov/22 15:49;gyfora;merged to main 99dc38f1270ae9b13a8b461df8a0bf66e9d8b5f7;;;",,,,,,,,,,,,,,,,,,,,,
Add new connector_artifact shortcode,FLINK-29958,13500708,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Nov/22 13:39,10/Nov/22 08:30,04/Jun/24 20:41,09/Nov/22 15:47,,,,,,,1.16.1,1.17.0,elasticsearch-3.0.1,elasticsearch-3.1.0,Documentation,,,,0,pull-request-available,,,,We need a new shortcode for connectors that allows them to specify the version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 15:47:26 UTC 2022,,,,,,,,,,"0|z1bu1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 15:47;chesnay;master: 421f057a7488fd64854a82424755f76b89561a0b
1.16: c2fe933e8a23af5d49a0df4110ccc6809b66a49d
elasticsearch-main: d4664b1112bae2b8c23330a5ac0684b860c25ebc
elasticsearch-v3.0: b32f832f19ee52c43cddc0f32e835ef9c208a770;;;",,,,,,,,,,,,,,,,,,,,,,
Rework connector docs integration,FLINK-29957,13500707,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Nov/22 13:37,10/Nov/22 08:25,04/Jun/24 20:41,10/Nov/22 08:25,,,,,,,1.16.1,1.17.0,elasticsearch-3.0.1,elasticsearch-3.1.0,Documentation,,,,0,pull-request-available,,,,"The current connector integration doesn't work properly with branches and is virtually impossible to maintain because of how obscure go modules are.

Re-implement it by checking out specific commits and copying files as necessary into a pseudo local module instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 08:25:20 UTC 2022,,,,,,,,,,"0|z1bu14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 08:25;chesnay;master: d8e89932068b18521724eab3c042ba3ab8fb90a0
1.16: 1b0f50b1f26513f3ff171426411d75cd424da154
elasticsearch-main: fbde503a812d6d56192fff39a13e13320833662d
elasticsearch-v3.0: 14ae7fddbb03437e9b2877470e99fd92cf4e9b92;;;",,,,,,,,,,,,,,,,,,,,,,
Kafka-related test infrastructure code is scattered over multiple classes/environments,FLINK-29956,13500691,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,09/Nov/22 10:58,20/Aug/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-major,starter,,,"We had a few issues with the test stability of Kafka-related tests (FLINK-24119, FLINK-29914). A workaround was to add randomness to topic identifiers. This change required touching multiple code locations that made it obvious that there is some room for improvement (see [Gabor's comment|https://github.com/apache/flink/pull/21247#issuecomment-1307084416] comment in the related PR). We could put some effort into unifying this by providing a common test environment that provides utility methods he mentioned in his comment:
{quote}
Topic creation
Topic deletion
Random name generation
AdminClient creation
Consumer creation
Producer creation
{quote}

Currently, we have Kafka instances created in different locations in our test suite (e.g. {{KafkaSourceTestEnv}}, {{KafkaTestEnvironmentImpl}}, {{KafkaTableTestBase}}). One idea is to provide a JUnit5 extension for this similarly to what we do with the [MiniClusterExtension|https://github.com/apache/flink/blob/a6db6ee5d0d6e9b50c6d110793e2efbd0d57cc38/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/junit5/MiniClusterExtension.java] for Flink. Maybe, there's already something out there that can be used/extended.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24119,FLINK-29914,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:16 UTC 2023,,,,,,,,,,"0|z1btxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 12:35;gaborgsomogyi;As mentioned I've time pressured works to do but when I've some spare time happy to come up w/ a consistent structure. If somebody picks it up in the meantime then just ping me in need, I know Kafka quite well...;;;","13/Jan/23 13:45;kamalesh0420;Hi [~gaborgsomogyi] [~mapohl] I am new to the library and looking to make a contribution. Is this issue still open or is it fixed? ;;;","13/Jan/23 14:21;gaborgsomogyi;This is still open but I don't think this is the jira you should start with(""I am new to the library""). It's not only to move some lines here and there but to plan end-to-end Kafka test lifecycle (including broker docker debugging in failure, creating common base classes, defining topic lifecycle, making the tests less flaky, etc...). If you still think it fits to you then go ahead :);;;","13/Jan/23 14:44;kamalesh0420;Oh okay, I commented since it was mentioned with a starter label. I think I can handle it but first I will try to solve another Jira task to get used to the codebase/dev setup and come back to this. ;;;","16/Jan/23 11:20;mapohl;I added the ""starter"" label because I expected it to not be too much internal Flink knowledge necessary to reorganize the test classes. Feel free to remove the label if you disagree. You might have a better bigger picture on that topic. ;;;","16/Jan/23 12:00;gaborgsomogyi;Flink internal knowledge is truly not needed. Additionally good to see motivated contributors so it's fine as-is :);;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,
Upgrade hogo version to v0.104.0,FLINK-29955,13500671,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,highfei2011@126.com,highfei2011@126.com,highfei2011@126.com,09/Nov/22 09:35,11/Nov/22 09:11,04/Jun/24 20:41,11/Nov/22 09:10,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Upgrade hogo version to v0.104.0, the current version is older.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 09:10:58 UTC 2022,,,,,,,,,,"0|z1btt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:10;gyfora;merged to main b2ac11f0a233a805c29417b4b8b403dce3300cc1;;;",,,,,,,,,,,,,,,,,,,,,,
Improve DynamoDB Connector Sample Code,FLINK-29954,13500652,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,09/Nov/22 07:56,09/Nov/22 08:58,04/Jun/24 20:41,09/Nov/22 08:58,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"As per https://lists.apache.org/thread/lhvhsyrl3o88lsb6so8158nz6qogsqvn.

The {{SinkIntoDynamoDb}} sample app should not use AWS SDK types between operators of the Flink job graph",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 08:58:18 UTC 2022,,,,,,,,,,"0|z1btow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 08:58;dannycranmer;Merged commit [{{bd1edd6}}|https://github.com/apache/flink-connector-aws/commit/bd1edd6005500d3dafa4dd30a7e85a33e5cc1dd7] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,
Get rid of flink-connector-hive dependency in flink-table-store-hive,FLINK-29953,13500651,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,09/Nov/22 07:55,29/Mar/23 01:52,04/Jun/24 20:41,29/Mar/23 01:52,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,It is unnecessary for the tablestore to rely on it in the test. Its incompatible modifications will make the tablestore troublesome.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 07:55:05.0,,,,,,,,,,"0|z1btoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using DROP TABLE XXX to drop a temporary table （or view） in sql-client can get an exception with incomplete explaination.,FLINK-29952,13500643,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,StarBoy1005,StarBoy1005,09/Nov/22 07:22,20/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / API,,,,0,auto-deprioritized-major,pull-request-available,,,"Drop temporary table by using DROP TABLE XXX...always get ""Drop it first before removing the permanent table"" but I just create a temporary table and have no relation with any permanent table. Like the official example :

-- create a word data generator table
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'datagen',
    'fields.word.length' = '1'
);

current exception:

 !image-2022-11-09-15-06-33-188.png! 


So I guess the exception  should append some notice like ""please check whether miss  a TEMPORARY before TABLE"".","Flink 1.16.0
Red Hat Enterprise Linux Server release 7.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/22 07:08;StarBoy1005;image-2022-11-09-15-06-33-188.png;https://issues.apache.org/jira/secure/attachment/13051985/image-2022-11-09-15-06-33-188.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:16 UTC 2023,,,,,,,,,,"0|z1btmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 07:49;yzl;Currently, dropping a temporary table should add the keyword 'TEMPORARY'. I think you are right, we need improve the exception message.;;;","09/Nov/22 08:04;StarBoy1005;[~yzl]Hi! I'm glad to do this and I'm fixing this now :);;;","09/Nov/22 08:27;yzl;[~StarBoy1005]  Thanks for your contribution!  [~fsk119] can you take a look?;;;","16/Nov/22 01:59;StarBoy1005;[~yzl] [~fsk119] Hi! After several retries, it just passed the flink-ci;)
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43193&view=results
Can somebody review this?;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,
Kafka SQL connector supports setting end offset/timestamp,FLINK-29951,13500628,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xuannan,xuannan,09/Nov/22 06:39,09/Nov/22 07:48,04/Jun/24 20:41,09/Nov/22 07:47,1.17.0,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"Currently, KafkaSource at DataStream API allows specifying end offset/timestamp by `KafkaSourceBuilder#setBounded`, while Kafka SQL connector has no way to do that.

To better align the functionality with DataStream and support bounded stream, we want to support setting end offset/timestamp in Kafka SQL connector.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24456,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 07:48:33 UTC 2022,,,,,,,,,,"0|z1btjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 07:48;martijnvisser;[~xuannan] Thanks for the ticket, but it's a duplicate. There's already a PR open for that, perhaps you can help with the review for it?;;;",,,,,,,,,,,,,,,,,,,,,,
Refactor ResultSet to an interface,FLINK-29950,13500625,13500273,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,yzl,yzl,09/Nov/22 06:32,12/Jan/23 11:42,04/Jun/24 20:41,12/Jan/23 11:42,1.17.0,,,,,,1.17.0,,,,Table SQL / Gateway,,,,0,pull-request-available,,,,"Currently, the 'ResultSet' only contains Schema and data for execution result, witch is not enough for Client to display necessary information. So we need to add more fields to improve 'ResultSet'.

We can refactor the ResultSet to an interface and hide the detail of implementation of ResultSet, such as how to Ser/De the data in ResultSet.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 11:42:45 UTC 2023,,,,,,,,,,"0|z1btiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 11:42;fsk119;Merged into master: 6a15b7e9b2459f2bb398c8d7cd062c8fb92da0f2;;;",,,,,,,,,,,,,,,,,,,,,,
Implement 'RemoteExecutor' to perform the communication with Gateway,FLINK-29949,13500622,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,yzl,yzl,09/Nov/22 06:10,14/Dec/22 11:54,04/Jun/24 20:41,14/Dec/22 11:54,1.17.0,,,,,,,,,,Table SQL / Client,,,,0,,,,,"‘RemoteExecutor’ is part of SQL Client. It performs the communication with Gateway (submitting sql, fetching result, etc.) through REST API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 06:10:49.0,,,,,,,,,,"0|z1bti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add rootCause for ErrorResponse,FLINK-29948,13500621,13500273,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fsk119,yzl,yzl,09/Nov/22 06:08,15/Aug/23 10:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,stale-assigned,,,"Since we need to introduce ‘ErrorDetail’, the old error handling is not suitable anymore. So we should Introduce ErrorResponse to return ErrorDetail in RestEndpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29946,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:09 UTC 2023,,,,,,,,,,"0|z1bti0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 06:10;fsk119;After offline discussion with [~lincoln.86xy] and [~xtsong], we decide to open another FLIP to discuss about the error processing in the REST parts. I will open a new PR after the new discussion is accepted.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
Support configuring session API in Gateway RestEndpoint,FLINK-29947,13500457,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,yzl,yzl,09/Nov/22 04:27,14/Dec/22 12:07,04/Jun/24 20:41,14/Dec/22 12:07,1.17.0,,,,,,,,,,Table SQL / Gateway,,,,0,,,,,"Since in Remote mode, Client communicates with Gateway through REST API, configuring session API should be introduced to RestEndpoint corresponding to [FLINK-29732|https://issues.apache.org/jira/browse/FLINK-29732https://issues.apache.org/jira/browse/FLINK-29732]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29732,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 04:27:33.0,,,,,,,,,,"0|z1bshk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ‘ErrorDetail’ in SQL Gateway Rest Endpoint for error handling,FLINK-29946,13500440,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,yzl,yzl,09/Nov/22 04:20,14/Dec/22 11:52,04/Jun/24 20:41,14/Dec/22 11:52,1.17.0,,,,,,,,,,Table SQL / Client,Table SQL / Gateway,,,0,,,,,"In embedded mode of SQL Client, the exception can be caught by the client finally, but in remote mode, all message return should be serialized and deserialized by REST API. Currently, the Gateway endpoint only returns the exception stack as List<String> when exceptions are thrown. It’s hard for Client to get more useful information. So it's necessary to introduce an 'ErrorDetail' to provide more information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29948,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 04:20:51.0,,,,,,,,,,"0|z1bsds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports to submit SQL to a embedded SQL Gateway in the SQL Client,FLINK-29945,13500414,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,yzl,yzl,09/Nov/22 04:10,15/Feb/23 14:47,04/Jun/24 20:41,29/Jan/23 03:57,1.17.0,,,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,,"The old Executor is not compatible with the SQL gateway, so it should be refactored.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31091,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 03:57:29 UTC 2023,,,,,,,,,,"0|z1bs80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 03:57;fsk119;Merged into master: 1534ea751613772d93699e1916e38f3af3dac7f9;;;",,,,,,,,,,,,,,,,,,,,,,
Support accumulator in source reader,FLINK-29944,13500309,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hk__lrzy,hk__lrzy,09/Nov/22 03:28,19/Aug/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,API / DataStream,,,,0,auto-deprioritized-critical,pull-request-available,,,"Source Reader is mainly for union batch and streaming logic in single interface, it's good point for the developer, but in the {{SourceFunction}} we can access {{runtimeconext}} to use accumulator before, now the {{SourceReaderContext}} have no method for it, this PR is mainly to support it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"link 
https://github.com/apache/flink/pull/21258",false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:05 UTC 2023,,,,,,,,,,"0|z1brko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 03:30;hk__lrzy;https://github.com/apache/flink/pull/21258;;;","09/Nov/22 06:31;luoyuxia;Thanks for contributing. From the pr, seems it changes the public interface, I think it'll need a FLIP.;;;","09/Nov/22 10:53;hk__lrzy;[~luoyuxia] this is from [FLIP-27|https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface]  i want to extend source reader context to align with runtimecontext.;;;","17/Nov/22 09:42;xtsong;Agree with [~luoyuxia].

[~hk__lrzy],
We are aware that the new source is from FLIP-27. However, the two public interfaces you are trying to add are not part of FLIP-27, which has been voted on. Any new changes to the public interface would need to go through the FLIP process.

Please see the following instructions for starting a FLIP.
https://flink.apache.org/contributing/contribute-code.html
https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals;;;","13/Jan/23 12:16;hk__lrzy;Ok, i will follow this one. thanks for your response.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,
Bump Table Store 0.2 Flink version to 1.15.2,FLINK-29943,13500294,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,09/Nov/22 03:22,09/Nov/22 08:08,04/Jun/24 20:41,09/Nov/22 08:08,table-store-0.2.2,,,,,,table-store-0.2.2,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,FLINK-29840,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 08:08:56 UTC 2022,,,,,,,,,,"0|z1brhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 08:08;TsReaper;release-0.2: 1de91d059cd4ec5e094f52d08b016582e030e944;;;",,,,,,,,,,,,,,,,,,,,,,
Improve average performance for TPC-DS,FLINK-29942,13500276,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jark,jark,09/Nov/22 03:08,09/Nov/22 03:08,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Hive,Table SQL / Planner,Table SQL / Runtime,,0,,,,,This is an umbrella issue to track the effort to improve the average performance of TPC-DS for Flink Batch SQL (50% queries to have 100% performance improvement). ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 03:08:40.0,,,,,,,,,,"0|z1brdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SQL Client connects to SQL Gateway,FLINK-29941,13500273,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jark,jark,09/Nov/22 02:57,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Table SQL / Client,Table SQL / Gateway,,,0,,,,,"This is an umbrella issue to track the effort to support SQL Client connects to SQL Gateway, including SQL Client refactoring, end-to-end tests, and documentation. 



Design Docs: https://docs.google.com/document/d/14cS4VBSamMUnlM_PZuK6QKLfriUuQU51iqET5oiYy_c/edit#heading=h.xfpxlv5v61pg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29073,,FLINK-30936,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 02:57:15.0,,,,,,,,,,"0|z1brco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraph logs job state change at ERROR level when job fails,FLINK-29940,13500258,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Do,,liuml07,liuml07,09/Nov/22 00:54,09/Dec/22 04:55,04/Jun/24 20:41,01/Dec/22 08:32,1.16.0,,,,,,,,,,Runtime / Coordination,,,,1,pull-request-available,,,,"When job switched to FAILED state, the log is very useful to understand why it failed along with the root cause exception stack. However, the current log level is INFO - a bit inconvenient for users to search from logging with so many surrounding log lines. We can log at ERROR level when the job switched to FAILED state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 19:55:23 UTC 2022,,,,,,,,,,"0|z1br9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 09:41;gaoyunhaii;Hi [~liuml07] It looks to me that it might not be an error from the JobManager's perspective since JM is able to recover from this status. Also may I have a double confirmation about why we want to make it to be ERROR level ? If for the purpose of monitoring, the log might not be very stable and it might be better to rely on metrics like numberOfRestarts. ;;;","18/Nov/22 06:19;liuml07;Thanks for the comment [~gaoyunhaii] . We have been using a comprehensive dashboard to show metrics including {{numberOfRestart}}. However, when we see multiple restarts for an interval, it's not clear which subtask / taskmanager caused the failure. This log is one of the most related places we can check the exception stack of the cause. With so many INFO level logging, it's not straightfoward to get this exact one. With ERROR level logging, it's simpler to spot it with eyeballs as well as setting up alerts. I think in increasingly more places, deployments are per-job-per-JM (e.g. application mode). -A job- *The job* failure is an ""error"" events to the dedicated JM. If JM can not recover from such error, it would be FATAL?;;;","01/Dec/22 08:32;xtsong;Agree with [~gaoyunhaii] that the state changing of a job, even the state is FAILED, should not be considered an error of the framework.

I think there're good reasons that support both ways, treating a job failure as an error or not. However, Flink has been the current way since most likely its first day, with lots of users being used to it. I don't think it's necessary to break this.

[~liuml07], for this or any other logs that are particularly interested, I think the proper way is to filter them out with keywords / regex, rather than promoting the logs to a higher level.

Closing the ticket as Won't Do.;;;","01/Dec/22 17:34;liuml07;Yes we now search for ""switched from RUNNING to FAILED"" string to get related logs and stack traces. I just learnt from here it's a convention to only log framework errors at ERROR level. Thanks;;;","05/Dec/22 12:22;rmetzger;I agree that ERROR is not the right log level here, because it is indeed not a log message indicating that the system can not proceed normally.
However, it is also not really an INFO event, since it is a pretty important message, that the user should notice. I was wondering whether a WARN log level would be a change we could consider here?;;;","05/Dec/22 19:55;liuml07;Yeah, WARN will be much better than INFO. I can update the PR if this looks good to [~gaoyunhaii] and [~xtsong].;;;",,,,,,,,,,,,,,,,,
Add metrics for Kubernetes Client Response 5xx count and rate,FLINK-29939,13500253,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhouJIANG,ZhouJIANG,ZhouJIANG,09/Nov/22 00:04,13/Dec/22 14:42,04/Jun/24 20:41,13/Dec/22 14:42,,,,,,,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Operator now publishes k8s client response count by response code. In addition to the accumulative count, adding rate for k8s client error responses could help to setup alerts detect underlying cluster API server status proactively. This is for enhancement of metrics when Flink Operator is deployed to shared / multi-tenant k8s clusters. 

 

Why is rate needed for certain response codes?

To detect issues proactively by setting up alerts in certain cases. It could not the total number but the rate indicates the start / end of unavailability issue.

 

Why do some 4xx matter in prod?

For example - noisy neighbor issue may happen at random time in shared clusters, and operator may start to see increased number of 429 if cluster does not have fairness in rate limiting. Another example is about churn: when the cluster has namespaces quota defined and namespace is under pod churn, there could be increasing number of 409. In these cases, metrics and alerting on count / rate of certain 4xx is critical to understand start / end of prod outage.

 

Why is 5xx needed ?

For faster identify infrastructure issue. With 5xx response count + rate, It's more straightforward than enumerating possible 5xx codes when setting up prod alerts.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 14:42:53 UTC 2022,,,,,,,,,,"0|z1br88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 09:35;gyfora;Sounds good +1;;;","12/Nov/22 11:29;mbalassi;[~ZhouJIANG] thanks. Would you like me to assign the ticket to you?;;;","12/Nov/22 19:45;ZhouJIANG;Yes, I'd like to work on this;;;","13/Dec/22 14:42;gyfora;merged to main f2f8bff7092b72d291d0cde5231266d574eacaaf;;;",,,,,,,,,,,,,,,,,,,
Add open() Method to AsyncSink ElementConverter,FLINK-29938,13500251,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,08/Nov/22 22:32,12/Dec/22 16:33,04/Jun/24 20:41,22/Nov/22 17:13,,,,,,,1.17.0,,,,Connectors / Common,,,,0,pull-request-available,,,,"The {{ElementConverter}} is used to convert records to objects that a Sink can deliver to a destination. In some sink implementations, the {{ElementConverter}} needs to be serialized and sent to TM, DynamoDB is a good example [1]. For DynamoDB we need to lazily instantiate some objects, and an `open()` method would provide a clean hook for this.


[1] https://github.com/apache/flink-connector-aws/blob/main/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDBEnhancedElementConverter.java#L57

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30388,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 17:13:32 UTC 2022,,,,,,,,,,"0|z1br7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 17:13;dannycranmer;Merged commit [{{91c4d86}}|https://github.com/apache/flink/commit/91c4d865eabad0f7f1b8c7426d87e86afa06d6f6] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,
Enhanced DynamoDB Element Converter,FLINK-29937,13500243,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,08/Nov/22 21:31,30/Nov/22 08:29,04/Jun/24 20:41,08/Nov/22 22:26,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"The new DynamoDB sink requires the user to supply an ElementConverter to convert their POJO into a {{DynamoDbWriteRequest}}. DynamoDB provide an ""enhanced client"" that supports annotations on your model class. https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/examples-dynamodb-enhanced.html

Add a new element converter, that can be optionally used to convert a {{@DynamoDbBean}} to a {{DynamoDbWriteRequest}}
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 22:26:33 UTC 2022,,,,,,,,,,"0|z1br60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 22:26;dannycranmer;Merged commit [{{a28dc94}}|https://github.com/apache/flink-connector-aws/commit/a28dc943fafb2c32c287f5e5948396ddfea224e8] into apache:main
Merged commit [{{611653b}}|https://github.com/apache/flink-connector-aws/commit/611653b906e3083be3d421d442fd25636b7f7bdb] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,
Make operator more robust to namespaces access errors,FLINK-29936,13499732,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,08/Nov/22 15:18,10/Nov/22 17:44,04/Jun/24 20:41,10/Nov/22 17:44,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,With some new features in the JOSDK ([https://javaoperatorsdk.io/docs/patterns-best-practices#stopping-or-not-operator-in-case-of-informer-errors-and-cache-sync-timeouts)]  we can now allow the operator to start even if not all namespace permissions are present.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 17:44:34 UTC 2022,,,,,,,,,,"0|z1bo0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 17:44;gyfora;merged to main 03416c345408b0facd9637c0f9efb9ff7514194d;;;",,,,,,,,,,,,,,,,,,,,,,
Setup CI runs against 1.16/1.17-SNAPSHOT,FLINK-29935,13499682,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Nov/22 12:43,14/Nov/22 09:28,04/Jun/24 20:41,14/Nov/22 09:28,,,,,,,elasticsearch-3.0.1,elasticsearch-3.1.0,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,We want to be notified early about breaking changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 15:28:46 UTC 2022,,,,,,,,,,"0|z1bnpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 15:28;chesnay;main: 54250c89496b7de98e17ba99c98ffd92df812cb3
v3.0: 7d4eb8d6ac184be807c7b8900869f1f5c89ba6b4

-Waiting with the backport until the first successful run.-;;;",,,,,,,,,,,,,,,,,,,,,,
maven-assembly-plugin 2.4 make assemble plugin could find xml in flink clients module,FLINK-29934,13499677,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,jackylau,jackylau,08/Nov/22 12:28,15/Dec/22 08:45,04/Jun/24 20:41,15/Dec/22 08:45,1.17.0,,,,,,,,,,Client / Job Submission,,,,0,pull-request-available,,,,!image-2022-11-08-20-28-00-814.png!,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 12:28;jackylau;image-2022-11-08-20-28-00-814.png;https://issues.apache.org/jira/secure/attachment/13051949/image-2022-11-08-20-28-00-814.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 08:36:23 UTC 2022,,,,,,,,,,"0|z1bnog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 13:15;chesnay;IIRC this usually meant that you had old stuff still around; i.e., do not have a clean git tree.;;;","11/Nov/22 04:07;jackylau;hi [~chesnay] what old stuff still around?;;;","11/Nov/22 14:09;chesnay;files from other branches that are covered by .gitignore but still picked up by maven.;;;","01/Dec/22 08:36;xtsong;I also cannot reproduce this.

[~jackylau], do you still have this problem?;;;",,,,,,,,,,,,,,,,,,,
Bump Flink version to 1.16.0,FLINK-29933,13499667,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,08/Nov/22 11:48,09/Nov/22 08:28,04/Jun/24 20:41,08/Nov/22 11:50,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 11:50:10 UTC 2022,,,,,,,,,,"0|z1bnm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 11:50;lzljs3620320;master: 668e857f0b088737a19bfb0ed489a00e0df7fe5d;;;",,,,,,,,,,,,,,,,,,,,,,
Upgrade Calcite version to 1.29.0,FLINK-29932,13499666,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Nov/22 11:44,19/Apr/23 06:54,04/Jun/24 20:41,31/Jan/23 13:23,,,,,,,1.17.0,,,,Table SQL / API,Table SQL / Runtime,,,0,pull-request-available,,,,"As it was suggested in comments here https://issues.apache.org/jira/browse/FLINK-27998
it's better to go version by version.
However i failed to find a jira issue to update to 1.29.0
so this is a jira issue for that",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31066,FLINK-27998,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 13:23:47 UTC 2023,,,,,,,,,,"0|z1bnm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 13:23;twalthr;Fixed in master: 6878fced84e86f7fa14358d04b3e5b4e0b5e3042;;;",,,,,,,,,,,,,,,,,,,,,,
Pin Flink binary URL and cache binary,FLINK-29931,13499665,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Nov/22 11:43,08/Nov/22 12:42,04/Jun/24 20:41,08/Nov/22 12:42,,,,,,,elasticsearch-3.0.1,elasticsearch-3.1.0,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 12:42:39 UTC 2022,,,,,,,,,,"0|z1bnls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 12:42;chesnay;main: 23dd29a43b63c5c95b5f60a677ca08c12dd6d77d
v3.0: 950c0b07d5d0f2077ca9721d34fd8c9c8eec6828;;;",,,,,,,,,,,,,,,,,,,,,,
ParquetFileStatsExtractorTest is unstable,FLINK-29930,13499664,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,08/Nov/22 11:37,08/Nov/22 11:53,04/Jun/24 20:41,08/Nov/22 11:53,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"https://github.com/apache/flink-table-store/actions/runs/3418971069/jobs/5691913303


[INFO] Running org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractorTest
Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.919 s <<< FAILURE! - in org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractorTest
Error:  testExtract  Time elapsed: 0.91 s  <<< ERROR!
java.lang.UnsupportedOperationException: type CHAR not supported for extracting statistics in parquet format
	at org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractor.toFieldStats(ParquetFileStatsExtractor.java:85)
	at org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractor.lambda$extract$0(ParquetFileStatsExtractor.java:73)
	at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250)
	at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110)
	at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
	at org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractor.extract(ParquetFileStatsExtractor.java:75)
	at org.apache.flink.table.store.format.FileStatsExtractorTestBase.testExtract(FileStatsExtractorTestBase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:109)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] Running org.apache.flink.table.store.format.parquet.ParquetFileFormatTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.037 s - in org.apache.flink.table.store.format.parquet.ParquetFileFormatTest
[INFO] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 11:53:06 UTC 2022,,,,,,,,,,"0|z1bnlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 11:53;lzljs3620320;master: 6104b46f608ae67c6704590045a83db58dd4444c;;;",,,,,,,,,,,,,,,,,,,,,,
Correct conversions from ResolvedSchema to TableSchema,FLINK-29929,13499661,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,liyubin117,liyubin117,08/Nov/22 11:34,18/Nov/22 03:10,04/Jun/24 20:41,18/Nov/22 03:10,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"In the new schema framework, DataType in Column indicate wether the column is a time attribute and the old api not, we can see that ```DefaultSchemaResolver.resolve()``` include above logic. but in TableSchema.fromResolvedSchema(), we can't see the time attribute removal logic, thus cause a wrong Timestamp DataType. the test case has been in the pull request.

!image-2022-11-08-19-32-05-894.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 11:32;liyubin117;image-2022-11-08-19-32-05-894.png;https://issues.apache.org/jira/secure/attachment/13051940/image-2022-11-08-19-32-05-894.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-08 11:34:51.0,,,,,,,,,,"0|z1bnkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow sharing (RocksDB) memory between slots,FLINK-29928,13499651,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,08/Nov/22 10:54,02/Dec/22 10:34,04/Jun/24 20:41,01/Dec/22 18:01,,,,,,,1.17.0,,,,Runtime / Configuration,Runtime / State Backends,Runtime / Task,,0,pull-request-available,,,,"h1. Background and motivation

RocksDB is one of the main consumers of off-heap memory, which it uses for BlockCache, MemTables, Indices and Bloom Filters.
Since 1.10 (FLINK-7289), it is possible to:
 - share these objects among RocksDB instances of the same slot
 - bound the total memory usage by all RocksDB instances of a TM

The memory is divided between the slots equally (unless using fine-grained resource control).
This is sub-optimal if some slots contain more memory intensive tasks than the others.

The proposal is to widen the scope of sharing memory to TM, so that it can be shared across all of its RocksDB instances.
That would reduce the overall memory consumption in exchange for resource isolation.
h1. Proposed changes
h2. Configuration
 - introduce ""state.backend.rocksdb.memory.fixed-per-tm"" (memory size, no default)
 -- cluster-level (yaml only)
 -- used by a job only if neither 'state.backend.rocksdb.memory.fixed-per-slot' nor 'state.backend.rocksdb.memory.fixed-per-slot' are not used for the job
 -- use cluster-level or default configuration when creating TM-wise shared RocksDB objects, e.g.  ""state.backend.rocksdb.memory.managed"", ""state.backend.rocksdb.memory.write-buffer-ratio""
 -- doesn't affect Flink memory calculations; user needs to take it into account when planning capacity (similar to fixed-per-slot)

h2. Example
{code:java}
# cluster-level configuration
taskmanager.memory.managed.size: 1gb
state.backend.rocksdb.memory.fixed-per-tm: 1gb
taskmanager.numberOfTaskSlots: 10
cluster.fine-grained-resource-management.enabled: false

# job 1:
state.backend.rocksdb.memory.managed: false # uses shared TM memory

# job 2:
state.backend.rocksdb.memory.managed: false # uses shared TM memory

# job 3:
state.backend.rocksdb.memory.managed: true # uses exclusive managed memory

# job 4:
state.backend.rocksdb.memory.managed: true # gets overriden below
state.backend.rocksdb.memory.fixed-per-slot: 50M # uses exclusive unmanaged memory

{code}
Jobs 1 and2 will use the same 1Gb of shared unmanaged memory and will compete with each other.
Their Python code (or other consumers) will be able to use up to ~100Mb per slot.

Jobs 3 and 4 are not affected as they specify using managed (3) or fixed-per-slot memory (4).
Python code (or other consumers) will be able to use up to ~100Mb per slot but will compete with RocksDB in job (3).

h2. Creating and sharing RocksDB objects

Introduce sharedResources to TaskManager.
Then, similarly to the current slot-wise sharing using MemoryManager:
 - put/get OpaqueMemoryResource
 - Creation of Cache object is done from the backend code on the first call
 - Release it when the last backend that uses it is destroyed
So flink-runtime doesn't have to depend on state backend.

h2. Class loading and resolution

RocksDB state backend is already a part of the distribution.
However, if a job also includes it then classloader.resolve-order should be set to parent-first to prevent conflicts.
h2. Lifecycle

The cache object should be destroyed on TM termnation; job or task completion should NOT close it.
h1. Testing
 * One way to test that the same RocksDB cache is used is via RocksDB metrics.
- -ITCases parameterization-
- manual and unit tests

h1. Limitations
 - classloader.resolve-order=child-first is not supported
 - fine-grained-resource-management is not supported
 - only RocksDB will be able to use TM-wise shared memory; other consumers may be adjusted later

h1. Rejected alternatives
 - set total ""fixed-per-slot"" to a larger value, essentially overcommitting unmanaged memory - doesn't work well in containerized environments (OOMErrors)
 - set numberOfTaskSlots=1 and allow sharing the same slot between any tasks - requires more invasive changes in scheduler and TM
- make part of managed memory shared; it is beleived that managed memory must preserve isolation proprty among other concerns

cc: [~yunta], [~ym], [~liyu]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30275,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 18:01:00 UTC 2022,,,,,,,,,,"0|z1bnio:",9223372036854775807,"Added new config parameters: 
- state.backend.rocksdb.memory.fixed-per-tm",,,,,,,,,,,,,,,,,,,"01/Dec/22 18:01;roman;Merged into master as 3b6d08e57f644cddcdac1fb5a110d44172652c3a.;;;",,,,,,,,,,,,,,,,,,,,,,
AkkaUtils#getAddress may cause memory leak,FLINK-29927,13499637,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,pltbkd,pltbkd,08/Nov/22 09:20,09/Nov/22 09:40,04/Jun/24 20:41,09/Nov/22 09:40,1.15.2,1.16.0,,,,,1.15.3,1.16.1,1.17.0,,Runtime / RPC,,,,0,pull-request-available,,,,"We found a slow memory leak in JM. When MetricFetcherImpl tries to retrieve metrics, it always call MetricQueryServiceRetriever#retrieveService first. And the method will acquire the address of a task manager, which will use AkkaUtil#getAddress internally. While the getAddress method is implemented like this:

{code:java}
    public static Address getAddress(ActorSystem system) {
        return new RemoteAddressExtension().apply(system).getAddress();
    }
{code}

and the RemoteAddressExtension#apply is like this:

{code:scala}
  def apply(system: ActorSystem): T = {
    java.util.Objects.requireNonNull(system, ""system must not be null!"").registerExtension(this)
  }
{code}

This means every call of AkkaUtils#getAddress will register a new extension to the ActorSystem, and can never be released until the ActorSystem exits.

Most of the usage of the method are called only once while initializing, but as described above, MetricFetcherImpl will also use the method. It can happens periodically while users open the WebUI, or happens when the users call the RESTful API directly to get metrics. This means the memory may keep leaking. 

The leak may be introduced in FLINK-23662 when porting the scala version of AkkaUtils to the java one, while I'm not sure if the scala version has the same issue.

The leak seems very slow. We observed it on a job running for more than one month with only 1G memory for job manager. So I suppose it's not an emergency one but still needs to fix.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 09:26;pltbkd;RemoteAddressExtensionLeaking.png;https://issues.apache.org/jira/secure/attachment/13051936/RemoteAddressExtensionLeaking.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 09:40:23 UTC 2022,,,,,,,,,,"0|z1bnfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 10:25;chesnay;Unless I'm several underestimating Scala magic this leak should've been there for a long time.

Should be easy to fix though; I've opened a PR.;;;","09/Nov/22 09:40;chesnay;master: 304122eadc52dd6ee8c04d9777d97eb66aec5e0e
1.16: 313e30483e9770d18461b5dc655da423d465b7e3
1.15: 6feaa440ffce2afc6b0222c49de598b94e60c825;;;",,,,,,,,,,,,,,,,,,,,,
File source continuous monitoring mode ignoring files during savepoint upgrade mode,FLINK-29926,13499628,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,avks,avks,08/Nov/22 08:45,20/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,1.15,auto-deprioritized-critical,Flink,ReadFile,"During a stateful application upgrade using flink kubernetes operator, the StreamExecutionEnvironment.readFile() with FileProcessingMode.PROCESS_CONTINUOUSLY mode operator fails to detect any new changes that has happened on the same file in the directory.
 
*Background* : Currently we have a fresh deployment of the application using kuberenetes operator using savepoint as the upgarde mode and checkpoint enabled.

env.readFile() with FileProcessingMode.PROCESS_CONTINUOUSLY mode operator starts continuosly monitoring the directory (S3 prefix) for any changes and also checkpoints for the provided duration.
{noformat}
2022-11-07 10:47:13.577 UTC taskmanager-1-1 [priority='DEBUG' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Opened ContinuousFileMonitoringFunction (taskIdx= 0) for path: s3://test-app/configs
...
...
2022-11-07 10:47:13.996 UTC taskmanager-1-1 [priority='DEBUG' thread='Legacy Source Thread - Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Ignoring s3://test-app/configs/control-event-config.json, with mod time= 1667817365000 and global mod time= 1667817365000
...
...
2022-11-07 10:51:40.896 UTC taskmanager-1-1 [priority='DEBUG' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - ContinuousFileMonitoringFunction checkpointed 1667817365000.{noformat}
Now we try to upgrade the application using the kubernetes operator, due to this the application tries to take savepoint by using the below Suspend Mechanism - Cancel with savepoint.
By doing this, the application calls the cancel methods which inturn sets the globalModificationTime = Long.MAX_VALUE and then the savepoint is taken.
{noformat}
2022-11-07 10:54:12.897 UTC taskmanager-1-1 [priority='DEBUG' thread='Legacy Source Thread - Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Ignoring s3://test-app/configs/control-event-config.json, with mod time= 1667817365000 and global mod time= 1667817365000
...
2022-11-07 10:55:12.899 UTC taskmanager-1-1 [priority='DEBUG' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - ContinuousFileMonitoringFunction checkpointed 9223372036854775807
....
2022-11-07 10:55:13.090 UTC taskmanager-1-1 [priority='DEBUG' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Closed File Monitoring Source for path: s3://test-app/{noformat}
Due to this, the globalModificationTime changed from 1667817365000 to MAX_VALUE (9223372036854775807) and gets stored in the savepoint state.

Once the application restarts with the new changes, the env.readFile() operator restores the previous state in which the globalModificationTime = Long.MAX_VALUE and starts ignoring any changes done to the file after upgrade
{noformat}
2022-11-07 11:00:13.577 UTC taskmanager-1-1 [priority='INFO' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Restoring state for the ContinuousFileMonitoringFunction
....
2022-11-07 11:00:13.577 UTC taskmanager-1-1 [priority='DEBUG' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - ContinuousFileMonitoringFunction retrieved a global mod time of 9223372036854775807
....
2022-11-07 11:00:13.577 UTC taskmanager-1-1 [priority='DEBUG' thread='Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Opened ContinuousFileMonitoringFunction (taskIdx= 0) for path: s3://test-app/configs
....
2022-11-07 11:00:13.996 UTC taskmanager-1-1 [priority='DEBUG' thread='Legacy Source Thread - Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Ignoring s3://test-app/configs/control-event-config.json, with mod time= 1667821399000 and global mod time= 9223372036854775807
...
...
2022-11-07 11:01:12.897 UTC taskmanager-1-1 [priority='DEBUG' thread='Legacy Source Thread - Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Ignoring s3://test-app/configs/control-event-config.json, with mod time= 1667821399000 and global mod time= 9223372036854775807
...
2022-11-07 11:02:12.897 UTC taskmanager-1-1 [priority='DEBUG' thread='Legacy Source Thread - Source: Custom File Source (1/1)#0'] org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction@ - Ignoring s3://test-app/configs/control-event-config.json, with mod time= 1667821399000 and global mod time= 9223372036854775807{noformat}
Cause : The above issue seems to be due the reassignment of the globalModificationTime to MAX_VALUE during cancel
[https://github.com/apache/flink/blob/a77747892b1724fa5ec388c2b0fe519db32664e9/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileMonitoringFunction.java#L389]
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:05 UTC 2023,,,,,,,,,,"0|z1bndk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 10:26;martijnvisser;bq. FileProcessingMode.PROCESS_CONTINUOUSLY mode operator fails to detect any new changes that has happened on the same file in the directory.

To be honest, if an existing file gets changed, I'm not sure that would be picked up by the FileSource. The JavaDoc https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/AbstractFileSource.AbstractFileSourceBuilder.html#monitorContinuously-java.time.Duration- mentions:

bq. This makes the source a ""continuous streaming"" source that keeps running, monitoring for new files, and reads these files when they appear and are discovered by the monitoring.

So my understanding is that the FileSource checks for new files, not for changes in file that have already been processed. ;;;","08/Nov/22 11:15;avks;[~martijnvisser] : we are not using FileSource to read the files. we are using the StreamExecutionEnvironment.readFile() with FileProcessingMode.PROCESS_CONTINUOUSLY mode (Flink version 1.15)
https://github.com/apache/flink/blob/b37999514cbbd019b31fb2d9c4ae751a956f6c87/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java#L1562;;;","08/Nov/22 11:22;martijnvisser;Ah. I was under the impression that that was already deprecated/using the FileSource, but I see that FLINK-25591 is still open;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
table ui of configure value is strange,FLINK-29925,13499617,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,junhan,bytesmith,bytesmith,08/Nov/22 07:43,12/Dec/22 03:39,04/Jun/24 20:41,12/Dec/22 03:38,1.15.3,,,,,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"As shown in the figure below, when the configure value is very large, the ui of the table is a bit strange  !截屏2022-11-08 15.37.04.png|width=856,height=496!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 07:40;bytesmith;截屏2022-11-08 15.37.04.png;https://issues.apache.org/jira/secure/attachment/13051934/%E6%88%AA%E5%B1%8F2022-11-08+15.37.04.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 03:39:40 UTC 2022,,,,,,,,,,"0|z1bnb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 09:26;gaoyunhaii;cc [~junhan] ;;;","01/Dec/22 08:44;xtsong;[~bytesmith], could you explain a bit more about what is being strange? I don't really get it from the screenshot.;;;","01/Dec/22 09:04;bytesmith;[~xtsong] hello,may be the screenshot is too big to find the problem. i've fixed it. As you can see, when the `config value` is long enough, the `config key` cell's width  become very small. It looks like so strange when i open the config page first time.;;;","01/Dec/22 09:08;xtsong;Ok, I see. That's a good point. Do you want to work on fixing this? Or I can try to get someone with frontend expertise to look into this.;;;","01/Dec/22 09:13;bytesmith;[~xtsong] sorry, I don't have enough frontend expertise to fix it.;;;","09/Dec/22 02:20;junhan;No worries, I will take care of this UI problem :);;;","12/Dec/22 03:39;junhan;master: 4e1e1a81ededd86199fcb43c933a198fcbdbe55b;;;",,,,,,,,,,,,,,,,
Update official document,FLINK-29924,13499353,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,StarBoy1005,StarBoy1005,StarBoy1005,08/Nov/22 04:20,08/Nov/22 06:22,04/Jun/24 20:41,08/Nov/22 06:21,table-store-0.2.1,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"Common missing of a period,in Description of ""kafka.bootstrap.servers"".
[link|https://nightlies.apache.org/flink/flink-table-store-docs-release-0.2/docs/development/configuration/#kafkalogoptions]",Flink Table Store v0.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 06:21:53 UTC 2022,,,,,,,,,,"0|z1blog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 06:21;TsReaper;master: 5e176d286c87a422a5eaab7a9b3e0a3d1546b45d;;;",,,,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle may face deadlock when running a task need to execute big size data,FLINK-29923,13499310,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,AlexXXX,AlexXXX,08/Nov/22 04:03,13/Dec/22 04:32,04/Jun/24 20:41,13/Dec/22 04:32,1.16.0,,,,,,1.16.1,1.17.0,,,Runtime / Network,,,,0,,,,,"The flink 1.16 offers hybrid shuffle to combine the superiority of blocking shuffle and pipeline shuffle. But when I want to test this new feature I face a problem that it may cause deadlock when it running. 

Actually, it will run well at beginning. However, when it runs to a certain number it may failure for the buffer size and if I set a bigger size it may running without data execution like the picture. So I want to ask the cause of this problem and a solution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29298,,,,,,,,,,,,"08/Nov/22 04:03;AlexXXX;性能差距.png;https://issues.apache.org/jira/secure/attachment/13051923/%E6%80%A7%E8%83%BD%E5%B7%AE%E8%B7%9D.png","08/Nov/22 04:03;AlexXXX;死锁2-select.png;https://issues.apache.org/jira/secure/attachment/13051925/%E6%AD%BB%E9%94%812-select.png","08/Nov/22 04:03;AlexXXX;死锁检测.png;https://issues.apache.org/jira/secure/attachment/13051924/%E6%AD%BB%E9%94%81%E6%A3%80%E6%B5%8B.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 04:32:51 UTC 2022,,,,,,,,,,"0|z1blew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 04:09;xtsong;Thanks for reporting this, [~AlexXXX].

[~Weijie Guo], could you take a look at this?;;;","08/Nov/22 04:22;Weijie Guo;[~AlexXXX] Thanks for the feedback. If I'm not wrong, the reason for the failure should be insufficient network memory or batch read memory, and this is an expected behavior. After all, pipelined execution requires more resources than all blocking. So now we have to solve the problem that the task thread is stuck. Can you provide more detailed information, such as the thread dump of the stuck subtask. In addition, if it is difficult to describe the problem clearly, you can communicate with me offline via wechat(a644813550) or any other contact ways you want.;;;","08/Nov/22 07:35;Weijie Guo;Through offline discussion with [~AlexXXX] , it is true that the task are stuck forever. Further, the cause of the problem should be the same as FLINK-29298 previously reported. It is a bug in the `LocalBufferPool`, and hybrid shuffle does increase the competition of network buffers, which makes it difficult to reproduce this bug under blocking shuffle, but it almost repeats under the specific query of hybrid shuffle, so I think it should be considered as a very serious bug.;;;","08/Nov/22 08:05;Weijie Guo;Casued by FLINK-29298;;;","13/Dec/22 04:32;xtsong;Should have been fixed by FLINK-29298.;;;",,,,,,,,,,,,,,,,,,
Table store hive catalog support create external table,FLINK-29922,13499161,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wukong,wukong,wukong,08/Nov/22 03:01,15/Nov/22 06:41,04/Jun/24 20:41,15/Nov/22 06:35,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Support create external table for table store hive catalog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 06:35:21 UTC 2022,,,,,,,,,,"0|z1bkhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 06:35;lzljs3620320;master:
feffebde7c3163232636c5b22add7d311198d48e
b80c6cde4f3a4804d1772f20e51ba1a7a960b245
74cd489109a8753319877b212a864a90aa242268;;;",,,,,,,,,,,,,,,,,,,,,,
Can't deserialize Avro type if schema contains a field with name `schema`.,FLINK-29921,13499123,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bill G,Bill G,07/Nov/22 23:45,11/Nov/22 08:29,04/Jun/24 20:41,,1.15.3,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,Flink's Avro deserializer can't deserialize messages with a schema including a field named {{{}schema{}}}. It will throw an exception {{{}Expecting type to be a PojoTypeInfo{}}}. The problem is that the Avro schema compiler generates the getter {{getSchema$()}} because {{getSchema()}} is always generated to return the field {{{}SCHEMA${}}}. This problem would probably also occur for fields such as {{encoder}} and {{decoder}} as these also have automatically generated getter functions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-07 23:45:24.0,,,,,,,,,,"0|z1bk9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor reformat Kafka connector documentation,FLINK-29920,13499119,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,07/Nov/22 22:59,09/Nov/22 02:01,04/Jun/24 20:41,09/Nov/22 02:01,1.16.0,,,,,,1.17.0,,,,Documentation,,,,0,pull-request-available,,,,"We used some HTML tag in the documentation which does not interpret Markdown format nicely. This fixes this by replacing with Markdown tags.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17831,,,,,,,"07/Nov/22 22:58;liuml07;Screenshot 2022-11-07 at 2.55.00 PM.png;https://issues.apache.org/jira/secure/attachment/13051909/Screenshot+2022-11-07+at+2.55.00+PM.png","07/Nov/22 22:59;liuml07;Screenshot 2022-11-07 at 2.55.08 PM.png;https://issues.apache.org/jira/secure/attachment/13051908/Screenshot+2022-11-07+at+2.55.08+PM.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 02:01:03 UTC 2022,,,,,,,,,,"0|z1bk8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 02:01;jark;Fixed in master: 9c1a1a18c4cf968e7e627cc9b542bb60faf9dfb3;;;",,,,,,,,,,,,,,,,,,,,,,
Support operator leader election,FLINK-29919,13498682,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,07/Nov/22 15:59,10/Nov/22 18:21,04/Jun/24 20:41,10/Nov/22 18:21,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"We should add configurable support to JOSDK leader election:
https://javaoperatorsdk.io/docs/features#leader-election",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 18:21:17 UTC 2022,,,,,,,,,,"0|z1bhjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 18:21;gyfora;merged to main 8c0de99fd25d2bbf99ea0742fb6e2607b8799d80;;;",,,,,,,,,,,,,,,,,,,,,,
Generalized delegation token support,FLINK-29918,13498461,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,07/Nov/22 11:28,07/Mar/23 08:58,04/Jun/24 20:41,16/Jan/23 14:44,1.16.0,,,,,,1.17.0,,,,,,,,0,,,,,Please see the linked draft FLIP document for further details.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 08:58:57 UTC 2023,,,,,,,,,,"0|z1bg6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 08:58;leonard;[~gaborgsomogyi] We're preparing the release of Flink 1.17, could you add the feature introduction into the release announcement [1]?

[1] https://docs.google.com/document/d/1aao4ATNcDBlDNdZ7VFFfrjrTGfUDcSTsvVzZrSSked4

 ;;;",,,,,,,,,,,,,,,,,,,,,,
In standalone mode can't view flink dashboard page in default,FLINK-29917,13498276,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Won't Fix,,StarBoy1005,StarBoy1005,07/Nov/22 09:55,07/Nov/22 10:31,04/Jun/24 20:41,07/Nov/22 10:16,1.16.0,,,,,,,,,,Runtime / Network,,,,0,,,,,"If ""rest.bind-address"" in conf/flink-conf.yaml default value is ""localhost"",user can't get the correct flink dashboard page in browser when use the standalone mode.In other situation like yarn-session,no difference between ""rest.bind-address: 0.0.0.0"" and ""rest.bind-address: localhost"",user could get the page in both env.
In some official documents,no more detail about ""Why 8081 port is keep listening but the error shows ERR_CONNECTION_REFUSED?""
So I guess maybe turn the default value to ""0.0.0.0"" can lead to beginners no more confused with error dashboard page:)","FLINK 0.16.0
CentOS Linux release 7.9.2009",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 10:31:55 UTC 2022,,,,,,,,,,"0|z1bf1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 10:16;chesnay;See FLINK-24474 for the reasoning why we bind to localhost by default.;;;","07/Nov/22 10:31;StarBoy1005;[~chesnay]Thanks! I ignored the security reason.;;;",,,,,,,,,,,,,,,,,,,,,
Levels in Table Store may mistakenly ignore level 0 files when two files have the same sequence number,FLINK-29916,13498269,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,07/Nov/22 09:30,08/Nov/22 12:20,04/Jun/24 20:41,08/Nov/22 12:20,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Current constructor of {{Levels}} class contains the following code:

{code:java}
this.level0 = new TreeSet<>(Comparator.comparing(DataFileMeta::maxSequenceNumber).reversed());
{code}

However when two or more jobs writing the same bucket, they may produce files containing the same sequence number. If two files have the same {{maxSequenceNumber}}, one of them will be mistakenly ignored by {{TreeSet}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 12:20:35 UTC 2022,,,,,,,,,,"0|z1bezs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 12:20;TsReaper;master: 95dd0c32da31b6528e96988b21e01126f3c24b7d
release-0.2: 1c66dbf7aa05e18acc8724eed3a48f005326311d;;;",,,,,,,,,,,,,,,,,,,,,,
netty-tcnative-static not built on CI,FLINK-29915,13498179,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,07/Nov/22 08:37,23/Nov/22 14:58,04/Jun/24 20:41,23/Nov/22 14:58,shaded-16.0,,,,,,shaded-17.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-28246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 14:58:08 UTC 2022,,,,,,,,,,"0|z1befs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 08:48;chesnay;LicenseChecker fails categorically because the static netty dependencies use qualifiers that aren't currently supported.;;;","23/Nov/22 14:58;chesnay;shaded-master: 0cf919601e310c4eb6f982ada2aa11625b3d26b5;;;",,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testKafkaSourceSink fails,FLINK-29914,13498101,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,mapohl,mapohl,07/Nov/22 08:07,30/Jan/23 08:02,04/Jun/24 20:41,24/Nov/22 11:07,1.15.3,,,,,,1.17.0,,,,Connectors / Kafka,,,,0,pull-request-available,test-stability,,,"[This build failed|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42857&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=36412] due to an error in {{{}KafkaTableITCase.testKafkaSourceSink{}}}:
{code:java}
Caused by: org.apache.kafka.common.errors.TimeoutException: Topic tstopic_avro not present in metadata after 60000 ms. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24119,,,FLINK-25438,FLINK-30822,,,,,FLINK-29956,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 07:57:49 UTC 2022,,,,,,,,,,"0|z1bdyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 08:55;gaborgsomogyi;All other Kafka tests are also unstable. While I was hanging around the mentioned test I've found a relatively big test code mistake which introduces race. Let me check whether it helps in this case...;;;","07/Nov/22 08:57;mapohl;Awesome, thanks for picking up on that one [~gaborgsomogyi];;;","07/Nov/22 09:19;gaborgsomogyi;Well, not saying I'm able to fix this issue but I've found a fundamental issue which is definitely a blocker in this area :);;;","07/Nov/22 09:28;gaborgsomogyi;I've added a PR, let's see the result. The test failures are super annoying and would be good to reach a better stability.;;;","23/Nov/22 09:01;gaborgsomogyi;Can we close this jira or the issue popped up again?;;;","24/Nov/22 11:09;gaborgsomogyi;master: 9b6bae4eb87e1f472fd0f6cf9403911a88ed89ce
release-1.16: 9e7ebdc671386a8127ddf5affb66e997d877cb7b
release-1.15: 0f2c6bc80cd8c8cb3dbc442f6c981ff4c095694a
;;;","25/Nov/22 03:40;mapohl;[~gaborgsomogyi] what's the intention behind the release notes in this ticket? This would show up in the release notes of 1.17 if we keep it like that. It feels like it should rather be a comment instead.;;;","25/Nov/22 07:56;gaborgsomogyi;Oh gosh, I've put it into the wrong place :/;;;","25/Nov/22 07:57;gaborgsomogyi;Please re-open it if the issue comes again.;;;","25/Nov/22 07:57;gaborgsomogyi;Moved to comment section, where it belongs. Thanks for notifying...;;;",,,,,,,,,,,,,
Shared state would be discarded by mistake when maxConcurrentCheckpoint>1,FLINK-29913,13498033,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Yanfei Lei,Yanfei Lei,07/Nov/22 06:07,05/Aug/23 17:56,04/Jun/24 20:41,05/Aug/23 17:56,1.15.0,1.16.0,1.17.0,,,,1.16.3,1.17.2,1.18.0,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"When maxConcurrentCheckpoint>1, the shared state of Incremental rocksdb state backend would be discarded by registering the same name handle. See [https://github.com/apache/flink/pull/21050#discussion_r1011061072]

cc [~roman] ",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32130,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 05 17:56:04 UTC 2023,,,,,,,,,,"0|z1bdjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 07:45;Yanfei Lei;I added [ConcurrentCheckpointITCase|https://github.com/apache/flink/commit/db27669d8bbfa995d961fae32f46bc4df676afb7] to reproduce this problem at https://github.com/fredia/flink/tree/test_concurrent_chk.;;;","07/Nov/22 11:30;klion26;[~Yanfei Lei]  thanks for creating this ticket and the IT Case,  would you like to contribute a fix for this problem?

For the priority, As this may lead to {{FileNotFoundExecption if set {{maxConcurrencthCheckpoint}}}} > 1, I think it at least needs to be Critical, what do you think about this?;;;","07/Nov/22 11:52;Yanfei Lei;[~klion26] Since maxConcurrentCheckpoint>1 &&MAX_RETAINED_CHECKPOINTS>1  are less frequently used, I set the priority to Minor.
Under the current checkpoint registration and deletion mechanism, I think this bug will take a while to fix, I'd like to fix this problem after discussion.;;;","09/Nov/22 19:04;roman;Thanks a lot for noticing and reporting this issue [~Yanfei Lei] and [~klion26]!

I think there is a (conceptually) simple solution: always generate unique state handle IDs.
(It was already discussed offline before in the context of similar problems while detecting duplicates in SharedStateRegistry).

The ID doesn't have to have any semantic meaning. 
RocksDB happens to [store the local path in ID|https://github.com/apache/flink/blob/421f057a7488fd64854a82424755f76b89561a0b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L397] and then uses it [on recovery|https://github.com/apache/flink/blob/421f057a7488fd64854a82424755f76b89561a0b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateDownloader.java#L105]; but it can be a separate field in the handle as well.

I don't see any issues with this approach, including recovery, rescaling, and JM failover cases.

What do you think? Maybe there are some alternatives?;;;","14/Nov/22 05:41;klion26;sorry for the late reply.

[~Yanfei Lei]  for the priority, IMHO, if the user set \{{ maxConcurrenctCheckpoint > 1 && MAX_RETAINED_CHECKPOINTS > 1 }} , then the checkpoints may be broken, and can't restore from the checkpoint because of the {{{}FileNotFoundException{}}}, so I think it deserves to escalate the priority.

[~roman] your proposal seems valid from my perspective, maybe changing the logic for {{generating the registry key(perhaps using the filename in the remote filesystem)is enough to solve the problem here?}}

please let me what do you think about this, thanks.;;;","14/Nov/22 10:19;Yanfei Lei;[~roman] Great proposal!  I think that maintaining the mapping of <state handle ID, file name> is necessary, because RocksDB needs the file name(xxx.sst) to rebuild its instance.

And I have a question about ""always generate unique state handle IDs"":

if the confirmation notification was missing, TM re-upload some state handles, how do we delete those duplicate state handles?

 ;;;","14/Nov/22 10:39;roman;[~klion26] , we could use remote filename as well, but we still have to store the mapping from remote to local to initialize RocksDB on recovery.

UUID are slightly better IMO because they are usually shorter and therefore consume less space in the final _metadata file and as keys in SharedStateRegistry.

[~Yanfei Lei] , good question. In case of re-upload, the old state should be discarded eventually when the checkpoint it was created for is subsumed.;;;","23/May/23 08:32;Feifan Wang;Sorry I didn't notice this ticket earlier so I submitted a duplicate one.

[~Yanfei Lei]  for the priority, I agree with [~klion26] , since it may break the checkpoint in valid use case, the priority at least be *Major* .

[~roman] I think your proposal is valid, but I still want to provide an alternative :
 # Make SharedStateRegistry allow register multi state object to same key. The generation strategy for SharedStateRegistryKey is still determined by CompositeStateHandle , as it is now. Different state objects may be registered under the same key in force-full-checkpoint (active re-upload).
 # SharedStateRegistry maintains an linked entry list sorted by registration time for each key. PlaceHolderStreamStateHandle will be replace by the last one in the entry list.

In this approach, SharedStateRegistry don't discard any state object in registration process. SharedStateRegistry only deleted state object when its lastUsedCheckpoint is subsumed.;;;","23/May/23 10:01;roman;Thanks for the proposal [~Feifan Wang]. I think it's easier to implement, compared to always unique state IDs. OTH, it complicates already complex part of the system; and has some runtime overhead. So I'd rather go with unique state IDs.

WDYT?

 

As for the priority, I'd change it to Major if there are no objections.;;;","23/May/23 12:21;Feifan Wang;One overhead I can see is that it will use more memory for storing the next pointer. On a 64-bit system, about 7.63MB more memory will be used for every one million entries, I think it is acceptable. Is there any other runtime overhead I missed ?

As for the complexity, this approach will indeed increase the operation of the linked list in the _registerReference()_ method and _unregisterUnusedState()_ method. But given that this is easy to implement, and the implementation is cohesive, I think the complexity is acceptable.

 

Just to clarify, I think using a unique ID is also a valid approach, but I want learn how you do the selection. Further, regarding the approach of using unique registry key, I agree with [~klion26] , we can just choose a stable register key generation method based on remote file name (such as use md5 digest of remote file name) , which can replace of IncrementalRemoteKeyedStateHandle#createSharedStateRegistryKeyFromFileName() . The mapping of local sst file name to StreamStateHandle never changed , so the part of RocksDB recovery does not need to be changed.

 

Whichever approach will be chosen, I am happy to implement it. Can you assign this ticket to me  [~roman] ? looking forward to hearing from you.;;;","23/May/23 13:47;roman;{quote}On a 64-bit system, about 7.63MB more memory will be used for every one million entries
Is there any other runtime overhead I missed ?
{quote}
I was more concerned about the additional time required to traverse the (mostly single-element) lists. When a checkpoint is subsumed, *all* entries need to be scanned. Adding pointer dereference(s) might break any optimizations that JVM and CPU would otherwise employ.

 
{quote}As for the complexity, this approach will indeed increase the operation of the linked list in the registerReference() method and unregisterUnusedState() method.
But given that this is easy to implement, and the implementation is cohesive, I think the complexity is acceptable.
{quote}
In my view, simplicity in this is part is worth the efforts. The problem that SharedStateRegistry solves is already tricky, and we shouldn't complicate it further (higher complexity potentially leads to more bugs and more maintanance efforts). 

 
{quote}Further, regarding the approach of using unique registry key, I agree with Congxian Qiu , we can just choose a stable register key generation method based on remote file name (such as use md5 digest of remote file name) , which can replace of IncrementalRemoteKeyedStateHandle#createSharedStateRegistryKeyFromFileName() .
The mapping of local sst file name to StreamStateHandle never changed , so the part of RocksDB recovery does not need to be changed.
{quote}
I don't fully understand what does ""never changed"" means here.
[Here|https://github.com/apache/flink/blob/fbf7b91424ec626ae56dd2477347a7759db6d5fe/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateDownloader.java#L105], the ID is used to create local path. If we change ID to remote path, local will change too. Per my understanding, we can not change the local path without updating metadata files.
Or am I missing something?

 
{quote}Whichever approach will be chosen, I am happy to implement it. Can you assign this ticket to me Roman Khachatryan ? looking forward to hearing from you.
{quote}
Sure, thanks for volounteering!;;;","23/May/23 15:37;Feifan Wang;Thanks for the clarification [~roman] ! 
{quote}Further, regarding the approach of using unique registry key, I agree with Congxian Qiu , we can just choose a stable register key generation method based on remote file name (such as use md5 digest of remote file name) , which can replace of IncrementalRemoteKeyedStateHandle#createSharedStateRegistryKeyFromFileName() .
The mapping of local sst file name to StreamStateHandle never changed , so the part of RocksDB recovery does not need to be changed.
{quote}
I mean we still use local file name as key of sharedState map in  _*IncrementalRemoteKeyedStateHandle*_ and use remote file path when generating SharedStateRegisterKey. Changes in _*IncrementalRemoteKeyedStateHandle*_ like this :
{code:java}
...

private final Map<StateHandleID, StreamStateHandle> sharedState;  // still use local file name as key of this map, corresponding to the “never change” I mentioned above

...


public void registerSharedStates(SharedStateRegistry stateRegistry, long checkpointID) {
...
    for (Map.Entry<StateHandleID, StreamStateHandle> sharedStateHandle :
            sharedState.entrySet()) {

        SharedStateRegistryKey registryKey = generateRegisterKey(sharedStateHandle.getValue);  // changed line


        StreamStateHandle reference =
                stateRegistry.registerReference(
                        registryKey, sharedStateHandle.getValue(), checkpointID);

        sharedStateHandle.setValue(reference);
    }
}

private static SharedStateRegistryKey generateRegisterKey(StreamStateHandle stateHandle) {
    String keyString = null;
    if (stateHandle instanceof FileStateHandle) {
        keyString = ((FileStateHandle) stateHandle).getFilePath().toString();
    } else if (stateHandle instanceof ByteStreamStateHandle) {
        keyString = ((ByteStreamStateHandle) stateHandle).getHandleName();
    } else {
        keyString = Integer.toString(System.identityHashCode(stateHandle));
    }
    return new SharedStateRegistryKey(md5sum(keyString)); // may be other digest algorithm
}

{code}
 

And we can only use normal handles (not PlaceholderStreamStateHandle) in IncrementalRemoteKeyedStateHandle to make sure IncrementalRemoteKeyedStateHandle#generateRegisterKey() method never get a PlaceholderStreamStateHandle.;;;","23/May/23 20:27;roman;Got it, thanks for clarifying. 

However, I think that using ""Integer.toString(System.identityHashCode(stateHandle))"" can easily lead to collisions, thereby causing checkpoint corruption.

But this seems to me an (important) implementation detail, that can be discussed in the PR. WDYT?;;;","24/May/23 03:49;Feifan Wang;Thanks [~roman] , I will prepare a pr.;;;","24/May/23 08:21;roman;Great! Thanks [~Feifan Wang] ;;;","25/May/23 07:37;klion26;thanks for the discuss above and contribution!

Using the UUID/filename as the key solves the problem here, and it also makes sense because the key and the remote file are one-to-one. In addition, it can also solve some other potential problems, for example, if the Flink job management platform uses the SharedRegistry here to maintain the checkpoints lifecycle, if a task has two ssts with the same name, it will now cause the file to be deleted by mistake (this situation occurs as follows: job A generates a checkpoint chk1, then stops, job B job B resumes from chk1, completes chk2, then stops, then job C resumes from chk1, completes chk3, after we register chk2 and chk3 in one SharedRegistry, we'll delete some remote files by mistake, because there will be some sst files in chk2 and chk3 with the same name);;;","28/May/23 12:02;Feifan Wang;Thanks for your information [~klion26] , I would like to add a case that I recently thought of a possible problem: Since 1.17, user can trigger checkpoint with assigned checkpoint type (CONFIGURED,FULL,INCREMENTAL). If user trigger a FULL checkpoint by rest api, this forces re-upload of snapshot files that have already been uploaded. Using remote file path based register key is still valid in this case.

I have submitted a PR, please help to review it [~roman] ,[~klion26] .;;;","03/Aug/23 08:45;roman;Merged into master as 6dcb10abf319b9e5494a82bee71f1ae1a4e4b211 ... 85f32d6bcb31708c9c2c845ea03ef117726b7c1a.

[~Feifan Wang] do you mind creating backport PRs for branches 1.17 and 1.16?;;;","03/Aug/23 09:04;Feifan Wang; Thanks to [~roman]  for helping to review the PR, I learned a lot from it.
{quote}do you mind creating backport PRs for branches 1.17 and 1.16?
{quote}
I am glad to.;;;","04/Aug/23 12:46;Feifan Wang;[~roman] ,  I submitted two PRs for [1.16|https://github.com/apache/flink/pull/23137] and [1.17|https://github.com/apache/flink/pull/23139], please take a look.;;;","05/Aug/23 11:24;Feifan Wang;Hi [~roman] , In addition backport pr of [1.16|https://github.com/apache/flink/pull/23137] and [1.17|https://github.com/apache/flink/pull/23139], I also submitted [a PR to fix outdated java doc in SharedStateRegistry|https://github.com/apache/flink/pull/23147], PTAL.;;;","05/Aug/23 12:35;roman;Thanks a lot [~Feifan Wang], happy to hear that :)

I've merged backports to 1.16 as aad658910fd6967199bce69f33872d245315bca1..c6c54412bf70af4432071dfdf131b62ff8236fe7
and to 1.17 as 65e7004dd32633f9dfc87b0808ffcb587daf525c..f3d212956660a77d58ab99f4de6611760e5f0b9a.

I've taken a look at javadoc update, thanks for noticing this (y);;;","05/Aug/23 17:56;roman;javadoc updated in 
c7d3e36053091b7d7b10e89c5f9dba3df46ec0bd (master)
f2d3ca0bf4d49621450c7e71c1bd7c9de2deb89b (1.16)
e213267ebf691f1e959be6f5154a790c2ec6a2e9 (1.17);;;"
jdbc scan.partition.column can specify any type of field,FLINK-29912,13498012,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,07/Nov/22 02:00,11/Mar/24 12:43,04/Jun/24 20:41,,1.16.0,,,,,,1.20.0,,,,Connectors / JDBC,,,,0,,,,,"scan.partition. column can specify any type of field.  At present, scan.partition. column must be a numeric, date, or timestamp column from the table in question.  You can specify any type of field, such as string type, which can satisfy all high concurrent read scenarios",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-07 02:00:22.0,,,,,,,,,,"0|z1bdeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve performance of AgglomerativeClustering,FLINK-29911,13497440,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,06/Nov/22 08:11,06/Jan/23 06:07,04/Jun/24 20:41,06/Jan/23 06:07,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-06 08:11:26.0,,,,,,,,,,"0|z1b9vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcNumericBetweenParametersProvider(Variables within the method are extracted),FLINK-29910,13497438,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,sunnny,sunnny,06/Nov/22 07:18,07/Nov/22 20:41,04/Jun/24 20:41,06/Nov/22 07:33,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,!image-2022-11-06-15-18-28-320.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/22 07:17;sunnny;image-2022-11-06-15-17-52-061.png;https://issues.apache.org/jira/secure/attachment/13051837/image-2022-11-06-15-17-52-061.png","06/Nov/22 07:18;sunnny;image-2022-11-06-15-18-28-320.png;https://issues.apache.org/jira/secure/attachment/13051836/image-2022-11-06-15-18-28-320.png","06/Nov/22 07:18;sunnny;image-2022-11-06-15-18-42-020.png;https://issues.apache.org/jira/secure/attachment/13051835/image-2022-11-06-15-18-42-020.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 20:41:28 UTC 2022,,,,,,,,,,"0|z1b9v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 20:41;martijnvisser;If this is indeed fixed, there should be a fix version. If this has been resolved in another way, that should be specified;;;",,,,,,,,,,,,,,,,,,,,,,
Standardise connector package names,FLINK-29909,13496899,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,05/Nov/22 16:54,06/Nov/22 19:26,04/Jun/24 20:41,06/Nov/22 19:26,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 06 19:26:13 UTC 2022,,,,,,,,,,"0|z1b6jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/22 19:26;dannycranmer;merged commit [{{3798aab}}|https://github.com/apache/flink-connector-aws/commit/3798aabfcc6f78645bf3d7255dfd6c336cd497f0] into main;;;",,,,,,,,,,,,,,,,,,,,,,
Externalize and configure E2E tests,FLINK-29908,13496667,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,05/Nov/22 15:41,05/Dec/22 10:36,04/Jun/24 20:41,05/Dec/22 10:36,,,,,,,aws-connector-4.0.0,,,,Connectors / AWS,,,,0,pull-request-available,,,,Migrate Amazon Kinesis and Firehose E2E test modules from Flink core to flink-connector-aws,,,,,,,,,,,,FLINK-29907,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29907,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 10:36:05 UTC 2022,,,,,,,,,,"0|z1b540:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 10:36;dannycranmer;Merged commit [{{df4e3d3}}|https://github.com/apache/flink-connector-aws/commit/df4e3d36017b2abaaaf9479389f220bd604a0307] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,
Externalize AWS connectors from Flink core,FLINK-29907,13496662,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,05/Nov/22 15:40,05/Dec/22 12:45,04/Jun/24 20:41,05/Dec/22 12:45,,,,,,,aws-connector-4.0.0,,,,Connectors / AWS,,,,0,pull-request-available,,,,"Externlize the following modules from Flink core to the connectors repo:
- {{flink-connector-aws-base}}
- {{flink-connector-kinesis}}
- {{flink-connector-sql-kinesis}}
- {{flink-connector-aws-kinesis-streams}}
- {{flink-connector-sql-aws-kinesis-streams}}
- {{flink-connector-aws-kinesis-firehose}}
- {{flink-connector-sql-aws-kinesis-firehose}}",,,,,,,,,,,,,,FLINK-29908,,,,,,,,,,,,,,FLINK-30066,FLINK-30065,,,,FLINK-29908,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 03 19:00:10 UTC 2022,,,,,,,,,,"0|z1b52w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/22 19:00;dannycranmer;Merged commit [{{aa10d88}}|https://github.com/apache/flink-connector-aws/commit/aa10d88f107dab8264ca6073b6cd64f599b34e93] into apache:main
Merged commit [{{da1ddb0}}|https://github.com/apache/flink-connector-aws/commit/da1ddb0b641d22cd758b49cd6f172d26383c8a6d] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,
Address tabs/spaces in checkstyle/spotless,FLINK-29906,13496604,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,dannycranmer,dannycranmer,05/Nov/22 15:13,29/Nov/22 14:53,04/Jun/24 20:41,23/Nov/22 13:45,,,,,,,aws-connector-3.0.0,,,,Connectors / AWS,,,,0,pull-request-available,,,,"The DynamoDB connector has a mix of tabs and spaces, the quality config does not seem to be enforcing anything. The code style guide says we should [use spaces|https://flink.apache.org/contributing/code-style-and-quality-formatting.html#whitespaces].

- Update code to use spaces
- Fix quality plugin to reject tabs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 14:53:49 UTC 2022,,,,,,,,,,"0|z1b4q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 13:46;dannycranmer;Merged commit [{{170947e}}|https://github.com/apache/flink-connector-aws/commit/170947e9a83e641a0a8ba0d584f800d058571d1c] into apache:main ;;;","23/Nov/22 14:46;chesnay;Just to point it out before we're full steam ahead of picking up (bad) old habits: If we spot an issue in one repo, then it likely is also a problem in others. This could be something that we could easily move to the parent pom and have it work for all connectors.;;;","23/Nov/22 18:02;darenwkt;Hi [~chesnay], thank you, that's a very good point. Currently, dynamodb connector parent pom is pointing to your repo ""flink-connector-parent"" [https://github.com/zentol/flink-connector-parent/blob/master/pom.xml.|https://github.com/zentol/flink-connector-parent/blob/master/pom.xml]

I can create a PR to that pom.xml, but I am wondering if I should do it on your repo or should it be in a ""shared"" flink-connector parent repo?;;;","23/Nov/22 18:17;dannycranmer;Thanks [~chesnay], good shout. [~darenwkt] please also check if this if an issue/would benefit the core Flink repo, and others (statefun, Kubernetes operator etc);;;","24/Nov/22 08:58;chesnay;> should do it on your repo or should it be in a ""shared"" flink-connector parent repo?

As explained in https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development this is just a temporary setup.;;;","24/Nov/22 09:00;chesnay;FYI the .editorconfig that other connector repos contain setup formatting for the xml files. It's not being verified though.;;;","29/Nov/22 14:53;darenwkt;Thanks Chesnay, just an update that this is applied in Flink Kubernetes Operator repo as well. [https://github.com/apache/flink-kubernetes-operator/pull/453#event-7913341426];;;",,,,,,,,,,,,,,,,
Migrate flink-connector-dynamodb-parent to flink-connector-aws,FLINK-29905,13496587,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,05/Nov/22 15:00,08/Nov/22 22:20,04/Jun/24 20:41,08/Nov/22 22:20,,,,,,,aws-connector-3.0.0,,,,Connectors / AWS,,,,0,pull-request-available,,,,Update {{flink-connector-dynamodb-parent}} pom to be more general for the AWS connector repo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-05 15:00:46.0,,,,,,,,,,"0|z1b4m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support “string1.notLike(string2)” built-in function in Table API,FLINK-29904,13496220,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,05/Nov/22 08:59,06/Nov/22 13:24,04/Jun/24 20:41,,,,,,,,,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 06 13:24:59 UTC 2022,,,,,,,,,,"0|z1b2co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/22 03:27;cun8cun8;Hi [~dianfu]   I'm interested in this ticket, could you please assign it to me?;;;","06/Nov/22 13:24;dianfu;[~cun8cun8] Have assigned it to you~;;;",,,,,,,,,,,,,,,,,,,,,
Support “string1.notSimilar(string2)” built-in function in Table API,FLINK-29903,13496219,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,05/Nov/22 08:57,06/Nov/22 13:24,04/Jun/24 20:41,,,,,,,,,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 06 13:24:18 UTC 2022,,,,,,,,,,"0|z1b2cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/22 03:28;cun8cun8;Hi [~dianfu]   I'm interested in this ticket, could you please assign it to me?;;;","06/Nov/22 13:24;dianfu;[~cun8cun8] Have assigned it to you~;;;",,,,,,,,,,,,,,,,,,,,,
Support “value1.notIn(TABLE)” built-in function in Table API,FLINK-29902,13496218,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,05/Nov/22 08:55,06/Nov/22 13:25,04/Jun/24 20:41,,,,,,,,,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 06 13:23:53 UTC 2022,,,,,,,,,,"0|z1b2c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/22 03:28;cun8cun8;Hi [~dianfu]   I'm interested in this ticket, could you please assign it to me?;;;","06/Nov/22 13:23;dianfu;[~cun8cun8] Have assigned it to you~;;;",,,,,,,,,,,,,,,,,,,,,
Support “NOT IN” built-in function in Table API,FLINK-29901,13496217,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,05/Nov/22 08:54,06/Nov/22 13:25,04/Jun/24 20:41,,,,,,,,,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 06 13:23:21 UTC 2022,,,,,,,,,,"0|z1b2c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/22 03:28;cun8cun8;Hi [~dianfu]   I'm interested in this ticket, could you please assign it to me?;;;","06/Nov/22 13:23;dianfu;[~cun8cun8] Have assigned it to you~;;;",,,,,,,,,,,,,,,,,,,,,
Table API for DynamoDB Sink,FLINK-29900,13496193,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,05/Nov/22 08:29,30/Nov/22 08:29,04/Jun/24 20:41,22/Nov/22 09:45,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,Implement table API support for DynamoDB sink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 09:45:09 UTC 2022,,,,,,,,,,"0|z1b26o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/22 10:35;dannycranmer;Thanks [~liangtl] I have assigned to you;;;","22/Nov/22 09:45;dannycranmer;Merged commit [{{b2606b6}}|https://github.com/apache/flink-connector-aws/commit/b2606b634df44c1bc412e72c9494842703052998] into apache:main 

Merged commit [{{a1d9391}}|https://github.com/apache/flink-connector-aws/commit/a1d939113fd997c9e96745d310273518d6d8cb7f] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,
Stacktrace printing in DefaultExecutionGraphCacheTest is confusing maven test log output,FLINK-29899,13495972,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,renqs,renqs,05/Nov/22 06:09,25/Nov/22 08:12,04/Jun/24 20:41,25/Nov/22 08:12,1.15.3,1.16.0,1.17.0,,,,1.15.4,1.16.1,1.17.0,,Runtime / Task,,,,0,pull-request-available,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42849&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8700]

 

 
{code:java}
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (408c1ab89f41c2d4f99c870e8abde94d)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.testImmediateCacheInvalidationAfterFailure(DefaultExecutionGraphCacheTest.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 08:12:42 UTC 2022,,,,,,,,,,"0|z1b0tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:20;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43045&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=41438;;;","11/Nov/22 09:20;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43045&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=38408;;;","17/Nov/22 09:55;zhuzh;The exception seems to be unrelated.

I saw in the weekly updates that ""Matthias and Qingsheng investigated the memory issue due to multiple azure agents on one machine use too much resources. We’ve reduced the agents number from 7 to 5, let’s keep an eyes on this issue.""
[~renqs] does it refer to this JIRA?;;;","25/Nov/22 04:19;mapohl;You are right, [~zhuzh] . The error presented in the issue description is misleading. We shouldn't print the stacktrace. I guess, that was accidentally added in [DefaultExecutionGraphCacheTest:151|https://github.com/apache/flink/blob/ad47fe246f7416c5685d1a38b8b8fb44ec503e86/flink-runtime/src/test/java/org/apache/flink/runtime/rest/handler/legacy/DefaultExecutionGraphCacheTest.java#L151].

Anyway, about the 137 exit code that's mentioned in the issue title: It's quite likely caused by some memory leak in one of the {{flink-table}} modules. It's just that other modules are affected by it if they run on the same machine at the same time.

I'm gonna go ahead and rephrase the title of this Jira issue and remove the stacktrace printing.;;;","25/Nov/22 08:12;mapohl;master: cc66d4855e6f8ee9986809a18f68a458bcfe3c12
1.16: 5f9d2519a3cf3adb9e76de416f5100db167d8afc
1.15: 60eedc15671a6bd8c98241b3180e25f9efb30c25;;;",,,,,,,,,,,,,,,,,,
ClusterEntrypointTest.testCloseAsyncShouldBeExecutedInShutdownHook fails on CI,FLINK-29898,13495971,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,renqs,renqs,05/Nov/22 05:46,15/Dec/22 08:49,04/Jun/24 20:41,15/Dec/22 08:49,1.17.0,,,,,,,,,,API / Core,,,,0,,,,,[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42848&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8345],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 08:49:29 UTC 2022,,,,,,,,,,"0|z1b0tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/22 05:47;renqs;[~wangyang0918] Any thought on his one?;;;","07/Nov/22 03:22;wangyang0918;From the current implementation, this issue could only happen when starting a new JVM process takes too long time(30s+). I assume something might be wrong with the virtual machine or disk.

Let's hold on and see whether there are more same failed pipelines.;;;","15/Dec/22 08:49;xtsong;Based on [~wangyang0918]'s explanation, and given that no occurrence of this issue has ever been reported after the first one in the past 1.5month, I'm closing the ticket for now. Feel free to re-open if this happens again.;;;",,,,,,,,,,,,,,,,,,,,
PyFlinkStreamUserDefinedFunctionTests.test_table_function fails on CI,FLINK-29897,13495970,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,05/Nov/22 05:35,05/Nov/22 05:38,04/Jun/24 20:41,,1.17.0,,,,,,,,,,API / Python,,,,0,,,,,"Link: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42838&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25259
 

Error message:
{code:java}
Nov 04 14:42:54 E                   	at sun.reflect.GeneratedMethodAccessor204.invoke(Unknown Source)
Nov 04 14:42:54 E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 04 14:42:54 E                   	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 04 14:42:54 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
Nov 04 14:42:54 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
Nov 04 14:42:54 E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
Nov 04 14:42:54 E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
Nov 04 14:42:54 E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
Nov 04 14:42:54 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
Nov 04 14:42:54 E                   	at java.lang.Thread.run(Thread.java:748)
Nov 04 14:42:54 
Nov 04 14:42:54 .tox/py38/lib/python3.8/site-packages/py4j/protocol.py:326: Py4JJavaError {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 05 05:36:32 UTC 2022,,,,,,,,,,"0|z1b0t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/22 05:36;renqs;[~hxbks2ks] Any thought on this one?;;;",,,,,,,,,,,,,,,,,,,,,,
DynamoDB CI Failing to run checks,FLINK-29896,13495886,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,dannycranmer,dannycranmer,04/Nov/22 20:21,04/Nov/22 22:03,04/Jun/24 20:41,04/Nov/22 22:03,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"The checks are failing to actually run the build and test step:
- https://github.com/apache/flink-connector-aws/actions/runs/3396739520/jobs/5648279513

{code}
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/%20-DaltDeploymentRepository=validation_repository/default/file/default-file.pom
Warning:  The POM for  -DaltDeploymentRepository=validation_repository:default:jar:file is missing, no dependency information available

Error:  Plugin  -DaltDeploymentRepository=validation_repository:default:file or one of its dependencies could not be resolved: Could not find artifact  -DaltDeploymentRepository=validation_repository:default:jar:file in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
Error:  
Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
Error:  Re-run Maven using the -X switch to enable full debug logging.
Error:  
Error:  For more information about the errors and possible solutions, please read the following articles:
Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 22:03:48 UTC 2022,,,,,,,,,,"0|z1b0ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 22:03;dannycranmer;merged commit [399b3af|https://github.com/apache/flink-connector-aws/commit/399b3af7f0400e22b200c949f07970e13c0a901b] into main;;;",,,,,,,,,,,,,,,,,,,,,,
Improve code coverage and integration tests for DynamoDB implementation of Async Sink,FLINK-29895,13495820,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Hong Teoh,Gusev,Gusev,04/Nov/22 19:53,28/Nov/22 14:12,04/Jun/24 20:41,28/Nov/22 14:12,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"*Scope:*
 * Reduce number of integration tests in favour of better unit test coverage
 * Make sure we have good unit test code coverage overall (>99%)
 * Add SQL IT case(s) (punted to https://issues.apache.org/jira/browse/FLINK-30229)",,,,,,,,,,,,,,,,,FLINK-24229,,,,,,,,,,,,,,,,FLINK-30229,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 16:50:52 UTC 2022,,,,,,,,,,"0|z1azvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 11:46;dannycranmer;[~Gusev] I will assign this to [~Hong Teoh], let me know if you were already working on it and I can reassign to you.
 ;;;","24/Nov/22 16:50;dannycranmer;Merged commit [{{89aba34}}|https://github.com/apache/flink-connector-aws/commit/89aba34abd6b80549949ca6a0e8e880f1fe20a03] into apache:main 
Merged commit [{{9e09d57}}|https://github.com/apache/flink-connector-aws/commit/9e09d57210c4b44a9f88cc25781e96c452b2ec49] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,
Rename flink-connector-dynamodb to flink-connector-aws,FLINK-29894,13495585,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,04/Nov/22 16:41,05/Nov/22 15:01,04/Jun/24 20:41,05/Nov/22 15:00,,,,,,,aws-connector-3.0.0,,,,Connectors / AWS,,,,0,,,,,"The existing [flink-connector-dynamodb|https://github.com/apache/flink-connector-dynamodb] repository should be renamed to {{flink-connector-aws}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 05 15:01:12 UTC 2022,,,,,,,,,,"0|z1ayfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 16:51;dannycranmer;Created INFRA ticket https://issues.apache.org/jira/browse/INFRA-23865;;;","05/Nov/22 15:01;dannycranmer;Done https://github.com/apache/flink-connector-aws;;;",,,,,,,,,,,,,,,,,,,,,
k8s python e2e test failed due to permission error when pulling the Docker image,FLINK-29893,13495583,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,04/Nov/22 16:25,14/Aug/23 12:33,04/Jun/24 20:41,14/Aug/23 12:33,1.17.0,,,,,,,,,,Test Infrastructure,,,,0,stale-major,test-stability,,,"[This build failed|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42415&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=11824] in {{test_kubernetes_pyflink_application}} due to permission issues when pulling the Docker image:

{quote}
Failed to pull image ""test_kubernetes_pyflink_application"": rpc error: code = Unknown desc = Error response from daemon: pull access denied for test_kubernetes_pyflink_application, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 12:33:32 UTC 2023,,,,,,,,,,"0|z1ayf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 12:33;mapohl;Looks like it was a one-time issue. I'm gonna close this one as cannot be reproduced.;;;",,,,,,,,,,,,,,,,,,,,,
flink-conf.yaml does not accept hash (#) in the env.java.opts property,FLINK-29892,13495582,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,sergiosp,sergiosp,04/Nov/22 15:58,08/Nov/22 02:05,04/Jun/24 20:41,08/Nov/22 02:05,1.15.2,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"When adding a string with hash (#) character in env.java.opts in flink-conf.yaml , the string will be truncated from the # onwards even when the value is surrounded by single quotes or double quotes.

example:

(in flink-conf.yaml):

env.java.opts: ""-Djavax.net.ssl.trustStorePassword=my#pwd""

 

the value shown on the flink taskmanagers or job managers is :

env.java.opts: -Djavax.net.ssl.trustStorePassword=my

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15358,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 02:04:53 UTC 2022,,,,,,,,,,"0|z1ayew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 03:26;wangyang0918;This is a known limit for current Flink options parser. Similar to FLINK-15358.;;;","08/Nov/22 02:04;xtsong;Let's close this duplicated ticket and keep the discussion in FLINK-15358;;;",,,,,,,,,,,,,,,,,,,,,
The RetryRule tests print some excessive stacktraces that might confuse people investigating actual issues,FLINK-29891,13495578,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,rskraba,mapohl,mapohl,04/Nov/22 15:39,07/Nov/22 15:23,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,Tests,,,,0,,,,,The changes introduced with FLINK-29198 cause excessive stacktraces in the CI logs. We might want to change that to avoid confusing people going through the logs for other errors. (see attached file),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29198,,,,,,,,,,,,"04/Nov/22 15:39;mapohl;RetryRule.log;https://issues.apache.org/jira/secure/attachment/13051812/RetryRule.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 17:50:24 UTC 2022,,,,,,,,,,"0|z1aye0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 15:40;mapohl;[~rskraba] could you have a look at this?;;;","04/Nov/22 17:50;rskraba;Sure, can you assign this to me?;;;",,,,,,,,,,,,,,,,,,,,,
UDFs classloading from JARs in 1.16 is broken,FLINK-29890,13495574,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,04/Nov/22 14:48,24/Feb/23 09:55,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"1.16 introduced a lot of changes with respect to classloading in the Table API. The way UDFs could previously be loaded from JARs in 1.15 does not work in 1.16 anymore - it fails with the ClassNotFound exception when UDFs are used at runtime. 

Here is a repository with a reproducible example:
[https://github.com/afedulov/udfs-flink-1.16/blob/main/src/test/java/com/example/UDFTest.java]
 
It works as is (Flink 1.15.2) and fails when switching the dependencies to 1.16.0.

Here are some of the PRs that might be related to the issue:
[https://github.com/apache/flink/pull/20001]
[https://github.com/apache/flink/pull/19845]
[https://github.com/apache/flink/pull/20211] (fixes a similar issue introduced after classloading changes in 1.16)
 
It is unclear how UDFs can be loaded from JARs in 1.16.
Ideally, this should be covered by tests and described in the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 09:55:11 UTC 2023,,,,,,,,,,"0|z1ayd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 14:57;mapohl;[~jark] may you have a look on that one?;;;","05/Nov/22 06:31;lsy;Thanks for report it, I will take a look. I can't access the link [https://github.com/afedulov/udfs-flink-1.16/blob/main/src/test/java/com/example/UDFTest.java,] can you give more context about how you use the UDF?;;;","05/Nov/22 06:33;lsy;We have e2e tests covering it in `UsingRemoteJarITCase`.;;;","07/Nov/22 11:43;afedulov;[~lsy] I was sure I made the repository public (was not the case), apologies for the inconvenience. 

You should have access now 

https://github.com/afedulov/udfs-flink-1.16;;;","08/Nov/22 02:00;lsy;[~afedulov] Thanks, I will see it as soon as possible.;;;","20/Nov/22 13:28;lsy;[~afedulov] Sorry for the later response, I have read your test case, you use table API and register a udf by your customer jar. Before 1.16, due to we don't support `add jar` or `create function ... using  jar` syntax in table API, so you have to  create a `URLClassLoader` that contains udf class, and then you replace the thread context classloader. Since 1.16, we introduce an `MutableURLClassLoader` in table module, this classloader is used to manage all the customer jars such as connector or udf jar. we can add the jar to `MutableURLClassLoader` via `add jar` or `create function ... using  jar` clause. This is helpful to avoid class conflict in table module. 

Regarding how to udf class can be loaded from jar in 1.16, you can use the `add jar` or `create function ... using  jar` syntax, more details to see [1] or [2] or FLIP-214. For your use case, you don't need to new a classloader and set it to thread context classloader in 1.16, you just need to simplify your code as follows：
{code:java}
@Test
public void shouldGenerateFlinkJobForInteractiveQueryWithUDFSuccessfully() throws Exception {
    final Path jarPath = Paths.get(TEST_FUNCTIONS_LOCATION, ""user-functions.jar"");
    final String jarPathString = String.format(""%s%s"", ""file://"", jarPath.toAbsolutePath());
    final Configuration config = new Configuration();
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);
    final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

    final String functionClassName = ""util.LowerCase"";

    tEnv.executeSql(String.format(""create temporary system function LowerCase as '%s' using jar '%s'"", functionClassName, jarPathString));

    final Table table = tEnv.fromValues(
        DataTypes.ROW(
            DataTypes.FIELD(""id"", DataTypes.DECIMAL(10, 2)),
            DataTypes.FIELD(""name"", DataTypes.STRING())
        ),
        row(1, ""ABC""),
        row(2L, ""ABCDE"")
    );

    final CloseableIterator<Row> iter = tEnv.sqlQuery(""SELECT LowerCase(name) as name FROM "" + table).execute().collect();
    final List<Row> list = new ArrayList<>();
    iter.forEachRemaining(list::add);

    System.out.println("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>"");
    System.out.println(list);
} {code}
 
 # [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#create-function]
 # https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/jar/#add-jar;;;","20/Nov/22 13:30;lsy;This is not a bug, is the by-design behavior. ;;;","13/Jan/23 17:52;charles-tan;[~lsy] I think there still exists a bug here. Using your example code from the comment above but enabling checkpointing, I ran into issues with compiling (full stack trace below). Reproducible code example: [https://github.com/charles-tan/udfs-flink-1.16].
{code:java}
2188 [main] WARN  org.apache.flink.table.runtime.generated.GeneratedClass [] - Failed to compile split code, falling back to original code
org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:120) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:51) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:498) ~[flink-streaming-java-1.16.0.jar:1.16.0]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:221) ~[flink-streaming-java-1.16.0.jar:1.16.0]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:153) ~[flink-streaming-java-1.16.0.jar:1.16.0]
    at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1017) ~[flink-streaming-java-1.16.0.jar:1.16.0]
    at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:56) ~[flink-clients-1.16.0.jar:1.16.0]
    at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:43) ~[flink-clients-1.16.0.jar:1.16.0]
    at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:59) ~[flink-clients-1.16.0.jar:1.16.0]
    at org.apache.flink.client.deployment.executors.LocalExecutor.getJobGraph(LocalExecutor.java:105) ~[flink-clients-1.16.0.jar:1.16.0]
    at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:82) ~[flink-clients-1.16.0.jar:1.16.0]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2188) ~[flink-streaming-java-1.16.0.jar:1.16.0]
    at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[flink-table-planner_2.12-1.16.0.jar:1.16.0]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:884) ~[flink-table-api-java-1.16.0.jar:1.16.0]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1382) ~[flink-table-api-java-1.16.0.jar:1.16.0]
    at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:475) ~[flink-table-api-java-1.16.0.jar:1.16.0]
    at com.example.UDFTest.shouldGenerateFlinkJobForInteractiveQueryWithUDFSuccessfullyWithCheckpointing(UDFTest.java:92) ~[test-classes/:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
    at java.lang.reflect.Method.invoke(Method.java:564) ~[?:?]
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) ~[junit-platform-commons-1.8.1.jar:1.8.1]
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66) ~[junit-jupiter-engine-5.8.1.jar:5.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at java.util.ArrayList.forEach(ArrayList.java:1511) ~[?:?]
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at java.util.ArrayList.forEach(ArrayList.java:1511) ~[?:?]
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) ~[junit-platform-engine-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) ~[junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) ~[junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) ~[junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) [junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) [junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) [junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) [junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) [junit-platform-launcher-1.8.1.jar:1.8.1]
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53) [junit-platform-launcher-1.8.1.jar:1.8.1]
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:71) [junit5-rt.jar:?]
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38) [junit-rt.jar:?]
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11) [idea_rt.jar:?]
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35) [junit-rt.jar:?]
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235) [junit-rt.jar:?]
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54) [junit-rt.jar:?]
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    ... 87 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    ... 87 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 9, Column 31: Cannot determine simple type name ""util""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6499) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6494) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[janino-3.0.11.jar:?]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[janino-3.0.11.jar:?]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[commons-compiler-3.0.11.jar:?]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[commons-compiler-3.0.11.jar:?]
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.16.0.jar:1.16.0]
    ... 87 more {code}
 

 ;;;","17/Jan/23 18:36;charles-tan;[~lsy] I believe this is the same issue as https://issues.apache.org/jira/browse/FLINK-28897 which has been marked as fix. However, the above example shows that in Flink 1.16.0, this has not been fixed.;;;","24/Feb/23 09:55;lsy;[~charles-tan] Yes, Please see the issue https://issues.apache.org/jira/browse/FLINK-29890 description, we need the runtime to provide a generic classloader to fix the problem. Currently, you can work around it by changing your code as follows:
{code:java}
@Test
public void shouldGenerateFlinkJobForInteractiveQueryWithUDFSuccessfully() throws Exception {
    final EnvironmentSettings settings = EnvironmentSettings.newInstance().build();
    final MutableURLClassLoader userClassLoader =
            FlinkUserCodeClassLoaders.create(
                    new URL[0], settings.getUserClassLoader(), settings.getConfiguration());

    StreamExecutionEnvironment streamExecEnv =
            new StreamExecutionEnvironment(settings.getConfiguration(), userClassLoader);
    final Executor executor =
            StreamTableEnvironmentImpl.lookupExecutor(userClassLoader, streamExecEnv);

    TableConfig tableConfig = TableConfig.getDefault();
    tableConfig.setRootConfiguration(executor.getConfiguration());
    tableConfig.addConfiguration(settings.getConfiguration());

    final ResourceManager resourceManager =
            new ResourceManager(settings.getConfiguration(), userClassLoader);
    final ModuleManager moduleManager = new ModuleManager();

    final CatalogManager catalogManager =
            CatalogManager.newBuilder()
                    .classLoader(userClassLoader)
                    .config(tableConfig)
                    .defaultCatalog(
                            settings.getBuiltInCatalogName(),
                            new GenericInMemoryCatalog(
                                    settings.getBuiltInCatalogName(),
                                    settings.getBuiltInDatabaseName()))
                    .executionConfig(streamExecEnv.getConfig())
                    .build();

    final FunctionCatalog functionCatalog =
            new FunctionCatalog(tableConfig, resourceManager, catalogManager, moduleManager);

    final Planner planner =
            PlannerFactoryUtil.createPlanner(
                    executor,
                    tableConfig,
                    userClassLoader,
                    moduleManager,
                    catalogManager,
                    functionCatalog);
    final StreamTableEnvironment tEnv =new StreamTableEnvironmentImpl(
            catalogManager,
            moduleManager,
            resourceManager,
            functionCatalog,
            tableConfig,
            streamExecEnv,
            planner,
            executor,
            settings.isStreamingMode());

    final Path jarPath = Paths.get("""", ""user-functions.jar"");
    final String jarPathString = String.format(""%s%s"", ""file://"", jarPath.toAbsolutePath());

    final String functionClassName = ""util.LowerCase"";

    tEnv.executeSql(
            String.format(
                    ""create temporary system function LowerCase as '%s' using jar '%s'"",
                    functionClassName, jarPathString));

    final Table table =
            tEnv.fromValues(
                    DataTypes.ROW(
                            DataTypes.FIELD(""id"", DataTypes.DECIMAL(10, 2)),
                            DataTypes.FIELD(""name"", DataTypes.STRING())),
                    row(1, ""ABC""),
                    row(2L, ""ABCDE""));

    final CloseableIterator<Row> iter =
            tEnv.sqlQuery(""SELECT LowerCase(name) as name FROM "" + table).execute().collect();
    final List<Row> list = new ArrayList<>();
    iter.forEachRemaining(list::add);

    System.out.println("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>"");
    System.out.println(list);
} {code}
It just looks a bit hack.;;;",,,,,,,,,,,,,
netty-tcnative-static does not bundle tcnative-classes,FLINK-29889,13495571,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Nov/22 14:36,08/Nov/22 10:44,04/Jun/24 20:41,08/Nov/22 10:43,shaded-16.0,,,,,,shaded-16.1,shaded-17.0,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,"The shade plugin configuration was not adjusted to also bundle the tcnative classes, which were previously pulled in via other bundled artifacts.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29886,FLINK-29874,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 10:43:57 UTC 2022,,,,,,,,,,"0|z1aycg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 10:43;chesnay;master: 3082afc952e68366e9fefe4d1181c4666969ee67
16: 8bd55d79dc721a6bb7749243e33e7e446081c8ca;;;",,,,,,,,,,,,,,,,,,,,,,
Improve MutatedConfigurationException for disallowed changes in CheckpointConfig and ExecutionConfig,FLINK-29888,13495536,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,04/Nov/22 12:10,22/Nov/22 16:58,04/Jun/24 20:41,08/Nov/22 13:21,1.17.0,,,,,,1.17.0,,,,API / DataStream,,,,0,pull-request-available,,,,"Currently if {{CheckpointConfig}} or {{ExecutionConfig}} are modified in a non-allowed way, user gets a generic error ""Configuration object ExecutionConfig changed"", without a hint of what has been modified. With FLINK-29379 we can improve this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30155,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 13:21:18 UTC 2022,,,,,,,,,,"0|z1ay4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 13:21;pnowojski;Merged to master as 69526c56a10..488db256914;;;",,,,,,,,,,,,,,,,,,,,,,
 Lookup cache in JDBC table connector  is not each process (i.e. TaskManager) will hold a cache,FLINK-29887,13495474,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,leen,leen,04/Nov/22 09:41,07/Nov/22 01:58,04/Jun/24 20:41,07/Nov/22 01:58,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"hi~ I saw the description of ""When lookup cache is enabled, each process (i.e. TaskManager) will hold a cache"" on the website. When I print out the hashCode of cache in each slot’s thread, I find that they are inconsistent. But according to the above website‘s instructions, cache's hashCode should be the same.

The context for verification is local IDEA.

Can you help me explain? thks~

 
{code:java}
//org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction # eval()  add a line of code: 
LOG.info(""cache hashCode is: {}"", cache);


//log
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (8/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@656ae7d9
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (6/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@5c3a31c2
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (5/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@598a856e
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (7/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@765328ef
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (3/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@47f36967
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (1/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@2b2ea2f
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (4/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@1901ad34
2022-11-04 17:22:53,118 INFO  org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction [Source: daily[1] -> Calc[2] -> LookupJoin[3] -> Calc[4] -> ConstraintEnforcer[5] (2/8)#0] [] - cache hashCode is: org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache@6c441f09
 {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 09:41:07.0,,,,,,,,,,"0|z1axqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"networkThroughput 1000,100ms,OpenSSL Benchmark is failing",FLINK-29886,13495460,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,pnowojski,pnowojski,04/Nov/22 09:20,04/Nov/22 14:37,04/Jun/24 20:41,04/Nov/22 14:37,1.17.0,,,,,,,,,,Benchmarks,Runtime / Network,,,0,,,,,"http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/837/console

{noformat}
04:09:40  java.lang.NoClassDefFoundError: org/apache/flink/shaded/netty4/io/netty/internal/tcnative/CertificateCompressionAlgo
04:09:40  	at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSslX509KeyManagerFactory$OpenSslKeyManagerFactorySpi.engineInit(OpenSslX509KeyManagerFactory.java:129)
04:09:40  	at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:256)
04:09:40  	at org.apache.flink.runtime.net.SSLUtils.getKeyManagerFactory(SSLUtils.java:279)
04:09:40  	at org.apache.flink.runtime.net.SSLUtils.createInternalNettySSLContext(SSLUtils.java:324)
04:09:40  	at org.apache.flink.runtime.net.SSLUtils.createInternalNettySSLContext(SSLUtils.java:303)
04:09:40  	at org.apache.flink.runtime.net.SSLUtils.createInternalClientSSLEngineFactory(SSLUtils.java:119)
04:09:40  	at org.apache.flink.runtime.io.network.netty.NettyConfig.createClientSSLEngineFactory(NettyConfig.java:147)
04:09:40  	at org.apache.flink.runtime.io.network.netty.NettyClient.init(NettyClient.java:115)
04:09:40  	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.start(NettyConnectionManager.java:87)
04:09:40  	at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start(NettyShuffleEnvironment.java:349)
04:09:40  	at org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.setUp(StreamNetworkBenchmarkEnvironment.java:133)
04:09:40  	at org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmark.setUp(StreamNetworkThroughputBenchmark.java:108)
04:09:40  	at org.apache.flink.benchmark.StreamNetworkThroughputBenchmarkExecutor$MultiEnvironment.setUp(StreamNetworkThroughputBenchmarkExecutor.java:117)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29883,,FLINK-29889,,,,,FLINK-29862,,,FLINK-29874,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:22:19 UTC 2022,,,,,,,,,,"0|z1axns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:22;pnowojski;Last successful build is on {{1c4081fa77}} flink/master commit

{noformat}
git ls 1c4081fa77..00a25808dfa
00a25808dfa [27 hours ago] (origin/master, origin/HEAD) [FLINK-29730][checkpoint] Do not support concurrent unaligned checkpoints in the ChannelStateWriteRequestDispatcherImpl [1996fanrui]
b0864c81d83 [13 days ago] [hotfix] Add final modifier to taskName [1996fanrui]
8e66be89dfc [7 hours ago] [FLINK-29831][hive] fix test failure for hive3 [yuxia Luo]
276bb778c7e [17 hours ago] [FLINK-28522][tests][JUnit5 migration] flink-sequence-file (#20258) [Ryan Skraba]
68adbcc8823 [2 weeks ago] [FLINK-26890][Connector/Kinesis] Handle invalid shards in DynamoDB streams consumer [Elphas Toringepi]
9e1ff648f18 [6 weeks ago] [FLINK-29862] Upgrade to flink-shaded 16.0 [Chesnay Schepler]
{noformat}
FLINK-29862 [~chesnay]? ;;;",,,,,,,,,,,,,,,,,,,,,,
SqlValidatorException :Column 'currency' is ambiguous,FLINK-29885,13495433,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,zmm_flink,zmm_flink,04/Nov/22 09:11,04/Nov/22 10:39,04/Jun/24 20:41,04/Nov/22 10:39,1.16.0,,,,,,,,,,Documentation,Table SQL / API,,,0,,,,,"When two tables are join, the two tables have the same field. When querying select, an exception will be thrown if the table name is not specified

exception content

Column 'currency' is ambiguous。

!image-2022-09-28-21-00-22-733.png!

!image-2022-09-28-21-00-09-054.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:13;zmm_flink;image-2022-09-28-21-00-09-054.png;https://issues.apache.org/jira/secure/attachment/13051802/image-2022-09-28-21-00-09-054.png","04/Nov/22 09:13;zmm_flink;image-2022-09-28-21-00-22-733.png;https://issues.apache.org/jira/secure/attachment/13051803/image-2022-09-28-21-00-22-733.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 09:11:26.0,,,,,,,,,,"0|z1axhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test failure in finegrained_resource_management/SortMergeResultPartitionTest.testRelease,FLINK-29884,13495417,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,nkruber,nkruber,04/Nov/22 09:05,02/Mar/23 15:31,04/Jun/24 20:41,02/Mar/23 15:31,1.16.1,1.17.0,,,,,1.17.0,,,,Runtime / Coordination,Runtime / Network,Tests,,0,pull-request-available,test-stability,,,"{{SortMergeResultPartitionTest.testRelease}} failed with a timeout in the finegrained_resource_management tests:
{code:java}
Nov 03 17:28:07 [ERROR] Tests run: 20, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 64.649 s <<< FAILURE! - in org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest
Nov 03 17:28:07 [ERROR] SortMergeResultPartitionTest.testRelease  Time elapsed: 60.009 s  <<< ERROR!
Nov 03 17:28:07 org.junit.runners.model.TestTimedOutException: test timed out after 60 seconds
Nov 03 17:28:07 	at org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.testRelease(SortMergeResultPartitionTest.java:374)
Nov 03 17:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 03 17:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 03 17:28:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 03 17:28:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 03 17:28:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Nov 03 17:28:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Nov 03 17:28:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Nov 03 17:28:07 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 03 17:28:07 	at java.lang.Thread.run(Thread.java:748) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42806&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7]",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29008,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 15:31:50 UTC 2023,,,,,,,,,,"0|z1axe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:17;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43045&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8360;;;","17/Nov/22 10:06;kevin.cyj;I will try to reproduce it and will update if any progress.;;;","28/Nov/22 09:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43512&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7604;;;","28/Nov/22 13:17;mapohl;I assigned the ticket to you for now, [~yingjie];;;","09/Jan/23 08:19;mapohl;This time, we have a thread dump provided for the timeout of this test:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44569&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8735;;;","09/Jan/23 08:20;mapohl;[~yingjie] did you manage to look into it?;;;","15/Jan/23 06:52;kevin.cyj;Sorry for the delay, I already found the root cause. It is only a test issue and I will submit a PR soon. ;;;","18/Jan/23 03:23;Weijie Guo;Thanks [~kevin.cyj] for analyzing and telling me the cause of this problem, I have opened a PR, PTAL~;;;","19/Jan/23 01:16;kevin.cyj;Merged into master via 0b8a83ce54d39d0d5a5b82573c5037f306e9f7f7.;;;","02/Mar/23 09:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46686&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7837

We ran into this issue in 1.16 as well. I'm reopening the issue. [~Weijie Guo] Can you provide a backport?;;;","02/Mar/23 09:47;Weijie Guo;[~mapohl] Thanks for reopening this, I have created the [backport pr|https://github.com/apache/flink/pull/22070], we can merge it after CI passed.;;;","02/Mar/23 15:31;Weijie Guo;release-1.16 via c05c7722e19f3525cf9444cc74ac8071480102d5.;;;",,,,,,,,,,,
Benchmarks are failing,FLINK-29883,13495316,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,chesnay,chesnay,04/Nov/22 08:07,04/Nov/22 10:07,04/Jun/24 20:41,04/Nov/22 10:07,1.17.0,,,,,,,,,,Benchmarks,,,,0,,,,,"Slack message report a failure of the scripts:

{code:java}
Failed build 837 of flink-master-benchmarks-java8 (Open): hudson.AbortException: script returned exit code 1
{code}

can't look further into it since the link requires credentials.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29886,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 08:07:48.0,,,,,,,,,,"0|z1awrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LargeDataITCase is not stable,FLINK-29882,13495311,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,qingyue,qingyue,04/Nov/22 07:58,19/Mar/23 05:43,04/Jun/24 20:41,19/Mar/23 05:43,table-store-0.3.0,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,https://github.com/apache/flink-table-store/actions/runs/3391781964/jobs/5637271002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 07:58:28.0,,,,,,,,,,"0|z1awqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when Fetch results in sql gateway, the result using open api is different  from using restful api  ",FLINK-29881,13495190,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yiwei93,yiwei93,04/Nov/22 06:19,04/Nov/22 09:11,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Gateway,,,,0,,,,,"use restful api , fetch result from  
{code:java}
  http://*****:8083/v1/sessions/9a8fcf37-73e5-43ca-bcc3-d44d8b71a24c/operations/b40085c1-a2c5-42f4-80e7-0971c5ef9710/result/0{code}
the result is 
{code:java}
{
  ""results"": {
    ""columns"": [
      {
        ""name"": ""localtimestamp"",
        ""logicalType"": {
          ""type"": ""TIMESTAMP_WITHOUT_TIME_ZONE"",
          ""nullable"": false,
          ""precision"": 3
        },
        ""comment"": null
      }
    ],
    ""data"": [
      {
        ""kind"": ""INSERT"",
        ""fields"": [
          ""2022-11-04T11:41:40.036""
        ]
      }
    ]
  },
  ""resultType"": ""PAYLOAD"",
  ""nextResultUri"": ""/v1/sessions/9a8fcf37-73e5-43ca-bcc3-d44d8b71a24c/operations/b40085c1-a2c5-42f4-80e7-0971c5ef9710/result/1""
}{code}
use api to fetch ,the code is 
{code:java}
ApiClient client = new ApiClient();
client.setHost(""hermes02"");
client.setPort(8083);
client.setScheme(""http"");
defaultApi = new DefaultApi(client);

OpenSessionRequestBody openSessionRequestBody = new OpenSessionRequestBody();
OpenSessionResponseBody openSessionResponseBody = defaultApi.openSession(openSessionRequestBody);

SessionHandle sessionHandle = new SessionHandle().identifier(UUID.fromString(openSessionResponseBody.getSessionHandle()));

ExecuteStatementRequestBody executeStatementRequestBody = new ExecuteStatementRequestBody().statement(""select localtimestamp"");
ExecuteStatementResponseBody executeStatementResponseBody = defaultApi.executeStatement(sessionHandle.getIdentifier(), executeStatementRequestBody);

FetchResultsResponseBody fetchResultsResponseBody = defaultApi.fetchResults(sessionHandle.getIdentifier(), UUID.fromString(executeStatementResponseBody.getOperationHandle()), 0L);{code}
the result is 
{code:java}
class FetchResultsResponseBody {
    results: class ResultSet {
        resultType: null
        nextToken: null
        resultSchema: null
        data: []
    }
    resultType: NOT_READY
    nextResultUri: /v1/sessions/9a8fcf37-73e5-43ca-bcc3-d44d8b71a24c/operations/b40085c1-a2c5-42f4-80e7-0971c5ef9710/result/0
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/22 06:47;yiwei93;image-2022-11-04-14-47-00-762.png;https://issues.apache.org/jira/secure/attachment/13051793/image-2022-11-04-14-47-00-762.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:09:59 UTC 2022,,,,,,,,,,"0|z1avzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 06:30;heigebupahei;The executestatment operation is asynchronous, you have to get the operation success status to get the result, there should be a rest interface to get the operation status;;;","04/Nov/22 06:48;yzl;Hi,

  Maybe it is a little misleading, let me explain.

  Currently, the `resultType` of the `ResultSet` has three value[1]: NOT_READY, PAYLOAD and EOS. If the result is NOT_READY, it doesn't mean the execution has error or there doesn't have result, but means the result is not ready yet, but you may get it from `nextResultUri`. PAYLOAD result means there is data in result , and EOS means all the result has been fetched (and the EOS result doesn't contain data).

  The result has a `Token`,  means the location of the result. Let's see, in `nextResultUri`, you can see your first nextResultUri is ""xxxx/1"", but your second nextResultUri is ""xxxxx/0"".  The first result's next token said you can move to fetch result 1, indicating the result 0 has been fetched (so the result is PAY_LOAD), and the second result's next token is still 0, means the data is not ready, so you have to try to fetch the result 0 again and this result is NOT_READY result (and has no data).

  You better use such codes to get all result and judge if you get all the results for complex query (that may return much results) follows:
{code:java}
// Pseudo code 
Response response = fetch(xxx, 0L);
SomeResultCollection allResults = xxx;
allResults.add(response.getData());
Long nextToken = response.getNextToken();  // maybe you should parse from nextResultUri
while (nextToken != null) { // which means there still has result
    response = fetch(xxx, nextToken);
    allResults.add(response.getData());
    nextToken = response.getNextToken();
}{code}
  And as yuanfenghu mentioned, the operation has status, if it is not FINISHED status, the resultType will be NOT_READY. For more information, please see [2]. And if the result is FINISHED, you can fetch PAY_LOAD result at first, but you should also judge if nextToken is null in case of there has results left.

[1] [ResultType|https://github.com/apache/flink/blob/8e66be89dfcb54b7256d51e9d89222ae6701061f/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ResultSet.java#L146]

[2][sql gateway design |https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Gateway#FLIP91:SupportSQLGateway-Architecture];;;","04/Nov/22 07:36;yiwei93;Hi,

  thank you for replying

  the problem your mentioned is what we did not consider.   To get all the data  ,we should judge the  `resulttype`.

There  is a other problem.the `FetchResultsResponseBody`  is defined in file `rest_v1_sql_gateway.yml` is not match   what we get from the rest interface.

  The `resultSet`  in   `FetchResultsResponseBody `  has a  attribute (List<RowData> data),    there are two  attribute  in  RowData(Integer arity,RowKind rowKind). What  we get from the rest interface  is also named `data`, but there are two  attribute(kind, fields).  

    ;;;","04/Nov/22 09:09;yzl;[~yiwei93] I believe the doc has some mistakes. The doc `rest_v1_sql_gateway.yml` is generated, and the ""Integer arity, RowKind rowKind"" is fields of the interface `RowData`. I think the doc generator has generated it from the source code, but the deserializer of the REST deserializes the data to the structure ""(RowKind kind, Object[] fields)"". I'll improve the generation when I have time.

For your problem, the actual return fields are:
 * kind : RowKind[1]. Equal to that in the doc;
 * fields: the actual data in the row, fields.length == arity.

[1] Describe the row in changelog. For more information: [RowKind |https://github.com/apache/flink/blob/00a25808dfac69ba8319b9c4dc365e13fd5b87d2/flink-core/src/main/java/org/apache/flink/types/RowKind.java#L25];;;",,,,,,,,,,,,,,,,,,,
Hive sink supports merge files in batch mode,FLINK-29880,13495188,13486231,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Nov/22 06:08,31/Jan/23 03:09,04/Jun/24 20:41,31/Jan/23 03:09,1.16.0,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 03:09:59 UTC 2023,,,,,,,,,,"0|z1avzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 03:09;jark;Fixed in master: cf358d7d55ca48b9d25e5217006898e3070a85ad;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce operators for files merging in batch mode,FLINK-29879,13495187,13486231,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Nov/22 06:07,17/Feb/23 18:42,04/Jun/24 20:41,17/Jan/23 09:56,1.16.0,,,,,,1.17.0,,,,Connectors / FileSystem,,,,0,pull-request-available,,,,"Similar to streaming mode, we introuce the following four operators:

BatchFileWriter -> BatchCompactCoordinator  ->  BatchCompactOperator -> BatchPartitionCommitter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 09:56:34 UTC 2023,,,,,,,,,,"0|z1avz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 09:56;jark;Fixed in master: b5f465036d28ed422a1218bfb8549cbf3e6ecd2e;;;",,,,,,,,,,,,,,,,,,,,,,
Fail to use flink-sql-connector-hive-3.1.3,FLINK-29878,13495179,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,luoyuxia,luoyuxia,04/Nov/22 04:19,31/Jan/23 04:21,04/Jun/24 20:41,31/Jan/23 04:21,1.17.0,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,"When I try to use flink-sql-connector-hive-3.1.3, it will throw the following exception:
{code:java}
ava.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConfUtil
    at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5170) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5114) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf(HiveCatalog.java:261) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.flink.table.endpoint.hive.HiveServer2EndpointFactory.createSqlGatewayEndpoint(HiveServer2EndpointFactory.java:71) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]{code}
After I don't exclude it in the pom,
{code:java}
<exclude>org/apache/hadoop/hive/conf/HiveConf.class</exclude>
<exclude>org/apache/hadoop/hive/metastore/HiveMetaStoreClient.class</exclude> {code}
it'll throw the exception:
{code:java}
aused by: java.lang.NoClassDefFoundError: com/facebook/fb303/FacebookService$Iface
    at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.8.0_252]
    at java.lang.ClassLoader.defineClass(ClassLoader.java:756) ~[?:1.8.0_252]
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.8.0_252]
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:468) ~[?:1.8.0_252]
    at java.net.URLClassLoader.access$100(URLClassLoader.java:74) ~[?:1.8.0_252]
    at java.net.URLClassLoader$1.run(URLClassLoader.java:369) ~[?:1.8.0_252]
    at java.net.URLClassLoader$1.run(URLClassLoader.java:363) ~[?:1.8.0_252]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_252]
    at java.net.URLClassLoader.findClass(URLClassLoader.java:362) ~[?:1.8.0_252]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_252]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_252]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_252]
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_252]
    at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_252]
    at org.apache.hadoop.hive.metastore.utils.JavaUtils.getClass(JavaUtils.java:52) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:146) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] {code}
Then i add the dependency. Seems it works now.
{code:java}
<dependency>
   <groupId>org.apache.thrift</groupId>
   <artifactId>libfb303</artifactId>
   <version>0.9.3</version>
</dependency> {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30034,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:20:59 UTC 2023,,,,,,,,,,"0|z1avxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 04:22;luoyuxia;Introduce by FLINK-29478.

[~samrat007] Could you please have a look? Is it possible to add these back?;;;","04/Nov/22 07:05;samrat007;Yes ! looking into it ;;;","08/Nov/22 08:18;dannycranmer;merged commit [a777478|https://github.com/apache/flink/commit/a77747892b1724fa5ec388c2b0fe519db32664e9] into master;;;","18/Nov/22 10:20;chesnay;The added dependency is not bundled in the sql jar, since the shade-plugin artifactSet was not adjusted accordingly.;;;","22/Nov/22 08:15;samrat007;I did the testing manually for the flinksql jar. 
I will recheck and update with changes . ;;;","31/Jan/23 04:20;renqs;Fixed on master: b1a9fe12f78999e555204a69d7e360705dc88450;;;",,,,,,,,,,,,,,,,,
Failed to fetch next result,FLINK-29877,13495172,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,penney,penney,04/Nov/22 03:09,04/Nov/22 06:33,04/Jun/24 20:41,,1.14.5,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"runTimeConfiguration.set(TaskManagerOptions.TASK_HEAP_MEMORY, MemorySize.ofMebiBytes(4096));
runTimeConfiguration.set(TaskManagerOptions.MANAGED_MEMORY_SIZE, MemorySize.ofMebiBytes(512));

 

I have set memory size,it still can't work.it is a simple union all.I can't understand why not work.please help me.Thanks !image-2022-11-04-11-13-48-282.png!"," 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/22 03:05;penney;image-2022-11-04-11-05-18-627.png;https://issues.apache.org/jira/secure/attachment/13051777/image-2022-11-04-11-05-18-627.png","04/Nov/22 03:13;penney;image-2022-11-04-11-13-48-282.png;https://issues.apache.org/jira/secure/attachment/13051779/image-2022-11-04-11-13-48-282.png","04/Nov/22 03:33;penney;image-2022-11-04-11-33-48-535.png;https://issues.apache.org/jira/secure/attachment/13051780/image-2022-11-04-11-33-48-535.png","04/Nov/22 03:34;penney;image-2022-11-04-11-34-11-994.png;https://issues.apache.org/jira/secure/attachment/13051781/image-2022-11-04-11-34-11-994.png","04/Nov/22 06:22;penney;image-2022-11-04-14-22-36-466.png;https://issues.apache.org/jira/secure/attachment/13051791/image-2022-11-04-14-22-36-466.png",,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 06:33:46 UTC 2022,,,,,,,,,,"0|z1avvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 03:24;Weijie Guo;[~penney] Could you show a complete error stack?;;;","04/Nov/22 03:34;penney;!image-2022-11-04-11-34-11-994.png!;;;","04/Nov/22 03:37;penney;And i found a strange question，If one of the three tables is empty, an error will be reported. If the table data is not empty, it seems that an error will not be reported;;;","04/Nov/22 04:32;luoyuxia;I'm not clear about your problem. Could you please expain a little bit more?

What's you sql/code and exception stacktrace?;;;","04/Nov/22 06:22;penney;It's TableA union all TableB union all TableC.The data volume of the three tables is not large. They are all about hundreds. I used the hive connector to read the three tables. The sqlmode is default.When it's running and report a error:Failed to fetch next result.Then I suspected that it was a memory problem, so I set the related memory to a large size,But it doesn't work.Thanks
!image-2022-11-04-14-22-36-466.png!;;;","04/Nov/22 06:33;leonard;I don't see a reasonable opinion that we should mark this issue as a blocker, I change the priority to major now.;;;",,,,,,,,,,,,,,,,,
Explicitly throw exception from Table Store sink when unaligned checkpoint is enabled,FLINK-29876,13495169,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,04/Nov/22 02:53,24/Nov/22 11:11,04/Jun/24 20:41,08/Nov/22 09:43,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,Currently table store sink does not support unaligned checkpoint but no exception is explicitly thrown. We should throw exception so that users can change their configurations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 11:11:08 UTC 2022,,,,,,,,,,"0|z1avv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 09:43;TsReaper;master: 9b5dc0cee57d2bad0d3f884202d00f7891e8540e
release-0.2: 2af8c0a4603807a1a2d21cfcefb1a6c6dee07a7f;;;","24/Nov/22 11:11;Gerrrr;Hey [~TsReaper] ,

Could you please explain why FTS does not support unaligned checkpoints and what is missing to add the support?

Thank you,
Alex

 ;;;",,,,,,,,,,,,,,,,,,,,,
Can not find JobConf when using Hive Connector ,FLINK-29875,13495165,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,04/Nov/22 02:19,04/Nov/22 03:31,04/Jun/24 20:41,,1.16.1,1.17.0,,,,,,,,,Connectors / Hive,,,,0,,,,,"
{code:java}

org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator Source: src[1] -> Sink: Collect table sink
  at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:229)
  at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:901)
  at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:891)
  at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:848)
  at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:830)
  at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:203)
  at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:156)
  at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:361)
  at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:206)
  at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134)
  at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
  at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
  at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:369)
  at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:346)
  at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
  at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
  at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
  ... 4 more
Caused by: java.lang.NoClassDefFoundError: Lorg/apache/hadoop/mapred/JobConf;
  at java.lang.Class.getDeclaredFields0(Native Method)
  at java.lang.Class.privateGetDeclaredFields(Class.java:2583)
  at java.lang.Class.getDeclaredField(Class.java:2068)
  at java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1871)
  at java.io.ObjectStreamClass.access$700(ObjectStreamClass.java:79)
  at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:506)
  at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:494)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:494)
  at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:391)
  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:681)
  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1941)
  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1807)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2098)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1624)
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2343)
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2267)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2125)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1624)
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2343)
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2267)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2125)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1624)
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2343)
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2267)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2125)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1624)
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:464)
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
  at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)
  at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)
  at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)
  at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)
  at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:488)
  at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286)
  at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:223)
  ... 20 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.JobConf
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  ... 56 more (state=,code=0)
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 03:31:26 UTC 2022,,,,,,,,,,"0|z1avu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 03:31;luoyuxia;I think it's expected. When use yarn to start cluster, we need to add the classpath of  mapreduce class to the classpath in yarn-site.xml .  

But it'll be better to hava a doc for it.;;;",,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink end-to-end test failed on azure due to ClassNotFoundException,FLINK-29874,13495163,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,leonard,leonard,04/Nov/22 01:57,04/Nov/22 14:37,04/Jun/24 20:41,04/Nov/22 14:37,,,,,,,,,,,BuildSystem / Shaded,Connectors / FileSystem,Tests,,0,,,,,"The failure should be related to the flink shaded netty dependency.
{code:java}
2022-11-03 12:45:04,240 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2022-11-03 12:45:04,283 INFO  org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStore [] - Initializing FileExecutionGraphInfoStore: Storage directory /tmp/executionGraphStore-461fd7ed-fbc0-4890-ae27-858af6391ea7, expiration time 3600000, maximum cache size 52428800 bytes.
2022-11-03 12:45:04,379 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:288)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:293)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
	at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:59)
Caused by: org.apache.flink.util.ConfigurationException: Failed to initialize SSLEngineFactory for REST server endpoint.
	at org.apache.flink.runtime.rest.RestServerEndpointConfiguration.fromConfiguration(RestServerEndpointConfiguration.java:161)
	at org.apache.flink.runtime.rest.RestServerEndpoint.<init>(RestServerEndpoint.java:118)
	at org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.<init>(WebMonitorEndpoint.java:225)
	at org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.<init>(DispatcherRestEndpoint.java:68)
	at org.apache.flink.runtime.rest.SessionRestEndpointFactory.createRestEndpoint(SessionRestEndpointFactory.java:62)
	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:166)
	... 6 more
Caused by: org.apache.flink.configuration.IllegalConfigurationException: openSSL not available
	at org.apache.flink.runtime.net.SSLUtils.getSSLProvider(SSLUtils.java:188)
	at org.apache.flink.runtime.net.SSLUtils.createRestNettySSLContext(SSLUtils.java:367)
	at org.apache.flink.runtime.net.SSLUtils.createRestServerSSLEngineFactory(SSLUtils.java:142)
	at org.apache.flink.runtime.rest.RestServerEndpointConfiguration.fromConfiguration(RestServerEndpointConfiguration.java:159)
	... 11 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.netty4.io.netty.internal.tcnative.SSLContext
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:136)
	at org.apache.flink.runtime.net.SSLUtils.getSSLProvider(SSLUtils.java:184)
	... 14 more

{code}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42785&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29889,,,,,FLINK-29862,FLINK-29886,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:20:06 UTC 2022,,,,,,,,,,"0|z1avts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 06:26;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42811&view=results;;;","04/Nov/22 06:27;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42808&view=results;;;","04/Nov/22 06:27;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42803&view=results;;;","04/Nov/22 09:20;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42819&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;",,,,,,,,,,,,,,,,,,,
"CannotPlanException raised after ""CROSS JOIN UNNEST""",FLINK-29873,13495157,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dy79,dy79,03/Nov/22 23:35,11/Nov/22 08:30,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"When I create a VIEW as a result of ""CROSS JOIN UNNEST"" and then use the condition in the WHERE clause of the VIEW, it throws an exception ""org.apache.calcite.plan.RelOptPlanner$CannotPlanException"".

Why am I getting this exception and how should I handle it the right way?

The following is the test code in which an error occurs.
{code:java}
it should ""filter with object_key"" in {
    tEnv.executeSql(
      s""""""CREATE TABLE s3_put_event (
         |  Records ARRAY<
         |    ROW<
         |      s3 ROW<
         |        bucket ROW<name STRING>,
         |        object ROW<key STRING, size BIGINT>
         |      >
         |    >
         |  >
         |) WITH (
         |  'connector' = 'datagen',
         |  'number-of-rows' = '3',
         |  'rows-per-second' = '1',
         |  'fields.Records.element.s3.bucket.name.length' = '8',
         |  'fields.Records.element.s3.object.key.length' = '15',
         |  'fields.Records.element.s3.object.size.min' = '1',
         |  'fields.Records.element.s3.object.size.max' = '1000'
         |)
         |"""""".stripMargin
    )

    tEnv.executeSql(
      s""""""CREATE TEMPORARY VIEW s3_objects AS
         |SELECT object_key, bucket_name
         |FROM (
         |  SELECT
         |    r.s3.bucket.name AS bucket_name,
         |    r.s3.object.key AS object_key,
         |    r.s3.object.size AS object_size
         |  FROM s3_put_event
         |  CROSS JOIN UNNEST(s3_put_event.Records) AS r(s3)
         |) rs
         |WHERE object_size > 0
         |"""""".stripMargin
    )

    tEnv.executeSql(
      s""""""CREATE TEMPORARY VIEW filtered_s3_objects AS
         |SELECT bucket_name, object_key
         |FROM s3_objects
         |WHERE object_key > ''
         |"""""".stripMargin)

    val result = tEnv.sqlQuery(""SELECT * FROM filtered_s3_objects"")
    tEnv.toChangelogStream(result).print()
    env.execute()
  } {code}
If I remove the condition object_key > '' in the ""filtered_s3_objects"" VIEW, and do it in the ""s3_objects"" VIEW, no exception is thrown.However, my actual query is complicated, so it is not easy to move the condition of the WHERE clause like this. It's hard to use especially if I need to separate the output stream.

I have attached the error log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/22 23:33;dy79;error-log.txt;https://issues.apache.org/jira/secure/attachment/13051763/error-log.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Tue Nov 08 04:30:08 UTC 2022,,,,,,,,,,"0|z1avsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 01:48;lsy;CC [~godfreyhe] ;;;","04/Nov/22 02:18;luoyuxia;I run the sql statements you post in the master branch, it won't throw any exception. The plan can be generated normally:
{code:java}
LogicalSink(table=[*anonymous_datastream_sink$1*], fields=[bucket_name, object_key])
  LogicalProject(bucket_name=[$1], object_key=[$0])
    LogicalFilter(condition=[>($0, _UTF-16LE'')])
      LogicalProject(object_key=[$1], bucket_name=[$0])
        LogicalFilter(condition=[>($2, 0)])
          LogicalProject(bucket_name=[$1.bucket.name], object_key=[$1.object.key], object_size=[$1.object.size])
            LogicalCorrelate(correlation=[$cor2], joinType=[inner], requiredColumns=[{0}])
              LogicalTableScan(table=[[default_catalog, default_database, s3_put_event]])
              Uncollect
                LogicalProject(Records=[$cor2.Records])
                  LogicalValues(tuples=[[{ 0 }]])StreamPhysicalSink(table=[*anonymous_datastream_sink$1*], fields=[bucket_name, object_key])
  StreamPhysicalCalc(select=[s3.bucket.name AS bucket_name, s3.object.key AS object_key])
    StreamPhysicalCorrelate(invocation=[$UNNEST_ROWS$1($cor2.Records)], correlate=[table($UNNEST_ROWS$1($cor2.Records))], select=[Records,s3], rowType=[RecordType(RecordType:peek_no_expand(RecordType:peek_no_expand(RecordType:peek_no_expand(VARCHAR(2147483647) name) bucket, RecordType:peek_no_expand(VARCHAR(2147483647) key, BIGINT size) object) s3) ARRAY Records, RecordType:peek_no_expand(RecordType:peek_no_expand(VARCHAR(2147483647) name) bucket, RecordType:peek_no_expand(VARCHAR(2147483647) key, BIGINT size) object) s3)], joinType=[INNER], condition=[AND(>($0.object.size, 0), >($0.object.key, _UTF-16LE''))])
      StreamPhysicalTableSourceScan(table=[[default_catalog, default_database, s3_put_event]], fields=[Records]) {code};;;","08/Nov/22 04:30;dy79;If it works well on the master branch, does it mean that the 1.15.2 version doesn't also throw an error?;;;",,,,,,,,,,,,,,,,,,,,
JobDetailsInfo Rest endpoint breaking change in 1.16,FLINK-29872,13494746,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gyfora,gyfora,03/Nov/22 16:32,04/Nov/22 08:36,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Runtime / REST,,,,0,,,,,"Flink 1.16 introduces a breaking change to the JobDetailsInfo endpoints by adding 3 required fields to: [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/rest/messages/job/metrics/IOMetricsInfo.java]

accumulatedBackpressured, accumulatedIdle, accumulatedBusy fields should not be primitive (which makes them required) but instead nullable.

This would allow the 1.16 restclient to read the jobdetails of previous cluster versions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 08:36:09 UTC 2022,,,,,,,,,,"0|z1at94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 20:25;chesnay;That's not a compatibility direction that we support in the first place though. We only make sure that a client can talk to future versions of Flink, not previous versions.

In particular the REST client has problems with compatibility anyway since it can submit job graphs that are subject to java serialization (breaking any compatibility guarantees anyway).;;;","03/Nov/22 20:39;gyfora;With components such as the Kubernetes Operator these compatibilty questions become more important. The current operator needs to talk to multiple Flink versions at the same time to perform basic cluster operations (ideally using the latest supported Flink client)

It is trivial to fix these incompatibilities on Flink rest side and that allows us to use the operator without having to add copies of fixed classes to shadow what comes from flink.

We simply should not introduce required fields in the rest api. It's easy not to do it with some attention and I don't see a reason why we would anyway.;;;","03/Nov/22 21:18;chesnay;Then frankly you should use the client from the oldest supported version, generate a client from the OpenAPI spec for the oldest supported version, or generate different clients for each Flink version from the OpenAPI spec.

??We simply should not introduce required fields in the rest api.??

Of course we should. Treating every single response field as nullable from now until the end of time is terrible. I'm not looking forward to having to null-check every little thing.;;;","03/Nov/22 21:27;gyfora;You are right we could do many complicated things that might solve a problem that is introduced by non backward compatible clients. It might not be a requirement at the moment that the Rest Client is backward compatible but maybe it should be. We have to understand the costs and the benefits. We can't simply look at the costs.

I understand that you have a different personal opinion, in that case the easiest is to discuss in the community because I disagree. If we decide to add backward compatibilty we might need some null checks or mechanism to set defaults , but what we get in return can be very valuable for certain users.;;;","04/Nov/22 00:28;chesnay;Can you expand on why you consider generating a client to be complicated?
Generating one for each Flink version, sure; but generating on for the 1.15 spec and using that going forward shouldn't be hard?
You'd also further decouple the operator from the actual Flink codebase (and certain modules/dependencies?), which should be a long-term goal anyway.
(additionally you wouldn't have to wait for a Flink release to benefit from changes to the spec)

You're probably gonna argue that you'd like to use 1.16+ features of the REST API; however you could probably even use a client generated from the 1.16 spec. Because we aren't actually marking fields as required in there a generated client _should_ treat everything as optional, exactly as you propose.

I really don't get what the big deal is supposed to be.
;;;","04/Nov/22 05:47;gyfora;Our goal is not to generate new clients if they are already available in Flink. Best would be to simply use it as we already use many other utilities.

You actually provided an excellent argument why we should make the RestClient backward compatible. As you said a simple generated client based on the latest spec would probably work. In that case I believe we should improve the RestClient so that it comes with this nice property also. Why would we design a non backward compatible RestClient if the spec itself is backward compatible?;;;","04/Nov/22 08:36;chesnay;??Why would we design a non backward compatible RestClient if the spec itself is backward compatible???

a) Remember that the spec came after the current rest client was created.
b) As I said before, the client within Flink can inherently not be fully compatible with all Flink versions because of the job submission.

That the fields aren't marked as required wasn't actually intentional and is more a limitation of the current spec.
Even if that were changed though I wouldn't oppose having a second variant that is less strict, i.e., is similar to the current spec.

Having fields being marked as required simplifies things on both ends; users know up front what they have to provide and what they get back, and on our side we don't need to have every handler check 20 preconditions for each request; instead this is handled generically by the rest framework.;;;",,,,,,,,,,,,,,,,
Upgrade operator Flink version and examples to 1.16,FLINK-29871,13494640,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rodmeneses,gyfora,gyfora,03/Nov/22 15:12,31/Jan/23 19:49,04/Jun/24 20:41,31/Jan/23 19:49,,,,,,,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,We should update our Flink dependency and the default example version to 1.16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 19:49:50 UTC 2023,,,,,,,,,,"0|z1aslk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 16:33;thw;X.Y.0 releases are not vetted and prone to surprises. Stable environments typically move to a X.Y.1 or later Flink version. Can we make this so that we support 1.16.x but make it the default for operator only once 1.16.1 comes out?;;;","04/Nov/22 16:36;gyfora;Thats a very good point [~thw] .

Let's do this in 2 steps then:
 1. Now we simply add E2E tests to verify 1.16 support
 2. Once 1.16.1 is out we update the client;;;","04/Nov/22 17:43;thw;+1;;;","07/Nov/22 03:54;wangyang0918;+1 for using the X.Y.1 in operator for stability.;;;","31/Jan/23 16:23;_anton;[https://flink.apache.org/news/2023/01/30/release-1.16.1.html]

1.16.1 is now out!;;;","31/Jan/23 19:49;morhidi;Fixed via aab14232721d49fe985ffaca5a8d175cd0f156df merged to main;;;",,,,,,,,,,,,,,,,,
split ResourceNotEnoughNotifier from ResourceActions,FLINK-29870,13494584,13352415,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,03/Nov/22 12:33,05/Jan/23 01:54,04/Jun/24 20:41,06/Dec/22 11:24,1.16.0,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"SlotManager determines whether a resource can be allocated by the return value of ResourceActions.allocateResource. It can reduce unnecessary waiting by quickly marking the slotRequest as a failure if the resource cannot be allocated.

The new declareResourceNeed function will not return whether resources could be allocated. This will cause useless waiting for SlotRequests.

Currently, ResourceActions does only two kinds of things.

allocate/release resource by ResourceManager, this only supported by ActiveResourceManager
notify resource not enough to JobMaster

So we can split ResourceNotEnoughNotifier from ResourceActions, and make ResourceActions only works for ActiveResourceManager

ResourceNotEnoughNotifier only send notifyNotEnoughResourcesAvailable to JobMaster
ResourceActions will deal with declareResourcesNeeded
    For StandaloneResourceManager, The SlotManager will not have one ResourceAllocator, so it can fail the SlotRequests quickly.",,,,,,,,,,,FLINK-29869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 11:24:29 UTC 2022,,,,,,,,,,"0|z1as94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 11:24;xtsong;master (1.17): 209df810f13e2fcbe5a4ca8bb015a8a5f662adc4;;;",,,,,,,,,,,,,,,,,,,,,,
make ResourceActions declarative ,FLINK-29869,13494583,13352415,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,03/Nov/22 12:33,05/Jan/23 01:54,04/Jun/24 20:41,26/Dec/22 10:17,1.16.0,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"As https://issues.apache.org/jira/browse/FLINK-18229 talked, we need make ResourceAction declarative in order to reduce the coupling between the SlotManager and the ResourceManager",,,,,,,,,,FLINK-29870,FLINK-18229,,,,,,,,,,FLINK-17372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 26 10:17:56 UTC 2022,,,,,,,,,,"0|z1as8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/22 10:17;xtsong;master (1.17): fc0cdeeaffb2ccdab720a8af68e337060668193a;;;",,,,,,,,,,,,,,,,,,,,,,
Dependency convergence error for org.osgi:org.osgi.core:jar,FLINK-29868,13494580,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,nkruber,nkruber,03/Nov/22 12:22,04/Nov/22 09:09,04/Jun/24 20:41,04/Nov/22 09:09,1.17.0,,,,,,1.17.0,,,,Build System,Table SQL / Runtime,,,0,,,,,"While working on FLINK-29867, the following new error is popping up while running

{code}
./mvnw clean install -pl flink-dist -am -DskipTests -Dflink.convergence.phase=install -Pcheck-convergence
{code}

(this is also done by CI which therefore fails)

{code}
[WARNING] 
Dependency convergence error for org.osgi:org.osgi.core:jar:4.3.0:runtime paths to dependency are:
+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT
  +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime
    +-org.apache.flink:flink-table-api-java-bridge:jar:1.17-SNAPSHOT:runtime
      +-org.apache.flink:flink-streaming-java:jar:1.17-SNAPSHOT:runtime
        +-org.apache.flink:flink-runtime:jar:1.17-SNAPSHOT:runtime
          +-org.xerial.snappy:snappy-java:jar:1.1.8.3:runtime
            +-org.osgi:org.osgi.core:jar:4.3.0:runtime
and
+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT
  +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime
    +-org.apache.flink:flink-scala_2.12:jar:1.17-SNAPSHOT:runtime
      +-org.apache.flink:flink-core:jar:1.17-SNAPSHOT:runtime
        +-org.apache.commons:commons-compress:jar:1.21:runtime
          +-org.osgi:org.osgi.core:jar:6.0.0:runtime
{code}",,,,,,,,,,,,,,FLINK-29867,,,,,,,,,,,,,,,,,,,MENFORCER-437,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:09:43 UTC 2022,,,,,,,,,,"0|z1as88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 12:45;chesnay;The strange thing is that osgi.core is listed as a runtime dependency, when it is actually provided.;;;","04/Nov/22 09:09;nkruber;fixed in {{master}} via [614fc2a5fd301789f7daa1282a5b500bf8d67d4b|https://github.com/apache/flink/commit/614fc2a5fd301789f7daa1282a5b500bf8d67d4b];;;",,,,,,,,,,,,,,,,,,,,,
Update maven-enforcer-plugin to 3.1.0,FLINK-29867,13494570,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,03/Nov/22 12:02,04/Nov/22 09:10,04/Jun/24 20:41,04/Nov/22 09:10,,,,,,,1.17.0,,,,Build System,,,,0,pull-request-available,,,,We currently rely on 3.0.0-M1 but will have to skip 3.0.0 (final) due to MENFORCER-394 which hits Flink's current code base as well,,,,,,,,,,,,FLINK-29868,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:10:26 UTC 2022,,,,,,,,,,"0|z1as60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:10;nkruber;fixed in {{master}} via [d0c6075be47e3d277a418d9fdefe513eeeb0c0f2|https://github.com/apache/flink/commit/d0c6075be47e3d277a418d9fdefe513eeeb0c0f2];;;",,,,,,,,,,,,,,,,,,,,,,
Split flink-dist,FLINK-29866,13494561,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,03/Nov/22 11:46,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Build System,,,,0,pull-request-available,stale-assigned,,,"Users currently have to setup dependencies to various Flink dependencies (APIs like streaming-java, formats, connectors etc), and have to set the scope for each of these correctly w.r.t. what is actually provided by the distribution.
This is unnecessarily tedious and error-prone, made worse by some special-cases like connector-files or the json/csv formats being bundled.

I propose to create a new module {{flink-dist-pub}} (name pending!) that acts as a sort-of BOM for what is in the distribution (specifically the flink-dist jar) that is relevant for common DataStream application development.

As follow-ups we could consider creating a table API extension of this module, and publish fat jars for the use-case of [~thw] in FLINK-29497.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30372,,,,,,,FLINK-30578,,,FLINK-29497,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:09 UTC 2023,,,,,,,,,,"0|z1as40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 12:04;chesnay;Note that this would also solve the ""flink-connector-base"" problem, where we don't really want connectors to bundle it, but it's currently required for a good user-experience (as otherwise it will not work during testing without adding an additional dependency).;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
Allow configuring the JDK in build-nightly-dist.yml,FLINK-29865,13494556,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,03/Nov/22 10:50,04/Nov/22 09:12,04/Jun/24 20:41,04/Nov/22 09:12,,,,,,,1.17.0,,,,Build System / Azure Pipelines,,,,0,pull-request-available,,,,"{{build-nightly-dist.yml}} currently uses the default JDK from https://github.com/flink-ci/flink-ci-docker which happens to be Java 1.8 that we use for releases. We should
# not rely on this default being set to 1.8 and
# be able to configure this in the workflows themselves",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:12:37 UTC 2022,,,,,,,,,,"0|z1as2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:12;nkruber;Fixed on {{master}} via [2306e448bfa8ca78e2bf13e34d2838939bce31e1|https://github.com/apache/flink/commit/2306e448bfa8ca78e2bf13e34d2838939bce31e1];;;",,,,,,,,,,,,,,,,,,,,,,
AvroDeserializationSchema.forSpecific still generates genericdata.record,FLINK-29864,13494555,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,lintermans,lintermans,03/Nov/22 10:30,03/Nov/22 12:50,04/Jun/24 20:41,03/Nov/22 12:50,1.15.2,,,,,,,,,,API / DataStream,,,,0,,,,,"I've generated AVRO classes based on AVRO specs.

But whatever I do, the AvroDeserializationSchema.forSpecific still generates GenericRecord objects instead of the desired destination classes.

I've also tried the RegistryAvroDeserializationSchema but the same issue pops up.
{code:java}
RegistryAvroDeserializationSchema<InputDataPoints> registryAvroDeserializationSchema = new RegistryAvroDeserializationSchema<>(
  InputDataPoints.class,
  null,
  () -> new SchemaCoder() {
          @Override
          public Schema readSchema(InputStream inputStream) throws IOException {
            return InputDataPoints.getClassSchema();
          }

          @Override
          public void writeSchema(Schema schema, OutputStream outputStream) throws IOException {
          }
});
 
{code}
 

When debugging, I notice SpecificDatumReader is being generated instead of GenericDatumReader, but still it produces the wrong types.

AvroSerializationSchema.forSpecific works fine and generates a correct byte array based on the AVRO classes objects.

Any suggestion how I can force the specific usage?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 03 12:50:05 UTC 2022,,,,,,,,,,"0|z1as2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 12:50;lintermans;Issue with namespaces,

it was trying to resolve the class based on namespace defined in Avro schema, but the namespace was not correct.;;;",,,,,,,,,,,,,,,,,,,,,,
Properly handle NaN/Infinity in OpenAPI spec,FLINK-29863,13494540,13540585,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,03/Nov/22 09:28,11/Mar/24 12:44,04/Jun/24 20:41,,1.15.0,,,,,,1.20.0,,,,Documentation,Runtime / REST,,,0,2.0-related,,,,"Our OpenAPI spec maps all float/double fields to float64, but we at times also return NaN/infinity which can't be represented as such since the JSON spec doesn't support it.

One alternative could be to document it as an either type, returning either a float64 or a string.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 14:29:28 UTC 2023,,,,,,,,,,"0|z1arzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 15:09;austince;What might be the issue of fixing the API to not return NaN/infinity instead?

 

> Numeric values that cannot be represented as sequences of digits (such as Infinity and NaN) are not permitted.

 

https://www.ecma-international.org/publications-and-standards/standards/ecma-404/;;;","13/Dec/22 17:55;chesnay;It may be difficult for us to guarantee that we never return NaN/infinity anywhere.;;;","13/Dec/22 18:57;austince;Because we're using Jackson to serialize and they support these number formats? If that's the case ( :( ), they seem to have been talking about this for years with no real progress: [https://github.com/FasterXML/jackson-databind/issues/911]

There's a couple of ideas in that issue for ways to correctly serialize NaN and infinities as `null`, but I can see that it would be hard to write test cases to enforce this throughout Flink's Rest API.

One thought might be to do not try to strictly enforce it, but treat all found occurrences as bugs — it would be nice if the JSON could be valid, as working with multi-type returns even if documented in OpenAPI is a bit painful. Wdyt?;;;","18/Jan/23 14:29;chesnay;We could add a custom serializer for all doubles that converts NaN/infinity to null. We can register this centrally so it applies to the entire REST API.;;;",,,,,,,,,,,,,,,,,,,
Upgrade to flink-shaded 16.1,FLINK-29862,13494522,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,03/Nov/22 08:24,30/Nov/22 11:39,04/Jun/24 20:41,30/Nov/22 11:39,,,,,,,1.17.0,,,,Build System,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29874,FLINK-29886,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 30 11:39:24 UTC 2022,,,,,,,,,,"0|z1arvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 11:52;chesnay;master: 9e1ff648f18286628649a83aeaead6cbdca85259;;;","04/Nov/22 10:06;chesnay;Reverted in cf3158b21bf16f998125dbd370509352334f9e43 due to strange SSL issues.;;;","30/Nov/22 11:36;chesnay;Benchmarks are still failing with 16.1.;;;","30/Nov/22 11:37;chesnay;FFS the benchmarks were just using the wrong flink-shaded _and_ tcnative version. ;;;","30/Nov/22 11:39;chesnay;master: 026a53e1b86e3ae2797fdc291c161ddb7e5e41b0;;;",,,,,,,,,,,,,,,,,,
Optimize logic of enumerateSplits in HiveSource ,FLINK-29861,13494518,13444738,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,03/Nov/22 07:56,03/Nov/22 08:03,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Hive,,,,0,,,,,"Currently, Hive source will call ` InputFormat#getSplits(JobConf conf, int minNumSplits)` to enumerate splits. The minNumSplits passed is the source's parallelism. 

For text format, the splits may be too many and each split contains much less data which is time costly for reader to get the split and do the reading.

We may need to revisit the logic for enumerating splits.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-03 07:56:13.0,,,,,,,,,,"0|z1arug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector bug when using Hybrid.Builder,FLINK-29860,13494512,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,noelo,noelo,noelo,03/Nov/22 07:15,10/Nov/22 11:28,04/Jun/24 20:41,10/Nov/22 11:28,,,,,,,1.17.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,"When using a HybridSource with a set of pulsar sources submitting a job to a flink cluster results in the following error

------------------------------------------------------------
 The program finished with the following exception:

The implementation of the BlockElement is not serializable. The object probably contains or references non serializable fields.
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:164)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:69)
    org.apache.flink.connector.base.source.hybrid.HybridSource$HybridSourceBuilder.addSource(HybridSource.java:246)
    org.apache.flink.connector.base.source.hybrid.HybridSource$HybridSourceBuilder.addSource(HybridSource.java:233)
    org.apache.flink.connector.base.source.hybrid.HybridSource.builder(HybridSource.java:104)

 

I think this is related to https://issues.apache.org/jira/browse/FLINK-25444

From a pulsar connector perspective it's simple fixed, just mark the ""private final InlineElement desc"" attribute in flink-connectors/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/source/config/CursorVerification.java as transient to avoid the serialisation process.

 

I've tested this and it seems to solve the issue. I can submit a PR with this fix.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 11:28:20 UTC 2022,,,,,,,,,,"0|z1art4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 01:34;syhily;I think we can just remove the extra document field for {{CursorVerification}}.;;;","08/Nov/22 07:01;tison;[~syhily] [~noelo] I'd prefer the solution by setting {{InlineElement}} as {{transient}}.

[~noelo] if you're still willing to submit a patch, please submit one and I'll assign to you.;;;","08/Nov/22 07:46;noelo;[~syhily] yep I do have a patch so assign me. I assume this is just for 1.17 snapshot and not other versions ?;;;","08/Nov/22 21:19;syhily;OK. I'll close my PR and in favor of the Noel's one. [~noelo] Can you help me add the {{transient}} field for {{TopicRoutingMode}} and {{MessageKeyHash}} in your PR?;;;","09/Nov/22 07:09;noelo;[~syhily] yep I'll do that today;;;","09/Nov/22 08:45;noelo;[~syhily] PR updated;;;","10/Nov/22 11:28;tison;master via fa6d18818348b052876ed5db16ecc6d5f5bad30c;;;",,,,,,,,,,,,,,,,
TPC-DS end-to-end test with adaptive batch scheduler failed due to oo non-empty .out files.,FLINK-29859,13494506,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,leonard,leonard,03/Nov/22 06:30,28/Feb/23 12:05,04/Jun/24 20:41,28/Feb/23 12:05,1.16.0,1.17.0,,,,,1.16.2,1.17.0,,,Tests,,,,0,pull-request-available,test-stability,,,"
Nov 03 02:02:12 [FAIL] 'TPC-DS end-to-end test with adaptive batch scheduler' failed after 21 minutes and 44 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files 


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 12:05:30 UTC 2023,,,,,,,,,,"0|z1arrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 03:33;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43077&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","05/Dec/22 10:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43707&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=9816;;;","13/Dec/22 07:20;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43904&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912;;;","13/Dec/22 08:26;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43910&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=9557;;;","19/Dec/22 09:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44025&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=11938;;;","20/Dec/22 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44084&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=9253;;;","23/Dec/22 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44184&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=7938;;;","23/Dec/22 07:52;mapohl;[~wanglijie] may you have a look at this issue?;;;","23/Dec/22 08:01;wanglijie;[~mapohl] I will take a look soon.;;;","28/Dec/22 07:14;wanglijie;Fixed via

master 14a61f368332320d7e38cc93a04f95bb63c66788

release-1.16 9e51cb8c117fd9a5f887e0a0e8faee4ff11462ea;;;","05/Jan/23 07:38;mapohl;I'm reopening this issue because the error reappeared even though it contained the fix of 9e51cb8c117fd9a5f887e0a0e8faee4ff11462ea in the {{release-1.16}} branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=8611

[~wanglijie] may you have a look?;;;","05/Jan/23 15:25;wanglijie;[~mapohl] I 'll take a look.;;;","19/Jan/23 13:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=6279;;;","19/Jan/23 13:38;mapohl;[~wanglijie] any updates on that one?;;;","25/Jan/23 08:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=8267;;;","30/Jan/23 14:06;mapohl;[~wanglijie] Have you had the chance to take a look at it?;;;","30/Jan/23 14:16;wanglijie;[~mapohl]  Sorry for the late reply, I'll take a look and prepare a fix this week.;;;","02/Feb/23 03:03;zhuzh;Fixed via
master:
19f2230a4758f471305727ad82d36984ebbc7e3a

release-1.16:
e6a91aa39fc425412a1a3ab32c2c479a970ae439;;;","22/Feb/23 09:55;mapohl;I'm reopening this issue again because of the following test failure in release-1.16:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46384&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=9287

There are exceptions caught in the TaskExecutor like:
{code}
2023-02-22 03:42:04,372 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Cannot find task to fail for execution d77a85f5ec62862896b264410c77e177_406c896546ef6157dd2c29ccb928b2d0_6_1 with exception:
org.apache.flink.runtime.jobmaster.ExecutionGraphException: The execution attempt d77a85f5ec62862896b264410c77e177_406c896546ef6157dd2c29ccb928b2d0_6_1 was not found.
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:483) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_362]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_362]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_362]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_362]
{code}

Other previously reported test instabilities contained this error as well. Therefore, it looks like it's not fixed, yet. [~wanglijie] may you have another look at it?;;;","22/Feb/23 15:00;wanglijie;[~mapohl] I 'll have a look.;;;","27/Feb/23 10:15;mapohl;1.16: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46543&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=9218];;;","28/Feb/23 12:05;zhuzh;master:
28e75620db7b0794ff1c19f8928ce7d33516ba64

release-1.17:
3de6b337cceda5b4b81ac8d34e099e3f1c802df9

release-1.16:
5b9398ae067a7073cadca25e883b278f6e7cd5bd;;;",
Jdbc reading supports setting multiple queryTemplates,FLINK-29858,13494499,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,03/Nov/22 05:50,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,1.20.0,,,,Connectors / JDBC,,,,0,jdbc_connector,,,,"Jdbc reading supports setting multiple queryTemplates. Currently, jdbc reading only supports reading one query template. Sometimes it is not enough. The queryTemplate in the JdbcRowDataInputFormat. Sometimes you may need to select * from table where col1>=? and col1 < ?  And select * from table where col1>=? and col1 <= ?  Both templates should be used

!image-2022-11-03-13-53-12-593.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/22 05:53;waywtdcc;image-2022-11-03-13-53-12-593.png;https://issues.apache.org/jira/secure/attachment/13051736/image-2022-11-03-13-53-12-593.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-03 05:50:33.0,,,,,,,,,,"0|z1arq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failure to connect to 'HiveServer2Endpoint' when using hive3 beeline,FLINK-29857,13494489,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,yzl,yzl,03/Nov/22 03:12,10/Nov/22 03:46,04/Jun/24 20:41,10/Nov/22 03:44,1.16.0,,,,,,1.16.1,,,,Connectors / Hive,,,,0,pull-request-available,,,,"Hive3 add a new 'TGetInfoType' value 'CLI_ODBC_KEYWORDS', but 'HiveServer2Endpoint' doesn't handle this value, causing crush when connecting to Hive metastore through 'HiveServer2Endpoint' by Hive3 Beeline.",,,,,,,,,,,,,,,,,,,,,FLINK-29839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 03:44:07 UTC 2022,,,,,,,,,,"0|z1aro0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 03:44;fsk119;release-1.16: a74023501e3f48c6a0c61cab8b1d212b8ea83ee9
master: 194df8de947baf30995371d87decec2b7c470610;;;",,,,,,,,,,,,,,,,,,,,,,
Triggering savepoint does not trigger operator notifyCheckpointComplete,FLINK-29856,13494487,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,mason6345,mason6345,03/Nov/22 02:49,08/Feb/23 14:53,04/Jun/24 20:41,08/Feb/23 14:53,1.15.0,1.16.0,,,,,,,,,Runtime / Checkpointing,,,,0,,,,,"When I trigger a savepoint with the Flink K8s operator, I verified for two sources (KafkaSource and MultiClusterKafkaSource) do not invoke notifyCheckpointComplete. This is easily reproducible in a simple pipeline (e.g. KafkaSource -> print). In this case, the savepoint is complete and successful, which is verified by the Flink Checkpoint UI tab and the jobmanager logs. e.g. `
Triggering checkpoint 3 (type=SavepointType\{name='Savepoint', postCheckpointAction=NONE, formatType=CANONICAL})`
 

However, when the checkpoint occurs via the interval, I do see the sources checkpointing properly and expected logs in the output.

After the ticket was initially filed, I also checked with other stateful UDFs and observed the same behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25191,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 08:29:06 UTC 2022,,,,,,,,,,"0|z1arnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 02:12;masteryhx;Hi, I may miss something.

You mean that the savepoint is completed successfully, but it should not be successful according to your analysis about the code ?;;;","16/Nov/22 12:33;mxm;I think the symptom described here manifests as a successful savepoint which does not include the mentioned sources because they have not been checkpointed as part of the savepoint. [~mason6345] I'm assuming this would mean that the resulting savepoint is corrupted?;;;","18/Nov/22 07:03;mason6345;Yes, Savepoint is completed successfully from Flink UI, metrics, and jobmanager logs but should be unsuccessful since the operators did not finish checkpointing. Note I didn't see any operator failures during the Savepoint process.

It's not only with the source–I also confirmed that the stateful sink operator also doesn't call snapshotState/notifyCheckpointComplete.

I haven't checked the Savepoint contents and I didn't notice the affects of corruption (e.g. missing source splits in state) since another checkpoint finished before I shutdown the job. I will test again tomorrow.

 ;;;","24/Nov/22 08:31;gaoyunhaii;Hi [~mason6345]  may I have a double confirmation that how you verified that snapshotState/notifyCheckpointComplete is not called?;;;","24/Nov/22 10:54;mxm;I think this might be caused by FLINK-25191. It was a conscious design choice as also explained in [FLIP-193|https://cwiki.apache.org/confluence/display/FLINK/FLIP-193%3A+Snapshots+ownership#FLIP193:Snapshotsownership-SkippingSavepointsforRecovery] to skip side effects like notifyCheckpointComplete() for intermediate savepoints.

The reason is that savepoints can be drawn at any point in time by the user who also controls the lifespan of the savepoint data. During recovery, savepoints were used just like checkpoints which could lead to issues when the savepoint had already been deleted or an option was used to skip savepoints during recovery. In this case, the recovery would use an older checkpoint and potentially cause side effects more than once, i.e. calling notifyCheckpointComplete() again for already processed records/state.

The new behavior is to treat ""intermediate"" savepoints independently of checkpoints which means they won't be used by normal recovery and hence should not be allowed to cause side effects. Intermediate savepoints can still be used to restore a job but notifyCheckpointComplete() will only be called upon creating the first checkpoint. Note that ""final"" savepoints which lead to termination of the job are still allowed to cause side effects via notifyCheckpointComplete().;;;","24/Nov/22 18:41;mason6345;[~masteryhx] [~mxm] [~gaoyunhaii] I double checked that snapshotState **is** called, but notifyCheckpointComplete **is not**–this is observable from FLIP 27 sources and custom stateful UDFs. It looks like an intended design decision like Max mentioned.

> i.e. calling notifyCheckpointComplete() again for already processed records/state.

Yes, I noticed this odd side effect when I tried to fix this issue, but it could be alleviated by adding the check for if the job is restoring or not.

—

Without notifyCheckpointComplete, it isn't clear what the benefit of an intermediate Savepoint is (need to re-read the FLIP). From the user perspective, trigger intermediate Savepoint has the benefit to be able to commit data to external systems like Kafka/Iceberg on demand for their operational procedures. Perhaps, the eventual solution is to replace operational procedure with triggering checkpoint with https://issues.apache.org/jira/browse/FLINK-29634 since that would match the effects (notifyCheckpointComplete, etc).

—

For the purposes of being backward compatible, does it make sense to enable the notifyCheckpointComplete behavior in intermediate savepoints for 1.16 and remove it when https://issues.apache.org/jira/browse/FLINK-29634 is released in 1.17?;;;","24/Nov/22 21:14;mason6345;{quote}Without notifyCheckpointComplete, it isn't clear what the benefit of an intermediate Savepoint is (need to re-read the FLIP).
{quote}
Okay, in my re-reading, it makes sense that Savepoint is an option to take a snapshot of the Flink job where user has full control over the lifecycle management. However, the semantics of stop with Savepoint supporting notifyCheckpointComplete and intermediate Savepoint not supporting it does not fully make sense to me, since the stop with Savepoint still ""commits side effects"".;;;","25/Nov/22 16:09;mxm;I think the reason why Flink doesn't call notifyCheckpointComplete() anymore on ""intermediate"" savepoints is purely for recovery reasons where we want to ensure that we only call notifyCheckpointComplete() once. Arguably, this isn't really the case because we can fail directly after restoring from a checkpoint but at least we will then only commit the already committed data and not any older data, as would be the case when a savepoint had been committing the data before we fell back to an earlier checkpoint.

The question is, would it be sufficient for your use case if the next checkpoint committed the data? Do you need notifyCheckpointComplete() to run immediately after you take the ""intermediate"" savepoint?
{quote}However, the semantics of stop with Savepoint supporting notifyCheckpointComplete and intermediate Savepoint not supporting it does not fully make sense to me, since the stop with Savepoint still ""commits side effects"".
{quote}
It kind of makes sense because on a stop-with-savepoint, the job will cease to exist and we have to run notifyCheckpointComplete() because we might otherwise never execute it. However, in the case of ""intermediate"" savepoints, the next checkpoint will eventually run and commit any pending data since the last checkpoint (the savepoint data being a subset of this data).;;;","29/Nov/22 05:04;mason6345;[~dwysakowicz] [~yunta] [~pnowojski] (contributors involved w/ FLIP-193) any thoughts on the discussion above?;;;","29/Nov/22 08:29;pnowojski;Yes, this is the intended behavour.
{quote}
I think the reason why Flink doesn't call notifyCheckpointComplete() anymore on ""intermediate"" savepoints is purely for recovery reasons where we want to ensure that we only call notifyCheckpointComplete() once. Arguably, this isn't really the case because we can fail directly after restoring from a checkpoint but at least we will then only commit the already committed data and not any older data, as would be the case when a savepoint had been committing the data before we fell back to an earlier checkpoint.
{quote}
The real rationale behind the current behaviour is if you imagine this scenario:
# triggering and completing {{chk42}}
# triggering and completing savepoint 
# job failover

Job can not recover to the savepoint, because there are no guarantees that it still exists (savepoint are always owned by the user). It has to failover to the {{chk42}}. However if we had committed the savepoint, that would have meant recovery to {{chk42}} would create either duplicate results (if the job is deterministic) OR inconsistent result (if the job is non-deterministic). 

[~mason6345], think about the savepoints as either state backups of your job, or terminal states after stopping the job (also kind of backups that out live your jobs). Ideally, neither of those should contain any pointers to an external state (like Kafka transactions), but this is not yet implemented (FLINK-30070). As it is, during recovery from savepoints it's recommended to drop the external state via replacing transactional operators {{uids}} (https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/savepoints/#triggering-savepoints). 
{quote}
one wants to be safe in those scenarios, we advise dropping the state of transactional sinks, by changing sinks uids.
{quote}
Generally speaking recovery to savepoints is always tricky, as in many cases it can violate exactly-once guarantees. Users have to take that into account, or use only stop-with-savepoint, to make sure that the original job won't be able to make any progress while we are doing something with that savepoint. 

{quote}
Without notifyCheckpointComplete, it isn't clear what the benefit of an intermediate Savepoint is (need to re-read the FLIP). From the user perspective, trigger intermediate Savepoint has the benefit to be able to commit data to external systems like Kafka/Iceberg on demand for their operational procedures. Perhaps, the eventual solution is to replace operational procedure with triggering checkpoint with https://issues.apache.org/jira/browse/FLINK-29634 since that would match the effects (notifyCheckpointComplete, etc).
{quote}
[~mason6345], can you explain what's the actual problem that you are experiencing and why are you using intermediate savepoints? It sounds to me like you are using it not the way they were intended. On demand committing data to external systems sounds like a use case for FLINK-27101. ;;;",,,,,,,,,,,,,
UDF randomly processed input data twice ,FLINK-29855,13494463,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,yanxinyi,yanxinyi,02/Nov/22 21:36,01/Sep/23 18:23,04/Jun/24 20:41,,1.14.4,1.16.0,1.17.0,1.18.0,,,,,,,,,,,0,,,,,"Local flink cluster env: 

1 task manager and 1 task slot.

To reproduce the issue:
 # create a datagen table with a single column int type of id with 1 row per second.
 # create a UDF that only mod input data with logging statements.
 # create a print table that prints the results.
 # insert data into the print table with UDF(input id column) execution from the datagen table.

The logging shows that some of the data have been processed twice, which is not expected I guess? This will totally change the behavior of the UDF if the data has been processed twice. I also attached main and UDF classes, as well as the logging file for additional info.

 

DDL

 
{code:java}
public static void main(String[] args) throws Exception {
        EnvironmentSettings settings = EnvironmentSettings.newInstance().build();
        
        TableEnvironment tEnv = TableEnvironment.create(settings);
        
        tEnv.executeSql(""CREATE FUNCTION IntInputUdf AS 'org.apache.flink.playgrounds.spendreport.IntInputUdf'"");        tEnv.executeSql(""CREATE TABLE datagenTable (\n"" +
                ""    id  INT\n"" +
                "") WITH (\n"" +
                ""    'connector' = 'datagen',\n"" +
                ""    'number-of-rows' = '100',\n"" +
                ""    'rows-per-second' = '1'\n"" +
                "")"");        
tEnv.executeSql(""CREATE TABLE print_table (\n"" +
                ""    id_in_bytes  VARBINARY,\n"" +
                ""    id  INT\n"" +
                "") WITH (\n"" +
                ""    'connector' = 'print'\n"" +
                "")"");        
tEnv.executeSql(""INSERT INTO print_table SELECT * FROM ( SELECT IntInputUdf(`id`) AS `id_in_bytes`, `id` FROM datagenTable ) AS ET WHERE ET.`id_in_bytes` IS NOT NULL"");
    }  {code}
 

UDF

 
{code:java}
public @DataTypeHint(""Bytes"") byte[] eval(@DataTypeHint(""INT"") Integer intputNum) {
    byte[] results = intputNum.toString().getBytes(StandardCharsets.UTF_8);
    if (intputNum % 2 == 0) {
      LOG.info(""### ### input bytes {} and num {}.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### "", results, intputNum);
      return results;
    }
    LOG.info(""*** *** input bytes {} and num {}."", results, intputNum);
    return null;
  } {code}
output

 

 
{code:java}
2022-11-02 13:38:56,765 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 51, 50, 52, 56, 51, 54, 53, 48, 50] and num -1324836502.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:38:56,766 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 51, 50, 52, 56, 51, 54, 53, 48, 50] and num -1324836502.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:38:57,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [49, 48, 56, 53, 52, 53, 54, 53, 52, 50] and num 1085456542.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:38:57,763 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [49, 48, 56, 53, 52, 53, 54, 53, 52, 50] and num 1085456542.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:38:58,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [49, 53, 48, 54, 51, 49, 49, 57, 53, 52] and num 1506311954.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:38:58,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [49, 53, 48, 54, 51, 49, 49, 57, 53, 52] and num 1506311954.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:38:59,759 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [45, 49, 56, 48, 48, 54, 57, 48, 52, 51, 55] and num -1800690437.
2022-11-02 13:39:00,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [49, 52, 50, 56, 56, 55, 55, 52, 56, 51] and num 1428877483.
2022-11-02 13:39:01,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 55, 57, 52, 50, 54, 51, 54, 56, 54] and num -1794263686.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:39:01,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 55, 57, 52, 50, 54, 51, 54, 56, 54] and num -1794263686.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:39:02,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 49, 54, 54, 56, 57, 56, 53, 52, 50] and num -1166898542.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:39:02,762 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 49, 54, 54, 56, 57, 56, 53, 52, 50] and num -1166898542.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:39:03,758 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [45, 49, 54, 54, 51, 53, 49, 53, 55, 53, 51] and num -1663515753.
2022-11-02 13:39:04,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [45, 52, 53, 53, 51, 52, 52, 50, 57] and num -45534429.
2022-11-02 13:39:05,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [49, 50, 55, 48, 55, 50, 52, 52, 57] and num 127072449.
2022-11-02 13:39:06,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [45, 52, 53, 51, 55, 48, 53, 54, 48, 55] and num -453705607.
2022-11-02 13:39:07,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 48, 57, 53, 57, 48, 56, 51, 50, 54] and num -1095908326.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:39:07,763 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [45, 49, 48, 57, 53, 57, 48, 56, 51, 50, 54] and num -1095908326.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-02 13:39:08,760 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [45, 49, 54, 50, 55, 53, 57, 55, 52, 49, 55] and num -1627597417.
2022-11-02 13:39:09,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [53, 57, 54, 53, 50, 48, 53, 48, 49] and num 596520501.
2022-11-02 13:39:10,761 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [49, 51, 54, 49, 49, 54, 50, 56, 52, 51] and num 1361162843.
2022-11-02 13:39:11,759 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [50, 48, 52, 56, 48, 53, 49, 55, 57, 49] and num 2048051791.
2022-11-02 13:39:12,759 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [45, 51, 48, 54, 54, 48, 51, 56, 51, 53] and num -306603835. {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/22 21:33;yanxinyi;IntInputUdf.java;https://issues.apache.org/jira/secure/attachment/13051729/IntInputUdf.java","02/Nov/22 21:33;yanxinyi;SpendReport.java;https://issues.apache.org/jira/secure/attachment/13051728/SpendReport.java","02/Nov/22 21:35;yanxinyi;example.log;https://issues.apache.org/jira/secure/attachment/13051727/example.log",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 19:27:48 UTC 2022,,,,,,,,,,"0|z1ari8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 02:02;lsy;Do the results in the print table as expected? Maybe if the datagen source produces two duplicate records, you can observe the records from the source first to find the root cause?;;;","03/Nov/22 10:02;luoyuxia;How do you know some of the data have been processed twice? 

The udf shouldn't process one row for twice, otherwise, it should be a critical bug.;;;","03/Nov/22 13:20;rovo98;Hi, Just for information. I modify the program provided by [~yanxinyi] and change the generator kind of field `id` from {color:#172b4d}*random*{color} (default) to *sequence.*  As the following,  datagen table DDL,

        tEnv.executeSql(
                String.format(
                        ""create table %s (\n""
                                + ""id INT \n""
                                + "") with (\n""
                                + ""'connector' = 'datagen',\n""
                                + ""'number-of-rows' = '10',\n""
                                + ""'rows-per-second' = '1',\n""
                                + ""'fields.id.kind' = 'sequence',\n""
                                + ""'fields.id.start' = '0',\n""
                                + ""'fields.id.end' = '100'\n""
                                + "")"",
                        ""datagenTable""));

 

The other code basically remains the same. I first try to examinate the records in the datagen table.

        tEnv.executeSql(""select `id` from datagenTable"")
                        .print();

The output is as expected, as shown below.

{+}---{-}{-}{+}------------+
|op|         id|

{+}---{-}{-}{+}------------+
|+I|          0|
|+I|          1|
|+I|          2|
|+I|          3|
|+I|          7|
|+I|          6|
|+I|          5|
|+I|          4| |
|+I|          9|
|+I|          8| |

 

The  key output log of the main program is shown below.

 

20:41:24,205 INFO  osp.udf.IntInputUdf                                          [] - *** *** input bytes [49] and num 1.
20:41:24,207 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [48] and num 0. +++ DEBUG +++ duplicated ?
20:41:24,207 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [48] and num 0. +++ DEBUG +++ duplicated ?

1> +I[[48], 0]
20:41:24,233 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [50] and num 2. +++ DEBUG +++ duplicated ?
20:41:24,233 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [50] and num 2. +++ DEBUG +++ duplicated ?
3> +I[[50], 2]
20:41:24,233 INFO  osp.udf.IntInputUdf                                          [] - *** *** input bytes [51] and num 3.
20:41:25,204 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [52] and num 4. +++ DEBUG +++ duplicated ?
20:41:25,204 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [52] and num 4. +++ DEBUG +++ duplicated ?
1> +I[[52], 4]
20:41:25,204 INFO  osp.udf.IntInputUdf                                          [] - *** *** input bytes [53] and num 5.
20:41:25,232 INFO  osp.udf.IntInputUdf                                          [] - *** *** input bytes [55] and num 7.
20:41:25,233 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [54] and num 6. +++ DEBUG +++ duplicated ?
20:41:25,233 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [54] and num 6. +++ DEBUG +++ duplicated ?
3> +I[[54], 6]
20:41:26,204 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [56] and num 8. +++ DEBUG +++ duplicated ?
20:41:26,204 INFO  osp.udf.IntInputUdf                                          [] - ### ### input bytes [56] and num 8. +++ DEBUG +++ duplicated ?
20:41:26,204 INFO  osp.udf.IntInputUdf                                          [] - *** *** input bytes [57] and num 9.
1> +I[[56], 8]

 

The results in print table are as expected. But the logging shows that  rows which met the condition(that is the not *null* result value) might processed twice.

Since I'm not familar with the flink udf internals, I can't find the cause of this issue.;;;","03/Nov/22 17:21;yanxinyi;[~lsy], thanks for the reply. I don't think the datagen produces the duplicated rows that often, especially 1 row per second already defined as part of the datagen config.

In addition to [~rovo98] sequence experiment, it's a clear signal that UDF has been randomly processed twice.  ;;;","03/Nov/22 17:28;yanxinyi;hi [~luoyuxia], I think this is a critical bug, and it can cause incorrect results based on the UDF logic. Let me make a modification and update it here shortly. ;;;","03/Nov/22 20:38;yanxinyi;I added a new UDF that randomly returns either null or bytes, and it resulted in an incorrect result. First of all, it randomly executed input data twice. Secondly, the where clause did not handle `{*}IS NOT NULL{*}`. This is definitely a big problem.

Datagen: sequence from 1 to 5.

 

Query
{code:java}
INSERT INTO print_table 
     SELECT * FROM ( 
           SELECT RandomUdf(`id`) AS `id_in_bytes`, `id` FROM datagenTable 
     ) 
AS ET WHERE ET.`id_in_bytes` IS NOT NULL"" {code}
Result: 
{code:java}
+I[null, 1] 
+I[[50], 2] 
+I[null, 4] {code}
UDF
{code:java}
public @DataTypeHint(""Bytes"") byte[] eval(@DataTypeHint(""INT"") Integer intputNum) {
    byte[] results = intputNum.toString().getBytes(StandardCharsets.UTF_8);
    int randomNumber = ((int) (Math.random() * (10 - 1))) + 1;
    LOG.info(""[*][*][*] input num is {} and random number is {}. [*][*][*]"", intputNum, randomNumber);
    if (randomNumber % 2 == 0) {
      LOG.info(""### ### input bytes {} and num {}.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### "", results, intputNum);
      return results;
    }
    LOG.info(""*** *** input bytes {} and num {}."", results, intputNum);
    return null;
  } {code}
Log:
{code:java}
2022-11-03 12:04:54,018 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 1 and random number is 4. [*][*][*]
2022-11-03 12:04:54,018 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [49] and num 1.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-03 12:04:54,019 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 1 and random number is 7. [*][*][*]
2022-11-03 12:04:54,019 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [49] and num 1.
2022-11-03 12:04:55,018 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 2 and random number is 6. [*][*][*]
2022-11-03 12:04:55,020 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [50] and num 2.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-03 12:04:55,020 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 2 and random number is 4. [*][*][*]
2022-11-03 12:04:55,021 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [50] and num 2.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-03 12:04:56,014 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 3 and random number is 9. [*][*][*]
2022-11-03 12:04:56,015 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [51] and num 3.
2022-11-03 12:04:57,014 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 4 and random number is 2. [*][*][*]
2022-11-03 12:04:57,015 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - ### ### input bytes [52] and num 4.   ### ### DEBUG ### ### duplicated call??? ### DEBUG  ### ### 
2022-11-03 12:04:57,015 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 4 and random number is 7. [*][*][*]
2022-11-03 12:04:57,015 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [52] and num 4.
2022-11-03 12:04:58,017 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - [*][*][*] input num is 5 and random number is 7. [*][*][*]
2022-11-03 12:04:58,018 INFO  org.apache.flink.playgrounds.spendreport.IntInputUdf         [] - *** *** input bytes [53] and num 5. {code}
 ;;;","03/Nov/22 21:32;yanxinyi;In case someone is trying to repro it locally or is interested in investigating this issue, I have pushed my main and UDFs classes into my personal [repro|https://github.com/yanxinyi/flink-playgrounds/tree/test_udf/table-walkthrough/src/main/java/org/apache/flink/playgrounds/spendreport] for your reference.  ;;;","04/Nov/22 19:27;yanxinyi;Thank you to [~luoyuxia] for confirming the root cause of the issue, and here is the finding from the email thread.
{code:java}
The execute plan for the sql `INSERT INTO print_table SELECT * FROM ( SELECT RandomUdf(`id`) AS `id_in_bytes`, `id` FROM datagenTable ) AS ET WHERE ET.`id_in_bytes` IS NOT NULL`  is :
`
StreamPhysicalSink(table=[default_catalog.default_database.print_table], fields=[id_in_bytes, id])
  StreamPhysicalCalc(select=[RandomUdf(id) AS id_in_bytes, id], where=[IS NOT NULL(RandomUdf(id))])
    StreamPhysicalTableSourceScan(table=[[default_catalog, default_database, datagenTable]], fields=[id])
`
and from the plan, we can see it'll call the udf for twice in the StreamPhysicalCalc, as of result of which, it seems the one row will be processed for twice. {code};;;",,,,,,,,,,,,,,,
Make Record Size Flush Strategy Optional for Async Sink,FLINK-29854,13494386,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chalixar,dannycranmer,dannycranmer,02/Nov/22 19:41,29/Apr/24 20:35,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Common,,,,0,,,,,"h3. Background

Currently AsyncSinkWriter supports three mechanisms that trigger a flush to the destination:
 * TIme based 
 * Batch size in bytes
 * Number of records in the batch

For ""batch size in bytes"" one must implement [getSizeInBytes|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/writer/AsyncSinkWriter.java#L202] in order for the base to calculate the total batch size. In some cases computing the batch size within the AsyncSinkWriter is an expensive operation, or not possible. For example, the DynamoDB connector needs to determine the serialized size of {{DynamoDbWriteRequest}}. (https://github.com/apache/flink-connector-dynamodb/pull/1/files#r1012223894)

h3. Scope

Add a feature to make ""size in bytes"" support optional, this includes:
- Connectors will not be required to implement {{getSizeInBytes}}
- Batches will not be validated for max size
- Records will not be validated for size
- Batches are not flushed when max size is exceeded

The sink implementer can decide if it is appropriate to enable this feature.



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 12:43:59 UTC 2023,,,,,,,,,,"0|z1ar14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 21:01;chalixar;Hi [~dannycranmer], could you please assign me this issue?;;;","02/Nov/22 21:09;dannycranmer;Done, thanks [~chalixar];;;","03/Jan/23 12:43;chalixar;Proposed Improvement under
https://cwiki.apache.org/confluence/display/FLINK/FLIP-284+%3A+Making+AsyncSinkWriter+Flush+triggers+adjustable
will update issue accordingly;;;",,,,,,,,,,,,,,,,,,,,
Older jackson-databind found in flink-kubernetes-operator-1.2.0-shaded.jar,FLINK-29853,13494372,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,02/Nov/22 17:44,13/Dec/22 01:41,04/Jun/24 20:41,10/Nov/22 10:34,kubernetes-operator-1.2.1,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"A Twistlock security scan of the existing 1.2.0 operator as well as the current main release shows a high vulnerability with the current jackson-databind version.

======
severity: High

cvss: 7.5

riskFactors:  Attack complexity: low,Attack vector: network,Has fix,High severity,Recent vulnerability

CVE link: [https://nvd.nist.gov/vuln/detail/CVE-2022-42003]

packageName: com.fasterxml.jackson.core_jackson-databind

packagePath: /flink-kubernetes-operator/flink-kubernetes-operator-1.2.0-shaded.jar and/or /flink-kubernetes-operator/flink-kubernetes-operator-1.3-SNAPSHOT-shaded.jar

description: In FasterXML jackson-databind before 2.14.0-rc1, resource exhaustion can occur because of a lack of a check in primitive value deserializers to avoid deep wrapper array nesting, when the UNWRAP_SINGLE_VALUE_ARRAYS feature is enabled. Additional fix version in 2.13.4.1 and 2.12.17.1

====

This is exactly like the older issue https://issues.apache.org/jira/browse/FLINK-27654 

I'm going to see if I can fix it myself and create a PR if I'm successful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 10:34:13 UTC 2022,,,,,,,,,,"0|z1aqy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 20:46;jbusche;OK - I tried changing the pom.xml from:

<version>2.13.4</version>

to

<version>2.13.4.1</version>

But that version wasn't found.

 

I then tried  <version>2.14.0-rc1</version>

and this solves the vulnerability, but not sure we'd want to accept a rc candidate as a permanent fix..  can maybe wait until the version is GA'd.

Testing the image with rc1 seems to work well:
{quote}oc get pods

NAME                                        READY   STATUS    RESTARTS   AGE

basic-example-56876dc586-9vlv7              1/1     Running   0          88s

basic-example-taskmanager-1-1               1/1     Running   0          48s

flink-kubernetes-operator-f7c8bc9b6-gct4d   2/2     Running   0          2m27s
{quote};;;","09/Nov/22 22:09;jbusche;OK, good news - 2.14.0 has GA'd and looks good.  Image scans clean (no fixable vulnerabilities) and the basic example works:
{quote}kubectl get pods

NAME                                         READY   STATUS    RESTARTS   AGE

basic-example-56876dc586-df769               1/1     Running   0          13m

basic-example-taskmanager-1-1                1/1     Running   0          13m

flink-kubernetes-operator-7d5c7b77f7-5wgb8   2/2     Running   0          14m
{quote}
I'll put a PR through, see how it looks...;;;","10/Nov/22 10:34;gyfora;merged to main 82fca328488be8ddf7d67ec6ce8ad1b20a6da3a6;;;",,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler duplicates operators for each parallel instance in the Web UI,FLINK-29852,13494357,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,JasonLee,JasonLee,02/Nov/22 15:58,05/May/23 15:37,04/Jun/24 20:41,02/Mar/23 17:18,1.16.0,1.16.1,,,,,1.16.2,1.17.0,,,Runtime / Coordination,Runtime / Web Frontend,,,0,pull-request-available,,,,"All the operators in the DAG are shown repeatedly

!image-2022-11-02-23-57-39-387.png!",Flink 1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/22 15:57;JasonLee;image-2022-11-02-23-57-39-387.png;https://issues.apache.org/jira/secure/attachment/13051718/image-2022-11-02-23-57-39-387.png","09/Nov/22 08:09;huwh;image-2022-11-09-16-09-44-233.png;https://issues.apache.org/jira/secure/attachment/13051987/image-2022-11-09-16-09-44-233.png","09/Nov/22 09:32;JasonLee;image-2022-11-09-17-32-27-377.png;https://issues.apache.org/jira/secure/attachment/13051988/image-2022-11-09-17-32-27-377.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 15:37:29 UTC 2023,,,,,,,,,,"0|z1aquo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 07:16;gaoyunhaii;Hi [~JasonLee] thanks for reporting the issue! Could this phenomenon be reproduced? And if possible could you provide the content of the response from the web server via http://<ip>:<port>/jobs/<job_id> ?;;;","03/Nov/22 08:18;JasonLee;Hi [~gaoyunhaii]  yes,Stably repeated, I tested two different SQL jobs and one datastream api job,There will be repetition.
{code:java}
// the content of the response from the web server

{
    ""jid"":""a294907b86943c3892070f07b98ec6e3"",
    ""name"":""FlinkStreamingBroadcastMysqlDemo"",
    ""isStoppable"":false,
    ""state"":""RUNNING"",
    ""start-time"":1667462380130,
    ""end-time"":-1,
    ""duration"":858837,
    ""maxParallelism"":-1,
    ""now"":1667463238967,
    ""timestamps"":Object{...},
    ""vertices"":Array[3],
    ""status-counts"":Object{...},
    ""plan"":{
        ""jid"":""a294907b86943c3892070f07b98ec6e3"",
        ""name"":""FlinkStreamingBroadcastMysqlDemo"",
        ""type"":""STREAMING"",
        ""nodes"":[
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""1a936cb48657826a536f331e9fb33b5e"",
                ""parallelism"":1,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Sink: Print to Std. Out<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                        ""ship_strategy"":""REBALANCE"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            }
        ]
    }
}{code}
 

 ;;;","08/Nov/22 13:14;huwh;Hi [~JasonLee] , I can't reproduce this problem with flink-1.16 WordCount, can you show how to reproduce this?;;;","08/Nov/22 13:34;JasonLee;Hi [~huwh] , I just used the command on the website like the following:
{code:java}
./flink run-application -t yarn-application ../examples/streaming/TopSpeedWindowing.jar
./flink run -t yarn-per-job --detached ../examples/streaming/TopSpeedWindowing.jar {code}
They all repeat as in the picture above.;;;","09/Nov/22 08:11;huwh;I can't reproduce this with this jar.{*}{*}

 

*My Flink version is* 

*Version:* 1.16.0 *Commit:* af6eff8 @ 2022-10-20T04:21:45+02:00 

 ;;;","09/Nov/22 10:10;JasonLee;Hi [~huwh] That's a little weird,I submit the task with the following command:
{code:java}
./flink run-application -t yarn-application ../examples/streaming/TopSpeedWindowing.jar {code}
The DAG is shown below,The window and sink operators have 4 parallelism, shown four times below

!image-2022-11-09-17-32-27-377.png!;;;","13/Dec/22 14:46;JasonLee;hi all, I re-downloaded and deployed the Flink cluster, the flink job is normal, and there is no duplication of operator. I do not know what the problem is, in a word, it is OK now, so I will close this issue, thanks.;;;","02/Mar/23 06:03;huwh;Hi, [~JasonLee] I found this is a bug while adaptiveScheduler update the ExecutionGraph#JsonPlan. It put all the execution vertices into the jsonPlan. Could you reopen this tickets, I would like to fix it. cc [~dwysakowicz] ;;;","02/Mar/23 06:11;JasonLee;[~huwh] yeah,Thank you for your reply,I've already open it.;;;","02/Mar/23 13:02;dmvk;master: 758ce72158922e00ed7078e00706f8502145e461

release-1.16: 57cabdccf88f03523edfab413c252c8f16e9890a

release-1.17: 30344f1c79f3760c88de73a0169f35dbe8951c3d;;;","05/May/23 14:58;Leone;We are facing this issue in 1.16.1 and need to make a go/no-go decision and wondering whether this problem is just affecting the endpoint /jobs/\{jobId} (and so the UI) or does it have deeper implications?;;;","05/May/23 15:37;dmvk;This is just a problem with the REST API representation, there are no broader consequences;;;",,,,,,,,,,,
Upgrade to Fabric8 6.x.x and JOSDK 4.x.x,FLINK-29851,13494338,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,02/Nov/22 14:25,08/Nov/22 06:33,04/Jun/24 20:41,04/Nov/22 11:08,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,In order to get the latest developments from fabric8 and the josdk we should upgrade to the latest version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 11:08:18 UTC 2022,,,,,,,,,,"0|z1aqqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 11:08;gyfora;Merged to main:
45a280aa2d6824ded3a03b503dc33208f159f5d9
0b82f860739152fef5bc7665d5e56d1484f67ac0;;;",,,,,,,,,,,,,,,,,,,,,,
Flink Table Store quick start guide does not work,FLINK-29850,13494337,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,Gerrrr,Gerrrr,02/Nov/22 14:15,24/Nov/22 10:08,04/Jun/24 20:41,24/Nov/22 10:08,,,,,,,,,,,Table Store,,,,0,,,,,"Following instructions in https://nightlies.apache.org/flink/flink-table-store-docs-master/docs/try-table-store/quick-start/ leads to empty results in {{word_count}} table.

Flink version 1.15.2, Flink Table Store version 0.2.1.

{noformat}
Flink SQL> show catalogs;
+-----------------+
|    catalog name |
+-----------------+
| default_catalog |
+-----------------+
1 row in set

Flink SQL> CREATE CATALOG my_catalog WITH (
>   'type'='table-store',
>   'warehouse'='file:/tmp/table_store'
> );
>
[INFO] Execute statement succeed.

Flink SQL> USE CATALOG my_catalog;
> USE CATALOG my_catalog;
>

Flink SQL> USE CATALOG my_catalog;
>
[INFO] Execute statement succeed.

Flink SQL> CREATE TABLE word_count (
>     word STRING PRIMARY KEY NOT ENFORCED,
>     cnt BIGINT
> );
>
[INFO] Execute statement succeed.

Flink SQL> CREATE TEMPORARY TABLE word_table (
>     word STRING
> ) WITH (
>     'connector' = 'datagen',
>     'fields.word.length' = '1'
> );
[INFO] Execute statement succeed.

Flink SQL> SET 'execution.checkpointing.interval' = '10 s';
>
[INFO] Session property has been set.

Flink SQL> INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word;
>
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 0c5f22c2ab3e83e1a1f9274818ff675b


Flink SQL> SET 'sql-client.execution.result-mode' = 'tableau';
>
[INFO] Session property has been set.

Flink SQL> RESET 'execution.checkpointing.interval';
>
[INFO] Session property has been reset.

Flink SQL> SET 'execution.runtime-mode' = 'batch';
>
[INFO] Session property has been set.

Flink SQL> SELECT * FROM word_count;
>
Empty set
{noformat}

Flink logs:

{noformat}
flink          | Starting standalonesession as a console application on host flink.
broker         | [2022-11-02 14:07:17,045] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
broker         | [2022-11-02 14:07:17,046] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
broker         | [2022-11-02 14:07:17,050] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
broker         | [2022-11-02 14:07:17,051] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
flink          | 2022-11-02 14:07:17,745 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
flink          | 2022-11-02 14:07:17,752 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Preconfiguration:
flink          | 2022-11-02 14:07:17,753 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -
flink          |
flink          |
flink          | RESOURCE_PARAMS extraction logs:
flink          | jvm_params: -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456
flink          | dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
flink          | logs: INFO  [] - Loading configuration property: jobmanager.rpc.address, flink
flink          | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
flink          | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
flink          | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
flink          | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
flink          | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
flink          | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
flink          | INFO  [] - Loading configuration property: parallelism.default, 1
flink          | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
flink          | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
flink          | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
flink          | INFO  [] - Loading configuration property: blob.server.port, 6124
flink          | INFO  [] - Loading configuration property: query.server.port, 6125
flink          | INFO  [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
flink          | INFO  [] - Final Master Memory configuration:
flink          | INFO  [] -   Total Process Memory: 1.563gb (1677721600 bytes)
flink          | INFO  [] -     Total Flink Memory: 1.125gb (1207959552 bytes)
flink          | INFO  [] -       JVM Heap:         1024.000mb (1073741824 bytes)
flink          | INFO  [] -       Off-heap:         128.000mb (134217728 bytes)
flink          | INFO  [] -     JVM Metaspace:      256.000mb (268435456 bytes)
flink          | INFO  [] -     JVM Overhead:       192.000mb (201326592 bytes)
flink          |
flink          | 2022-11-02 14:07:17,753 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
flink          | 2022-11-02 14:07:17,754 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting StandaloneSessionClusterEntrypoint (Version: 1.15.2, Scala: 2.12, Rev:69e8126, Date:2022-08-17T14:58:06+02:00)
flink          | 2022-11-02 14:07:17,754 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: flink
flink          | 2022-11-02 14:07:18,137 WARN  org.apache.hadoop.util.NativeCodeLoader                      [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
flink          | 2022-11-02 14:07:18,225 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: flink
flink          | 2022-11-02 14:07:18,226 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Temurin - 1.8/25.345-b01
flink          | 2022-11-02 14:07:18,226 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
flink          | 2022-11-02 14:07:18,226 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 981 MiBytes
flink          | 2022-11-02 14:07:18,227 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /opt/java/openjdk
flink          | 2022-11-02 14:07:18,230 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop version: 2.8.3
flink          | 2022-11-02 14:07:18,230 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
flink          | 2022-11-02 14:07:18,230 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx1073741824
flink          | 2022-11-02 14:07:18,231 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms1073741824
flink          | 2022-11-02 14:07:18,231 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
flink          | 2022-11-02 14:07:18,231 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/opt/flink/log/flink--standalonesession-0-flink.log
flink          | 2022-11-02 14:07:18,232 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
flink          | 2022-11-02 14:07:18,232 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
flink          | 2022-11-02 14:07:18,232 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
flink          | 2022-11-02 14:07:18,233 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
flink          | 2022-11-02 14:07:18,234 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --configDir
flink          | 2022-11-02 14:07:18,234 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     /opt/flink/conf
flink          | 2022-11-02 14:07:18,234 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --executionMode
flink          | 2022-11-02 14:07:18,235 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     cluster
flink          | 2022-11-02 14:07:18,235 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
flink          | 2022-11-02 14:07:18,235 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
flink          | 2022-11-02 14:07:18,235 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
flink          | 2022-11-02 14:07:18,236 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
flink          | 2022-11-02 14:07:18,236 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
flink          | 2022-11-02 14:07:18,236 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
flink          | 2022-11-02 14:07:18,236 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
flink          | 2022-11-02 14:07:18,237 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=1073741824b
flink          | 2022-11-02 14:07:18,237 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
flink          | 2022-11-02 14:07:18,237 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
flink          | 2022-11-02 14:07:18,238 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: /opt/flink/lib/flink-cep-1.15.2.jar:/opt/flink/lib/flink-connector-files-1.15.2.jar:/opt/flink/lib/flink-connector-kafka-1.15.2.jar:/opt/flink/lib/flink-csv-1.15.2.jar:/opt/flink/lib/flink-faker-0.5.0.jar:/opt/flink/lib/flink-json-1.15.2.jar:/opt/flink/lib/flink-scala_2.12-1.15.2.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-zookeeper-3.5.9.jar:/opt/flink/lib/flink-table-api-java-uber-1.15.2.jar:/opt/flink/lib/flink-table-planner-loader-1.15.2.jar:/opt/flink/lib/flink-table-runtime-1.15.2.jar:/opt/flink/lib/flink-table-store-dist-0.2.1.jar:/opt/flink/lib/kafka-clients-3.3.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.15.2.jar:::
flink          | 2022-11-02 14:07:18,238 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
flink          | 2022-11-02 14:07:18,240 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
flink          | 2022-11-02 14:07:18,259 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, flink
flink          | 2022-11-02 14:07:18,259 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
flink          | 2022-11-02 14:07:18,260 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
flink          | 2022-11-02 14:07:18,260 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
flink          | 2022-11-02 14:07:18,260 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
flink          | 2022-11-02 14:07:18,261 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
flink          | 2022-11-02 14:07:18,261 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
flink          | 2022-11-02 14:07:18,262 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
flink          | 2022-11-02 14:07:18,262 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
flink          | 2022-11-02 14:07:18,263 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
flink          | 2022-11-02 14:07:18,263 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
flink          | 2022-11-02 14:07:18,264 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
flink          | 2022-11-02 14:07:18,264 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
flink          | 2022-11-02 14:07:18,314 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting StandaloneSessionClusterEntrypoint.
flink          | 2022-11-02 14:07:18,371 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
flink          | 2022-11-02 14:07:18,442 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
flink          | 2022-11-02 14:07:18,460 WARN  org.apache.flink.runtime.util.HadoopUtils                    [] - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
flink          | 2022-11-02 14:07:18,490 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to flink (auth:SIMPLE)
flink          | 2022-11-02 14:07:18,497 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-5958803181883791922.conf.
flink          | 2022-11-02 14:07:18,511 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
flink          | 2022-11-02 14:07:18,522 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Using working directory: WorkingDirectory(/tmp/jm_3fa14cf24c77ac8d356de9bc3b5b07ae).
flink          | 2022-11-02 14:07:18,890 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address flink:6123, bind address 0.0.0.0:6123.
taskmanager_2  | 2022-11-02 14:07:18,937 WARN  org.apache.flink.runtime.net.ConnectionUtils                 [] - Could not connect to flink/172.23.0.4:6123. Selecting a local address using heuristics.
taskmanager_2  | 2022-11-02 14:07:18,938 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address '48d9697ca669' (172.23.0.5) for communication.
taskmanager_2  | 2022-11-02 14:07:19,015 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.23.0.5:0, bind address 0.0.0.0:0.
taskmanager_1  | 2022-11-02 14:07:19,036 WARN  org.apache.flink.runtime.net.ConnectionUtils                 [] - Could not connect to flink/172.23.0.4:6123. Selecting a local address using heuristics.
taskmanager_1  | 2022-11-02 14:07:19,036 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address '583c1c3a0ea1' (172.23.0.6) for communication.
taskmanager_1  | 2022-11-02 14:07:19,107 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.23.0.6:0, bind address 0.0.0.0:0.
taskmanager_2  | 2022-11-02 14:07:19,634 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
taskmanager_2  | 2022-11-02 14:07:19,680 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
taskmanager_2  | 2022-11-02 14:07:19,681 INFO  akka.remote.Remoting                                         [] - Starting remoting
taskmanager_1  | 2022-11-02 14:07:19,762 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
flink          | 2022-11-02 14:07:19,769 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
taskmanager_1  | 2022-11-02 14:07:19,803 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
taskmanager_1  | 2022-11-02 14:07:19,804 INFO  akka.remote.Remoting                                         [] - Starting remoting
flink          | 2022-11-02 14:07:19,813 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
flink          | 2022-11-02 14:07:19,814 INFO  akka.remote.Remoting                                         [] - Starting remoting
taskmanager_2  | 2022-11-02 14:07:19,917 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@172.23.0.5:42023]
taskmanager_1  | 2022-11-02 14:07:20,064 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@172.23.0.6:42189]
flink          | 2022-11-02 14:07:20,065 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@flink:6123]
taskmanager_2  | 2022-11-02 14:07:20,187 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@172.23.0.5:42023
taskmanager_2  | 2022-11-02 14:07:20,216 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/tmp/tm_172.23.0.5:42023-f6c8bb)
taskmanager_2  | 2022-11-02 14:07:20,229 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
taskmanager_2  | 2022-11-02 14:07:20,235 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.23.0.5:0, bind address 0.0.0.0:0.
taskmanager_2  | 2022-11-02 14:07:20,263 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
taskmanager_2  | 2022-11-02 14:07:20,267 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
taskmanager_2  | 2022-11-02 14:07:20,268 INFO  akka.remote.Remoting                                         [] - Starting remoting
taskmanager_2  | 2022-11-02 14:07:20,290 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@172.23.0.5:34613]
taskmanager_2  | 2022-11-02 14:07:20,319 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@172.23.0.5:34613
flink          | 2022-11-02 14:07:20,320 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@flink:6123
taskmanager_2  | 2022-11-02 14:07:20,347 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_172.23.0.5:42023-f6c8bb .
taskmanager_1  | 2022-11-02 14:07:20,356 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@172.23.0.6:42189
taskmanager_2  | 2022-11-02 14:07:20,367 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/tm_172.23.0.5:42023-f6c8bb/blobStorage
flink          | 2022-11-02 14:07:20,380 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory /tmp/jm_3fa14cf24c77ac8d356de9bc3b5b07ae/blobStorage
taskmanager_2  | 2022-11-02 14:07:20,379 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/tm_172.23.0.5:42023-f6c8bb/blobStorage
flink          | 2022-11-02 14:07:20,386 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:6124 - max concurrent requests: 50 - max backlog: 1000
taskmanager_1  | 2022-11-02 14:07:20,389 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/tmp/tm_172.23.0.6:42189-46dab1)
taskmanager_2  | 2022-11-02 14:07:20,391 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
taskmanager_2  | 2022-11-02 14:07:20,392 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 172.23.0.5:42023-f6c8bb
taskmanager_1  | 2022-11-02 14:07:20,405 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
flink          | 2022-11-02 14:07:20,410 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
taskmanager_1  | 2022-11-02 14:07:20,410 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.23.0.6:0, bind address 0.0.0.0:0.
flink          | 2022-11-02 14:07:20,417 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address flink:0, bind address 0.0.0.0:0.
taskmanager_2  | 2022-11-02 14:07:20,427 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 298 GB, usable 197 GB (66.11% usable)
taskmanager_2  | 2022-11-02 14:07:20,438 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
taskmanager_2  | 	/tmp/flink-io-a33a42a4-12bd-4241-ad32-a69da961dcd7
taskmanager_1  | 2022-11-02 14:07:20,454 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
taskmanager_2  | 2022-11-02 14:07:20,458 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: /0.0.0.0, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 1 (manual), number of client threads: 1 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
taskmanager_1  | 2022-11-02 14:07:20,461 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
taskmanager_1  | 2022-11-02 14:07:20,462 INFO  akka.remote.Remoting                                         [] - Starting remoting
flink          | 2022-11-02 14:07:20,466 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
flink          | 2022-11-02 14:07:20,479 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
flink          | 2022-11-02 14:07:20,480 INFO  akka.remote.Remoting                                         [] - Starting remoting
taskmanager_1  | 2022-11-02 14:07:20,510 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@172.23.0.6:36747]
flink          | 2022-11-02 14:07:20,522 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@flink:37609]
taskmanager_1  | 2022-11-02 14:07:20,522 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@172.23.0.6:36747
flink          | 2022-11-02 14:07:20,543 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@flink:37609
taskmanager_1  | 2022-11-02 14:07:20,562 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_172.23.0.6:42189-46dab1 .
flink          | 2022-11-02 14:07:20,580 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
taskmanager_1  | 2022-11-02 14:07:20,582 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/tm_172.23.0.6:42189-46dab1/blobStorage
taskmanager_2  | 2022-11-02 14:07:20,587 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
taskmanager_2  | 	/tmp/flink-netty-shuffle-c3d2b336-07e9-4ccf-8667-0da2c0e75c9b
taskmanager_1  | 2022-11-02 14:07:20,592 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/tm_172.23.0.6:42189-46dab1/blobStorage
taskmanager_1  | 2022-11-02 14:07:20,610 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
taskmanager_1  | 2022-11-02 14:07:20,611 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 172.23.0.6:42189-46dab1
flink          | 2022-11-02 14:07:20,631 INFO  org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStore [] - Initializing FileExecutionGraphInfoStore: Storage directory /tmp/executionGraphStore-35629f69-cfa4-4988-b484-fbef12d681d8, expiration time 3600000, maximum cache size 52428800 bytes.
taskmanager_1  | 2022-11-02 14:07:20,648 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 298 GB, usable 197 GB (66.11% usable)
taskmanager_1  | 2022-11-02 14:07:20,657 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
taskmanager_1  | 	/tmp/flink-io-ad0d6004-a678-46b8-a875-27409a4613b7
taskmanager_1  | 2022-11-02 14:07:20,673 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: /0.0.0.0, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 1 (manual), number of client threads: 1 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
taskmanager_2  | 2022-11-02 14:07:20,673 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
taskmanager_2  | 2022-11-02 14:07:20,705 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
flink          | 2022-11-02 14:07:20,737 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Upload directory /tmp/flink-web-348451b6-b4b5-4332-a8eb-4195cf0f370a/flink-web-upload does not exist.
flink          | 2022-11-02 14:07:20,739 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Created directory /tmp/flink-web-348451b6-b4b5-4332-a8eb-4195cf0f370a/flink-web-upload for file uploads.
flink          | 2022-11-02 14:07:20,743 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
taskmanager_1  | 2022-11-02 14:07:20,781 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
taskmanager_1  | 	/tmp/flink-netty-shuffle-cecc60b4-f0e0-448a-b024-1be6fc330a49
taskmanager_2  | 2022-11-02 14:07:20,820 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
taskmanager_2  | 2022-11-02 14:07:20,823 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 117 ms).
taskmanager_2  | 2022-11-02 14:07:20,832 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
taskmanager_1  | 2022-11-02 14:07:20,859 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
taskmanager_1  | 2022-11-02 14:07:20,887 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
taskmanager_2  | 2022-11-02 14:07:20,895 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 68 ms). Listening on SocketAddress /0.0.0.0:37525.
taskmanager_2  | 2022-11-02 14:07:20,898 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
taskmanager_2  | 2022-11-02 14:07:20,949 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
taskmanager_2  | 2022-11-02 14:07:21,000 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
taskmanager_2  | 2022-11-02 14:07:21,004 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-15ff4df8-b6f5-4356-959e-73a7d8e09c0c
taskmanager_2  | 2022-11-02 14:07:21,011 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
taskmanager_1  | 2022-11-02 14:07:21,036 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
taskmanager_1  | 2022-11-02 14:07:21,042 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 154 ms).
taskmanager_1  | 2022-11-02 14:07:21,053 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
taskmanager_1  | 2022-11-02 14:07:21,133 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 86 ms). Listening on SocketAddress /0.0.0.0:35461.
taskmanager_1  | 2022-11-02 14:07:21,136 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
taskmanager_1  | 2022-11-02 14:07:21,203 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
taskmanager_1  | 2022-11-02 14:07:21,243 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
taskmanager_1  | 2022-11-02 14:07:21,245 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-cbe37ac6-95c7-4ea1-b41c-c0be94c0a181
taskmanager_1  | 2022-11-02 14:07:21,254 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
flink          | 2022-11-02 14:07:21,282 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component log file: /opt/flink/log/flink--standalonesession-0-flink.log
flink          | 2022-11-02 14:07:21,282 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component stdout file: /opt/flink/log/flink--standalonesession-0-flink.out
taskmanager_2  | 2022-11-02 14:07:21,381 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*.
taskmanager_1  | 2022-11-02 14:07:21,467 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*.
flink          | 2022-11-02 14:07:21,521 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at 0.0.0.0:8081
flink          | 2022-11-02 14:07:21,523 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://0.0.0.0:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000
flink          | 2022-11-02 14:07:21,524 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Web frontend listening at http://0.0.0.0:8081.
flink          | 2022-11-02 14:07:21,556 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new DispatcherLeaderProcess.
flink          | 2022-11-02 14:07:21,565 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
flink          | 2022-11-02 14:07:21,569 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
flink          | 2022-11-02 14:07:21,571 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.
flink          | 2022-11-02 14:07:21,576 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs that are not finished, yet.
flink          | 2022-11-02 14:07:21,576 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
flink          | 2022-11-02 14:07:21,600 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_0 .
flink          | 2022-11-02 14:07:21,602 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
flink          | 2022-11-02 14:07:21,633 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
taskmanager_2  | 2022-11-02 14:07:31,422 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
taskmanager_1  | 2022-11-02 14:07:31,494 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
flink          | 2022-11-02 14:07:31,522 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 172.23.0.5:42023-f6c8bb (akka.tcp://flink@172.23.0.5:42023/user/rpc/taskmanager_0) at ResourceManager
taskmanager_2  | 2022-11-02 14:07:31,548 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@flink:6123/user/rpc/resourcemanager_* under registration id 7f8dcb16a15971a1ad69c555269560f9.
flink          | 2022-11-02 14:07:31,553 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 172.23.0.6:42189-46dab1 (akka.tcp://flink@172.23.0.6:42189/user/rpc/taskmanager_0) at ResourceManager
taskmanager_1  | 2022-11-02 14:07:31,566 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@flink:6123/user/rpc/resourcemanager_* under registration id 304fd4e226851d53127be09eebd39228.
flink          | 2022-11-02 14:09:41,130 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'insert-into_my_catalog.default.word_count' (0c5f22c2ab3e83e1a1f9274818ff675b).
flink          | 2022-11-02 14:09:41,131 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'insert-into_my_catalog.default.word_count' (0c5f22c2ab3e83e1a1f9274818ff675b).
flink          | 2022-11-02 14:09:41,182 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
flink          | 2022-11-02 14:09:41,198 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'insert-into_my_catalog.default.word_count' (0c5f22c2ab3e83e1a1f9274818ff675b).
flink          | 2022-11-02 14:09:41,251 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for insert-into_my_catalog.default.word_count (0c5f22c2ab3e83e1a1f9274818ff675b).
flink          | 2022-11-02 14:09:41,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job insert-into_my_catalog.default.word_count (0c5f22c2ab3e83e1a1f9274818ff675b).
flink          | 2022-11-02 14:09:41,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
flink          | 2022-11-02 14:09:41,366 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 2 ms, total 1 pipelined regions currently.
flink          | 2022-11-02 14:09:41,380 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@66e2f2aa
flink          | 2022-11-02 14:09:41,380 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
flink          | 2022-11-02 14:09:41,382 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
flink          | 2022-11-02 14:09:41,435 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
flink          | 2022-11-02 14:09:41,453 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@7366e212 for insert-into_my_catalog.default.word_count (0c5f22c2ab3e83e1a1f9274818ff675b).
flink          | 2022-11-02 14:09:41,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'insert-into_my_catalog.default.word_count' (0c5f22c2ab3e83e1a1f9274818ff675b) under job master id 00000000000000000000000000000000.
flink          | 2022-11-02 14:09:41,477 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
flink          | 2022-11-02 14:09:41,477 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_my_catalog.default.word_count (0c5f22c2ab3e83e1a1f9274818ff675b) switched from state CREATED to RUNNING.
flink          | 2022-11-02 14:09:41,488 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_table[1] (1/1) (7a150a66716e80d16e4f4d21450a843e) switched from CREATED to SCHEDULED.
flink          | 2022-11-02 14:09:41,489 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1) (e234b4060f0956d22ad81cf7edf1197d) switched from CREATED to SCHEDULED.
flink          | 2022-11-02 14:09:41,489 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Writer -> Global Committer -> Sink: end (1/1) (d1372739faeaa4f7c2d5dc3157c38091) switched from CREATED to SCHEDULED.
flink          | 2022-11-02 14:09:41,516 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
flink          | 2022-11-02 14:09:41,521 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
flink          | 2022-11-02 14:09:41,523 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@flink:6123/user/rpc/jobmanager_2 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
flink          | 2022-11-02 14:09:41,527 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink:6123/user/rpc/jobmanager_2 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
flink          | 2022-11-02 14:09:41,531 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
flink          | 2022-11-02 14:09:41,533 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 0c5f22c2ab3e83e1a1f9274818ff675b: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
taskmanager_2  | 2022-11-02 14:09:41,544 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 2a3be1da0a0a5737027c89101aa867e6 for job 0c5f22c2ab3e83e1a1f9274818ff675b from resource manager with leader id 00000000000000000000000000000000.
taskmanager_2  | 2022-11-02 14:09:41,551 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,553 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 0c5f22c2ab3e83e1a1f9274818ff675b for job leader monitoring.
taskmanager_2  | 2022-11-02 14:09:41,555 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@flink:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
taskmanager_2  | 2022-11-02 14:09:41,575 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
taskmanager_2  | 2022-11-02 14:09:41,598 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@flink:6123/user/rpc/jobmanager_2 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:09:41,599 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:09:41,604 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 0c5f22c2ab3e83e1a1f9274818ff675b.
flink          | 2022-11-02 14:09:41,623 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_table[1] (1/1) (7a150a66716e80d16e4f4d21450a843e) switched from SCHEDULED to DEPLOYING.
flink          | 2022-11-02 14:09:41,624 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: word_table[1] (1/1) (attempt #0) with attempt id 7a150a66716e80d16e4f4d21450a843e and vertex id bc764cd8ddf7a0cff126f51c16239658_0 to 172.23.0.5:42023-f6c8bb @ flink-sandbox_taskmanager_2.flink-sandbox_default (dataPort=37525) with allocation id 2a3be1da0a0a5737027c89101aa867e6
flink          | 2022-11-02 14:09:41,631 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1) (e234b4060f0956d22ad81cf7edf1197d) switched from SCHEDULED to DEPLOYING.
flink          | 2022-11-02 14:09:41,631 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate[3] -> ConstraintEnforcer[4] (1/1) (attempt #0) with attempt id e234b4060f0956d22ad81cf7edf1197d and vertex id 20ba6b65f97481d5570070de90e4e791_0 to 172.23.0.5:42023-f6c8bb @ flink-sandbox_taskmanager_2.flink-sandbox_default (dataPort=37525) with allocation id 2a3be1da0a0a5737027c89101aa867e6
flink          | 2022-11-02 14:09:41,639 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Writer -> Global Committer -> Sink: end (1/1) (d1372739faeaa4f7c2d5dc3157c38091) switched from SCHEDULED to DEPLOYING.
flink          | 2022-11-02 14:09:41,640 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Writer -> Global Committer -> Sink: end (1/1) (attempt #0) with attempt id d1372739faeaa4f7c2d5dc3157c38091 and vertex id b5c8d46f3e7b141acf271f12622e752b_0 to 172.23.0.5:42023-f6c8bb @ flink-sandbox_taskmanager_2.flink-sandbox_default (dataPort=37525) with allocation id 2a3be1da0a0a5737027c89101aa867e6
taskmanager_2  | 2022-11-02 14:09:41,657 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,679 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
taskmanager_2  | 2022-11-02 14:09:41,708 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: word_table[1] (1/1)#0 (7a150a66716e80d16e4f4d21450a843e), deploy into slot with allocation id 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_table[1] (1/1)#0 (7a150a66716e80d16e4f4d21450a843e) switched from CREATED to DEPLOYING.
taskmanager_2  | 2022-11-02 14:09:41,712 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,716 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: word_table[1] (1/1)#0 (7a150a66716e80d16e4f4d21450a843e) [DEPLOYING].
taskmanager_2  | 2022-11-02 14:09:41,721 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading 0c5f22c2ab3e83e1a1f9274818ff675b/p-2fcfa81de948a930c635827bf3c1c2dd73a1cc34-73131fc0e3af0bde76e76fed81b34031 from flink/172.23.0.4:6124
taskmanager_2  | 2022-11-02 14:09:41,736 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading 0c5f22c2ab3e83e1a1f9274818ff675b/p-35ffaba250050d7c3202364ee51d6170823060af-8c7ee9ac1cd70b6d02680461334f2e15 from flink/172.23.0.4:6124
taskmanager_2  | 2022-11-02 14:09:41,743 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate[3] -> ConstraintEnforcer[4] (1/1)#0 (e234b4060f0956d22ad81cf7edf1197d), deploy into slot with allocation id 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,743 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1)#0 (e234b4060f0956d22ad81cf7edf1197d) switched from CREATED to DEPLOYING.
taskmanager_2  | 2022-11-02 14:09:41,744 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,745 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate[3] -> ConstraintEnforcer[4] (1/1)#0 (e234b4060f0956d22ad81cf7edf1197d) [DEPLOYING].
taskmanager_2  | 2022-11-02 14:09:41,752 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Writer -> Global Committer -> Sink: end (1/1)#0 (d1372739faeaa4f7c2d5dc3157c38091), deploy into slot with allocation id 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,753 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Writer -> Global Committer -> Sink: end (1/1)#0 (d1372739faeaa4f7c2d5dc3157c38091) switched from CREATED to DEPLOYING.
taskmanager_2  | 2022-11-02 14:09:41,755 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a3be1da0a0a5737027c89101aa867e6.
taskmanager_2  | 2022-11-02 14:09:41,754 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Writer -> Global Committer -> Sink: end (1/1)#0 (d1372739faeaa4f7c2d5dc3157c38091) [DEPLOYING].
taskmanager_2  | 2022-11-02 14:09:42,019 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4cd774ec
taskmanager_2  | 2022-11-02 14:09:42,019 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
taskmanager_2  | 2022-11-02 14:09:42,022 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
taskmanager_2  | 2022-11-02 14:09:42,039 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Writer -> Global Committer -> Sink: end (1/1)#0 (d1372739faeaa4f7c2d5dc3157c38091) switched from DEPLOYING to INITIALIZING.
taskmanager_2  | 2022-11-02 14:09:42,045 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@39a0f61
taskmanager_2  | 2022-11-02 14:09:42,046 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
taskmanager_2  | 2022-11-02 14:09:42,046 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
taskmanager_2  | 2022-11-02 14:09:42,048 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_table[1] (1/1)#0 (7a150a66716e80d16e4f4d21450a843e) switched from DEPLOYING to INITIALIZING.
flink          | 2022-11-02 14:09:42,048 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Writer -> Global Committer -> Sink: end (1/1) (d1372739faeaa4f7c2d5dc3157c38091) switched from DEPLOYING to INITIALIZING.
flink          | 2022-11-02 14:09:42,051 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_table[1] (1/1) (7a150a66716e80d16e4f4d21450a843e) switched from DEPLOYING to INITIALIZING.
taskmanager_2  | 2022-11-02 14:09:42,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_table[1] (1/1)#0 (7a150a66716e80d16e4f4d21450a843e) switched from INITIALIZING to RUNNING.
flink          | 2022-11-02 14:09:42,194 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_table[1] (1/1) (7a150a66716e80d16e4f4d21450a843e) switched from INITIALIZING to RUNNING.
taskmanager_2  | 2022-11-02 14:09:42,549 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4c0769d9
taskmanager_2  | 2022-11-02 14:09:42,549 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
taskmanager_2  | 2022-11-02 14:09:42,549 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
taskmanager_2  | 2022-11-02 14:09:42,550 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1)#0 (e234b4060f0956d22ad81cf7edf1197d) switched from DEPLOYING to INITIALIZING.
flink          | 2022-11-02 14:09:42,552 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1) (e234b4060f0956d22ad81cf7edf1197d) switched from DEPLOYING to INITIALIZING.
taskmanager_2  | 2022-11-02 14:09:42,587 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
taskmanager_2  | 2022-11-02 14:09:42,601 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
taskmanager_2  | 2022-11-02 14:09:42,646 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1)#0 (e234b4060f0956d22ad81cf7edf1197d) switched from INITIALIZING to RUNNING.
flink          | 2022-11-02 14:09:42,649 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate[3] -> ConstraintEnforcer[4] (1/1) (e234b4060f0956d22ad81cf7edf1197d) switched from INITIALIZING to RUNNING.
taskmanager_2  | 2022-11-02 14:09:42,664 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Writer -> Global Committer -> Sink: end (1/1)#0 (d1372739faeaa4f7c2d5dc3157c38091) switched from INITIALIZING to RUNNING.
flink          | 2022-11-02 14:09:42,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Writer -> Global Committer -> Sink: end (1/1) (d1372739faeaa4f7c2d5dc3157c38091) switched from INITIALIZING to RUNNING.
flink          | 2022-11-02 14:09:44,421 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398184398 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:09:44,550 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.MemoryManagerImpl [] - orc.rows.between.memory.checks=5000
taskmanager_2  | 2022-11-02 14:09:44,564 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.OrcCodecPool [] - Got brand-new codec LZ4
taskmanager_2  | 2022-11-02 14:09:44,635 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 88bc8525-373e-4037-b4dd-c6dd5f58d73f with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:09:44,904 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=498 ms, finalizationTime=8 ms).
flink          | 2022-11-02 14:09:54,398 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398194396 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:09:54,591 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 574446aa-36c2-49aa-a0d6-edc3b8b72c56 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:09:54,618 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=218 ms, finalizationTime=4 ms).
flink          | 2022-11-02 14:10:04,362 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 3 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398204361 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:10:04,521 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: dac80290-db56-4ccb-8075-b3c75c11adea with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:04,546 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 3 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=184 ms, finalizationTime=1 ms).
flink          | 2022-11-02 14:10:07,010 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'collect' (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:07,010 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'collect' (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:07,013 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
flink          | 2022-11-02 14:10:07,015 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'collect' (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:07,018 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for collect (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:07,022 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job collect (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:07,023 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
flink          | 2022-11-02 14:10:07,096 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 1 ms, total 1 pipelined regions currently.
flink          | 2022-11-02 14:10:07,100 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@351187aa
flink          | 2022-11-02 14:10:07,100 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as BatchExecutionStateBackend
flink          | 2022-11-02 14:10:07,101 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@218b842e
flink          | 2022-11-02 14:10:07,102 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
flink          | 2022-11-02 14:10:07,105 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@34f4197c for collect (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:07,105 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'collect' (b99dc3a03951e172bab40bdc3867a11a) under job master id 00000000000000000000000000000000.
flink          | 2022-11-02 14:10:07,107 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: word_count[5].
flink          | 2022-11-02 14:10:07,565 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
flink          | 2022-11-02 14:10:07,565 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job collect (b99dc3a03951e172bab40bdc3867a11a) switched from state CREATED to RUNNING.
flink          | 2022-11-02 14:10:07,566 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1) (2f57d5f86e90495f0103c37122559ca1) switched from CREATED to SCHEDULED.
flink          | 2022-11-02 14:10:07,567 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@flink:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
flink          | 2022-11-02 14:10:07,568 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
flink          | 2022-11-02 14:10:07,569 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@flink:6123/user/rpc/jobmanager_3 for job b99dc3a03951e172bab40bdc3867a11a.
flink          | 2022-11-02 14:10:07,571 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink:6123/user/rpc/jobmanager_3 for job b99dc3a03951e172bab40bdc3867a11a.
flink          | 2022-11-02 14:10:07,572 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
flink          | 2022-11-02 14:10:07,573 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job b99dc3a03951e172bab40bdc3867a11a: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
taskmanager_1  | 2022-11-02 14:10:07,579 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 9eaf993bc7b1eb47e8cce2ce7ef851ce for job b99dc3a03951e172bab40bdc3867a11a from resource manager with leader id 00000000000000000000000000000000.
taskmanager_1  | 2022-11-02 14:10:07,585 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 9eaf993bc7b1eb47e8cce2ce7ef851ce.
taskmanager_1  | 2022-11-02 14:10:07,587 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job b99dc3a03951e172bab40bdc3867a11a for job leader monitoring.
taskmanager_1  | 2022-11-02 14:10:07,589 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@flink:6123/user/rpc/jobmanager_3 with leader id 00000000-0000-0000-0000-000000000000.
taskmanager_1  | 2022-11-02 14:10:07,608 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
taskmanager_1  | 2022-11-02 14:10:07,623 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@flink:6123/user/rpc/jobmanager_3 for job b99dc3a03951e172bab40bdc3867a11a.
taskmanager_1  | 2022-11-02 14:10:07,624 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job b99dc3a03951e172bab40bdc3867a11a.
taskmanager_1  | 2022-11-02 14:10:07,628 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job b99dc3a03951e172bab40bdc3867a11a.
flink          | 2022-11-02 14:10:07,633 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1) (2f57d5f86e90495f0103c37122559ca1) switched from SCHEDULED to DEPLOYING.
flink          | 2022-11-02 14:10:07,633 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1) (attempt #0) with attempt id 2f57d5f86e90495f0103c37122559ca1 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 172.23.0.6:42189-46dab1 @ flink-sandbox_taskmanager_1.flink-sandbox_default (dataPort=35461) with allocation id 9eaf993bc7b1eb47e8cce2ce7ef851ce
taskmanager_1  | 2022-11-02 14:10:07,642 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9eaf993bc7b1eb47e8cce2ce7ef851ce.
taskmanager_1  | 2022-11-02 14:10:07,661 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
taskmanager_1  | 2022-11-02 14:10:07,681 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1), deploy into slot with allocation id 9eaf993bc7b1eb47e8cce2ce7ef851ce.
taskmanager_1  | 2022-11-02 14:10:07,682 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1) switched from CREATED to DEPLOYING.
taskmanager_1  | 2022-11-02 14:10:07,687 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1) [DEPLOYING].
taskmanager_1  | 2022-11-02 14:10:07,691 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9eaf993bc7b1eb47e8cce2ce7ef851ce.
taskmanager_1  | 2022-11-02 14:10:07,695 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading b99dc3a03951e172bab40bdc3867a11a/p-2fcfa81de948a930c635827bf3c1c2dd73a1cc34-a67d489b268205f020dffb083ac03b73 from flink/172.23.0.4:6124
taskmanager_1  | 2022-11-02 14:10:07,709 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading b99dc3a03951e172bab40bdc3867a11a/p-35ffaba250050d7c3202364ee51d6170823060af-4a4384fee2d2573445ec6a38d289c704 from flink/172.23.0.4:6124
taskmanager_1  | 2022-11-02 14:10:07,969 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@58668db5
taskmanager_1  | 2022-11-02 14:10:07,969 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as BatchExecutionStateBackend
taskmanager_1  | 2022-11-02 14:10:07,974 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@3f8ed41d
taskmanager_1  | 2022-11-02 14:10:07,985 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1) switched from DEPLOYING to INITIALIZING.
flink          | 2022-11-02 14:10:07,990 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1) (2f57d5f86e90495f0103c37122559ca1) switched from DEPLOYING to INITIALIZING.
taskmanager_1  | 2022-11-02 14:10:08,470 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Initializing collect sink state with offset = 0, buffered results bytes = 0
taskmanager_1  | 2022-11-02 14:10:08,475 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Collect sink server established, address = /172.23.0.6:46067
flink          | 2022-11-02 14:10:08,483 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator [] - Received sink socket server address: /172.23.0.6:46067
flink          | 2022-11-02 14:10:08,495 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: word_count[5] registering reader for parallel task 0 @ 172.23.0.6
flink          | 2022-11-02 14:10:08,496 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: word_count[5] received split request from parallel task 0
taskmanager_1  | 2022-11-02 14:10:08,497 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1) switched from INITIALIZING to RUNNING.
flink          | 2022-11-02 14:10:08,501 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1) (2f57d5f86e90495f0103c37122559ca1) switched from INITIALIZING to RUNNING.
taskmanager_1  | 2022-11-02 14:10:08,511 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
taskmanager_1  | 2022-11-02 14:10:08,524 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
taskmanager_1  | 2022-11-02 14:10:08,528 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1) switched from RUNNING to FINISHED.
taskmanager_1  | 2022-11-02 14:10:08,528 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 (2f57d5f86e90495f0103c37122559ca1).
taskmanager_1  | 2022-11-02 14:10:08,530 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1)#0 2f57d5f86e90495f0103c37122559ca1.
flink          | 2022-11-02 14:10:08,547 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: word_count[5] -> ConstraintEnforcer[6] -> Sink: Collect table sink (1/1) (2f57d5f86e90495f0103c37122559ca1) switched from RUNNING to FINISHED.
flink          | 2022-11-02 14:10:08,551 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job collect (b99dc3a03951e172bab40bdc3867a11a) switched from state RUNNING to FINISHED.
flink          | 2022-11-02 14:10:08,551 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job b99dc3a03951e172bab40bdc3867a11a
flink          | 2022-11-02 14:10:08,551 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job b99dc3a03951e172bab40bdc3867a11a.
flink          | 2022-11-02 14:10:08,559 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job b99dc3a03951e172bab40bdc3867a11a reached terminal state FINISHED.
flink          | 2022-11-02 14:10:08,579 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job b99dc3a03951e172bab40bdc3867a11a has been registered for cleanup in the JobResultStore after reaching a terminal state.
flink          | 2022-11-02 14:10:08,582 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'collect' (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:08,588 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: word_count[5].
flink          | 2022-11-02 14:10:08,589 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: word_count[5] closed.
flink          | 2022-11-02 14:10:08,593 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
flink          | 2022-11-02 14:10:08,594 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [9eaf993bc7b1eb47e8cce2ce7ef851ce].
flink          | 2022-11-02 14:10:08,595 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 3fa14cf24c77ac8d356de9bc3b5b07ae: Stopping JobMaster for job 'collect' (b99dc3a03951e172bab40bdc3867a11a).
flink          | 2022-11-02 14:10:08,597 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@flink:6123/user/rpc/jobmanager_3 for job b99dc3a03951e172bab40bdc3867a11a from the resource manager.
taskmanager_1  | 2022-11-02 14:10:08,605 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: 9eaf993bc7b1eb47e8cce2ce7ef851ce, jobId: b99dc3a03951e172bab40bdc3867a11a).
taskmanager_1  | 2022-11-02 14:10:08,608 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job b99dc3a03951e172bab40bdc3867a11a from job leader monitoring.
taskmanager_1  | 2022-11-02 14:10:08,611 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job b99dc3a03951e172bab40bdc3867a11a.
flink          | 2022-11-02 14:10:14,362 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 4 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398214360 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:10:14,532 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 5b845b0e-2915-4dfb-80d9-d669d9d638e5 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:14,550 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 4 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=190 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:10:24,362 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398224361 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:10:24,563 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 4b3812a4-94dd-46cd-9984-d1fee2e7afac with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:24,599 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=237 ms, finalizationTime=1 ms).
taskmanager_2  | 2022-11-02 14:10:24,661 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-4.orc with {include: [false, true, true, true, true, true], offset: 3, length: 402, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:10:24,685 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.OrcCodecPool [] - Got brand-new codec LZ4
taskmanager_2  | 2022-11-02 14:10:24,686 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-2.orc with {include: [false, true, true, true, true, true], offset: 3, length: 402, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:10:24,687 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.OrcCodecPool [] - Got brand-new codec LZ4
taskmanager_2  | 2022-11-02 14:10:24,687 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-1.orc with {include: [false, true, true, true, true, true], offset: 3, length: 401, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:10:24,688 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.OrcCodecPool [] - Got brand-new codec LZ4
taskmanager_2  | 2022-11-02 14:10:24,689 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-0.orc with {include: [false, true, true, true, true, true], offset: 3, length: 383, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:10:24,690 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.OrcCodecPool [] - Got brand-new codec LZ4
taskmanager_2  | 2022-11-02 14:10:24,690 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-3.orc with {include: [false, true, true, true, true, true], offset: 3, length: 402, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:10:24,713 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.MemoryManagerImpl [] - orc.rows.between.memory.checks=5000
taskmanager_2  | 2022-11-02 14:10:24,714 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.OrcCodecPool [] - Got brand-new codec LZ4
taskmanager_2  | 2022-11-02 14:10:24,715 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 20f29b63-c459-489a-adf1-f394ed078e66 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:34,326 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398234326 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:10:34,487 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 85a8279b-961c-43a2-9938-45fc06431e6b with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:34,513 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job 0c5f22c2ab3e83e1a1f9274818ff675b (9883 bytes, checkpointDuration=186 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:10:44,327 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398244325 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:10:44,491 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 05e6b1e3-2282-4299-b1df-f71b271a0629 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:44,510 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=185 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:10:54,326 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 8 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398254325 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:10:54,520 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 8149f4f8-5f71-41a5-bf6a-6b0f492b8366 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:10:54,536 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 8 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=211 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:11:04,291 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 9 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398264290 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:11:04,428 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 13526b37-54e5-4db0-a12f-0e55c0e641bf with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:04,448 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 9 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7315 bytes, checkpointDuration=158 ms, finalizationTime=0 ms).
taskmanager_2  | 2022-11-02 14:11:04,465 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-8.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:04,467 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-6.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:04,468 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-5.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:04,469 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-67352189-bdc4-4305-9504-c7cf2d4f7d0c-0.orc with {include: [false, true, true, true, true, true], offset: 3, length: 402, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:04,470 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-7.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:04,472 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: aa2e96ce-91d8-4691-83af-e640d4dda870 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:14,292 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 10 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398274290 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:11:14,464 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: bc74d0fb-084b-4ad6-8a44-fc7f379decd9 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:14,484 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 10 for job 0c5f22c2ab3e83e1a1f9274818ff675b (9884 bytes, checkpointDuration=194 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:11:24,291 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 11 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398284290 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:11:24,428 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 2174fcd8-87e2-43d4-a8b2-16ee8cef2c78 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:24,444 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 11 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7316 bytes, checkpointDuration=154 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:11:34,256 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 12 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398294255 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:11:34,413 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: ce1e4a3a-804d-4dfe-9b71-94b18f0288c6 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:34,429 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 12 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7316 bytes, checkpointDuration=174 ms, finalizationTime=0 ms).
flink          | 2022-11-02 14:11:44,255 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 13 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398304255 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:11:44,408 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 60ad313d-d8ce-4b0b-96db-877d6b3fc18a with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:44,424 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 13 for job 0c5f22c2ab3e83e1a1f9274818ff675b (7316 bytes, checkpointDuration=168 ms, finalizationTime=1 ms).
taskmanager_2  | 2022-11-02 14:11:44,442 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-12.orc with {include: [false, true, true, true, true, true], offset: 3, length: 421, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:44,443 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-10.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:44,444 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-9.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:44,445 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-67352189-bdc4-4305-9504-c7cf2d4f7d0c-1.orc with {include: [false, true, true, true, true, true], offset: 3, length: 418, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:44,446 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl [] - Reading ORC rows from file:/tmp/table_store/default.db/word_count/bucket-0/data-c818e7f8-5688-46ce-a1c2-441bc788e589-11.orc with {include: [false, true, true, true, true, true], offset: 3, length: 421, schema: struct<_KEY_word:string,_SEQUENCE_NUMBER:bigint,_VALUE_KIND:tinyint,word:string,cnt:bigint>, includeAcidColumns: true}
taskmanager_2  | 2022-11-02 14:11:44,448 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: eada9552-1ebb-4e2b-a8d1-78a19cc63b69 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:54,256 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 14 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667398314255 for job 0c5f22c2ab3e83e1a1f9274818ff675b.
taskmanager_2  | 2022-11-02 14:11:54,413 INFO  org.apache.flink.table.store.shaded.org.apache.orc.impl.WriterImpl [] - ORC writer created for path: 6f1a8224-91e1-4727-b024-a54f22066842 with stripeSize: 67108864 blockSize: 268435456 compression: LZ4 bufferSize: 262144
flink          | 2022-11-02 14:11:54,427 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 14 for job 0c5f22c2ab3e83e1a1f9274818ff675b (9884 bytes, checkpointDuration=172 ms, finalizationTime=0 ms).
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 13:56:11 UTC 2022,,,,,,,,,,"0|z1aqq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 19:08;knaufk;I can reproduce this.;;;","03/Nov/22 00:36;zjureel;Hi [~knaufk] I try the steps above with flink 1.15.2 and table sotre 0.2.1, it works. Can you show me more informations about streaming job `insert-into_my_catalog.default.word_count` in flink ui? THX;;;","04/Nov/22 13:56;Gerrrr;After debugging this for a bit, I know why this error happened. In my setup, 2 TMs and a JM were running in separate containers. As the example I attached in the issue description uses a local filesystem, Flink Table Store splits data between the TMs' local filesystems:

Say, there were only 2 inserts:

{noformat}
INSERT INTO word_count VALUES ('foo', 1);
INSERT INTO word_count VALUES ('bar', 2);
{noformat}

The files on the TM file systems looked like this:

{noformat}
✗ docker exec -it flink-sandbox_taskmanager_1 bash
root@162c299b3173:/opt/flink# ls /tmp/filesystem_store/
'word=foo'

✗ docker exec -it flink-sandbox_taskmanager_2 bash
root@bba60c909ee0:/opt/flink# ls /tmp/filesystem_store/
'word=bar'
{noformat}

After I plugged in an object store, the quick start guide worked like a charm. FWIW making TMs and JM share the tmp directory worked as well. Since the issue was caused by user misconfiguration, we can close it.

Even though Flink Table Store started to work after making JM and TMs shared the tmp directory, it is not clear to me why this is the case. I am new to Flink and would appreciate it if someone could elaborate why {{SELECT * FROM word_count}} returned nothing when the data was partitioned between TMs.;;;",,,,,,,,,,,,,,,,,,,,
Event time temporal join on an upsert source may produce incorrect execution plan,FLINK-29849,13494261,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,02/Nov/22 08:27,21/Feb/23 06:21,04/Jun/24 20:41,21/Dec/22 07:23,1.15.3,1.16.0,,,,,1.16.1,1.17.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,"For current implementation, the execution plan is incorrect when do event time temporal join on an upsert source. There's two problems:
1.  for an upsert source, we should not add a ChangelogNormalize node under a temporal join input, or it will damage the versions of the version table. For versioned tables, we use a single-temporal mechanism which relies sequencial records of a same key to ensure the valid period of each version, so if the ChangelogNormalize was added then an UB message will be produced based on the previous  UA or Insert message, and all the columns are totally same include event time, e.g., 
original upsert input
{code}
+I (key1, '2022-11-02 10:00:00', a1)
+U (key1, '2022-11-02 10:01:03', a2)
{code}

the versioned data should be:
{code}
v1  [~, '2022-11-02 10:00:00')
v2  ['2022-11-02 10:00:00', '2022-11-02 10:01:03')
{code}

after ChangelogNormalize's processing, will output:
{code}
+I (key1, '2022-11-02 10:00:00', a1)
-U (key1, '2022-11-02 10:00:00', a1)
+U (key1, '2022-11-02 10:01:03', a2)
{code}

versions are incorrect:
{code}
v1  ['2022-11-02 10:00:00', '2022-11-02 10:00:00')  // invalid period
v2  ['2022-11-02 10:00:00', '2022-11-02 10:01:03')
{code}

2. semantically, a filter cannot be pushed into an event time temporal join, otherwise, the filter may also corrupt the versioned table
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 06:17:19 UTC 2023,,,,,,,,,,"0|z1aq9c:",9223372036854775807,"This resolves the correctness issue when do event time temporal join with a versioned table backed by an upsert source. When the right input of the join is an upsert source, it no longer generates a ChangelogNormalize node for it.
Note this is an incompatible plan change compare to 1.16.0",,,,,,,,,,,,,,,,,,,"21/Dec/22 07:23;godfrey;Fixed in master: eb44ac01c9969cb22ab832b6b2155b109f015b06

1.16.1: dbb6654c9d211e0944a0a1f58921c12bda6916cf;;;","21/Feb/23 06:17;lincoln.86xy;Just for record: the conclusions of reviewing the semantic impact of filter-related optimizations/rewrites on streaming scenarios(filter pushdown disturb time attributes). The following releated operations were checked:

1. temporal join, including eventtime and proctime(proctime temporal join is unsupported yet, one similar solution is lookup join), the essence is that filter should not go through snapshot to pre-filter data which may destroy the data version (corresponding to the original data may be changed over time), otherwise it affects the result. FLINK-28988 prevent the filter push down through an eventime temporal join. 
2. window related, does not affect the result, but only affects the density of watermark generation and the timing of the output result

3. interval join, does not affect the result(the on condition in left join can be pushdown, the post where condition can change join to inner join)
4. deduplication, does not affect the result, the current filter push down already consider the 'containsOver' protection judgment;;;",,,,,,,,,,,,,,,,,,,,,
Port BinaryRowDataUtil to Flink Table Store,FLINK-29848,13494259,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,02/Nov/22 08:05,10/Nov/22 16:15,04/Jun/24 20:41,04/Nov/22 04:07,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Port org.apache.flink.table.data.binary.BinaryRowDataUtil to flink table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 04:07:59 UTC 2022,,,,,,,,,,"0|z1aq8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 04:07;TsReaper;master: 18c01207839a6b2eade3a010538a4aa1610a2eb6;;;",,,,,,,,,,,,,,,,,,,,,,
Store/cache JarUploadResponseBody,FLINK-29847,13494258,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,darenwkt,darenwkt,02/Nov/22 08:03,07/May/23 15:17,04/Jun/24 20:41,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Kubernetes operator currently uploadJar always even when the same JAR has been uploaded to JM previously. For example, this occurs during rollback and suspend-run operation.

To improve the performance, we want to cache the JarUploadResponseBody so that in the next runJar operation, kubernetes operator will check the cache and reuse the jar if it's the same JAR. This ""cache"" can be as simple as storing the uploaded JarFilePath in the CR Status field.

Ref: [https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L188-L199]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 07 15:17:54 UTC 2023,,,,,,,,,,"0|z1aq8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 08:30;Wencong Liu;Hello [~darenwkt] , I'm wondering how to identify that two jars are same?;;;","07/May/23 15:17;zxcoccer;hi, I would like to give a try on this, can  I take this ticket?I will try my best to complete it
 ;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade Archunit to 1.0.0,FLINK-29846,13494252,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,02/Nov/22 07:40,12/Dec/22 16:05,04/Jun/24 20:41,12/Dec/22 16:05,,,,,,,1.17.0,,,,Build System,Tests,,,0,pull-request-available,,,,"Flink still uses Archunit version 0.22.0. Recently Archunit 1.0.0 has been introduced; we should upgrade to this major version and remove all the deprecated usages",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 16:05:02 UTC 2022,,,,,,,,,,"0|z1aq7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 08:58;Sergey Nuyanzin;Hi [~martijnvisser]
do you mind if i help with this one?;;;","02/Nov/22 10:18;martijnvisser;[~Sergey Nuyanzin] Not at all, I've assigned it to you;;;","12/Dec/22 16:05;chesnay;master: ba6c0c7da89746511c567bb12ff62b9315ecfd9b;;;",,,,,,,,,,,,,,,,,,,,
ThroughputCalculator throws java.lang.IllegalArgumentException: Time should be non negative under very low throughput cluster,FLINK-29845,13494245,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,dawnwords,dawnwords,02/Nov/22 07:32,15/Aug/23 10:35,04/Jun/24 20:41,,1.14.6,,,,,,,,,,Runtime / Network,Runtime / Task,,,0,pull-request-available,stale-assigned,,,"Our team are using Flink@1.14.6 to process data from Kafka.

It works all fine unless the same job jar with same arguments deployed in an environment with{color:#ff0000} *very low kafka source throughput.*{color} The job crashed sometimes with the following Exception and could not be able to recover unless we restarted TaskManagers, which is unacceptable for a production environment.
{code:java}
[2022-10-31T15:33:57.153+08:00] [o.a.f.runtime.taskmanager.Task#cess (2/16)#244] - [WARN ] KeyedProcess (2/16)#244 (b9b54f6445419fc43c4d58fcd95cee82) switched from RUNNING to FAILED with failure cause: java.lang.IllegalArgumentException: Time should be non negative
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.runtime.throughput.ThroughputEMA.calculateThroughput(ThroughputEMA.java:44)
	at org.apache.flink.runtime.throughput.ThroughputCalculator.calculateThroughput(ThroughputCalculator.java:80)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.debloat(StreamTask.java:789)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$4(StreamTask.java:781)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
{code}
After checking the source code roughly, we found if buffer debloating is disabled ([https://github.com/apache/flink/blob/release-1.14.6/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L427] ), the buffer debloater will still be scheduled ([https://github.com/apache/flink/blob/release-1.14.6/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L755] ) so that the {{ThrouputCalculator}}  keeps calculating the throughput ([https://github.com/apache/flink/blob/release-1.14.6/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L789] ) which causes the division of zero and seems useless as i suppose.

Currently, we tried to workaround by setting {{taskmanager.network.memory.buffer-debloat.period: 365d}} to avoid the buffer debloater being scheduled frequently causing the random crash.

P.S. We found a bug with similar stacktrace https://issues.apache.org/jira/browse/FLINK-25454 which was fixed in 1.14.6.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25454,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:10 UTC 2023,,,,,,,,,,"0|z1aq5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 08:17;chesnay;Did you actually still see this issue 1.14.6 or a previous version?;;;","02/Nov/22 08:41;dawnwords;[~chesnay] yes in 1.14.6, and actually our stacktrace are not exactly the same with FLINK-25454;;;","17/Nov/22 09:56;kevin.cyj;I think it is not guaranteed that System.currentTimeMillis is monotonic. It is better to give it a fix. Do you already have a fix? [~dawnwords] ;;;","22/Dec/22 08:40;kevin.cyj;[~dawnwords] Would you like to fix it? If not, we will assign this issue to other contributors.;;;","06/Jan/23 05:59;Weijie Guo;[~dawnwords] Thanks for reporting this.

After FLINK-24189, BufferDebloater will not be scheduled if `buffer-debloat.enabled` is false. So, I think this problem is fixed in flink-1.15`.

As for the concerns raised by [~kevin.cyj] , I also think it's better to give it a fix. If you don't have enough time, I can fix this.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
FLIP-263: Improve resolving schema compatibility,FLINK-29844,13494219,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,masteryhx,masteryhx,02/Nov/22 04:47,15/Jan/24 08:47,04/Jun/24 20:41,,,,,,,,,,,,API / Type Serialization System,,,,0,,,,,"Details can be seen in
https://cwiki.apache.org/confluence/display/FLINK/FLIP-263%3A+Improve+resolving+schema+compatibility",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 11:20:38 UTC 2023,,,,,,,,,,"0|z1aq00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:20;knaufk;I will mark this as ""Not finished"" for Flink 1.18 and remove the fixVersion from the ticket as the feature freeze has passed. Thanks, Konstantin (one of the release managers for Flink 1.18);;;",,,,,,,,,,,,,,,,,,,,,,
Euclidean Distance Measure generates NAN distance values,FLINK-29843,13494217,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,02/Nov/22 04:17,10/Jan/23 05:17,04/Jun/24 20:41,02/Nov/22 07:07,ml-2.1.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Currently Flink ML's `EuclideanDistanceMeasure.distance(...)` method might return a negative value as the distance between two vectors given the calculation accuracy of java doubles. This bug should be fixed to guarantee that the distance is a non-negative value.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-02 04:17:21.0,,,,,,,,,,"0|z1apzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change commitIdentifier in Table Store snapshot to long value,FLINK-29842,13494213,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,02/Nov/22 03:22,08/Nov/22 06:12,04/Jun/24 20:41,08/Nov/22 06:12,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Currently {{commitIdentifier}} in {{Snapshot}} is a {{String}} value. However there are many scenarios where we need to compare two identifiers to find out which one is newer. For example
* In FLINK-29840, we need to store the latest modified commit for each writer. Only when the latest snapshot is newer than this commit can we safely close the writer.
* In FLINK-29805, we can read the commit identifier of the latest snapshot. All identifiers older than that should be filtered out.
* In FLINK-29752, we need to trigger full compaction once in a few commits. We can read the latest commit identifier and compare it with the full compaction identifier to check if full compaction is successfully committed.
 ",,,,,,,,,,,,,,FLINK-29840,FLINK-29805,FLINK-29752,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 06:12:16 UTC 2022,,,,,,,,,,"0|z1apyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 06:12;TsReaper;master: 350627439a70b187f7270278b497841fa3f2c554
release-0.2: bc5646cc62bb341954d6320c7f3ed375195e747f;;;",,,,,,,,,,,,,,,,,,,,,,
docs/content/docs/deployment/config.md has error,FLINK-29841,13494212,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Wencong Liu,hcj,hcj,02/Nov/22 03:21,11/May/23 08:07,04/Jun/24 20:41,11/May/23 08:07,1.18.0,,,,,,1.18.0,,,,Documentation,,,,0,pull-request-available,,,,"line 64 in docs/content/docs/deployment/config.md has error.

line 64 is :

These value are configured as memory sizes, for example *1536m* or *2g*.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 08:07:15 UTC 2023,,,,,,,,,,"0|z1apyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 03:33;leonard;[~hcj] I checked our docs website, the display is ok. What errors do you mean?;;;","02/Nov/22 03:46;hcj;[~leonard] ""these values are"" is  more appropriate?;;;","18/Apr/23 08:46;Wencong Liu;I'd like to fix this. WDYT? [~Weijie Guo] ;;;","11/May/23 08:07;Weijie Guo;master(1.18) via 69aca3e064bd30efa45802fbc9f14c2885e4fd1e.;;;",,,,,,,,,,,,,,,,,,,
Old record may overwrite new record in Table Store when snapshot committing is slow,FLINK-29840,13494210,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,02/Nov/22 03:12,09/Nov/22 08:08,04/Jun/24 20:41,09/Nov/22 08:08,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Consider the following scenario when snapshot committing is slow:
* A writer produces some records at checkpoint T.
* It produces no record at checkpoint T+1 and is closed.
* It produces some records at checkpoint T+2. It will be reopened and read the latest sequence number from disk. However snapshot at checkpoint T may not be committed so the sequence number it reads might be too small.

In this scenario, records from checkpoint T may overwrite records from checkpoint T+2 because they have larger sequence numbers.",,,,,,,,,,,,FLINK-29842,FLINK-29943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 08:08:29 UTC 2022,,,,,,,,,,"0|z1apy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 08:08;TsReaper;master: f54008c98f51e419fa4a80d6fe0e62ab97358c54
release-0.2: fe0f333f5289081ca210540576e81c469ba18dd6;;;",,,,,,,,,,,,,,,,,,,,,,
HiveServer2 endpoint doesn't support TGetInfoType value 'CLI_ODBC_KEYWORDS',FLINK-29839,13494207,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,libra_816,libra_816,02/Nov/22 02:53,09/Dec/22 03:35,04/Jun/24 20:41,10/Nov/22 03:46,1.16.0,,,,,,1.16.1,1.17.0,,,Connectors / Hive,Table SQL / Gateway,,,0,,,,," I had starting the SQL Gateway with the HiveServer2 Endpoint, and then I submit SQL with Hive Beeline, but I get the following exception:
{code:java}
java.lang.UnsupportedOperationException: Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.
at org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.GetInfo(HiveServer2Endpoint.java:371) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1537) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1522) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]
2022-11-01 13:55:33,885 ERROR org.apache.thrift.server.TThreadPoolServer                   [] - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'infoValue' is unset! Struct:TGetInfoResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*java.lang.UnsupportedOperationException:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.:9:8, org.apache.flink.table.endpoint.hive.HiveServer2Endpoint:GetInfo:HiveServer2Endpoint.java:371, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1537, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1522, org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39, org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39, org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286, java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1128, java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:628, java.lang.Thread:run:Thread.java:834], errorMessage:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.), infoValue:null)
at org.apache.hive.service.rpc.thrift.TGetInfoResp.validate(TGetInfoResp.java:379) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.validate(TCLIService.java:5228) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5285) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5254) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.write(TCLIService.java:5205) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]
2022-11-01 13:55:33,886 WARN  org.apache.thrift.transport.TIOStreamTransport               [] - Error closing output stream.
java.net.SocketException: Socket closed
at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113) ~[?:?]
at java.net.SocketOutputStream.write(SocketOutputStream.java:150) ~[?:?]
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81) ~[?:?]
at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142) ~[?:?]
at java.io.FilterOutputStream.close(FilterOutputStream.java:182) ~[?:?]
at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.transport.TSocket.close(TSocket.java:235) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:303) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]
{code}

 I found the following code is where the exception is thrown, but I don’t know how to solve it.

{code:java}
public TGetInfoResp GetInfo(TGetInfoReq tGetInfoReq) throws TException {
    TGetInfoResp resp = new TGetInfoResp();
try {
        GatewayInfo info = service.getGatewayInfo();
TGetInfoValue tInfoValue;
switch (tGetInfoReq.getInfoType()) {
            case CLI_SERVER_NAME:
            case CLI_DBMS_NAME:
                tInfoValue = TGetInfoValue.stringValue(info.getProductName());
             break;
            case CLI_DBMS_VER:
                tInfoValue = TGetInfoValue.stringValue(info.getVersion().toString());
             break;
            default:
                throw new UnsupportedOperationException(
                        String.format(
                                ""Unrecognized TGetInfoType value: %s."",
tGetInfoReq.getInfoType()));
}
        resp.setStatus(OK_STATUS);
resp.setInfoValue(tInfoValue);
} catch (Throwable t) {
        LOG.error(""Failed to GetInfo."", t);
resp.setStatus(toTStatus(t));
}
    return resp;
}
{code}

CLI_ODBC_KEYWORDS----- It's a new definition in the TGetInfoType enumeration class in the Hive dependency package. It seems to be in a high version, but my Hive environment and Flink reference connector should be 3.1.2, which is available to the enumeration Definition, but from the source code of Flink-1.16.0, only three enumerations of TGetInfoType are supported: CLI_SERVER_NAME, CLI_DBMS_NAME, CLI_DBMS_VER.
Is it a problem that Flink-1.16.0 does not support, or is there something wrong with my environment or configuration? How can i fix it?


","Flink version: 1.16.0
Hive version: 3.1.2",,,,,,,,,,,,,,,,,,,FLINK-29857,,,,,,,,FLINK-30347,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 09:15:08 UTC 2022,,,,,,,,,,"0|z1apxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 03:29;leonard;CC [~fsk119] Could you take a look this issue？;;;","02/Nov/22 08:26;yzl;Hi, I have replied in mailing list. Please check out.;;;","02/Nov/22 09:15;libra_816;[~yzl] Thank you. I got it;;;",,,,,,,,,,,,,,,,,,,,
Hive streaming sink for partitioned table should contain metastore by default,FLINK-29838,13494198,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,02/Nov/22 01:56,02/Nov/22 01:56,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Hive,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-02 01:56:46.0,,,,,,,,,,"0|z1apvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL API does not expose the RowKind of the Row for processing Changelogs,FLINK-29837,13494190,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,eric.xiao,eric.xiao,01/Nov/22 23:58,03/Nov/22 15:03,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / API,,,,0,,,,,"When working with `{{{}ChangeLog{}}}` data in the SQL API it was a bit misleading to see that the `{{{}op{}}}` column appears{^}[1]{^}  the type of  in the table schema of print results but it is not available to be used in a the SQL API:
{code:java}
val tableEnv = StreamTableEnvironment.create(env)

val dataStream = env.fromElements(
  Row.ofKind(RowKind.INSERT, ""Alice"", Int.box(12)),
  Row.ofKind(RowKind.INSERT, ""Bob"", Int.box(5)),
  Row.ofKind(RowKind.UPDATE_AFTER, ""Alice"", Int.box(100))
)(Types.ROW(Types.STRING, Types.INT))

// interpret the DataStream as a Table
val table =
  tableEnv.fromChangelogStream(dataStream, Schema.newBuilder().primaryKey(""f0"").build(), ChangelogMode.upsert())

// register the table under a name and perform an aggregation
tableEnv.createTemporaryView(""InputTable"", table)
tableEnv
  .sqlQuery(""SELECT * FROM InputTable where op = '+I'"")
  .execute()
  .print() {code}
The error logs.

 

 
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 32 to line 1, column 33: Column 'op' not found in any table
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:184)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:109)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:237)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:105)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:675){code}
It would be nice to expose the `op` column to be usable in the Flink SQL APIs as it is in the DataStream APIs.

[1] [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/data_stream_api/#examples-for-fromchangelogstream] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 03 15:03:11 UTC 2022,,,,,,,,,,"0|z1aptk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 02:54;luoyuxia;> it was a bit misleading to see that the `{{{}op{}}}` column appears{^}[1]{^}  the type of  in the table schema of print results

Maybe is was a bit misleadning, but please remember it doesn't mean the phsical `{{{}op{}}}` column exist. It  just the printer's scope to decide how to show the row. 

I'm wondering which we need to expose the `op` column. Could you please explain in which case do we need to expose the `op` column  ?

 ;;;","03/Nov/22 15:03;eric.xiao;Hi [~luoyuxia] thanks for responding!

We are exploring the Flink SQL API and have been translating some of our DataStream API pipelines to Flink SQL. In our pipelines we have business logic that depend on the kind of changelog event this particular row is associated with.

I saw that there is the configuration parameter  [1] that you can pass in `ChangelogMode` to the `fromChangelogStream`, but I don't think that addresses our needs.

 [1] https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/data_stream_api/#handling-of-changelog-streams;;;",,,,,,,,,,,,,,,,,,,,,
PulsarUnorderedSourceITCase#testMultipleSplits failed due to an expected job restart not happening,FLINK-29836,13493985,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,mapohl,mapohl,01/Nov/22 09:45,04/Jan/23 02:52,04/Jun/24 20:41,04/Jan/23 02:52,1.15.3,,,,,,,,,,Connectors / Pulsar,,,,0,test-stability,,,,"[This build failed|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42681&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27377] due to a missing job restart:
{code:java}
 Nov 01 02:04:20 Expecting
Nov 01 02:04:20   <CompletableFuture[Failed with the following stack trace:
Nov 01 02:04:20 java.lang.RuntimeException: Job restarted
Nov 01 02:04:20 	at org.apache.flink.streaming.api.operators.collect.UncheckpointedCollectResultBuffer.sinkRestarted(UncheckpointedCollectResultBuffer.java:42)
Nov 01 02:04:20 	at org.apache.flink.streaming.api.operators.collect.AbstractCollectResultBuffer.dealWithResponse(AbstractCollectResultBuffer.java:87)
Nov 01 02:04:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:147)
Nov 01 02:04:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Nov 01 02:04:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Nov 01 02:04:20 	at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.compareWithExactlyOnceSemantic(UnorderedCollectIteratorAssert.java:108)
Nov 01 02:04:20 	at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.matchesRecordsFromSource(UnorderedCollectIteratorAssert.java:79)
Nov 01 02:04:20 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.lambda$checkResultWithSemantic$0(UnorderedSourceTestSuiteBase.java:53)
Nov 01 02:04:20 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
Nov 01 02:04:20 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
Nov 01 02:04:20 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Nov 01 02:04:20 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Nov 01 02:04:20 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Nov 01 02:04:20 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Nov 01 02:04:20 ]>
Nov 01 02:04:20 to be completed within 2M.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,FLINK-29755,,,FLINK-30340,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 02:52:58 UTC 2023,,,,,,,,,,"0|z1aokg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 06:23;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42809&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","04/Nov/22 15:58;mapohl;[~leonard] is this a mistake or what's the reason you annotated this issue {{1.16.0}} and {{1.17.0}}. So far, there have been only two build failures based on the {{release-1.15}} branch.;;;","04/Nov/22 16:01;mapohl;{code}
java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: Coordinator of operator 7df19f87deec5680128845fd9a6ca18d does not exist
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_292]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.compareWithExactlyOnceSemantic(UnorderedCollectIteratorAssert.java:108) ~[flink-connector-test-utils-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.matchesRecordsFromSource(UnorderedCollectIteratorAssert.java:79) ~[flink-connector-test-utils-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.lambda$checkResultWithSemantic$0(UnorderedSourceTestSuiteBase.java:53) ~[test-classes/:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640) [?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: org.apache.flink.util.FlinkException: Coordinator of operator 7df19f87deec5680128845fd9a6ca18d does not exist
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$deliverCoordinationRequestToCoordinator$20(AdaptiveScheduler.java:726) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_292]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.deliverCoordinationRequestToCoordinator(AdaptiveScheduler.java:724) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.sendRequestToCoordinator(JobMaster.java:572) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.deliverCoordinationRequestToCoordinator(JobMaster.java:890) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at sun.reflect.GeneratedMethodAccessor252.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304) ~[?:?]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[?:?]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[?:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[?:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[?:?]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
        ... 4 more
{code}
There has been a stacktrace like the one above. It seems to be caused by the test collecting the results for a Operator with a not existing OperatorID. Not sure though, whether that is expected or it contributed to the test failure.;;;","04/Nov/22 16:02;mapohl;I'm linking FLINK-29755. They might be caused by the same issue.;;;","04/Nov/22 16:20;leonard;[~mapohl] My mistake, udpated;;;","21/Nov/22 03:34;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43308&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","21/Nov/22 04:36;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43278&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","21/Nov/22 09:33;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43253&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","22/Nov/22 11:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43215&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=28131;;;","23/Nov/22 08:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43399&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=28131;;;","23/Nov/22 08:42;mapohl;Interestingly (after double-checking), this failure really only affects 1.15 builds so far.;;;","29/Nov/22 06:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43573&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27663;;;","30/Nov/22 06:53;mapohl;I moved the issue under the Pulsar stability umbrella ticket.;;;","01/Dec/22 15:23;martijnvisser;https://github.com/apache/flink-connector-pulsar/actions/runs/3593169763/jobs/6049780821#step:9:33272;;;","05/Dec/22 10:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43701&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27405;;;","22/Dec/22 08:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44161&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=28143;;;","04/Jan/23 02:52;tison;After FLINK-30413 dropped the related support, this test was dropped also. Invalid now.;;;",,,,,,
NoClassDefFoundError in PulsarSourceUnorderedE2ECase,FLINK-29835,13493983,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Invalid,,mapohl,mapohl,01/Nov/22 09:36,04/Jan/23 02:53,04/Jun/24 20:41,04/Jan/23 02:53,1.15.3,1.16.0,pulsar-3.0.0,pulsar-4.0.0,,,,,,,Connectors / Pulsar,Runtime / Coordination,,,0,test-stability,,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=16001] failed in {{PulsarSourceUnorderedE2ECase.testSavepoint}} due to some job timeout.

The logs reveal {{{}NoClassDefFoundErrors{}}}:
{code:java}
2022-11-14 15:36:59,696 WARN  org.apache.pulsar.shade.org.asynchttpclient.DefaultAsyncHttpClient [] - Unexpected error on ChannelManager close
java.lang.NoClassDefFoundError: org/apache/pulsar/shade/io/netty/util/concurrent/DefaultPromise$1
     at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:499) ~[?:?]
     at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[?:?]
     at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[?:?]
     at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[?:?]
     at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor.ensureThreadStarted(SingleThreadEventExecutor.java:970) ~[?:?]
     at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor.shutdownGracefully(SingleThreadEventExecutor.java:661) ~[?:?]
     at org.apache.pulsar.shade.io.netty.util.concurrent.MultithreadEventExecutorGroup.shutdownGracefully(MultithreadEventExecutorGroup.java:163) ~[?:?]
     at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.ChannelManager.close(ChannelManager.java:307) ~[?:?]
     at org.apache.pulsar.shade.org.asynchttpclient.DefaultAsyncHttpClient.close(DefaultAsyncHttpClient.java:120) ~[?:?]
     at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.close(AsyncHttpConnector.java:346) ~[?:?]
     at org.apache.pulsar.client.admin.internal.PulsarAdminImpl.close(PulsarAdminImpl.java:490) ~[?:?]
     at org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.close(PulsarSourceEnumerator.java:172) ~[?:?]
     at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:255) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
     at org.apache.flink.runtime.source.coordinator.SourceCoordinator.close(SourceCoordinator.java:265) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
     at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:76) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
     at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
Caused by: java.lang.ClassNotFoundException: org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise$1
     at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_342]
     at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_342]
     at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
     at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
     at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
     at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_342]
     ... 16 more {code}
Outdated:

-The issue seems to be related to the BlobServer failing to provide some artifacts ({{{}java.io.FileNotFoundException{}}}) that consequently causes classes not being found.-

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,,,,,,,,FLINK-29755,,,"01/Nov/22 09:36;mapohl;PulsarSourceUnorderedE2ECase.testSavepoint.FileNotFoundException.log;https://issues.apache.org/jira/secure/attachment/13051674/PulsarSourceUnorderedE2ECase.testSavepoint.FileNotFoundException.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 02:53:10 UTC 2023,,,,,,,,,,"0|z1aok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 08:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42863&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=16034;;;","08/Nov/22 07:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42910&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=13776;;;","09/Nov/22 08:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42957&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=16788;;;","11/Nov/22 09:13;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43043&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=16549;;;","22/Nov/22 08:33;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43149&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15929];;;","22/Nov/22 08:35;mapohl;My initial judgement might be wrong because the {{FileNotFoundException}} is actually only logged in {{INFO}} level. Checking [AbstractBlobCache:169|https://github.com/apache/flink/blob/ea52732dc48a4f1c5be0925890cd8aa1ea2a11ed/flink-runtime/src/main/java/org/apache/flink/runtime/blob/AbstractBlobCache.java#L169] also reveals that this error is handled by downloading the files from the BlobServer as stated in the log message.;;;","22/Nov/22 08:45;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43174&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15988];;;","22/Nov/22 10:46;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43206&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15424];;;","25/Nov/22 04:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43461&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15883;;;","25/Nov/22 07:20;mapohl;[~syhily] could you have a look at it?;;;","28/Nov/22 10:22;mapohl;Same build, multiple failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43514&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=16454
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43514&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=17073;;;","28/Nov/22 10:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43519&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=12836;;;","29/Nov/22 06:01;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15319;;;","02/Dec/22 10:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43661&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=17056;;;","02/Dec/22 10:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43664&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=13641;;;","05/Dec/22 10:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43693&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=16995;;;","05/Dec/22 10:17;mapohl;Several stacktrace show up in the logs:

NoClassDefFoundError (not exclusively for {{{}SingletonContext$GenerationComparator{}}}!):
{code:java}
 2022-12-03 03:54:32,784 WARN  org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyClient [] - Client shutdown hook java.lang.ref.WeakReference failed.
 java.lang.NoClassDefFoundError: org/apache/pulsar/shade/org/jvnet/hk2/internal/SingletonContext$GenerationComparator
   at org.apache.pulsar.shade.org.jvnet.hk2.internal.SingletonContext.shutdown(SingletonContext.java:142) ~[?:?]
   at org.apache.pulsar.shade.org.jvnet.hk2.internal.ServiceLocatorImpl.shutdown(ServiceLocatorImpl.java:920) ~[?:?]
   at org.apache.pulsar.shade.org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.shutdown(AbstractHk2InjectionManager.java:183) ~[?:?]
   at org.apache.pulsar.shade.org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.shutdown(ImmediateHk2InjectionManager.java:30) ~[?:?]
   at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.close(ClientRuntime.java:371) ~[?:?]
   at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.onShutdown(ClientRuntime.java:353) ~[?:?]
   at org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyClient.release(JerseyClient.java:190) ~[?:?]
   at org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyClient.close(JerseyClient.java:180) ~[?:?]
   at org.apache.pulsar.client.admin.internal.PulsarAdminImpl.close(PulsarAdminImpl.java:488) ~[?:?]
   at org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.close(PulsarSourceEnumerator.java:164) ~[?:?]
   at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.source.coordinator.SourceCoordinator.close(SourceCoordinator.java:216) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:76) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
 Caused by: java.lang.ClassNotFoundException: org.apache.pulsar.shade.org.jvnet.hk2.internal.SingletonContext$GenerationComparator
   at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_342]
   at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_342]
   at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:68) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:52) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_342]
   ... 14 more{code}
{{{}CoordinatorException$InvalidTxnStatusException{}}}:
{code:java}
2022-12-03T03:54:39,784+0000 [BookKeeperClientWorker-OrderedExecutor-0-0] ERROR org.apache.pulsar.transaction.coordinator.impl.MLTransactionMetadataStore - TxnID : (0,3643) add acked subscription error with TxnStatus : COMMITTING
org.apache.pulsar.transaction.coordinator.exceptions.CoordinatorException$InvalidTxnStatusException: Expect Txn `(0,3643)` to be in OPEN status but it is in COMMITTING status
   at org.apache.pulsar.transaction.coordinator.impl.TxnMetaImpl.checkTxnStatus(TxnMetaImpl.java:96) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.pulsar.transaction.coordinator.impl.TxnMetaImpl.addAckedPartitions(TxnMetaImpl.java:127) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.pulsar.transaction.coordinator.impl.TxnMetaImpl.addAckedPartitions(TxnMetaImpl.java:37) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.pulsar.transaction.coordinator.impl.MLTransactionMetadataStore.lambda$addAckedPartitionToTxn$7(MLTransactionMetadataStore.java:330) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:714) ~[?:?]
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]
   at org.apache.pulsar.transaction.coordinator.impl.MLTransactionLogImpl$3.addComplete(MLTransactionLogImpl.java:158) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.bookkeeper.mledger.impl.OpAddEntry.safeRun(OpAddEntry.java:232) ~[org.apache.pulsar-managed-ledger-2.10.2.jar:2.10.2]
   at org.apache.bookkeeper.common.util.SafeRunnable.run(SafeRunnable.java:36) ~[org.apache.bookkeeper-bookkeeper-common-4.14.5.jar:4.14.5]
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[io.netty-netty-common-4.1.77.Final.jar:4.1.77.Final]
   at java.lang.Thread.run(Thread.java:829) ~[?:?]
2022-12-03T03:54:39,784+0000 [BookKeeperClientWorker-OrderedExecutor-0-0] ERROR org.apache.pulsar.broker.service.ServerCnx - Send response error for ADD_SUBSCRIPTION_TO_TXN request 2032153913178166290.
org.apache.pulsar.transaction.coordinator.exceptions.CoordinatorException$InvalidTxnStatusException: Expect Txn `(0,3643)` to be in OPEN status but it is in COMMITTING status
   at org.apache.pulsar.transaction.coordinator.impl.TxnMetaImpl.checkTxnStatus(TxnMetaImpl.java:96) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.pulsar.transaction.coordinator.impl.TxnMetaImpl.addAckedPartitions(TxnMetaImpl.java:127) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.pulsar.transaction.coordinator.impl.TxnMetaImpl.addAckedPartitions(TxnMetaImpl.java:37) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.pulsar.transaction.coordinator.impl.MLTransactionMetadataStore.lambda$addAckedPartitionToTxn$7(MLTransactionMetadataStore.java:330) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:714) ~[?:?]
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]
   at org.apache.pulsar.transaction.coordinator.impl.MLTransactionLogImpl$3.addComplete(MLTransactionLogImpl.java:158) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.2.jar:2.10.2]
   at org.apache.bookkeeper.mledger.impl.OpAddEntry.safeRun(OpAddEntry.java:232) ~[org.apache.pulsar-managed-ledger-2.10.2.jar:2.10.2]
   at org.apache.bookkeeper.common.util.SafeRunnable.run(SafeRunnable.java:36) ~[org.apache.bookkeeper-bookkeeper-common-4.14.5.jar:4.14.5]
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[io.netty-netty-common-4.1.77.Final.jar:4.1.77.Final]
   at java.lang.Thread.run(Thread.java:829) ~[?:?] {code};;;","05/Dec/22 10:23;mapohl;{{{}CoordinatorException$TransactionNotFoundException{}}}:

{code:java}
 org.apache.pulsar.transaction.coordinator.exceptions.CoordinatorException$TransactionNotFoundException: The transaction with this txdID `(0,2745)`not found 
   at org.apache.pulsar.transaction.coordinator.impl.MLTransactionMetadataStore.getTxnPositionPair(MLTransactionMetadataStore.java:434) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.1.jar:2.10.1]
   at org.apache.pulsar.transaction.coordinator.impl.MLTransactionMetadataStore.lambda$addAckedPartitionToTxn$10(MLTransactionMetadataStore.java:304) ~[org.apache.pulsar-pulsar-transaction-coordinator-2.10.1.jar:2.10.1]
   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
   at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [io.netty-netty-common-4.1.77.Final.jar:4.1.77.Final]
   at java.lang.Thread.run(Thread.java:829) [?:?]{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43694&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=14358;;;","08/Dec/22 11:57;mapohl;2x in a single build:
* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=16857]
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=13613;;;","03/Jan/23 09:26;mapohl;I updated the affected versions. FLINK-30397 removed Pulsar-related code from {{master}}. Therefore, it's only an issue on {{release-1.16}} and {{release-1.15}} where the test disabled (see FLINK-30351). {{pulsar-3.0.0}} and {{pulsar-4.0.0}} were added as affected versions.;;;","04/Jan/23 02:53;tison;After FLINK-30413 dropped the related support, this test was dropped also. Invalid now.;;;",,
Clear static Jackson TypeFactory cache on CL release,FLINK-29834,13493981,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,01/Nov/22 09:26,02/Nov/22 11:38,04/Jun/24 20:41,02/Nov/22 11:38,,,,,,,1.16.1,1.17.0,,,Runtime / Coordination,,,,0,pull-request-available,,,,"The Jackson TypeFactory contains a singleton instance that is at times used by Jackson, potentially containing user-classes for longer than necessary.

https://github.com/FasterXML/jackson-databind/issues/1363

We could clear this cache whenever a user code CL is being released similar to what was done in BEAM-6460.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 11:38:44 UTC 2022,,,,,,,,,,"0|z1aojk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 11:38;chesnay;master: 00cda36859f4f9f7b9ff9980fd81fa6515badb9b
1.16: f5dd37d1e51963ed3abe318d1e95a34110bc54d2;;;",,,,,,,,,,,,,,,,,,,,,,
Improve PyFlink support in Windows,FLINK-29833,13493971,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxbks2ks,hxbks2ks,hxbks2ks,01/Nov/22 08:49,22/Nov/22 12:18,04/Jun/24 20:41,,1.15.3,1.16.0,,,,,,,,,API / Python,,,,0,,,,,Many users are used to developing PyFlink jobs on Windows. It is necessary to improve the simplicity of PyFlink job development on Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-01 08:49:17.0,,,,,,,,,,"0|z1aohc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve switch to default database in docs,FLINK-29832,13493969,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,01/Nov/22 08:41,19/Mar/23 05:43,04/Jun/24 20:41,19/Mar/23 05:43,table-store-0.2.2,table-store-0.3.0,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"`FlinkCatalogFactory` creates a default database named `default` in table store. The `default` is a keyword in SQL, and when we create a new database, we cant execute `use default` to switch to `default` directly. We can switch to default database ""use `default`;"" in flink table store",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/22 08:40;zjureel;image-2022-11-01-16-40-47-539.png;https://issues.apache.org/jira/secure/attachment/13051671/image-2022-11-01-16-40-47-539.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 02:43:22 UTC 2022,,,,,,,,,,"0|z1aogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 01:35;TsReaper;Hi [~zjureel].

Before diving into your implementation, I'd like to discuss the necessity of this ticket. There are many other catalog (for example, {{HiveCatalog}}) whose default database name is just ""default"". To switch to this database, we should run {{USE `default`}}, using backtick(`) to escape the keyword.

Why do you think this change is necessary? I'd like to hear your options.;;;","04/Nov/22 02:43;zjureel;Thanks [~TsReaper] When i tried to fixed this issue, i found the names of default database in hive and spark are also `default`. So i think the `default` is better than `default_database`. I'm new to sql, and when using hive, i can switch to default database `use default;` directly, and it cause in flink table store. I will change this issue to improve the document in flink table store. Can you help to review it? THX again :);;;",,,,,,,,,,,,,,,,,,,,,
Multiple Hive tests are failing,FLINK-29831,13493967,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,martijnvisser,martijnvisser,01/Nov/22 08:30,04/Nov/22 02:15,04/Jun/24 20:41,04/Nov/22 02:15,1.16.0,1.17.0,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 01 01:56:03 [ERROR] org.apache.flink.table.module.hive.HiveModuleTest.testNumberOfBuiltinFunctions  Time elapsed: 0.042 s  <<< FAILURE!

Nov 01 01:57:17 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 12.846 s  <<< FAILURE!

Nov 01 01:57:17 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testMapRedOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 10.355 s  <<< FAILURE!

Nov 01 01:57:17 [ERROR]   HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport:124->assertHiveTableOrcFormatTableStatsEquals:368 

Nov 01 01:57:17 [ERROR]   HiveTableSourceStatisticsReportTest.testMapRedOrcFormatHiveTableSourceStatisticsReport:164->assertHiveTableOrcFormatTableStatsEquals:368 

Nov 01 01:57:17 [ERROR]   HiveModuleTest.testNumberOfBuiltinFunctions:53->verifyNumBuiltInFunctions:75 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=24818",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 02:14:50 UTC 2022,,,,,,,,,,"0|z1aogg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 09:20;martijnvisser;1.16:
{code:java}
Nov 01 02:28:13 [ERROR] Errors: 
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testComplexDataTypes:186->verifyDataTypes:212 » Catalog
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testDataTypes:118->verifyDataTypes:212 » Catalog Faile...
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testNonSupportedBinaryDataTypes:127 » Catalog Failed t...
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testNonSupportedVarBinaryDataTypes:139 » Catalog Faile...
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42682&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26311

[~luoyuxia] Any ideas on this?;;;","02/Nov/22 04:13;luoyuxia;The HiveTableSourceStatisticsReportTest / testNumberOfBuiltinFunctions failure  is caused by upgrading Hive to 3.1.3 in [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=24818].

To fix it, we need to do adjustment in test code.

But for the  HiveCatalogDataTypeTest failure in [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42682&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26311], it really confused me.I couldn't reproduce it in my local env and our test ci with a [pr|[https://github.com/apache/flink/pull/21213]] to trigger it . But I will still try to debug to see why it happens.;;;","02/Nov/22 08:11;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25263;;;","04/Nov/22 02:14;renqs;master: 8e66be89dfcb54b7256d51e9d89222ae6701061f;;;",,,,,,,,,,,,,,,,,,,
PulsarSinkITCase$DeliveryGuaranteeTest.writeRecordsToPulsar failed,FLINK-29830,13493964,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,martijnvisser,martijnvisser,01/Nov/22 08:25,19/Jan/23 10:27,04/Jun/24 20:41,02/Dec/22 14:23,1.15.3,1.16.0,1.17.0,,,,1.15.4,1.16.1,1.17.0,pulsar-3.0.1,Connectors / Pulsar,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 01 01:28:03 [ERROR] Failures: 
Nov 01 01:28:03 [ERROR]   PulsarSinkITCase$DeliveryGuaranteeTest.writeRecordsToPulsar:140 
Nov 01 01:28:03 Actual and expected should have same size but actual size is:
Nov 01 01:28:03   0
Nov 01 01:28:03 while expected size is:
Nov 01 01:28:03   115
Nov 01 01:28:03 Actual was:
Nov 01 01:28:03   []
Nov 01 01:28:03 Expected was:
Nov 01 01:28:03   [""AT_LEAST_ONCE-isxrFGAL-0-kO65unDUKX"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-1-4tBNu1UmeR"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-2-9PTnEahlNU"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-3-GjWqEp21yz"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-4-jnbJr9C0w8"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-5-e8Wacz5yDO"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-6-9cW53j3Zcf"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-7-jk8z3m2Aa5"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-8-VU56KmMeiz"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-9-uvMdFxxDAj"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-10-FQyWfwJFbH"",
...
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37544",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30109,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 14:23:10 UTC 2022,,,,,,,,,,"0|z1aofs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 08:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42681&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=27843;;;","02/Nov/22 13:09;fpaul;[~martijnvisser] do you think this is a block for the 1.15.3 release? The pulsar tests seem to be flaky for a while.;;;","02/Nov/22 13:45;martijnvisser;This is not a blocker for 1.15.3;;;","07/Nov/22 08:01;mapohl;Same build:
* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42857&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=28624]
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42857&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=28463;;;","07/Nov/22 13:30;syhily;The log couldn't show the real cause. We shall add some log to print the hidden error stack.

{{Timeout for waiting the records from Pulsar. We have consumed 0 messages, expect 137 messages.}}

Aha, I find the cause, it's my mistake in consuming messages.;;;","07/Nov/22 13:56;mapohl;There is some warning in the logs about some timeout on the Pulsar side:
{code}
160037 01:57:02,826 [Legacy Source Thread - Source: Custom Source (1/1)#0] WARN  org.apache.flink.connector.pulsar.testutils.function.ControlSource$StopSignal [] - Timeout for waiting the records from Pulsar. We have consumed 0 messages, expect 137 messages.
 160038 01:57:02,987 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 600 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667699822986 for job 6fa5e9f506b63276a4933daa137fdf43.
 160039 01:57:02,994 [jobmanager-io-thread-13] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 600 for job 6fa5e9f506b63276a4933daa137fdf43 (314 bytes, checkpointDuration=8 ms, finalizationTime=0 ms).
 160040 01:57:02,996 [Source: Custom Source (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source (1/1)#0 (3cfeb328eb182887d817886aa848606d) switched from RUNNING to FINISHED.
{code}

Subsequent checkpoints are declined as a consequence (AFAIU).;;;","07/Nov/22 14:03;syhily;[~mapohl] It's my mistake in {{ControlSource.StopSignal}}. I consume messages with the latest stop cursor. Which would cause the race condition that all message has been sent before I start the consumer.

Can you add 1.16.0 in affected version?;;;","07/Nov/22 14:05;mapohl;Done. Are you going to provide a fix for that? I would assign this Jira issue to you in that case.;;;","07/Nov/22 14:10;syhily;Yep, I'll submit a fix right now. Plz assign this ticket to me and thanks for helping me find this undetectable bug.;;;","07/Nov/22 15:00;syhily;[~mapohl] Can you help review the PR? https://github.com/apache/flink/pull/21252;;;","14/Nov/22 03:38;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43089&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","14/Nov/22 03:43;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43089&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","21/Nov/22 09:32;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43253&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","24/Nov/22 03:35;mapohl;[~syhily] may you create a backport PR for 1.16 and 1.15 as well, please?;;;","30/Nov/22 06:54;mapohl;I moved this under the Umbrella Pulsar stability ticket.;;;","01/Dec/22 13:46;martijnvisser;Fixed in Pulsar external connector repo: 5162b766bb042705ecafefc773bb2600b7e56263;;;","02/Dec/22 14:23;mapohl;master: f8b3b33ce1c2b36aa8e0011131a1ba74f540035f
1.16: 0ff47bedba48714c79b82cfb0379768d79e2d6ef
1.15: 3eb002eb30ebffba6306081f79ead3a487b2cb5c
flink-connector-pulsar:v3.0: 5162b766bb042705ecafefc773bb2600b7e56263;;;",,,,,,
align explain results in different platforms,FLINK-29829,13493959,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,liyubin117,liyubin117,01/Nov/22 08:13,08/Nov/22 12:22,04/Jun/24 20:41,08/Nov/22 12:22,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Delete last line separator during explain results generation in design, but in fact, just delete the last character, it will result in the outputs in Windows has one more line than Linux. It will confuse users, and block test cases in platforms differ from Linux.

 
{code:java}
//Windows
LegacySink(name=[`default_catalog`.`default_database`.`appendSink2`], fields=[a, b])
+- GroupWindowAggregate(groupBy=[id1], window=[SlidingGroupWindow('w$, rowtime, 6000, 12000)], select=[id1, LISTAGG(text, $f3) AS EXPR$1])
   +- Exchange(distribution=[hash[id1]])
      +- Calc(select=[id1, rowtime, text, '*' AS $f3])
         +- Reused(reference_id=[1])


{code}
 

 
{code:java}
//linux
LegacySink(name=[`default_catalog`.`default_database`.`appendSink2`], fields=[a, b])
+- GroupWindowAggregate(groupBy=[id1], window=[SlidingGroupWindow('w$, rowtime, 6000, 12000)], select=[id1, LISTAGG(text, $f3) AS EXPR$1])
   +- Exchange(distribution=[hash[id1]])
      +- Calc(select=[id1, rowtime, text, '*' AS $f3])
         +- Reused(reference_id=[1])
 {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-01 08:13:52.0,,,,,,,,,,"0|z1aoeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to read data from table with given snapshot id,FLINK-29828,13493956,13493853,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,01/Nov/22 08:09,19/Mar/23 05:44,04/Jun/24 20:41,19/Mar/23 05:44,table-store-0.2.2,table-store-0.3.0,,,,,,,,,Table Store,,,,0,,,,,"Support reading data from table with given snapshot id from table, users can query table data as follows:

`SELECT t1.word, t1.cnt, t2.cnt FROM word_count$snapshot$10 t1 JOIN word_count$snapshot$11 t2 ON t1.word = t2.word and t1.cnt != t2.cnt`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 26 02:25:18 UTC 2022,,,,,,,,,,"0|z1aoe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/22 02:25;liliwei;hi, [~zjureel] , Why not we use SQL Hints?  IMO, it seems more user friendly and the syntax is more standardized.;;;",,,,,,,,,,,,,,,,,,,,,,
[Connector][AsyncSinkWriter] Checkpointed states block writer from sending records,FLINK-29827,13493947,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,mc8max,mc8max,01/Nov/22 07:45,10/Nov/22 08:37,04/Jun/24 20:41,10/Nov/22 08:37,1.15.2,1.16.0,,,,,1.15.3,1.16.1,1.17.0,,Connectors / Common,,,,0,pull-request-available,,,,"Hi every one,

Recently we discovered an issue which blocks Sink operators from sending records to client's endpoint.

To *reproduce* the issue, we started our Flink app from an existing savepoint, in which some Sink operators hold some buffered records. For instance, app employs KinesisStreamSink with a parallelism of 4. 2 of them has no buffered records, the other 2 start with existing states of some records, which are leftover from the previous run. 

{*}Behavior{*}: during runtime, we sent records (let's say 200) to this sink in rebalance mode. But only 100 of them (50%) were dispatched from the sink operators.

After {*}investigation{*}, we found that the implementation AsyncSinkWriter invokes submitRequestEntries() to send the records to their destination. This invocation is performed when a callback is performed, a flush(true) or forced-flush is called, or when the buffered is full (either in size or in quantity).

The case falls in the first scenario: the _callback is not registered_ {_}when the writer starts with some existing buffered records{_}, initialized from savepoint. Hence in our case, those operators were holding records till their buffers become full, while other operators still perform the usual sending.

Impacted {*}scope{*}: flink-1.15.2 or later version, for any Sink that implements AsyncSinkWriter.

We currently treat this as an abnormal behavior of Flink, but please let me know if this behavior is intended by design.

Thanks in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 08:49:43 UTC 2022,,,,,,,,,,"0|z1aoc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 11:21;dannycranmer;[~mc8max] Thanks for reporting this issue. I agree it sounds like a bug, [~chalixar] is going to take a look.;;;","07/Nov/22 08:49;dannycranmer;Merged commit [{{d76053a}}|https://github.com/apache/flink/commit/d76053a0e670f45b478841f92ef27f9b641643f5] into master 
Merged commit [{{f5f8060}}|https://github.com/apache/flink/commit/f5f806044db373971b3cd611180d1e0227ceaedd] into release-1.16
Merged commit [{{7b50b3a}}|https://github.com/apache/flink/commit/7b50b3a8d13be8931de07897e326cb96022ebdbe] into release-1.15;;;",,,,,,,,,,,,,,,,,,,,,
"flink-sql-connector-hbase-1.4 netty class not found, because of shade not work",FLINK-29826,13493944,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lambdaoo,lambdaoo,01/Nov/22 07:21,11/Nov/22 08:31,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Connectors / HBase,,,,0,,,,,"flink-connectors/flink-sql-connector-hbase-1.4/pom.xml ,flink-connectors/flink-sql-connector-hbase-2.2/pom.xml shade plugin only include <include>io.netty:netty-all</include> artifactId. this jar does not contains classes. it noly has META-INF.this cause somebody use flink-sql-connector-hbase-1.4 dependency netty classe not found .

I also tried flink-sql-connector-hbase-2.2, it workes perfected. Because hbase-client itself shaded netty. So this does not affect this connector.

When I change <include>io.netty:netty-all</include> to <include>io.netty:*</include> and repackage this, it workes.

Please fix it, thanks","flink 1.15.1

hbase 1.4",86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/22 07:19;lambdaoo;2022-11-01 22-55-12屏幕截图-1.png;https://issues.apache.org/jira/secure/attachment/13051663/2022-11-01+22-55-12%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE-1.png","01/Nov/22 07:19;lambdaoo;2022-11-01 23-06-41屏幕截图.png;https://issues.apache.org/jira/secure/attachment/13051662/2022-11-01+23-06-41%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png","01/Nov/22 07:19;lambdaoo;2022-11-01 23-18-00屏幕截图.png;https://issues.apache.org/jira/secure/attachment/13051661/2022-11-01+23-18-00%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Tue Nov 01 07:52:40 UTC 2022,,,,,,,,,,"0|z1aobc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 07:52;martijnvisser;This most likely won't be fixed, since most likely we'll drop support for HBase 1.4 in the next release. ;;;",,,,,,,,,,,,,,,,,,,,,,
Improve benchmark stability,FLINK-29825,13493900,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,01/Nov/22 04:35,22/Mar/23 11:31,04/Jun/24 20:41,10/Mar/23 07:36,1.17.0,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,,"Currently, regressions are detected by a simple script which may have false positives and false negatives, especially for benchmarks with small absolute values, small value changes would cause large percentage changes. see [here|https://github.com/apache/flink-benchmarks/blob/master/regression_report.py#L132-L136] for details.
And all benchmarks are executed on one physical machine, it might happen that hardware issues affect performance, like ""[FLINK-18614] Performance regression 2020.07.13"".
 
This ticket aims to improve the precision and recall of the regression-check script.
 ",,,,,,,,,,,,,,,,,,FLINK-27571,,,,,,,,,,,,,,FLINK-31561,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 07:36:00 UTC 2023,,,,,,,,,,"0|z1ao1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/23 09:16;pnowojski;Thanks for picking this up [~Yanfei Lei]. Are you planning to improve benchmarks stability itself (1), or only the regression detection script (2)?

For the (2), for sometime I was playing with idea how to more reliably detect regression in noisy benchmarks like this:
http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=fireProcessingTimers&extr=on&quarts=on&equid=off&env=2&revs=200
My assumption is/was that this must be a pretty well known problem, and it's only a matter of finding the right algorithm to do it. For example maybe [Two sample Kolmogorov-Smirnov test|https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov%E2%80%93Smirnov_test]. It looks like if we split the samples into two distributions, for example:
1. all latest samples, from the latest one (1st one), until N'th latest sample 
2. samples from N'th latest to M'th latest (M>N)

We would have two distributions and we could use the Komnongorov-Smirnov test to compare those two distributions, how similar are they. And report if the difference is greater then some threshold.

Probably there are also other ways.

Could you share your thoughts/plans on this issue [~Yanfei Lei]?;;;","31/Jan/23 07:20;Yanfei Lei;Thanks for the suggestion and sorry for the late reply, I'm planning to improve the stability regression detection script.

Leveraging [Two sample Kolmogorov-Smirnov test|https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov%E2%80%93Smirnov_test] to detect regression is a good idea:

(1) I‘m concerned it's not easy to construct the first distribution: ""all latest samples, from the latest one (1st one), until N'th latest sample"". If some regressions have occurred before, it will ""distort"" the first distribution, possibly leading to false positive; meanwhile, optimization can also cause ""distortion"" which possibly leads to false negative, It's relatively acceptable. Maybe we can filter out outliers to avoid this problem. 

(2) Also, [Kolmogorov-Smirnov test|https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov%E2%80%93Smirnov_test] is an absolute value, and it is impossible to identify whether it is an improvement or a regression.

 

My plan is/was to use the error to adjust each threshold, each benchmark run has an error value in the result, 

let thr=0.75*(max error of N'th latest sample), and the other logic is the same as the existing detection script.;;;","04/Feb/23 15:53;lindong;According to the wiki, two sample Kolmogorov-Smirnov test is used to determine whether two distributions (i.e. collection of values) are close enough. On the other hand, regression detection is more about determining whether a single value (e.g. latest performance) is observably worse than the the best performance in the past. These are two quite different problems.

Is there any success story of using Kolmogorov-Smirnov to detect regression in practice?

I drafted this [doc|https://docs.google.com/document/d/1Bvzvq79Ll5yxd1UtC0YzczgFbZPAgPcN3cI0MjVkIag] to explain the algorithm that I would like to try to detect Flink regression. It is not exactly the same as the one I used before for TensorFlow (because I lost that doc) but the ideas are pretty much the same. Using the heuristics described in this doc, I am confident it should have much lower false positive rate than the relatively simple formula used in the existing [script|https://github.com/apache/flink-benchmarks/blob/master/regression_report.py].

The parameters (e.g. threshold for regression detection) of this algorithm need to be tuned based on the benchmark data.

Hopefully I can get time to implement and evaluate this algorithm in the coming 2 weeks. The main issue that I don't know how to address yet is how to update the script to get the maximum and deviation of throughput across multiple runs for a given commit point.;;;","06/Feb/23 13:28;pnowojski;I have responded on the dev mailing list, but let's maybe move the discussion here.

[~lindong] , the Kolmogorov-Smirnov test was a just a result of a quick google search for relevant mathematical concepts. I have a feeling that it could be adapted to something that would work for us. For example instead of checking the supremum between two empirical distribution functions (EDF), we could add up differences between those distribution functions. If the new EDF has on lower values, the sum of differences would be negative, that would point toward a regression. But maybe there are better approaches.

I think the drawback of your proposal is that it wouldn't detect if there is visible performance regression within benchmark noise. While this should be do-able with large enough number of samples (ad for example described above). For example if the results are oscillating randomly around 1000 ({+}/- 150), and there is performance regression that changes the result to 900 ({+}/- 135). And we have quite a lot of noisy benchmarks, like [this|http://codespeed.dak8s.net:8000/timeline/?ben=fireProcessingTimers&env=2] or [this|http://codespeed.dak8s.net:8000/timeline/?ben=serializerTuple&env=2]. 

I was also informed about some tooling created exactly for detecting performance regressions from benchmark results: 

> fork of  Hunter - a perf change detection tool, originally from DataStax:
> Blog post - [https://medium.com/building-the-open-data-stack/detecting-performance-regressions-with-datastax-hunter-c22dc444aea4]
> Paper - [https://arxiv.org/pdf/2301.03034.pdf]
> Our fork - [https://github.com/gerrrr/hunter]

The algorithm that's used underneath ""E-divisive Means"" sounds promising. ;;;","07/Feb/23 02:08;lindong;[~pnowojski] I think one drawback with your proposal is that it is comparing two distributions and depends on having large enough number of samples in both distributions. It means that after a regression has happened, you need to run engouh commit-points so that the recent distribution starts to be considerably different from the previous distribution according to Kolmogorov-Smirnov test. This would considerably delay the time-to-regression-detection. It seems that my proposal would not suffer from this issue since it allows users to specify how many commit-points we need to repeat the regression before sending alert. And this number can be 1-3 commit points.

Regarding the drawback of not detecting ""there is visible performance regression within benchmark noise"", my proposal is to either exclude noisy benchmark completely, or we can require the regression to be 2X the noise (the ratio is also tunable). These sound like a reasonable practical solution, right?

I don't think we will be able to have perfect regression detection without any drawback(e.g. 0 false positive and 0 false negative). The question is whether the proposed solution can be useful enough (i.e. low false positive and low false negative) and whether it is the best solution across all available choices. So it can be OK if some regression is not detected, like the one mentioned above


BTW, regarding the noisy benchmark mentioned above, I am curious how Kolmogorov-Smirnov test can address issue. Maybe I can update my proposal to re-use the idea. Can you help explain it?

I will take a look at the tooling mentioned above later to see if we can learn from them or re-use them.;;;","07/Feb/23 10:30;Yanfei Lei;[~lindong] 
Thanks for the algorithm you proposed, I wrote a [script|https://github.com/fredia/flink-benchmarks/blob/FLINK-29825/check_regression.py] to test it briefly, new algorithm shows better sensitivity than existing median-based method.
I did two kinds of tests:
1. For benchmarks where regression has occurred:
    a. Under the appropriate parameters, the new algorithm has higher precision and recall on most benchmarks.
    b. The new algorithm can find the regression faster, and the current algorithm needs to wait until the median window slides into the corresponding interval, which means that the regression may have occurred for several days.
2. For noisy benchmarks:
    a. New algorithm produces fewer false positives for most benchmarks. like fireProcessingTimers of Flink (Java11) and fireProcessingTimers of Flink.
     b. For the benchmark with regression in the noise(like serializerTuple of Flink (Java11)), the new algorithm can also detect it, but the existing median-based method cannot detect it.

In my opinion, the new algorithm is very concise and efficient, it can also avoid the effects of distorted baselines caused by regression.;;;","07/Feb/23 15:55;pnowojski;[~lindong], I don't think having to wait a couple of days to sometimes (for noisy benchmarks) to reliably detect a performance regression is an issue. We can not run regressions checks per each PR before PR is merged, so it really doesn't matter much if the regression will be detected 12h after merging or 72h after merging.  

Thanks for the investigation [~Yanfei Lei]. As I said, I have a feeling we should be able to find a better, more sophisticated solution, but at the same time I can not dive deeper into this myself. I would encourage one of you to take a look at the Hunter tool that I mentioned above, and maybe include it in the comparison. But at the same time if you are strongly inclined towards [~lindong]'s idea, I wouldn't block it, as it's indeed most likely an improvement over what we have right now.

{quote}
BTW, regarding the noisy benchmark mentioned above, I am curious how Kolmogorov-Smirnov test can address issue. Maybe I can update my proposal to re-use the idea. Can you help explain it?
{quote}
I've just realised that my naive idea (basically comparing integrals of two EDFs) would be unable to detect if benchmark suddenly became very noisy, maintaining. the same average/mean. I think I would need to think/do some research how to clarify my thoughts. Roughly speaking I wanted to run some comparison on two empirical distribution functions. Human via looking at two EDFs can very easily detect that they are coming from two different distributions:
https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2021/06/empirical_cdf_plot_multiple.png?w=576&ssl=1;;;","08/Feb/23 01:48;lindong;Thanks [~Yanfei Lei] for implementing and evaluating the algorithm!

[~pnowojski] Cool, I think we have agreed to make incremental improvements and used the algorithm proposed in the above doc to detect regression for Flink benchmarks.

We probably still have different understandings regarding the pros/cons of these alternative choices. It will be great if you or someone else can help implement an alternative choice and show that it can do better than the one we are going to use. I probably won't have time to try the Hunter algorithm myself in the near future.


;;;","10/Feb/23 13:38;Yanfei Lei;[~pnowojski]  I tried to use hunter to detect regression, and [here|https://docs.google.com/document/d/1coI4eJsauBtrlS1Z77bhGf-hNtDEXbzuwacG5ZPCMc8/edit?usp=sharing] are some evaluation results of the three algorithms. I'm not sure I fully understand the usage of hunter, it looks like hunter can only detect regressions in the history sequence, I modified it a little bit to detect regressions in the latest commit, correct me if something is wrong in the document:D.;;;","10/Feb/23 14:43;pnowojski;Thanks a lot for the very detailed comparison [~Yanfei Lei]. Let's go with the [~lindong]'s proposal!;;;","10/Feb/23 15:10;lindong;Thanks [~Yanfei Lei] for the detailed evaluation results! Maybe we can write a blog together based on your evaluation results.;;;","10/Feb/23 16:08;pnowojski;Yes, that's a good idea :);;;","13/Feb/23 02:23;Yanfei Lei;Thanks for taking the time to review the evaluation results. Writing a blog is a good idea👍,  and I also intend to implement Dong's algorithm completely (only the max-based algorithm under “moreisbetter"" is implemented during evaluation) to replace the median-based algorithm.;;;","10/Mar/23 07:36;lindong;Merged to apache/flink-benchmarks master branch 7d2013a9f401366bc9073857175f434882867bfe;;;",,,,,,,,,
AgglomerativeClustering fails when the distanceThreshold is very large,FLINK-29824,13493898,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,01/Nov/22 04:17,10/Jan/23 05:19,04/Jun/24 20:41,02/Nov/22 07:08,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"The current implementation did not consider following case:

When distanceThreshold not null and set as a large value, all data points are supposed to be assigned into a single cluster.

 

In this case, we should stop training when the number of the active clusters is one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-01 04:17:20.0,,,,,,,,,,"0|z1ao14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support get schema for table snapshot,FLINK-29823,13493885,13493853,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,01/Nov/22 02:30,19/Mar/23 05:44,04/Jun/24 20:41,19/Mar/23 05:44,table-store-0.2.2,table-store-0.3.0,,,,,,,,,Table Store,,,,0,pull-request-available,,,,Support to create schema from table snapshot with given snapshot id,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-01 02:30:22.0,,,,,,,,,,"0|z1any8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong description in comments of StreamExecutionEnvironment#setMaxParallelism(),FLINK-29822,13493869,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,01/Nov/22 01:50,03/Apr/23 14:25,04/Jun/24 20:41,03/Apr/23 14:25,1.18.0,,,,,,1.18.0,,,,API / DataStream,,,,0,pull-request-available,,,,"The upper limit (inclusive) of max parallelism is Short.MAX_VALUE + 1, not Short.MAX_VALUE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-5473,,FLINK-4380,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 14:25:44 UTC 2023,,,,,,,,,,"0|z1anuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 14:25;pnowojski;merged commit ea9c9d9 into apache:master

Thanks [~Feifan Wang]!;;;",,,,,,,,,,,,,,,,,,,,,,
Schema.Builder how to set not null column?,FLINK-29821,13493865,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Information Provided,,ge.bugman,ge.bugman,01/Nov/22 01:14,12/Apr/23 13:41,04/Jun/24 20:41,14/Nov/22 14:46,shaded-16.0,,,,,,,,,,Table SQL / API,,,,0,,,,,"{code:java}
// code placeholder
Schema.Builder schemaBuilder = Schema.newBuilder();
schemaBuilder.column(""id"", DataTypes.BIGINT())
        .column(""value"", DataTypes.STRING())
        .primaryKey(""id""); {code}
When I was setting primary key, console print primary key can not be null, but I don't know how to setting my primay key not null....",Windows local development env.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 13:41:33 UTC 2023,,,,,,,,,,"0|z1ants:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 01:52;luoyuxia;schemaBuilder.column(""id"", DataTypes.BIGINT().notNull());;;;","01/Nov/22 01:59;yzl;Hi Alvin,

  I did a simple test under the flink master branch and it works. And I have checked the `primaryKey` implementation:
{code:java}
public Builder primaryKey(String... columnNames) {               
    Preconditions.checkNotNull(columnNames, ""Primary key column names must not be null."");            
    return primaryKey(Arrays.asList(columnNames)); 
} {code}
  I think it won't cause problem. Can you share more information, such as the context of your code?;;;","12/Apr/23 13:41;ge.bugman;Thanks all of you guys.;;;",,,,,,,,,,,,,,,,,,,,
Support to read data from table store with specify snapshot id,FLINK-29820,13493853,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,01/Nov/22 00:28,19/Mar/23 05:45,04/Jun/24 20:41,19/Mar/23 05:45,table-store-0.2.2,table-store-0.3.0,,,,,,,,,Table Store,,,,0,,,,,"Currently our queries can only read the latest snapshot from table store. We need to support reading the data of the specified snapshot, in this way, we can read historical versions or compare data across versions as needed. In [#FLINK-29735] we have supported to query snapshots with `SELECT * FROM MyTable$snapshots`, we can query data from table `wordcount` with `snapshot 10` as follows:

`SELECT * FROM wordcount$snapshot$10 WHERE count>10`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:45:04 UTC 2023,,,,,,,,,,"0|z1anr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:45;lzljs3620320;We have already supported this.;;;",,,,,,,,,,,,,,,,,,,,,,
Record an error event when savepoint fails within grace period,FLINK-29819,13493791,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,claraxiong,claraxiong,claraxiong,31/Oct/22 15:47,31/Oct/22 17:28,04/Jun/24 20:41,31/Oct/22 17:28,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"As of now, SavepointObserver retries if savepoint fails within grace period until success or failure happens after the grace period. The grace period is for each retry.  If underlying problem for quick failure is not transient, such as a mis-configured path or a perisistent storage failure, retries keep going on without recording any error event. 

We should first add logic to record an error event per failed attempt. We can consider capping the retries if it become a pain for users.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 17:28:37 UTC 2022,,,,,,,,,,"0|z1ane8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 17:28;gyfora;merged to main c406db0dae7000d8d1f75cb6d93545180f6ca173;;;",,,,,,,,,,,,,,,,,,,,,,
HsResultPartitionTest.testAvailability fails  ,FLINK-29818,13493785,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Sergey Nuyanzin,Sergey Nuyanzin,31/Oct/22 14:51,16/Nov/22 09:53,04/Jun/24 20:41,16/Nov/22 09:51,1.17.0,,,,,,1.17.0,,,,Runtime / Network,,,,0,pull-request-available,test-stability,,,"{noformat}
13:13:31,079 [ForkJoinPool-27-worker-25] ERROR org.apache.flink.util.TestLoggerExtension                    [] - 
--------------------------------------------------------------------------------
Test org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testAvailability[testAvailability()] failed with:
org.opentest4j.AssertionFailedError: 
Expecting value to be false but was true
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testAvailability(HsResultPartitionTest.java:414)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
        at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
        at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)

        at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
        at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42653&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8390",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/22 14:50;Sergey Nuyanzin;logs-ci-test_ci_finegrained_resource_management-1667221819.zip;https://issues.apache.org/jira/secure/attachment/13051643/logs-ci-test_ci_finegrained_resource_management-1667221819.zip",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 08:06:28 UTC 2022,,,,,,,,,,"0|z1ancw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 01:10;Weijie Guo;[~Sergey Nuyanzin] Thanks for reporting this, I will fix this unstable test today.;;;","02/Nov/22 08:09;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8199;;;","02/Nov/22 15:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42733&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8195;;;","02/Nov/22 15:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42749&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8427;;;","02/Nov/22 16:17;Weijie Guo;I have found the reason and proposed pull request, it will merge as soon as possible.;;;","03/Nov/22 06:12;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","03/Nov/22 06:13;leonard;[~Weijie Guo] Could you find someone who are familiar with runtime module to review this PR？;;;","03/Nov/22 06:21;Weijie Guo;[~leonard] Sure, [~xtsong] will take a look later.;;;","04/Nov/22 10:34;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42795&view=results;;;","16/Nov/22 08:06;xtsong;- master (1.17): 39f0e9bfadf0317328ede1d8c7dd37894beaae38;;;",,,,,,,,,,,,,
Published metadata for apache-flink in pypi are inconsistent and causes poetry to fail,FLINK-29817,13493780,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,ah.casimiro,ah.casimiro,31/Oct/22 14:26,18/Nov/22 07:58,04/Jun/24 20:41,18/Nov/22 07:58,1.16.0,,,,,,1.16.1,1.17.0,,,API / Python,,,,0,pull-request-available,,,,"Hi, 

Following the debug steps described in [this github thread|https://github.com/python-poetry/poetry/issues/3011] I got to the conclusion that the metadata of the apache-flink 1.16.0 package is wrong. And because of that I cannot properly manage my dependencies using poetry.

I can successfully install it with pip (runs with no errors), as stated in the docs:
{code:java}
python -m pip install apache-flink {code}
But when I try to include the dependency in my poetry project I got the following error:

 
{code:java}
❯ poetry add apache-flink@1.16.0Updating dependencies
Resolving dependencies... (2.0s)Because pemja (0.2.6) depends on numpy (1.21.4)
 and apache-flink (1.16.0) depends on numpy (>=1.14.3,<1.20), pemja (0.2.6) is incompatible with apache-flink (1.16.0).
So, because cv-features depends on apache-flink (1.16.0) which depends on pemja (0.2.6), version solving failed. {code}
 

I've followed the same debug steps as in [this github thread|https://github.com/python-poetry/poetry/issues/3011] and can confirm that apache-flink has exactly the same problem as described in the thread: the wheel package in pypi has correct dependency metadata but the pypi published don't.

 
{code:java}
❯ pkginfo -f requires_dist /Users/andre/Downloads/apache_flink-1.16.0-cp39-cp39-macosx_11_0_arm64.whl
requires_dist: ['py4j (==0.10.9.3)', 'python-dateutil (==2.8.0)', 'apache-beam (==2.38.0)', 'cloudpickle (==2.1.0)', 'avro-python3 (!=1.9.2,<1.10.0,>=1.8.1)', 'pytz (>=2018.3)', 'fastavro (<1.4.8,>=1.1.0)', 'requests (>=2.26.0)', 'protobuf (<3.18)', 'httplib2 (<=0.20.4,>=0.19.0)', 'apache-flink-libraries (<1.16.1,>=1.16.0)', 'numpy (<1.22.0,>=1.21.4)', 'pandas (<1.4.0,>=1.3.0)', 'pyarrow (<9.0.0,>=5.0.0)', 'pemja (==0.2.6) ; python_full_version >= ""3.7"" and platform_system != ""Windows""'] {code}
but the pipy json metadata is wrong:

 

 
{code:java}
❯ curl -sL https://pypi.org/pypi/apache-flink/json | jq '.info.requires_dist'[
  ""py4j (==0.10.9.3)"",
  ""python-dateutil (==2.8.0)"",
  ""apache-beam (==2.38.0)"",
  ""cloudpickle (==2.1.0)"",
  ""avro-python3 (!=1.9.2,<1.10.0,>=1.8.1)"",
  ""pytz (>=2018.3)"",
  ""fastavro (<1.4.8,>=1.1.0)"",
  ""requests (>=2.26.0)"",
  ""protobuf (<3.18)"",
  ""httplib2 (<=0.20.4,>=0.19.0)"",
  ""apache-flink-libraries (<1.16.1,>=1.16.0)"",
  ""numpy (<1.20,>=1.14.3)"",
  ""pandas (<1.2.0,>=1.0)"",
  ""pyarrow (<7.0.0,>=0.15.1)"",
  ""pemja (==0.2.6) ; python_full_version >= \""3.7\"" and platform_system != \""Windows\""""
]{code}
 

As per [this comment|https://github.com/python-poetry/poetry/issues/3011#issuecomment-702826616], could you please republish the package correcting this metadata information, please? This [other comment|https://github.com/apple/turicreate/issues/3342#issuecomment-702957550] can help gain more context.

 

Thanks

 

 ","macos 12.6 (M1)

Poetry Version: 1.2.2
Python Version:  3.9.12",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 07:58:05 UTC 2022,,,,,,,,,,"0|z1anbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 15:13;martijnvisser;CC [~hxbks2ks] [~dianfu];;;","01/Nov/22 02:07;hxbks2ks;The `install_requirements` of PyFlink depends on your `python_version`.   
{code:python}
if sys.version_info < (3, 7):
        # python 3.6 upper and lower limit
        install_requires.append('numpy>=1.14.3,<1.20')
        install_requires.append('pandas>=1.0,<1.2.0')
        install_requires.append('pyarrow>=0.15.1,<7.0.0')
    else:
        # python 3.7, 3.8 and 3.9 upper limit and M1 chip lower limit,
        install_requires.append('numpy>=1.21.4,<1.22.0')
        install_requires.append('pandas>=1.3.0,<1.4.0')
        install_requires.append('pyarrow>=5.0.0,<9.0.0')
{code}
 So I guess Pypi collects the `metadata` used in Python3.6.
 
;;;","02/Nov/22 02:01;hxbks2ks;I think we can optimize the `metadata` to solve this problem in the 1.16.1.;;;","18/Nov/22 07:58;hxbks2ks;Merged into master via e5762a558f3697294cd73da4247a741fc6f73456
Merged into release-1.16 via d5b10d8ec9ca9fa03201ce57421bb0e714e224a7;;;",,,,,,,,,,,,,,,,,,,
Fix the bug that StreamTask doesn't handle exception during restoring,FLINK-29816,13493754,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,xieyi,xieyi,31/Oct/22 12:52,10/Oct/23 08:11,04/Jun/24 20:41,27/Feb/23 15:31,1.15.3,1.16.1,1.17.0,,,,1.17.0,,,,Runtime / Task,,,,0,pull-request-available,,,,"h4. 1. How to repeat 

ProcessWindowFunction, and make some exception in process()
test code
{code:java}
public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.enableCheckpointing(60 * 1000);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().setCheckpointTimeout(60000);

        KafkaSource<String> kafkaConsumer = KafkaSource.<String>builder()
                .setBootstrapServers(""****"")
                .setTopics(""****"")
                .setGroupId(""****"")
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .setStartingOffsets(OffsetsInitializer.earliest())
                .build();

        DataStreamSource<String> kafkaSource = env.fromSource(kafkaConsumer, WatermarkStrategy.noWatermarks(), ""Kafka Source"");

        SingleOutputStreamOperator<String> mapSourse = kafkaSource.keyBy(s -> s).window(TumblingProcessingTimeWindows.of(Time.seconds(15)))
                .process(new ProcessWindowFunction<String, String, String, TimeWindow>() {
                    @Override
                    public void process(String s, ProcessWindowFunction<String, String, String, TimeWindow>.Context context, Iterable<String> iterable, Collector<String> collector) throws Exception {
                        //when process event:""abc"" .It causes java.lang.NumberFormatException
                        Integer intS = Integer.valueOf(s);
                        collector.collect(s);
                    }
                })
                .name(""name-process"").uid(""uid-process"");

        mapSourse.print();
        env.execute();
    }
{code}
kafka input event
{code:java}
>1
>1
>2
>2
>3
>3
>abc
>abc
>
{code}
h4. 2. fault phenomena

when job process the event:""abc"",It will cause java.lang.NumberFormatException and failover ,Then attempt and failover continuously.
However, it only failover 2 times(attempt 0, attempt 1) and when attempt for third time, It work normally, and no exception
!image-2022-10-31-19-54-12-546.png!

checkpoint 1  complete in attempt 1,before failover exception 1
{code:java}
2022-10-31 16:59:53,644 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1667206793605 for job 7bca78a75b089d447bb4c99efcfd6527.2022-10-31 16:59:54,010 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Completed checkpoint 1 for job 7bca78a75b089d447bb4c99efcfd6527 (21630 bytes, checkpointDuration=333 ms, finalizationTime=72 ms).  {code}
 

attempt 2 was restore from checkpoint
{code:java}
2022-10-31 17:00:30,033 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 7bca78a75b089d447bb4c99efcfd6527 from Checkpoint 1 @ 1667206793605 for 7bca78a75b089d447bb4c99efcfd6527 located at hdfs://eadhadoop/user/sloth/sloth-fs-checkpoints/meta/1_7/7bca78a75b089d447bb4c99efcfd6527/chk-1.
{code}
 

 
h4. 3. possible reasons

during attempt 2 , task restore from checkpoint, userfunction in ProcessWindowFunction was called in SteamTask.restore and produce ""java.lang.NumberFormatException"", However, SteamTask catch exception and didn't handle exception because subtask is not in RUNNING state.

*the stack trace in attempt 2*
user function was called in SteamTask.restore(subtask state is INITIALIZING)
{code:java}
java.lang.Thread.getStackTrace(Thread.java:1552)
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:45)
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:40)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:57)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:32)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:568)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:524)
org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)
org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1693)
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1684)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:690)
org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
java.lang.Thread.run(Thread.java:745)
{code}
stack trace(which cause failover) in attempt 0 and attempt 1
user function was called in SteamTask.invoke
{code:java}
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:45)
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:40)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:57)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:32)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:568)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:524)
org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)
org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1693)
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1684)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
java.lang.Thread.run(Thread.java:745)
{code}
in org.apache.flink.streaming.runtime.tasks.StreamTask handleAsyncException
SteamTask only handleAsyncException when is Running==true
[https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L1540]
{code:java}
    @Override
    public void handleAsyncException(String message, Throwable exception) {
        if (isRunning) {
            // only fail if the task is still running
            asyncExceptionHandler.handleAsyncException(message, exception);
        }
    }
{code}
but during restore,isRunning==false
[https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L673]

 

So during Steam.restore, SteamTask skip exception in userfunction of ProcessWindowFunction.

 

 
h4.  

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30511,,,,,,,,,,,,,,,"31/Oct/22 11:49;xieyi;image-2022-10-31-19-49-52-432.png;https://issues.apache.org/jira/secure/attachment/13051642/image-2022-10-31-19-49-52-432.png","31/Oct/22 11:54;xieyi;image-2022-10-31-19-54-12-546.png;https://issues.apache.org/jira/secure/attachment/13051641/image-2022-10-31-19-54-12-546.png","02/Nov/22 02:42;xieyi;image-2022-11-02-10-42-21-099.png;https://issues.apache.org/jira/secure/attachment/13051687/image-2022-11-02-10-42-21-099.png","02/Nov/22 02:57;xieyi;image-2022-11-02-10-57-08-064.png;https://issues.apache.org/jira/secure/attachment/13051688/image-2022-11-02-10-57-08-064.png","02/Nov/22 03:06;xieyi;image-2022-11-02-11-06-37-925.png;https://issues.apache.org/jira/secure/attachment/13051692/image-2022-11-02-11-06-37-925.png","02/Nov/22 03:10;xieyi;image-2022-11-02-11-10-25-508.png;https://issues.apache.org/jira/secure/attachment/13051691/image-2022-11-02-11-10-25-508.png","22/Feb/23 09:26;RocMarshal;image-2023-02-22-17-26-06-200.png;https://issues.apache.org/jira/secure/attachment/13055722/image-2023-02-22-17-26-06-200.png",,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 14:49:51 UTC 2023,,,,,,,,,,"0|z1an60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 10:18;Weijie Guo;[~xieyi] Thanks for reporting this, which version of flink do you use when you encounter this problems? And did you use unalignCheckpoint?;;;","02/Nov/22 02:23;xieyi;[~Weijie Guo]  I didn't use unalignCheckpoint, I had test flink 1.14.0, 1.15.2, 1.16.0,  All of the version can repeat the problem.;;;","02/Nov/22 02:48;xieyi;I also found some phenomenon
h4. 1. userfunction in   ProcessWindowFunction was called during SteamTask.restore, because MailboxProcessore is doing Mail(""Timer callback for org.apache.flink.streaming.api.operators.InternalTimerServiceImpl"")

 

here is the Timer callback for '1667358089999'

!image-2022-11-02-11-06-37-925.png!

 
h4.  2. The  Mail (""Timer callback for org.apache.flink.streaming.api.operators.InternalTimerServiceImpl"") was put in Mailbiox during restoreGates and initializeStateAndOpenOperators

 

Timer callback for '1667358089999' was put in Mailbox, because during restoreGates and initializeStateAndOpenOperators, register timer '1667358089999'(it seems restored from state)

!image-2022-11-02-11-10-25-508.png!

 

 
 
 
 

 ;;;","02/Nov/22 03:14;Weijie Guo;[~xieyi] Yes,This phenomenon is normal. When restoring, the mailbox is working, which means that the recovered channel can process data normally . The key to the problem is that only after all channels are recovered, the StreamTask will become the running state, but all exceptions will be caught when the processFunction callback executes, and will only be thrown when the running state is reached.;;;","02/Nov/22 03:57;Weijie Guo;# Because the restored channel can work normally in this stage, and the registerTimer is part of the working logic of the windowOperator. If I remember correctly, the defaultAction of mailbox (the logic that the operator actually processes data) will not be run in the state recovery phase before the introduction of unaligned checkpoint. But the current implementation is that no matter whether unaligned checkpoint is enabled or not, it will undergo a transformation from recoveredInputChannel ->normal InputChannel, which brings some confusions.
 # This is exactly the workflow of window operator. We will register timers to trigger window calculation(including user function) when they are out of date. And we expect execute these callback in mailbox thread.
 # I still need to think about the solution to the problem that the error was swallowed in the recovery phase.;;;","17/Nov/22 09:43;kevin.cyj;Any update on this issue?;;;","20/Feb/23 10:57;RocMarshal;hi, [~Weijie Guo] Any process on this issue ? ;;;","20/Feb/23 11:14;Weijie Guo;[~RocMarshal] Thanks for the reminder. TBH, I don't have time to do this asap, feel free to take over this if you want.;;;","21/Feb/23 02:27;RocMarshal;[~Weijie Guo] thanks for the reply.
I'm interested in it. May I get the ticket ?
IMO, before starting the process, we need sort out the `handleAsyncException` mechanism & state-switch of `SteamTask` ;;;","21/Feb/23 02:37;Weijie Guo;[~RocMarshal] You are assigned.;;;","22/Feb/23 09:31;RocMarshal;As described in the historical comments and description text,

The exceptions happen to `StreamTask` during the `restore()` was ignored by `asyncExceptionHandler`.
At the `Execution` side, it is possible to enter the \{@code FAILED} state from any other state described at `ExecutionState` class. However, here's no `isInitializing` flag or `Initializing` state in StreamTask.
We can deal the issue with the state rule of `ExecutionState`.

- Introduce is `isInitializing` flag for `StreamTask` in order to help `asyncExceptionHandler` judge handle branch.   It is worth noting that such an approach would result in two adjacent states where it is unsafe to change the value of the flags, and we can only rely on overlapping boundary conditions to ensure that exceptions can be handled

!image-2023-02-22-17-26-06-200.png!


 * Or we can introduce a State Enum for `StreamTask` like `ExecutionState`, If so, we should ensure that the state introduced is simple and overrides the current StreamTask state transition as a basic standard,  and the security of state transitions(thread-safe).


Please let me know what's your opinon. Thanks so much~

CC [~xieyi] [~Weijie Guo] [~kevin.cyj] ;;;","22/Feb/23 10:32;Weijie Guo;Thanks [~RocMarshal] for your analysis. It is a good proposal to change the state of `StreamTask` to enumeration. I have a look at the recently active tickets and just found FLINK-13871 seems to be doing the same thing.;;;","22/Feb/23 13:56;fanrui;Hi [~Weijie Guo] [~RocMarshal] , thanks for your analysis and PR.

StreamTask should `handleAsyncException` during initializing, so introducing isInitializing can solve this bug. But it makes the state of StreamTask difficult to maintain.

 

Hi [~pnowojski] [~akalashnikov] , nice to see TaskState introduced in FLINK-13871 to improve the state of StreamTask.

I recommend fixing the bug before refactoring the state machine for two reasons:
1. After the introduction of initializing, the state of TaskState will become more complicated, and FLINK-13871 will do more thought when designing TaskState. If we design TaskState first, not sure if it will be easily compatible with initializing in the future.
2. Fix bugs before refactoring the state machine, it will be easier to backport the bugfix to 1.16 and 1.17.

WDYT?;;;","22/Feb/23 14:56;Weijie Guo;[~fanrui] If we prefer to fix it in the first way (that is, without introducing state enumeration), I agree to fix it before refactoring. But before starting this work, we'd better make sure that all participants in `FLINK-13871` knows this in advance or even directly participates in the code review, which will be more conducive to their overall control of the state machine design.;;;","22/Feb/23 15:39;fanrui;Hi [~Weijie Guo] , thanks for your reminder. I have requested them, and I can go ahead after they agree. 

Anyway, I don't have strong opinion about which PR is merged first.;;;","27/Feb/23 02:24;fanrui;Thanks [~RocMarshal] for your contribution, and [~Weijie Guo] [~akalashnikov] for the discussion and review:)

Merged commit [a98bb9a6c978d5458cf4cc11dc7b68034f4def4b|https://github.com/apache/flink/commit/a98bb9a6c978d5458cf4cc11dc7b68034f4def4b] into apache:master;;;","27/Feb/23 14:49;Weijie Guo;Thanks [~fanrui] for merging this to master.

All the commits are as follows:
master 1.18 via a98bb9a6c978d5458cf4cc11dc7b68034f4def4b.
release 1.17 via 65e455189d2a88fd62be2e8a464feab4687ee088.
release 1.16 via 2f0df8076cff97b579f739b9605d32704c428213.;;;",,,,,,
image apache/flink:1.16.0-scala_2.12 does not exist,FLINK-29815,13493752,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mbalassi,voidking,voidking,31/Oct/22 12:23,29/Nov/22 13:33,04/Jun/24 20:41,22/Nov/22 08:20,1.16.0,,,,,,,,,,,,,,0,,,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/]

In the document, image `apache/flink:1.16.0-scala_2.12` was used.

!image-2022-10-31-20-17-38-977.png!

However, there is no image named `apache/flink:1.16.0-scala_2.12` in docker hub. !image-2022-10-31-20-18-35-718.png!

[https://hub.docker.com/r/apache/flink/tags?page=1&name=1.16.0]

So, we should either modify the document or upload the image.

If the document should be modified, I'd like to make a pull request to github repo.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/22 12:17;voidking;image-2022-10-31-20-17-38-977.png;https://issues.apache.org/jira/secure/attachment/13051640/image-2022-10-31-20-17-38-977.png","31/Oct/22 12:18;voidking;image-2022-10-31-20-18-35-718.png;https://issues.apache.org/jira/secure/attachment/13051639/image-2022-10-31-20-18-35-718.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 13:33:03 UTC 2022,,,,,,,,,,"0|z1an5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 13:41;mbalassi;Thanks for reporting [~voidking] , taking a look.;;;","21/Nov/22 15:42;mbalassi;[~voidking] I reuploaded the 1.16.0 images, please check again:

[https://hub.docker.com/r/apache/flink/tags?page=1&name=1.16.0]

cc [~hxbks2ks] ;;;","29/Nov/22 13:33;voidking;Good job! The image now exists!;;;",,,,,,,,,,,,,,,,,,,,
Upgrade Stateful Functions to use Flink 1.15,FLINK-29814,13493733,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,galenwarren,Fil Karnicki,Fil Karnicki,31/Oct/22 11:43,26/Mar/23 20:04,04/Jun/24 20:41,01/Nov/22 16:54,,,,,,,,,,,Stateful Functions,,,,0,pull-request-available,,,,"Upgrade Statefun to use the latest Flink 1.15.x

 

There already exists a pull request without a jira. Perhaps it can be reviewed/amended as part of this task?

[https://github.com/apache/flink-statefun/pull/314]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 16:54:13 UTC 2022,,,,,,,,,,"0|z1an1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 16:54;tzulitai;Merged to flink-statefun/master: b4a85ef031e539e7c2580a796246e2896d533d8f;;;",,,,,,,,,,,,,,,,,,,,,,
Remove deprecated Curator API usages,FLINK-29813,13493726,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,31/Oct/22 11:22,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32204,FLINK-31995,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 13:30:57 UTC 2023,,,,,,,,,,"0|z1amzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/23 07:04;mapohl;Are you already working on that one? I could pick it up since I'm going through the leader election code for [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+refactoring+leaderelection+to+make+flink+support+multi-component+leader+election+out-of-the-box]/FLINK-26522 anyway.;;;","05/Apr/23 07:21;chesnay;I have some WIP code here: https://github.com/zentol/flink/commits/deprecation_curator

IIRC I stopped because it wasn't immediately clear what the repercussions of these changes would be when running with older ZK versions.;;;","30/May/23 13:30;mapohl;Just as a fyi: FLINK-32204 was caused by the {{TreeCache}} not being thread-safe when shutting down the passed {{executorService}}. If there's a late event coming in that is processed in the curator's event thread while the cache is closed in another thread, it might be that we run into a {{RejectedExecutionException}} because the executor might have been shutdown while the late event is submitted to be processed (see the [FLINK-32204 comment|https://issues.apache.org/jira/browse/FLINK-32204?focusedCommentId=17726613&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17726613]). This became apparent when FLINK-31995 was introduced which added shutdown handling to the {{DirectExecutorService}}.;;;",,,,,,,,,,,,,,,,,,,,
Remove deprecated Netty API usages,FLINK-29812,13493725,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,31/Oct/22 11:22,02/Nov/22 07:12,04/Jun/24 20:41,02/Nov/22 07:12,,,,,,,1.17.0,,,,Runtime / REST,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 07:12:00 UTC 2022,,,,,,,,,,"0|z1amzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 07:12;chesnay;master: 03c0f155c2f5198bf1fda35de43afc34a4b12f6e;;;",,,,,,,,,,,,,,,,,,,,,,
HadoopOSSFileSystemBehaviorITCase is not executed on AzureCI,FLINK-29811,13493724,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,31/Oct/22 11:16,11/Nov/22 08:31,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,Connectors / FileSystem,Test Infrastructure,Tests,,0,,,,,{{HadoopOSSFileSystemBehaviorITCase}} is not executed in AzureCI due to missing setup of the corresponding environment (i.e. missing environment variables and corresponding environment).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29810,,,,,,,FLINK-28542,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-31 11:16:03.0,,,,,,,,,,"0|z1amzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AzureFileSystemBehaviorITCase is not executed on AzureCI,FLINK-29810,13493723,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,31/Oct/22 11:13,11/Nov/22 08:31,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,Connectors / FileSystem,Test Infrastructure,Tests,,0,,,,,"{{AzureFileSystemBehaviorITCase}} is not executed in AzureCI due to missing setup of the corresponding environment (i.e. missing environment variables and corresponding environment).

It's related to FLINK-12654 which addresses the e2e tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12654,,,FLINK-29811,,,,FLINK-28542,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-31 11:13:17.0,,,,,,,,,,"0|z1amz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST API for running a Flink job,FLINK-29809,13493713,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,nguyenmphu,nguyenmphu,31/Oct/22 10:33,02/Dec/22 03:20,04/Jun/24 20:41,02/Dec/22 03:20,,,,,,,,,,,Runtime / REST,,,,0,,,,,"When I want to submit a Flink job, I have to run `flink run ...` or submit a jar via Flink web. But in our production environment, we cannot connect to the flink server and run the command or submit a jar file via the web. So I need a REST API to trigger a jar file in Flink server.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 09:39:57 UTC 2022,,,,,,,,,,"0|z1amww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 11:53;chesnay;How exactly do you expect the jar to transferred to the Flink server if you're unable to submit it via any existing mean?;;;","31/Oct/22 14:28;nguyenmphu;Our dev team deploys all Flink servers and applications (jar files) to the production environment. But they don't have permission to access the servers, can't get jar files, and only access the flink web through the remote desktop.;;;","03/Nov/22 07:16;zhuzh;Flink already supports submitting a job using REST API and this is how submitting from Flink web UI works. (https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-1)
However, the question is, you should be able to interact with the web UI if you can have access to Flink REST API. ;;;","17/Nov/22 09:39;kevin.cyj;Do you mean to pass an URL to Flink through the REST API and Flink APP itself gets the jar from the URL and runs it?;;;",,,,,,,,,,,,,,,,,,,
Grant leadership to leaderContender while it is closing may lead to deadlock,FLINK-29808,13493707,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Weijie Guo,Weijie Guo,31/Oct/22 10:00,09/Feb/23 09:59,04/Jun/24 20:41,09/Feb/23 09:59,1.17.0,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"If we grant leadership to a leaderContender, but it is closing at the same time, deadlock may occur. This phenomenon was observed in 'JobMasterServiceLeadershipRunner'. Through some investigation, I found that `ResourceManagerServiceImpl` took some measures to avoid this problem, and FLINK-29234 try to fix this problem in a similar way.

However, there are other implementations of leaderContender. We need to check the code and introduce some tests when necessary to ensure that they will not suffer the same problem.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30195,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-31 10:00:49.0,,,,,,,,,,"0|z1amvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop TypeSerializerConfigSnapshot and savepoint support from Flink versions < 1.8.0,FLINK-29807,13493704,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,31/Oct/22 09:54,14/Dec/23 15:15,04/Jun/24 20:41,04/Nov/22 16:46,,,,,,,1.17.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"The motivation behind this move is two fold. One reason is that it complicates our code base unnecessarily and creates confusion on how to actually implement custom serializers. The immediate reason is that I wanted to clean up Flink's configuration stack a bit and refactor the ExecutionConfig class FLINK-29379. This refactor would keep the API compatibility of the ExecutionConfig, but it would break savepoint compatibility with snapshots written with some of the old serializers, which had ExecutionConfig as a field and were serialized in the snapshot. This issue has been resolved by the introduction of TypeSerializerSnapshot in Flink 1.7 FLINK-9377, where serializers are no longer part of the snapshot.

TypeSerializerConfigSnapshot has been deprecated and no longer used by built-in serializers since Flink 1.8 FLINK-9376 and FLINK-11323. Users were encouraged to migrate to TypeSerializerSnapshot since then with their own custom serializers. That has been plenty of time for the migration.

This proposal would have the following impact for the users:
1. we would drop support for recovery from savepoints taken with Flink < 1.7.0 for all built in types serializers
2. we would drop support for recovery from savepoints taken with Flink < 1.8.0 for built in kryo serializers
3. we would drop support for recovery from savepoints taken with Flink < 1.17 for custom serializers using deprecated TypeSerializerConfigSnapshot

1. and 2. would have a simple migration path. Users migrating from those old savepoints would have to first start his job using a Flink version from the [1.8, 1.16] range, and take a new savepoint that would be compatible with Flink 1.17.
3. This is a bit more problematic, because users would have to first migrate their own custom serializers to use TypeSerializerSnapshot (using a Flink version from the [1.8, 1.16]), take a savepoint, and only then migrate to Flink 1.17. However users had already 4 years to migrate, which in my opinion has been plenty of time to do so.

*As discussed and vote is currently in progress:* https://lists.apache.org/thread/x5d0p08pf2wx47njogsgqct0k5rpfrl4
",,,,,,,,,,,,,,FLINK-29379,,,,,,,,,,,,,,,,,,,FLINK-11323,FLINK-9376,FLINK-9377,FLINK-31167,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 16:46:11 UTC 2022,,,,,,,,,,"0|z1amuw:",9223372036854775807,"Savepoints using `TypeSerializerConfigSnapshot` are no longer supported. That means, all savepoints from Flink < 1.8.0 are no longer supported. Furthermore, savepoints from Flink < 1.17.0 created with custom serialiser using deprecated since Flink 1.8.0 class `TypeSerializerConfigSnapshot` are also no longer supported.

If you are only using built-in serialisers (Pojo, Kryo, Avro, Tuple, ...), and your savepoint is from 1.8.0 <= Flink, you don't have to do anything.
If you are only using built-in serialisers (Pojo, Kryo, Avro, Tuple, ...), and your savepoint is from Flink < 1.8.0, please first upgrade your job to 1.8.x <= Flink <= 1.16.x, before upgrading in the second step to Flink >= 1.17.x.
If previously you were using a custom serialiser that depends on `TypeSerializerConfigSnapshot`, please first while still using 1.8.x <= Flink <= 1.16.x upgrade your serialiser to `TypeSerializerSnapshot`, take a savepoint and restore from that savepoint in Flink >= 1.7.0",,,,,,,,,,,,,,,,,,,"04/Nov/22 16:46;pnowojski;Merged to master as 1e131d2a777..32cc0143753;;;",,,,,,,,,,,,,,,,,,,,,,
Move related class  StateChangeOperation  into package org.apache.flink.state.changelog.restore,FLINK-29806,13493697,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,senbork,senbork,31/Oct/22 09:45,31/Oct/22 15:15,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,,"*Issue Description*

 ** 

Package _*org.apache.flink.state.changelog*_ groups together miscellaneous classes that might be useful to different packages. The package structure violates the “high cohesion and low coupling” design rules. I found that class *_StateChangeOperation_* strongly interacts with classes in the other package ({*}_org.apache.flink.state.changelog.restore_{*}) by checking the source code and the snippet of the dependency graph in the attachments. The project has grown larger, leading to becoming increasingly hard to maintain. During the development process, one groups together classes (that often co-change) with similar responsibilities in one package to facilitate maintenance, which prevents a change that causes other packages to be modified. For example, if one modifies package _*org.apache.flink.state.changelog*_ (i.e., package rename), resulting in multiple classes of package *_org.apache.flink.state.changelog.restore_* __ to be changed.

 

Location: The source file can be found at path File  flink-statebackend-changelog/src/main/java/org/apache/flink/state/changelog/StateChangeOperation.java

 ** 

*Refactoring suggestions*

 

I suggest to move related class  {*}_StateChangeOperation  into package org.apache.flink.state.changelog.restore_{*}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/22 09:44;senbork;StateChangeOperation-java.png;https://issues.apache.org/jira/secure/attachment/13051630/StateChangeOperation-java.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 09:57:39 UTC 2022,,,,,,,,,,"0|z1amtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 09:57;Yanfei Lei;Hi [~senbork], thanks for your suggestion, but I don't think the refactoring is necessary, because

1){{{}StateChangeOperation{}}} is also used by {{{}AbstractStateChangeLogger{}}}(located in {*}_org.apache.flink.state.changelog_{*}{_}),{_}  

2) _org.apache.flink.state.changelog.restore_ is the sub-directory of  {_}org.apache.flink.state.changelog{_}, I don't this violates the “high cohesion and low coupling” design rules.;;;",,,,,,,,,,,,,,,,,,,,,,
"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow",FLINK-29805,13493693,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,31/Oct/22 09:17,08/Nov/22 07:00,04/Jun/24 20:41,08/Nov/22 07:00,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow.

This is due to a bug in {{FileStoreCommitImpl#filterCommitted}}. When this method finds an identifier, it removes the identifier from a map. However different snapshots may have the same identifier (for example an APPEND commit and the following COMPACT commit will have the same identifier), so we need to use another set to check for identifiers.

When snapshot committing is fast there is at most 1 identifier to check after the job restarts, so nothing happens. However when snapshot committing is slow, there will be multiple identifiers to check and some identifiers will be mistakenly kept.",,,,,,,,,,,,FLINK-29842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 07:00:21 UTC 2022,,,,,,,,,,"0|z1amsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 07:00;TsReaper;master: 0a193a8110aa4716250fed3c4223018ab519c9b1
release-0.2: d41d14a5921aa27ebd903eed45365841093e97ed;;;",,,,,,,,,,,,,,,,,,,,,,
Support convertion of Correlate to Join if correlation variable is unused introduced in Calcite 1.28,FLINK-29804,13493688,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,31/Oct/22 08:38,15/Nov/22 00:29,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"It was introduced in https://issues.apache.org/jira/browse/CALCITE-4668
 and leads to issues in a number of tests like {{SetOperatorsTest}}, {{CorrelateTest}}, {{SetOperatorsTest}}, {{TemporalTableFunctionJoinTest}} and probably some integration tests

An example of failure
{noformat}

org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

FlinkLogicalJoin(condition=[true], joinType=[inner])
:- FlinkLogicalCalc(select=[c])
:  +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+- FlinkLogicalCalc(select=[d], where=[>(e, 20)])
   +- FlinkLogicalTableFunctionScan(invocation=[*org.apache.flink.table.planner.utils.TableFunc0*($2)], rowType=[RecordType(VARCHAR(2147483647) d, INTEGER e)])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.

	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:93)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:982)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:896)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:658)
	at org.apache.flink.table.planner.plan.batch.table.CorrelateTest.testCorrelateWithMultiFilterAndWithoutCalcMergeRules(CorrelateTest.scala:106)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=BATCH_PHYSICAL, FlinkRelDistributionTraitDef=any, sort=[].
Missing conversion is FlinkLogicalTableFunctionScan[convention: LOGICAL -> BATCH_PHYSICAL]
There is 1 empty subset: rel#394:RelSubset#8.BATCH_PHYSICAL.any.[], the relevant part of the original plan is as follows
377:FlinkLogicalTableFunctionScan(invocation=[*org.apache.flink.table.planner.utils.TableFunc0*($2)], rowType=[RecordType(VARCHAR(2147483647) d, INTEGER e)])

Root: rel#388:RelSubset#10.BATCH_PHYSICAL.any.[]
Original rel:
FlinkLogicalJoin(subset=[rel#344:RelSubset#5.LOGICAL.any.[]], condition=[true], joinType=[inner]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.00000001E8 cpu, 1.200000001E9 io, 0.0 network, 0.0 memory}, id = 356
  FlinkLogicalCalc(subset=[rel#354:RelSubset#1.LOGICAL.any.[]], select=[c]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 357
    FlinkLogicalLegacyTableSourceScan(subset=[rel#347:RelSubset#0.LOGICAL.any.[]], table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}, id = 346
  FlinkLogicalCalc(subset=[rel#355:RelSubset#4.LOGICAL.any.[]], select=[d], where=[>(e, 20)]): rowcount = 1.0, cumulative cost = {1.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 363
    FlinkLogicalTableFunctionScan(subset=[rel#350:RelSubset#2.LOGICAL.any.[]], invocation=[*org.apache.flink.table.planner.utils.TableFunc0*($2)], rowType=[RecordType(VARCHAR(2147483647) d, INTEGER e)]): rowcount = 1.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 349

Sets:
Set#6, type: RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c)
	rel#380:RelSubset#6.LOGICAL.any.[], best=rel#346
		rel#346:FlinkLogicalLegacyTableSourceScan.LOGICAL.any.[](table=[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]],fields=a, b, c), rowcount=1.0E8, cumulative cost={1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}
	rel#391:RelSubset#6.BATCH_PHYSICAL.any.[], best=rel#390
		rel#390:BatchPhysicalLegacyTableSourceScan.BATCH_PHYSICAL.any.[](table=[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]],fields=a, b, c), rowcount=1.0E8, cumulative cost={1.0E8 rows, 0.0 cpu, 2.4E9 io, 0.0 network, 0.0 memory}
Set#7, type: RecordType(VARCHAR(2147483647) c)
	rel#382:RelSubset#7.LOGICAL.any.[], best=rel#381
		rel#381:FlinkLogicalCalc.LOGICAL.any.[](input=RelSubset#380,select=c), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}
	rel#393:RelSubset#7.BATCH_PHYSICAL.any.[], best=rel#392
		rel#392:BatchPhysicalCalc.BATCH_PHYSICAL.any.[](input=RelSubset#391,select=c), rowcount=1.0E8, cumulative cost={2.0E8 rows, 0.0 cpu, 2.4E9 io, 0.0 network, 0.0 memory}
Set#8, type: RecordType(VARCHAR(2147483647) d, INTEGER e)
	rel#383:RelSubset#8.LOGICAL.any.[], best=rel#377
		rel#377:FlinkLogicalTableFunctionScan.LOGICAL.any.[](invocation=*org.apache.flink.table.planner.utils.TableFunc0*($2),rowType=RecordType(VARCHAR(2147483647) d, INTEGER e)), rowcount=1.0, cumulative cost={1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}
	rel#394:RelSubset#8.BATCH_PHYSICAL.any.[], best=null
Set#9, type: RecordType(VARCHAR(2147483647) d)
	rel#385:RelSubset#9.LOGICAL.any.[], best=rel#384
		rel#384:FlinkLogicalCalc.LOGICAL.any.[](input=RelSubset#383,select=d,where=>(e, 20)), rowcount=1.0, cumulative cost={2.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}
	rel#396:RelSubset#9.BATCH_PHYSICAL.any.[], best=null
		rel#395:BatchPhysicalCalc.BATCH_PHYSICAL.any.[](input=RelSubset#394,select=d,where=>(e, 20)), rowcount=1.0, cumulative cost={inf}
		rel#399:AbstractConverter.BATCH_PHYSICAL.broadcast.[](input=RelSubset#396,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=broadcast,sort=[]), rowcount=1.0, cumulative cost={inf}
		rel#403:BatchPhysicalExchange.BATCH_PHYSICAL.broadcast.[](input=RelSubset#396,distribution=broadcast), rowcount=1.0, cumulative cost={inf}
	rel#398:RelSubset#9.BATCH_PHYSICAL.broadcast.[], best=null
		rel#399:AbstractConverter.BATCH_PHYSICAL.broadcast.[](input=RelSubset#396,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=broadcast,sort=[]), rowcount=1.0, cumulative cost={inf}
		rel#403:BatchPhysicalExchange.BATCH_PHYSICAL.broadcast.[](input=RelSubset#396,distribution=broadcast), rowcount=1.0, cumulative cost={inf}
Set#10, type: RecordType(VARCHAR(2147483647) c, VARCHAR(2147483647) d)
	rel#387:RelSubset#10.LOGICAL.any.[], best=rel#386
		rel#386:FlinkLogicalJoin.LOGICAL.any.[](left=RelSubset#382,right=RelSubset#385,condition=true,joinType=inner), rowcount=1.0E8, cumulative cost={3.00000002E8 rows, 2.00000002E8 cpu, 3.600000001E9 io, 0.0 network, 0.0 memory}
	rel#388:RelSubset#10.BATCH_PHYSICAL.any.[], best=null
		rel#389:AbstractConverter.BATCH_PHYSICAL.any.[](input=RelSubset#387,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#400:BatchPhysicalNestedLoopJoin.BATCH_PHYSICAL.any.[](left=RelSubset#393,right=RelSubset#398,joinType=InnerJoin,where=true,select=c, d,build=right), rowcount=1.0E8, cumulative cost={inf}

Graphviz:
digraph G {
	root [style=filled,label=""Root""];
	subgraph cluster6{
		label=""Set 6 RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c)"";
		rel346 [label=""rel#346:FlinkLogicalLegacyTableSourceScan\ntable=[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]],fields=a, b, c\nrows=1.0E8, cost={1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel390 [label=""rel#390:BatchPhysicalLegacyTableSourceScan\ntable=[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]],fields=a, b, c\nrows=1.0E8, cost={1.0E8 rows, 0.0 cpu, 2.4E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		subset380 [label=""rel#380:RelSubset#6.LOGICAL.any.[]""]
		subset391 [label=""rel#391:RelSubset#6.BATCH_PHYSICAL.any.[]""]
	}
	subgraph cluster7{
		label=""Set 7 RecordType(VARCHAR(2147483647) c)"";
		rel381 [label=""rel#381:FlinkLogicalCalc\ninput=RelSubset#380,select=c\nrows=1.0E8, cost={2.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel392 [label=""rel#392:BatchPhysicalCalc\ninput=RelSubset#391,select=c\nrows=1.0E8, cost={2.0E8 rows, 0.0 cpu, 2.4E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		subset382 [label=""rel#382:RelSubset#7.LOGICAL.any.[]""]
		subset393 [label=""rel#393:RelSubset#7.BATCH_PHYSICAL.any.[]""]
	}
	subgraph cluster8{
		label=""Set 8 RecordType(VARCHAR(2147483647) d, INTEGER e)"";
		rel377 [label=""rel#377:FlinkLogicalTableFunctionScan\ninvocation=*org.apache.flink.table.planner.utils.TableFunc0*($2),rowType=RecordType(VARCHAR(2147483647) d, INTEGER e)\nrows=1.0, cost={1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		subset383 [label=""rel#383:RelSubset#8.LOGICAL.any.[]""]
		subset394 [label=""rel#394:RelSubset#8.BATCH_PHYSICAL.any.[]"",color=red]
	}
	subgraph cluster9{
		label=""Set 9 RecordType(VARCHAR(2147483647) d)"";
		rel384 [label=""rel#384:FlinkLogicalCalc\ninput=RelSubset#383,select=d,where=>(e, 20)\nrows=1.0, cost={2.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel395 [label=""rel#395:BatchPhysicalCalc\ninput=RelSubset#394,select=d,where=>(e, 20)\nrows=1.0, cost={inf}"",shape=box]
		rel399 [label=""rel#399:AbstractConverter\ninput=RelSubset#396,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=broadcast,sort=[]\nrows=1.0, cost={inf}"",shape=box]
		rel403 [label=""rel#403:BatchPhysicalExchange\ninput=RelSubset#396,distribution=broadcast\nrows=1.0, cost={inf}"",shape=box]
		subset385 [label=""rel#385:RelSubset#9.LOGICAL.any.[]""]
		subset396 [label=""rel#396:RelSubset#9.BATCH_PHYSICAL.any.[]""]
		subset398 [label=""rel#398:RelSubset#9.BATCH_PHYSICAL.broadcast.[]""]
		subset396 -> subset398;	}
	subgraph cluster10{
		label=""Set 10 RecordType(VARCHAR(2147483647) c, VARCHAR(2147483647) d)"";
		rel386 [label=""rel#386:FlinkLogicalJoin\nleft=RelSubset#382,right=RelSubset#385,condition=true,joinType=inner\nrows=1.0E8, cost={3.00000002E8 rows, 2.00000002E8 cpu, 3.600000001E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel389 [label=""rel#389:AbstractConverter\ninput=RelSubset#387,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]\nrows=1.0E8, cost={inf}"",shape=box]
		rel400 [label=""rel#400:BatchPhysicalNestedLoopJoin\nleft=RelSubset#393,right=RelSubset#398,joinType=InnerJoin,where=true,select=c, d,build=right\nrows=1.0E8, cost={inf}"",shape=box]
		subset387 [label=""rel#387:RelSubset#10.LOGICAL.any.[]""]
		subset388 [label=""rel#388:RelSubset#10.BATCH_PHYSICAL.any.[]""]
	}
	root -> subset388;
	subset380 -> rel346[color=blue];
	subset391 -> rel390[color=blue];
	subset382 -> rel381[color=blue]; rel381 -> subset380[color=blue];
	subset393 -> rel392[color=blue]; rel392 -> subset391[color=blue];
	subset383 -> rel377[color=blue];
	subset385 -> rel384[color=blue]; rel384 -> subset383[color=blue];
	subset396 -> rel395; rel395 -> subset394;
	subset398 -> rel399; rel399 -> subset396;
	subset398 -> rel403; rel403 -> subset396;
	subset387 -> rel386[color=blue]; rel386 -> subset382[color=blue,label=""0""]; rel386 -> subset385[color=blue,label=""1""];
	subset388 -> rel389; rel389 -> subset387;
	subset388 -> rel400; rel400 -> subset393[label=""0""]; rel400 -> subset398[label=""1""];
}
	at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:709)
	at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:390)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:533)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:317)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
	... 55 more

{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-5381,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-31 08:38:41.0,,,,,,,,,,"0|z1amrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API Scala APIs lack proper source jars,FLINK-29803,13493683,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,31/Oct/22 08:11,01/Nov/22 09:23,04/Jun/24 20:41,01/Nov/22 09:23,1.15.0,,,,,,1.15.3,1.16.1,1.17.0,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 09:23:48 UTC 2022,,,,,,,,,,"0|z1amq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 09:23;chesnay;master: 0bc9c538ac71249d39a27a0a6cca0ad9a0a87d8f
1.16: e9a3f7ca1002be5d119efce1aad79c7023bc1bf0
1.15: e45e5c3fa7d6c5d7d6b88e8a99cf0028987014a8;;;",,,,,,,,,,,,,,,,,,,,,,
ChangelogStateBackend supports native savepoint,FLINK-29802,13493663,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,31/Oct/22 06:16,22/Feb/24 05:39,04/Jun/24 20:41,22/Feb/24 05:39,,,,,,,1.20.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 05:39:00 UTC 2024,,,,,,,,,,"0|z1amls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 05:39;masteryhx;merged b62de02f...9308e10c into master.;;;",,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinator need open the way to operate metricGroup interface,FLINK-29801,13493662,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,MengYue,MengYue,MengYue,31/Oct/22 06:15,01/Mar/23 03:48,04/Jun/24 20:41,16/Feb/23 13:46,,,,,,,1.18.0,,,,Runtime / Metrics,,,,0,pull-request-available,,,,"Currently, We have no way to get metric group instances in OperatorCoordinator

In some cases, we may report some metric in OperatorCoordinator such as Flink hudi integrate scene， some meta will send to operator coordinator to commit to hdfs or hms

but we also need to report some metrics in operator coordinator for monitor purpose",,,,,,,,,,,,,,,,,,FLINK-31268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 13:46:49 UTC 2023,,,,,,,,,,"0|z1amlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 06:25;MengYue;[~jark] [~rmetzger]  Can you review this improvement, because we really need it in Hudi to report some metric;;;","31/Oct/22 06:27;MengYue;[~danny0405] Can you review this improvement, we need this to report some metrics in stream write coordinator;;;","31/Oct/22 06:37;jark;I think this is a nice feature.  [~zhuzh] what do you think? ;;;","03/Nov/22 06:50;zhuzh;The requirement makes sense. I will take a look at the pull request.;;;","17/Nov/22 10:56;ruanhang1993;Hi, [~MengYue] ,

I may rely on this feature to accomplish https://issues.apache.org/jira/browse/FLINK-21000. In the feature https://issues.apache.org/jira/browse/FLINK-21000, I provide a POC which contains some parts about the metric group in OperatorCoordinator. 

With these research, I have some question about this PR:
 # How should we use the operator coordinator metric group when implement the OperatorCoordinator? For example, we always get the metric group from one context in the SourceCoordinator. Why not pass the metric group in its context?
 # Is there some problems to use the JobManagerJobMetricGroup? There may be serval operator coordinators in the same job. It seems that every operator coordinator should make sure that its metric group does not conflict with others. 

ps: It is my POC(https://github.com/ruanhang1993/flink/commit/dc41d3ab465d03e2829e5f1d10137ea34621ad87).  I am very glad to help if you need.

Best,  Hang;;;","17/Nov/22 12:44;MengYue;[~ruanhang1993] [~zhuzh] 

Firstly, thanks a lot for your reply

The following answer to your 2 question
 # Each implementation of OperatorCoordinator will hold the metric group instance that can be registered for anything they wanted. this feature is not only for SourceCoordinator, eg: what about the other operator coordinators who don't have a context
 # Each operator coordinator's metric group is distinguished by metric group name

Secondly

This implement already worked in our product env, if we had or wanted a new design, we can push it forward and finish quickly because we real need it;;;","21/Nov/22 03:52;ruanhang1993;Hi, [~MengYue] ,

For the question 1, what I mean is better to pass the metric group in the `OperatorCoordinator.Context` instead of using it by invoking the method `registeMetric` provided in the PR. 

For the question 2, it seems that the implementations of `OperatorCoordinator` need to distinguish the metric group name by self, which may need to be done by Flink.

IMO, I think we need a new metric group for the `OperatorCoordinator`. And we need the FLIP for the change and a discuss it in the community.

This needs to spend some time in the discussion it in the community.  ;;;","21/Nov/22 06:46;MengYue;Hi [~ruanhang1993], [~zhuzh] 

A further consideration is good, very happy to hear that and looking forward to this Flip, I will focus on any progress of this Flip and It would be even better if  I could help it finish, If any part needs me to join in， just let me know

 

 ;;;","24/Nov/22 03:41;ruanhang1993;Hi, [~MengYue] ,

I have created a FLIP([https://cwiki.apache.org/confluence/display/FLINK/FLIP-274%3A+Introduce+metric+group+for+OperatorCoordinator]) about this issue. Could you help to review this FLIP when you get time? Any comments are appreciated.

Maybe we could discuss through Slack or Dingding. Here is my e-mail(ruanhang1993@hotmail.com).

Thanks~;;;","24/Nov/22 06:49;MengYue;[~ruanhang1993]   I will review this FLIP later and looking forward to working with you on this FLIP, can you tell me your Slack or Dingding Id;;;","24/Nov/22 08:47;ruanhang1993;Hi，[~MengYue] , what is your e-mail? I will send an e-mail to you. Thanks.;;;","24/Nov/22 08:50;MengYue;[~ruanhang1993]  Here is my email(272614347@qq.com);;;","29/Nov/22 00:22;stevenz3wu;This is an important missing part of coordinator. I am also very interested in this for FLIP-27 Iceberg source. [~MengYue] [~ruanhang1993] do you want to start a discussion thread of the FLIP-274?

I have two questions.

1. There is already an empty `OperatorCoordinatorMetricGroup` interface in Flink. what is the reason to add this method to the interface?
{code}
    /**
     * The total number of events received since the operator coordinator started.
     */
    Counter getNumEventsInCounter();
{code}

2. How can the metric group be passed to `SourceCoordinatorContext`?;;;","01/Dec/22 07:48;ruanhang1993;Hi, [~stevenz3wu] ,

For question 1,  we want to provide some common metrics in the `OperatorCoordinatorMetricGroup` like other metric groups, such as `SinkWriterMetricGroup`. So we provide the `numEventsIn` for it.

For question 2, when invoking `SourceCoordinatorProvider#getCoordinator(OperatorCoordinator.Context context)`, we could get the `OperatorCoordinatorMetricGroup` by the method `metricGroup()` in `OperatorCoordinator.Context`. The detail will be contained in the other issue https://issues.apache.org/jira/browse/FLINK-21000 after this issue.

The discussion will be sent this week or later. Thanks for the reply.;;;","01/Dec/22 16:57;stevenz3wu;@ruanhang1993 I am not sure if `numEventsIn` is a very useful metrics. maybe hold back adding it until there is a clear value of it. If we do want to expose it, it will be better to add a tag of the event type/class name.;;;","16/Feb/23 13:46;chesnay;master: 7bebd2d9fac517c28afc24c0c034d77cfe2b43a6;;;",,,,,,,
Continuous failover will leak the inprogress output file,FLINK-29800,13493656,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,31/Oct/22 04:54,01/Nov/22 03:09,04/Jun/24 20:41,,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"When running job which sink to the file system, the inprogress files will keep growing when job keeps failover, it will do harm to the filesystem. I think the clean up to the file which is currently written to should be performed when job failing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 03:08:58 UTC 2022,,,,,,,,,,"0|z1amk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 06:08;luoyuxia;What's you idea for cleaning  up these files? From my sides, it may be not a trivial work.;;;","31/Oct/22 08:21;martijnvisser;I also doubt that cleaning-up this is easy, especially with exactly-once semantics involved;;;","31/Oct/22 10:46;aitozi;Thanks [~luoyuxia]  [~martijnvisser]  for your comments. In my case, the residual files is caused by the inprogress files between the checkpoint is not recorded in the {{{}BucketState{}}}. So it will generate a new part file to write to after restore.

My proposal is to clean the inprogress files which are not a part of the {{inProgressFileRecoverablesPerCheckpoint}} and {{{}pendingFileRecoverablesPerCheckpoint{}}}. This means these files can not be used during the restoring. So I think these inprogress files can be clean up when closing the {{Bucket}} during the failover.

Looking forward to your opinions;;;","31/Oct/22 11:37;luoyuxia;[~aitozi] Thanks for explaination. I have two question:

1: How do you know these files that should be cleaned? List the written files and then exclude the files recorded in  `inProgressFileRecoverablesPerCheckpoint` and `pendingFileRecoverablesPerCheckpoint`?

2: When to clean up these files? More exactly, in which method do we clean up these files?;;;","31/Oct/22 12:57;aitozi;[~luoyuxia] 
1. The current inprogress files can be record in the {{Bucket}} as a new field. Each new created inprogress will be append to it. When prepareCheckpoint, the correspond file will be marked as checkpointed. 
2. Then, during failover, the close -> disposePartFile will be executed (or a new dedicated method can be introduced which will be executed during close). The inprogress files which are not marked checkpointed can be clean up safely. 

IMO, there is no exactly point can guarantee the inprogress file checkpointed atomic. If checkpointed, the inprogress file should not be deleted. So , the safe way in my mind is to mark it as checkpointed when preparing checkpoint.

By this way, we can clean the most residual files except the failure during the preparing checkpoint and checkpoint succeed;;;","01/Nov/22 02:53;luoyuxia;> The current inprogress files can be record in the {{Bucket}} as a new field

Seems the existing `inProgressPart` in Bucket can meet such need?

If so, the ` inProgressPart` will be disposed in close method in the current code base.

 ;;;","01/Nov/22 03:08;aitozi;The inProgressPart seems only record current one inprogress file. Between the checkpoints, maybe serval files will rolled out. IIUC, these inprogress/pending files only should be kept when they are part of the checkpoints.

And the dispose only close the output stream of the file, the files will still there.;;;",,,,,,,,,,,,,,,,
How to override ConfigMap values while deploying the Operator via OLM?,FLINK-29799,13493634,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Won't Do,,ckavili,ckavili,30/Oct/22 16:30,31/Oct/22 08:05,04/Jun/24 20:41,31/Oct/22 08:05,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Hi,

Flink Kubernetes Operator is great - thank you! I deployed it via OLM on OpenShift 4.10 and would like to override some config from `flink-operator-config` CM. When I override a value manually, it persists (which is strange - it's a resource managed via Operator) so when I change a value in CM and restart Operator, it works. How can I give the parameters I want while installing the operator? Some operators support setting environment variables in `Subscription` object but I couldn't find such a thing in the documentation. I can do it easily with Helm installation but I want to use OLM. It'd be appreciated if you can guide me. Many many thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 08:05:00 UTC 2022,,,,,,,,,,"0|z1amfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 08:05;gyfora;The OLM installation is currently not supported by the community. You can ask on the user mailing list / slack you might be able to find some help.;;;",,,,,,,,,,,,,,,,,,,,,,
Rename K8s operator client code module,FLINK-29798,13493627,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,30/Oct/22 14:36,15/Nov/22 21:44,04/Jun/24 20:41,15/Nov/22 21:44,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The example code module in the k8s operator is named simply kubernetes-client-examples, and thus is published like so:

[https://repo1.maven.org/maven2/org/apache/flink/kubernetes-client-examples/1.2.0/]

We should make this more specific.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 21:44:56 UTC 2022,,,,,,,,,,"0|z1ame0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/22 12:21;mbalassi;After more consideration I decided to exclude these modules from the maven deployment without renaming.;;;","15/Nov/22 21:44;mbalassi;[{{757a507}}|https://github.com/apache/flink-kubernetes-operator/commit/757a5079a50625de1c46ea5556cdd0766dc00361] in main;;;",,,,,,,,,,,,,,,,,,,,,
"can't run a job on yarn, if set fs.default-scheme",FLINK-29797,13493603,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Melon Wang,Melon Wang,30/Oct/22 02:54,27/Apr/24 11:50,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Deployment / YARN,,,,0,pull-request-available,,,,"If I set the value of *fs.default-scheme* to other scheme and run a job on yarn.

I got this exception:
{code:java}
Caused by: java.io.FileNotFoundException: File does not exist: /tmp/application_1667097340114_0001-flink-conf.yaml9110057305807570477.tmp
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1128)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1120)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1120)
        at org.apache.flink.yarn.YarnApplicationFileUploader.registerSingleLocalResource(YarnApplicationFileUploader.java:168)
        at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1047)
        at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:623)
        at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:490)
        ... 24 more {code}
I think the cause of this problem may be 

*tmpConfigurationFile.getAbsolutePath() !issue.png!*

 ",,,,,,,,,,,,,,,,,,,,,FLINK-33424,,,,,,,FLINK-33424,,,,,,,,FLINK-33472,,,,,,,"30/Oct/22 02:53;Melon Wang;issue.png;https://issues.apache.org/jira/secure/attachment/13051612/issue.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 27 11:49:47 UTC 2024,,,,,,,,,,"0|z1am8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/24 11:49;Adrian Z;Please assign to me... ;;;",,,,,,,,,,,,,,,,,,,,,,
pyflink protobuf requirement out of date,FLINK-29796,13493599,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jorgev,jorgev,29/Oct/22 20:08,05/Jan/23 02:12,04/Jun/24 20:41,05/Jan/23 02:12,1.16.0,,,,,,,,,,API / Python,,,,0,,,,,"The setup.py file for pyflink currently requires protobuf<3.18 but the dev-requirements.txt file lists protubuf<=3.21 which seems to indicate that the library works with newer version of protobuf. The latest version of protobuf which satisfies the requirement was 3.17.3 which was released over a year ago, and notably the various gcloud api packages all require much newer versions (3.19+ I think). Obviously there are ways around this but the right answer is likely to ease/change the requirement.",,,,,,,,,,,,,,,,,,,,FLINK-28786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 18:42:55 UTC 2022,,,,,,,,,,"0|z1am7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 08:24;martijnvisser;I think the minimum version needs to be bumped; do you want to open a PR for it?;;;","04/Nov/22 05:20;dianfu;cc [~hxb];;;","15/Dec/22 17:51;engnatha;Just wanted to bump this. Looking at [https://github.com/apache/flink/blob/release-1.16/flink-python/setup.py#L314,] it seems this has already made its way back to 1.16 if I'm understanding this correctly? This is blocking me from pulling apache-flink in through our requirements.txt since we require protobuf > 3.19 due to the security vulnerabilities detailed [here|https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf]. We use [pantsbuild|https://www.pantsbuild.org/] for python repo management so there's no easy way to separate out our requirements for a temporary solution.;;;","15/Dec/22 18:42;martijnvisser;It looks this was changed as part of FLINK-28786 which is scheduled for 1.16.1 indeed. So if this change is sufficient, we can close this ticket (while linking them to each other);;;",,,,,,,,,,,,,,,,,,,
The source.file.stream.io-fetch-size can not be set by table properties,FLINK-29795,13493571,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,29/Oct/22 08:03,31/Oct/22 03:14,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"The {{source.file.stream.io-fetch-size}} is used in the bulk format mode, but it is not exposed to the filesystem connector options. If I try to use it in the with property, it will fails with {{{}Unsupported options{}}}. It can only be set by add it to the {{flink-conf.yaml}} now, and the same session cluster share the same config value. It's not convenient to adjust it and I think it should be scoped to the table's property.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 29 08:18:53 UTC 2022,,,,,,,,,,"0|z1am1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/22 08:06;aitozi;BTW, I think this option is used in the format's code, but actually it's not bounded with a specific format;;;","29/Oct/22 08:18;aitozi;A reasonable fix to my mind, is to expose the original options in the properties to the format builder. So that it can fallback to the original options to get the \{{source.file.stream.io-fetch-size}}, then it will work I think.

cc [~martijnvisser] can you give me some suggestion on this ?;;;",,,,,,,,,,,,,,,,,,,,,
Flink Operator Need to Include Cause for Exceptions If Possible,FLINK-29794,13493544,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhouJIANG,ZhouJIANG,ZhouJIANG,28/Oct/22 21:01,13/Dec/22 01:39,04/Jun/24 20:41,01/Nov/22 09:50,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"For easier debugging and consistent stack trace, it would be appreciated if Exceptions are always thrown with cause (if exists), instead of persisting string message & silently swallow the cause. 

This issue currently present in apache.flink.kubernetes.operator.reconciler.diff.ReflectiveDiffBuilder and apache.flink.kubernetes.operator.utils.EnvUtils

 

We would like to suggest this as a pattern for future exception handling as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 09:50:50 UTC 2022,,,,,,,,,,"0|z1alvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 21:12;gyfora;I agree with the suggestion, but I think this is general good practice and if the pattern wasn’t used its an honest developer/reviewer mistake.

probably not intentional in most cases:);;;","28/Oct/22 21:42;ZhouJIANG;(y) So we'll grep current Exceptions, and add cause if that's missing;;;","01/Nov/22 09:50;gyfora;merged to main 3cbe58eee04bfd195520f172b56350077acb1875;;;",,,,,,,,,,,,,,,,,,,,
Scala Code 2 Java Code,FLINK-29793,13493454,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,leen,leen,28/Oct/22 10:30,28/Oct/22 12:03,04/Jun/24 20:41,28/Oct/22 12:03,,,,,,,,,,,,,,,0,,,,,"Is there any plan to replace all Scala code with Java, thks~",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 12:03:34 UTC 2022,,,,,,,,,,"0|z1albk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 12:03;martijnvisser;No, there are no plans to replace Scala code with Java code. There's no benefit for this at the moment;;;",,,,,,,,,,,,,,,,,,,,,,
FileStoreCommitTest is unstable and may stuck,FLINK-29792,13493443,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,28/Oct/22 09:45,01/Nov/22 04:27,04/Jun/24 20:41,01/Nov/22 04:26,table-store-0.3.0,,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"{{FileStoreCommitTest}} may stuck because the {{FileStoreCommit}} in {{TestCommitThread}} does not commit APPEND snapshot when no new files are produced. In this case, if the following COMPACT snapshot conflicts with the current merge tree, the test will stuck.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 04:26:50 UTC 2022,,,,,,,,,,"0|z1al94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 04:26;TsReaper;master: 153804749deeb335083aeefc619b44a9276310e2
release-0.2: 2f97ff0c51d189685a1b6ca81414d12487c5725a;;;",,,,,,,,,,,,,,,,,,,,,,
Print sink result mess up with GC log in E2eTestBase ,FLINK-29791,13493264,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,qingyue,qingyue,28/Oct/22 07:16,04/Nov/22 09:09,04/Jun/24 20:41,04/Nov/22 09:09,table-store-0.3.0,,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,https://github.com/apache/flink-table-store/actions/runs/3343373246/jobs/5536523910,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:09:57 UTC 2022,,,,,,,,,,"0|z1ak5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:09;TsReaper;master: 534f8b5588dc6a96c3f92f9d9b4ea9230f1e550b
release-0.2: a50f29c1f06534faadedf5f531d6f2de64af2cee;;;",,,,,,,,,,,,,,,,,,,,,,
Implement Catalog#getDatabase,FLINK-29790,13493169,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,28/Oct/22 06:37,28/Oct/22 09:01,04/Jun/24 20:41,28/Oct/22 09:01,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,This is a basic API and should be supported.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 09:01:55 UTC 2022,,,,,,,,,,"0|z1ajk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 09:01;lzljs3620320;master: 4b5f78119a5d520223f4e726063d588af5522033;;;",,,,,,,,,,,,,,,,,,,,,,
Fix flaky tests in CheckpointCoordinatorTest,FLINK-29789,13492900,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,sopan98,sopan98,28/Oct/22 04:44,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,auto-deprioritized-minor,pull-request-available,,,"The test org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.testTriggerAndDeclineCheckpointComplex is flaky and has the following failure:

Failures:
[ERROR] Failures:
[ERROR]   CheckpointCoordinatorTest.testTriggerAndDeclineCheckpointComplex:1054 expected:<2> but was:<1>

I used the tool [NonDex|https://github.com/TestingResearchIllinois/NonDex] to find this flaky test.
Command: mvn -pl flink-runtime edu.illinois:nondex-maven-plugun:1.1.2:nondex -Dtest=org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest#testTriggerAndDeclineCheckpointComplex

I analyzed the assertion failure and found that checkpoint1Id and checkpoint2Id are getting assigned by iterating over a HashMap.
As we know, iterator() returns elements in a random order [(JavaDoc|https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html#entrySet--]) and this might cause test failures for some orders.

Therefore, to remove this non-determinism, we would change HashMap to LinkedHashMap.
On further analysis, it was found that the Map is getting initialized on line 1894 of org.apache.flink.runtime.checkpoint.CheckpointCoordinator class.

After changing from HashMap to LinkedHashMap, the above test is passing without any non-determinism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:04 UTC 2023,,,,,,,,,,"0|z1ahwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 03:18;sopan98;[~martijnvisser] any thoughts?;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,
StatefulJobWBroadcastStateMigrationITCase failed in native savepoints,FLINK-29788,13492693,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,masteryhx,hxbks2ks,hxbks2ks,28/Oct/22 03:19,31/Oct/22 08:07,04/Jun/24 20:41,31/Oct/22 08:07,1.16.0,,,,,,1.16.1,1.17.0,,,Release System,Runtime / State Backends,,,0,pull-request-available,,,, !image-2022-10-28-11-18-45-471.png! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/22 03:18;hxbks2ks;image-2022-10-28-11-18-45-471.png;https://issues.apache.org/jira/secure/attachment/13051534/image-2022-10-28-11-18-45-471.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 08:07:15 UTC 2022,,,,,,,,,,"0|z1agmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 03:20;hxbks2ks;[~masteryhx] Could you help take a look? Thx.;;;","28/Oct/22 04:27;masteryhx;Sure. It's known limitation that changelog hasn't supported native savepoint in 1.16.
So I think we could just disable changelog when triggering native savepoint in this case.
It will not block the release of 1.16.
BTW, IIUC, Should the migration it case be executed before we decide to release 1.16 ? [~hxbks2ks] 

   ;;;","28/Oct/22 07:46;hxbks2ks;It is not the blocker of release 1.16.0，but it belongs to the finalized step of a release.;;;","31/Oct/22 06:41;masteryhx;I have commited a pr to fix it in the test.


I think the release processes could be improved here.

We could update the FlinkVersion of these classes finally, but the migration test should be executed when we prepare for the release.

It could help us to find the migration exception earily.

(Some exceptions maybe not so simple)
WDYT? [~hxbks2ks] ;;;","31/Oct/22 06:59;hxbks2ks;[~masteryhx] Thanks a lot for the fix. Before the release, the code may change, so I think preparing the migrated test data in advance may result in some problems.
;;;","31/Oct/22 08:07;hxbks2ks;Merged into master via c89e400ae379c8b7490d9af20f82f49319895dce
Merged into release-1.16 via 87d86c05c7a91d7546935ec1ceee3c0c55e2e191;;;",,,,,,,,,,,,,,,,,
fix ci METHOD_NEW_DEFAULT issue,FLINK-29787,13492677,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyubin117,liyubin117,liyubin117,28/Oct/22 03:12,31/Aug/23 09:38,04/Jun/24 20:41,28/Oct/22 09:19,1.17.0,,,,,,1.17.0,,,,Build System / CI,,,,0,pull-request-available,,,,"`org.apache.flink.api.connector.source.SourceReader` declared a new default function `pauseOrResumeSplits()`, japicmp plugin failed during ci running, we should configure the plugin to make it compatible.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29784,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 09:19:02 UTC 2022,,,,,,,,,,"0|z1agiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 06:00;aitozi;I also encountered this issue. Does it related to this commit ? [https://github.com/apache/flink/commit/82567cc9e9a23a2b6ca41f433c4b9310c0075767]

cc [~Yellow] ;;;","28/Oct/22 09:19;mapohl;master: cb442936156583d6cf42550f3d9187da427b9c68;;;",,,,,,,,,,,,,,,,,,,,,
VarianceThresholdSelector should implement HasInputCol,FLINK-29786,13492618,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,28/Oct/22 01:47,23/Nov/22 09:28,04/Jun/24 20:41,23/Nov/22 09:28,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Flink ML has both `FeaturesCol` and `InputCol` to be used to set input. The `FeaturesCol` should only be used with `LabelCol`, while `InputCol` can be used more generally.

The `VarianceThresholdSelector` should use `InputCol` instead of `FeaturesCol`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-28 01:47:22.0,,,,,,,,,,"0|z1ag5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Flink Elasticsearch-7 connector elasticsearch.version to 7.17.0,FLINK-29785,13492608,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,elkhand,elkhand,27/Oct/22 23:24,31/Oct/22 08:26,04/Jun/24 20:41,31/Oct/22 08:26,,,,,,,,,,,Connectors / ElasticSearch,,,,0,,,,,"`elasticsearch-7` connector uses ElasticSearch `7.10.2` version.

[https://github.com/apache/flink-connector-elasticsearch/blob/main/flink-connector-elasticsearch7/pom.xml#L39]

 

When ElasticSearch server side is `8.X.X` then, `elasticsearch-7` connector does not work. For Example for ElasticSearch `8.2.2` version on the server side, the minimum required version on the `elasticsearch-7` Flink connector side is `""minimum_wire_compatibility_version"" : ""7.17.0""`.

As of today `elasticsearch-8` does not exist yet. [ Add Elasticsearch 8.0 support Jira ticket |https://issues.apache.org/jira/browse/FLINK-26088 ] .

With this intermediare step - upgrading `elasticsearch-7` connector `elasticsearch.version` to 7.17.0 can help Flink users still ingest into ElasticSearch which has `8.X.X` version deployed on the server side. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 08:26:46 UTC 2022,,,,,,,,,,"0|z1ag3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 08:26;martijnvisser;This isn't possible, because that newer version of the Elasticsearch connector is released under a new license that's not compatible with the Apache License. See FLINK-26088;;;",,,,,,,,,,,,,,,,,,,,,,
Build fails with There is at least one incompatibility: org.apache.flink.api.connector.source.SourceReader,FLINK-29784,13492575,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,27/Oct/22 18:42,28/Oct/22 08:54,04/Jun/24 20:41,28/Oct/22 08:54,1.17.0,,,,,,,,,,API / Core,,,,0,,,,,"currently {{./mvnw clean install -DskipTests}} fails with 
{noformat}
----------------------------------------------------
[ERROR] Failed to execute goal io.github.zentol.japicmp:japicmp-maven-plugin:0.16.0_m325:cmp (default) on project flink-core: There is at least one incompatibility: org.apache.flink.api.connector.source.SourceReader.pauseOrResumeSplits(java.util.Collection,java.util.Collection):METHOD_NEW_DEFAULT -> [Help 1]
[ERROR] 

{noformat}
It starts failing after this commit https://github.com/apache/flink/commit/82567cc9e9a23a2b6ca41f433c4b9310c0075767
removal of that commit fixes the behavior 
// cc [~hxbks2ks] may be you know more details about that",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29787,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 08:54:02 UTC 2022,,,,,,,,,,"0|z1afw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 08:54;mapohl;I know that your Jira issue was created first, [~Sergey Nuyanzin] . But I'm closing that one anyway in favor of FLINK-29787 because there's already a PR attached to that one.;;;",,,,,,,,,,,,,,,,,,,,,,
Flaky test: KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecoveryEventTime,FLINK-29783,13492560,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,gaborgsomogyi,gaborgsomogyi,27/Oct/22 16:35,30/Oct/22 14:50,04/Jun/24 20:41,27/Oct/22 21:13,1.17.0,,,,,,,,,,Tests,,,,0,pull-request-available,,,,"{code:java}
Oct 27 15:07:54 java.lang.AssertionError: Create test topic : partition_failure_recovery_EventTime failed, org.apache.kafka.common.errors.TopicExistsException: Topic 'partition_failure_recovery_EventTime' already exists.
Oct 27 15:07:54 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:207)
Oct 27 15:07:54 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
Oct 27 15:07:54 	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:217)
Oct 27 15:07:54 	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecovery(KafkaShuffleExactlyOnceITCase.java:158)
Oct 27 15:07:54 	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecoveryEventTime(KafkaShuffleExactlyOnceITCase.java:101)
Oct 27 15:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Oct 27 15:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Oct 27 15:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Oct 27 15:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
Oct 27 15:07:54 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Oct 27 15:07:54 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Oct 27 15:07:54 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Oct 27 15:07:54 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Oct 27 15:07:54 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Oct 27 15:07:54 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Oct 27 15:07:54 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Oct 27 15:07:54 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Oct 27 15:07:54 	at java.lang.Thread.run(Thread.java:748)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 21:13:33 UTC 2022,,,,,,,,,,"0|z1afsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 16:35;gaborgsomogyi;It's super annoying to restart CI and hope that it works. Trying to add a quick fix.;;;","27/Oct/22 21:13;martijnvisser;Duplicate of FLINK-24119;;;",,,,,,,,,,,,,,,,,,,,,
Bash-Java-utils.jar still uses Log4j 2.16.0,FLINK-29782,13492532,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,bharathkopparapu,bharathkopparapu,27/Oct/22 14:00,27/Oct/22 15:02,04/Jun/24 20:41,27/Oct/22 14:11,1.11.6,,,,,,,,,,Client / Job Submission,,,,0,,,,,"Hello Apache Flink team,

 

Flink-1.11.6
/bin/bash-java-utils.jar uses log4j 2.16.0
can that be upgraded to 2.17.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 15:02:47 UTC 2022,,,,,,,,,,"0|z1afmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 14:11;martijnvisser;[~bharathkopparapu] You should upgrade to a later version of Flink, Fink 1.11 is no longer supported by the community. ;;;","27/Oct/22 14:21;bharathkopparapu;[~martijnvisser] Thank you. Which version do you suggest?;;;","27/Oct/22 15:02;martijnvisser;[~bharathkopparapu] I would currently recommend Flink 1.15.2, the latest version;;;",,,,,,,,,,,,,,,,,,,,
ChangelogNormalize uses wrong keys after transformation by WatermarkAssignerChangelogNormalizeTransposeRule ,FLINK-29781,13492530,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,27/Oct/22 13:22,21/Dec/22 14:25,04/Jun/24 20:41,15/Nov/22 09:06,1.15.3,1.16.0,,,,,1.16.1,1.17.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,"currently WatermarkAssignerChangelogNormalizeTransposeRule didn't remap the uniquekey indexes for its new input after plan rewrite, this may produce wrong result.

A simple case:
{code}

  @Test
  def testPushdownCalcNotAffectChangelogNormalizeKey(): Unit = {
    util.addTable(""""""
                    |CREATE TABLE t1 (
                    |  ingestion_time TIMESTAMP(3) METADATA FROM 'ts',
                    |  a VARCHAR NOT NULL,
                    |  b VARCHAR NOT NULL,
                    |  WATERMARK FOR ingestion_time AS ingestion_time
                    |) WITH (
                    | 'connector' = 'values',
                    | 'readable-metadata' = 'ts:TIMESTAMP(3)'
                    |)
      """""".stripMargin)
    util.addTable(""""""
                    |CREATE TABLE t2 (
                    |  k VARBINARY,
                    |  ingestion_time TIMESTAMP(3) METADATA FROM 'ts',
                    |  a VARCHAR NOT NULL,
                    |  f BOOLEAN NOT NULL,
                    |  WATERMARK FOR `ingestion_time` AS `ingestion_time`,
                    |  PRIMARY KEY (`a`) NOT ENFORCED
                    |) WITH (
                    | 'connector' = 'values',
                    | 'readable-metadata' = 'ts:TIMESTAMP(3)',
                    | 'changelog-mode' = 'I,UA,D'
                    |)
      """""".stripMargin)
    val sql =
      """"""
        |SELECT t1.a, t1.b, t2.f
        |FROM t1 INNER JOIN t2 FOR SYSTEM_TIME AS OF t1.ingestion_time
        | ON t1.a = t2.a WHERE t2.f = true
        |"""""".stripMargin
    util.verifyRelPlan(sql, ExplainDetail.CHANGELOG_MODE)
  }
{code}

the generated plan is incorrect for now:  {color:red}ChangelogNormalize(key=[ingestion_time]){color} uses wrong key 'ingestion_time' (should be 'a')

optimize result: 
{code}
Calc(select=[a, b, f])
+- TemporalJoin(joinType=[InnerJoin], where=[AND(=(a, a0), __TEMPORAL_JOIN_CONDITION(ingestion_time, ingestion_time0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(a0), __TEMPORAL_JOIN_LEFT_KEY(a), __TEMPORAL_JOIN_RIGHT_KEY(a0)))], select=[ingestion_time, a, b, ingestion_time0, a0, f])
   :- Exchange(distribution=[hash[a]])
   :  +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time])
   :     +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, b])
   :        +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, ingestion_time])
   +- Exchange(distribution=[hash[a]])
      +- Calc(select=[ingestion_time, a, f], where=[f])
         +- ChangelogNormalize(key=[ingestion_time])
            +- Exchange(distribution=[hash[a]])
               +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time])
                  +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, f])
                     +- TableSourceScan(table=[[default_catalog, default_database, t2, project=[a, f], metadata=[ts]]], fields=[a, f, ingestion_time])
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 09:25:25 UTC 2022,,,,,,,,,,"0|z1afm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 09:25;godfreyhe;Fixed in master: 5463f244ec69f623d75c15374b55bb8695e92b3e

in 1.16.1: 5466716b20d5c720bf29dea560909e7055870555;;;",,,,,,,,,,,,,,,,,,,,,,
how to persist a flink table like spark persist a dataset,FLINK-29780,13492526,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zqw99,zqw99,27/Oct/22 13:02,27/Oct/22 13:27,04/Jun/24 20:41,27/Oct/22 13:27,,,,,,,,,,,Table SQL / API,,,,0,,,,,"I have code:

table1 = table...

table2 = table1....

table3 = table1...

 

How many times would (table1 = table...) be executed?

Once or twice?

I think it executes once, but it actually executes twice, so how do I make it only execute once",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/22 13:13;zqw99;1027.jpg;https://issues.apache.org/jira/secure/attachment/13051513/1027.jpg",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 13:27:15 UTC 2022,,,,,,,,,,"0|z1aflc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 13:10;zqw99;spark provides a DataSet.persist() method to cache data. I was wondering if flink has a similar method, or how can I achieve the same results as spark.;;;","27/Oct/22 13:27;martijnvisser;[~zqw99] This question is better suited for the Flink User mailing list or the Slack channel. See https://flink.apache.org/community.html#how-do-i-get-help-from-apache-flink;;;",,,,,,,,,,,,,,,,,,,,,
Allow using MiniCluster with a PluginManager to use metrics reporters,FLINK-29779,13492486,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,27/Oct/22 10:56,24/Nov/22 12:15,04/Jun/24 20:41,24/Nov/22 12:15,1.16.0,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"Currently, using MiniCluster with a metric reporter loaded as a plugin is not supported, because the {{ReporterSetup.fromConfiguration(config, null));}} gets passed {{null}} for the PluginManager.

I think it generally valuable to allow passing a PluginManager to the MiniCluster.

I'll open a PR for this.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 12:15:54 UTC 2022,,,,,,,,,,"0|z1afcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 11:28;morhidi;+1 I just realized the other day, it's missing.;;;","24/Nov/22 12:15;rmetzger;Merged to master (for 1.17) in https://github.com/apache/flink/commit/9984b09cb9a2af9f2bde0e973cb8ce375942bd8c;;;",,,,,,,,,,,,,,,,,,,,,
fix error in flink-1.13.md,FLINK-29778,13492423,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,hcj,hcj,27/Oct/22 08:27,27/Oct/22 11:31,04/Jun/24 20:41,27/Oct/22 11:31,1.13.0,1.13.1,1.13.2,1.13.3,1.13.5,1.13.6,,,,,Documentation,,,,0,,,,,"In flink-1.13.md,line 53 has syntax error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 11:31:28 UTC 2022,,,,,,,,,,"0|z1aeyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 09:07;hcj;could someone help assign this ticket to me? Thanks~;;;","27/Oct/22 10:07;zhangjilong;I‘m interesting in fixing doc，can you assign this ticket to me?;;;","27/Oct/22 11:31;martijnvisser;It doesn't make much sense to fix this issue if it only exists in the Flink 1.13 documentation, because Flink 1.13 is no longer supported by the community;;;",,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-dstl,FLINK-29777,13492390,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,masteryhx,masteryhx,masteryhx,27/Oct/22 07:59,07/Nov/22 04:58,04/Jun/24 20:41,01/Nov/22 10:14,,,,,,,1.17.0,,,,Runtime / State Backends,Tests,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25549,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 10:15:53 UTC 2022,,,,,,,,,,"0|z1aer4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 10:15;masteryhx;TaskChangelogRegistryImplTest will be migrated in FLINK-29776;;;",,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-statebackend-changelog,FLINK-29776,13492387,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xiarui,masteryhx,masteryhx,27/Oct/22 07:57,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Runtime / State Backends,Tests,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:10 UTC 2023,,,,,,,,,,"0|z1aeqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 10:10;xiarui;Hi Yu, could you assign this issue to me? I work on state management of Flink and am willing to help the community to migrate the unit tests related to state backend.;;;","01/Nov/22 10:19;masteryhx;Sure. 
BTW, I found TaskChangelogRegistryImplTest which is in dstl module has not been migrated.
I think you could also migrate it together in a pr with two commits.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-statebackend-rocksdb,FLINK-29775,13492386,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,zhangyy91,masteryhx,masteryhx,27/Oct/22 07:56,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Runtime / State Backends,Tests,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 10:17:59 UTC 2023,,,,,,,,,,"0|z1aeq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 10:17;joern;Could you assign this ticket to me?

I would like to help with it and already worked on the migration to Junit 5 on the flink-filesystem module.;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce options metadata table,FLINK-29774,13492155,13490538,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Oct/22 03:25,27/Oct/22 08:45,04/Jun/24 20:41,27/Oct/22 08:45,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"SELECT * FROM T$options;
KEY | VALUE
...     | ...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 08:45:29 UTC 2022,,,,,,,,,,"0|z1adaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 08:45;lzljs3620320;master: 3959440576f6d8e8e3dc88eb4130a04cdce4c3b1;;;",,,,,,,,,,,,,,,,,,,,,,
PreAggregationITCase.LastValueAggregation and PreAggregationITCase.LastNonNullValueAggregation are unstable,FLINK-29773,13492154,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,27/Oct/22 03:20,27/Oct/22 07:46,04/Jun/24 20:41,27/Oct/22 07:46,table-store-0.3.0,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"{{PreAggregationITCase.LastValueAggregation}} and {{PreAggregationITCase.LastNonNullValueAggregation}} need to make sure that the order of input data is determined. However the default parallelism of {{FileStoreTableITCase}} is 2, so the order of input data might change across tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:46:47 UTC 2022,,,,,,,,,,"0|z1adao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 07:46;TsReaper;master: 147ea7528edfade6025e76fc66252765dc6f8fe4;;;",,,,,,,,,,,,,,,,,,,,,,
Kafka table source scan blocked,FLINK-29772,13492145,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tinny,tinny,27/Oct/22 02:13,27/Oct/22 02:49,04/Jun/24 20:41,,1.13.2,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"{code:java}
//
""Source: TableSourceScan(table=[[ostream, user_mart, dwd_ads_isms_msgmiddle, watermark=[-(toTimeStamps($2), 10000:INTERVAL SECOND)]]], fields=[data_type, cluster_name, server_time, server_time_s, client_time, client_time_s, imei, request_id, owner_id, service_id, content_id, sign_id, receiver_type, msg_type, handle_type, reach_type, source_type, create_time, msg_id, imsi, array_info_imei, phone, channel_id, process_time, code, msg, receiver, content_type, android_version, apk_version]) -> Calc(select=[data_type, server_time, client_time, msg_id, array_info_imei, code, PROCTIME() AS proctime, Reinterpret(toTimeStamps(server_time)) AS rowtime]) -> Calc(select=[array_info_imei AS imei, REPLACE(msg_id, _UTF-16LE'#', _UTF-16LE'') AS msg_id, CASE((SEARCH(server_time, Sarg[(-∞.._UTF-16LE'NULL'), (_UTF-16LE'NULL'.._UTF-16LE'null'), (_UTF-16LE'null'..+∞)]:CHAR(4) CHARACTER SET ""UTF-16LE"") AND server_time IS NOT NULL), server_time, CAST(FROM_UNIXTIME(CAST(SUBSTRING(CAST(PROCTIME_MATERIALIZE(proctime)), 0, 10))))) AS server_time, CASE((SEARCH(client_time, Sarg[(-∞.._UTF-16LE'NULL'), (_UTF-16LE'NULL'.._UTF-16LE'null'), (_UTF-16LE'null'..+∞)]:CHAR(4) CHARACTER SET ""UTF-16LE"") AND client_time IS NOT NULL), client_time, CAST(FROM_UNIXTIME(CAST(SUBSTRING(CAST(PROCTIME_MATERIALIZE(proctime)), 0, 10))))) AS client_time, IF(((data_type = _UTF-16LE'sms-netmsg-send':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (code = _UTF-16LE'0':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")), 1, 0) AS send_cnt, IF(((data_type = _UTF-16LE'sms-netmsg-callback':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (code = _UTF-16LE'0':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")), 1, 0) AS reach_cnt, rowtime], where=[SEARCH(data_type, Sarg[_UTF-16LE'sms-netmsg-callback':VARCHAR(19) CHARACTER SET ""UTF-16LE"", _UTF-16LE'sms-netmsg-send':VARCHAR(19) CHARACTER SET ""UTF-16LE""]:VARCHAR(19) CHARACTER SET ""UTF-16LE"")]) (22/24)#0"" Id=77 BLOCKED on java.lang.Object@4aa3fe44 owned by ""Legacy Source Thread - Source: TableSourceScan(table=[[ostream, user_mart, dwd_ads_isms_msgmiddle, watermark=[-(toTimeStamps($2), 10000:INTERVAL SECOND)]]], fields=[data_type, cluster_name, server_time, server_time_s, client_time, client_time_s, imei, request_id, owner_id, service_id, content_id, sign_id, receiver_type, msg_type, handle_type, reach_type, source_type, create_time, msg_id, imsi, array_info_imei, phone, channel_id, process_time, code, msg, receiver, content_type, android_version, apk_version]) -> Calc(select=[data_type, server_time, client_time, msg_id, array_info_imei, code, PROCTIME() AS proctime, Reinterpret(toTimeStamps(server_time)) AS rowtime]) -> Calc(select=[array_info_imei AS imei, REPLACE(msg_id, _UTF-16LE'#', _UTF-16LE'') AS msg_id, CASE((SEARCH(server_time, Sarg[(-∞.._UTF-16LE'NULL'), (_UTF-16LE'NULL'.._UTF-16LE'null'), (_UTF-16LE'null'..+∞)]:CHAR(4) CHARACTER SET ""UTF-16LE"") AND server_time IS NOT NULL), server_time, CAST(FROM_UNIXTIME(CAST(SUBSTRING(CAST(PROCTIME_MATERIALIZE(proctime)), 0, 10))))) AS server_time, CASE((SEARCH(client_time, Sarg[(-∞.._UTF-16LE'NULL'), (_UTF-16LE'NULL'.._UTF-16LE'null'), (_UTF-16LE'null'..+∞)]:CHAR(4) CHARACTER SET ""UTF-16LE"") AND client_time IS NOT NULL), client_time, CAST(FROM_UNIXTIME(CAST(SUBSTRING(CAST(PROCTIME_MATERIALIZE(proctime)), 0, 10))))) AS client_time, IF(((data_type = _UTF-16LE'sms-netmsg-send':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (code = _UTF-16LE'0':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")), 1, 0) AS send_cnt, IF(((data_type = _UTF-16LE'sms-netmsg-callback':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (code = _UTF-16LE'0':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")), 1, 0) AS reach_cnt, rowtime], where=[SEARCH(data_type, Sarg[_UTF-16LE'sms-netmsg-callback':VARCHAR(19) CHARACTER SET ""UTF-16LE"", _UTF-16LE'sms-netmsg-send':VARCHAR(19) CHARACTER SET ""UTF-16LE""]:VARCHAR(19) CHARACTER SET ""UTF-16LE"")]) (22/24)#0"" Id=87
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
    -blocked on java.lang.Object@4aa3fe44
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:344)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:684)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:639)
    at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$556/1624913200.run(Unknown Source)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 02:49:09 UTC 2022,,,,,,,,,,"0|z1ad8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 02:49;tinny;my case is kafka interval join，deadlock occurs when using rocksdb;;;",,,,,,,,,,,,,,,,,,,,,,
Fix flaky RollbackTest,FLINK-29771,13491892,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,darenwkt,gyfora,gyfora,26/Oct/22 15:15,11/Nov/22 08:47,04/Jun/24 20:41,11/Nov/22 08:47,kubernetes-operator-1.3.0,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 13:58:35 UTC 2022,,,,,,,,,,"0|z1aboo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 13:58;gyfora;merged to main 3b59de9129c0677dd41a454fe5178103ce29f815;;;",,,,,,,,,,,,,,,,,,,,,,
hbase connector supports out-of-order data,FLINK-29770,13491806,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Bo Cui,Bo Cui,26/Oct/22 13:14,08/Dec/23 02:23,04/Jun/24 20:41,,,,,,,,,,,,Connectors / HBase,,,,0,auto-deprioritized-major,pull-request-available,,,"The data may be out of order and has no timestamp. As a result, the data written to the HBase is incorrect",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 02:23:04 UTC 2023,,,,,,,,,,"0|z1ab5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Dec/23 02:23;tanjialiang;[~Bo Cui] Thanks for reporting this issue, but i think it can be closed because it had been support in [FLINK-33208|https://issues.apache.org/jira/browse/FLINK-33208].;;;",,,,,,,,,,,,,,,,,,,,
Further limit the explosion range of failover in hybrid shuffle mode,FLINK-29769,13491755,13491747,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,Weijie Guo,Weijie Guo,Weijie Guo,26/Oct/22 12:03,15/Feb/23 05:44,04/Jun/24 20:41,15/Feb/23 05:44,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"Under the current failover strategy, if a region changes to the failed state, all its downstream regions must be restarted. For ALL_ EDGE_BLOCKING type jobs, since they are scheduled stage by stage, no additional overhead. However, for the hybrid shuffle mode, the upstream and downstream can both run at the same time. If the upstream task fails, we hope that it will not affect the downstream tasks that do not consume it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-26 12:03:19.0,,,,,,,,,,"0|z1aaug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle supports consume partial finished subtask in speculative execution mode,FLINK-29768,13491753,13491747,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,26/Oct/22 11:54,06/Jan/23 08:06,04/Jun/24 20:41,06/Jan/23 08:06,1.17.0,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"At present, downstream task can be scheduled only if all upstream tasks finished in speculative execution mode. We can do some improvements to support schedule downstream tasks when the upstream's some subtask is not completely finished. After the upstream task's data is totally produced, we need to update the result partition's information of this subtask to trigger downstream task's consumption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 08:06:39 UTC 2023,,,,,,,,,,"0|z1aau0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:06;xtsong;master (1.17): d65f7c1c2f3bf3076fee6940e538c3725eee0a05;;;",,,,,,,,,,,,,,,,,,,,,,
VertexWiseSchedulingStrategy supports hybrid shuffle,FLINK-29767,13491748,13491747,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,26/Oct/22 11:43,21/Dec/22 08:30,04/Jun/24 20:41,15/Dec/22 02:17,1.17.0,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"`AdaptiveBatchScheduler` using `VertexWiseSchedulingStrategy` to schedule tasks, but it only supports blocking edge,  so we should make it also support hybrid shuffle edge.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 02:17:02 UTC 2022,,,,,,,,,,"0|z1aasw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 02:17;xtsong;master (1.17): a6a321777a53b8d7807078cb8d98c7b3de242fed;;;",,,,,,,,,,,,,,,,,,,,,,
AdaptiveBatchScheduler should also work with hybrid shuffle mode,FLINK-29766,13491747,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,26/Oct/22 11:37,15/Feb/23 05:45,04/Jun/24 20:41,15/Feb/23 05:45,1.17.0,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,Umbrella,,,,"AdaptiveBatchScheduler(including speculative execution) is a very important piece for large-scale batch jobs, but it can only work in the BLOCKING shuffle mode, I propose to also support this feature for HYBRID shuffle mode to combine the power of hybrid shuffle and adaptiveBatchScheduler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30938,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-26 11:37:48.0,,,,,,,,,,"0|z1aaso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL query not executing properly,FLINK-29765,13491721,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aqib.mehmood,aqib.mehmood,26/Oct/22 09:03,28/Oct/22 08:42,04/Jun/24 20:41,,1.15.0,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"I am using this query to compare difference between last and second last price of our order sku
WITH CTE AS (
    SELECT a.sku, a.name, a.updatedAt, b.price FROM (    
        SELECT sku, name, max(updatedAt) AS updatedAt from (
            SELECT sku, name, updatedAt FROM wms.PurchaseOrderProduct
            WHERE CONCAT(sku, DATE_FORMAT(updatedAt, '%Y-%m-%d %H:%m:%s')) not in (
                SELECT CONCAT(sku, DATE_FORMAT(updatedAt, '%Y-%m-%d %H:%m:%s')) FROM (
                    SELECT sku, max(updatedAt) as updatedAt from wms.PurchaseOrderProduct
                    GROUP BY sku
                ) AS x
            )
        ) AS z
        GROUP BY sku, name
    ) AS a
    LEFT JOIN wms.PurchaseOrderProduct b
    ON a.sku=b.sku AND a.name=b.name and a.updatedAt=b.updatedAt
)
SELECT a.sku, a.name, a.updatedAt AS latestupdatedAt, a.price AS latestPrice, b.updatedAt AS lastUpdatedAt, b.price AS lastPrice
FROM (
    SELECT a.sku, a.name, a.updatedAt, b.price from (
        SELECT sku, name, max(updatedAt) as updatedAt from wms.PurchaseOrderProduct
        GROUP BY sku, name
    ) AS a
    LEFT JOIN wms.PurchaseOrderProduct b
    ON a.sku=b.sku AND a.name=b.name and a.updatedAt=b.updatedAt
) AS a
LEFT JOIN CTE AS b
ON a.sku=b.sku AND a.name=b.name;
This issue is that Im getting *NULLs* for columns *lastUpdatedAt* and {*}lastPrice{*}. But when I run the same query on our prod database, I'm getting desired results.I suspect that flink is not processing the entire query before giving the results.

I get desired results for a couple of rows in while *lastUpdatedAt* and *lastPrice* are not *NULL* in the beginning of the table{*}.{*} But then after that the entire two columns return *NULLs*
I would like to know why flink is not executing the above query properly?TIA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Fri Oct 28 08:42:05 UTC 2022,,,,,,,,,,"0|z1aamw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 09:12;martijnvisser;[~godfreyhe] [~jark] WDYT? ;;;","28/Oct/22 02:39;fsk119;It's better you can share more infos, e.g. source table schema, json plan?;;;","28/Oct/22 08:42;aqib.mehmood;{{We're using a kafka connector. Below is the configs and source table}}


{{        EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();}}
{{        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);}}
{{        tEnv.executeSql(""CREATE TABLE orders (\n"" +}}
{{            ""    data ROW(\n"" +}}
{{            ""    id INT,\n"" +}}
{{            ""    poId INT,\n"" +}}
{{            ""    productId INT,\n"" +}}
{{            ""    name varchar(191),\n"" +}}
{{            ""    quantity  INT,\n"" +}}
{{            ""    price decimal(15,4),\n"" +}}
{{            ""    taxType varchar(191),\n"" +}}
{{            ""    taxAmount decimal(15,4),\n"" +}}
{{            ""    subTotalWithoutTax  decimal(15,4),\n"" +}}
{{            ""    subTotalWithTax  decimal(15,4),\n"" +}}
{{            ""    createdAt TIMESTAMP_LTZ,\n"" +}}
{{            ""    updatedAt TIMESTAMP_LTZ,\n"" +}}
{{            ""    sku varchar(191),\n"" +}}
{{            ""    mrp decimal(15,4)\n"" +}}
{{            "")) WITH (\n"" +}}
{{            ""    'connector' = 'kafka',\n"" +}}
{{            ""    'topic'     = 'orders'""}}
{{            ""    'scan.startup.mode'    = 'earliest-offset',\n"" +}}
{{            ""    'format'    = 'json',\n"" +}}
{{            ""    'json.timestamp-format.standard' = 'ISO-8601',\n"" +}}
{{            ""    'properties.security.protocol' = 'SASL_SSL',\n"" +);}};;;",,,,,,,,,,,,,,,,,,,,
Automatic judgment of parallelism of source,FLINK-29764,13491559,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,waywtdcc,waywtdcc,26/Oct/22 05:27,27/Oct/22 00:34,04/Jun/24 20:41,27/Oct/22 00:34,1.15.2,1.16.0,,,,,1.17.0,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,The parallelism of the source is automatically judged. The parallelism of the source should not be determined by jobmanager. adaptive batch scheduler The default source parallelism is judged by the two configurations of jobmanager.adaptive batch-scheduler.min-parallelism and jobmanager.adaptive batch-scheduler.max-parallelism and the number of partitions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 00:33:41 UTC 2022,,,,,,,,,,"0|z1a9mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 06:20;Weijie Guo;[~waywtdcc] Can you explain your problem in more detail? As far as I know, the source parallelism under the adaptive batch scheduler subject the following rule:
 # Kind of source can infer parallelism according to the catalog. For example, HiveTableSource.
 # Considering other sources, using the configuration ""{*}jobmanager.adaptive-batch-scheduler.default-source-parallelism{*}” to manually configure source parallelism.;;;","27/Oct/22 00:33;waywtdcc;Oh, sorry, I misunderstood. ;;;",,,,,,,,,,,,,,,,,,,,,
TaskManager heatbeat timeout exception in Github CI for python tests,FLINK-29763,13491434,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,yunfengzhou,yunfengzhou,26/Oct/22 03:59,10/Jan/23 04:55,04/Jun/24 20:41,10/Jan/23 04:55,ml-2.1.0,,,,,,,,,,Library / Machine Learning,,,,0,test-stability,,,,"https://github.com/apache/flink-ml/actions/runs/3322007330/jobs/5490434747
https://github.com/apache/flink-ml/actions/runs/3321223124/jobs/5488576891
https://github.com/apache/flink-ml/actions/runs/3319920091/jobs/5485672250
https://github.com/apache/flink-ml/actions/runs/3319722473/jobs/5485231041
https://github.com/apache/flink-ml/actions/runs/3319599111/jobs/5484952148
https://github.com/apache/flink-ml/actions/runs/3318938657/jobs/5483471010",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-26 03:59:16.0,,,,,,,,,,"0|z1a8v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not create a standalone cluster with reactive mode using the operator,FLINK-29762,13491359,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,yuvipanda,yuvipanda,25/Oct/22 18:37,26/Oct/22 16:17,04/Jun/24 20:41,25/Oct/22 19:51,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"I'm trying to create a minimal running flink cluster with reactive scaling using the kubernetes operator (running v1.2.0), with the following YAML:

 

{{
kind: FlinkDeployment
metadata:
  name: test-flink-cluster
spec:
  flinkConfiguration:
    scheduler-mode: reactive
  flinkVersion: v1_15
  image: flink:1.15
  jobManager:
    replicas: 1
    resource:
      cpu: 0.2
      memory: 1024m
  mode: standalone
  serviceAccount: flink
  taskManager:
    replicas: 1
    resource:
      cpu: 0.2
      memory: 1024m}}

 

However, this causes the jobmanager to crash with the following:

 

{{sed: couldn't open temporary file /opt/flink/conf/sedLX7Jx8: Read-only file system}}
{{sed: couldn't open temporary file /opt/flink/conf/sed1vva8t: Read-only file system}}
{{/docker-entrypoint.sh: line 73: /opt/flink/conf/flink-conf.yaml: Read-only file system}}
{{/docker-entrypoint.sh: line 89: /opt/flink/conf/flink-conf.yaml.tmp: Read-only file system}}
{{Starting Job Manager}}
{{Starting standalonesession as a console application on host test-flink-cluster-58cd584fdd-xwbtf.}}
{{2022-10-25 18:32:00,422 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------}}
{{2022-10-25 18:32:00,510 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Preconfiguration: }}
{{2022-10-25 18:32:00,512 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - }}


{{RESOURCE_PARAMS extraction logs:}}
{{jvm_params: -Xmx469762048 -Xms469762048 -XX:MaxMetaspaceSize=268435456}}
{{dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=469762048b -D jobmanager.memory.jvm-overhead.max=201326592b}}
{{logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.}}
{{INFO  [] - Loading configuration property: blob.server.port, 6124}}
{{INFO  [] - Loading configuration property: kubernetes.jobmanager.annotations, flinkdeployment.flink.apache.org/generation:1}}
{{INFO  [] - Loading configuration property: kubernetes.jobmanager.replicas, 1}}
{{INFO  [] - Loading configuration property: scheduler-mode, reactive}}
{{INFO  [] - Loading configuration property: ""kubernetes.operator.metrics.reporter.prom.port"", ""9999""}}
{{INFO  [] - Loading configuration property: jobmanager.rpc.address, test-flink-cluster.default}}
{{INFO  [] - Loading configuration property: kubernetes.taskmanager.cpu, 0.2}}
{{INFO  [] - Loading configuration property: ""prometheus.io/port"", ""9999""}}
{{INFO  [] - Loading configuration property: kubernetes.service-account, flink}}
{{INFO  [] - Loading configuration property: kubernetes.cluster-id, test-flink-cluster}}
{{INFO  [] - Loading configuration property: kubernetes.container.image, flink:1.15}}
{{INFO  [] - Loading configuration property: parallelism.default, 2}}
{{INFO  [] - Loading configuration property: kubernetes.namespace, default}}
{{INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2}}
{{INFO  [] - Loading configuration property: kubernetes.rest-service.exposed.type, ClusterIP}}
{{INFO  [] - Loading configuration property: ""prometheus.io/scrape"", ""true""}}
{{INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1024m}}
{{INFO  [] - Loading configuration property: ""kubernetes.operator.metrics.reporter.prom.class"", ""org.apache.flink.metrics.prometheus.PrometheusReporter""}}
{{INFO  [] - Loading configuration property: web.cancel.enable, false}}
{{INFO  [] - Loading configuration property: execution.target, remote}}
{{INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1024m}}
{{INFO  [] - Loading configuration property: taskmanager.rpc.port, 6122}}
{{INFO  [] - Loading configuration property: kubernetes.internal.cluster-mode, SESSION}}
{{INFO  [] - Loading configuration property: kubernetes.jobmanager.cpu, 0.2}}
{{INFO  [] - Loading configuration property: $internal.flink.version, v1_15}}
{{INFO  [] - The derived from fraction jvm overhead memory (102.400mb (107374184 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead}}
{{INFO  [] - Final Master Memory configuration:}}
{{INFO  [] -   Total Process Memory: 1024.000mb (1073741824 bytes)}}
{{INFO  [] -     Total Flink Memory: 576.000mb (603979776 bytes)}}
{{INFO  [] -       JVM Heap:         448.000mb (469762048 bytes)}}
{{INFO  [] -       Off-heap:         128.000mb (134217728 bytes)}}
{{INFO  [] -     JVM Metaspace:      256.000mb (268435456 bytes)}}
{{INFO  [] -     JVM Overhead:       192.000mb (201326592 bytes)}}

{{2022-10-25 18:32:00,514 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------}}
{{2022-10-25 18:32:00,516 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting StandaloneSessionClusterEntrypoint (Version: 1.15.1, Scala: 2.12, Rev:f494be6, Date:2022-06-20T14:40:28+02:00)}}
{{2022-10-25 18:32:00,516 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: flink}}
{{2022-10-25 18:32:00,517 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>}}
{{2022-10-25 18:32:00,517 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Oracle Corporation - 11/11.0.16+8}}
{{2022-10-25 18:32:00,519 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64}}
{{2022-10-25 18:32:00,519 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 433 MiBytes}}
{{2022-10-25 18:32:00,520 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /usr/local/openjdk-11}}
{{2022-10-25 18:32:00,520 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  No Hadoop Dependency available}}
{{2022-10-25 18:32:00,522 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:}}
{{2022-10-25 18:32:00,523 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx469762048}}
{{2022-10-25 18:32:00,523 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms469762048}}
{{2022-10-25 18:32:00,523 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456}}
{{2022-10-25 18:32:00,524 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/opt/flink/log/flink--standalonesession-0-test-flink-cluster-58cd584fdd-xwbtf.log}}
{{2022-10-25 18:32:00,524 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties}}
{{2022-10-25 18:32:00,524 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties}}
{{2022-10-25 18:32:00,524 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml}}
{{2022-10-25 18:32:00,524 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:}}
{{2022-10-25 18:32:00,526 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --configDir}}
{{2022-10-25 18:32:00,528 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     /opt/flink/conf}}
{{2022-10-25 18:32:00,528 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --executionMode}}
{{2022-10-25 18:32:00,528 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     cluster}}
{{2022-10-25 18:32:00,529 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D}}
{{2022-10-25 18:32:00,529 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b}}
{{2022-10-25 18:32:00,529 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D}}
{{2022-10-25 18:32:00,533 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b}}
{{2022-10-25 18:32:00,533 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D}}
{{2022-10-25 18:32:00,534 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b}}
{{2022-10-25 18:32:00,534 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D}}
{{2022-10-25 18:32:00,534 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=469762048b}}
{{2022-10-25 18:32:00,534 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D}}
{{2022-10-25 18:32:00,534 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b}}
{{2022-10-25 18:32:00,534 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: /opt/flink/lib/flink-cep-1.15.1.jar:/opt/flink/lib/flink-connector-files-1.15.1.jar:/opt/flink/lib/flink-csv-1.15.1.jar:/opt/flink/lib/flink-json-1.15.1.jar:/opt/flink/lib/flink-scala_2.12-1.15.1.jar:/opt/flink/lib/flink-shaded-zookeeper-3.5.9.jar:/opt/flink/lib/flink-table-api-java-uber-1.15.1.jar:/opt/flink/lib/flink-table-planner-loader-1.15.1.jar:/opt/flink/lib/flink-table-runtime-1.15.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.15.1.jar:::}}
{{2022-10-25 18:32:00,535 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------}}
{{2022-10-25 18:32:00,611 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]}}
{{2022-10-25 18:32:00,635 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124}}
{{2022-10-25 18:32:00,715 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.jobmanager.annotations, flinkdeployment.flink.apache.org/generation:1}}
{{2022-10-25 18:32:00,716 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.jobmanager.replicas, 1}}
{{2022-10-25 18:32:00,716 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: scheduler-mode, reactive}}
{{2022-10-25 18:32:00,716 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: ""kubernetes.operator.metrics.reporter.prom.port"", ""9999""}}
{{2022-10-25 18:32:00,717 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, test-flink-cluster.default}}
{{2022-10-25 18:32:00,717 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.taskmanager.cpu, 0.2}}
{{2022-10-25 18:32:00,717 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: ""prometheus.io/port"", ""9999""}}
{{2022-10-25 18:32:00,717 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.service-account, flink}}
{{2022-10-25 18:32:00,717 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.cluster-id, test-flink-cluster}}
{{2022-10-25 18:32:00,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.container.image, flink:1.15}}
{{2022-10-25 18:32:00,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2}}
{{2022-10-25 18:32:00,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.namespace, default}}
{{2022-10-25 18:32:00,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2}}
{{2022-10-25 18:32:00,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.rest-service.exposed.type, ClusterIP}}
{{2022-10-25 18:32:00,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: ""prometheus.io/scrape"", ""true""}}
{{2022-10-25 18:32:00,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1024m}}
{{2022-10-25 18:32:00,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: ""kubernetes.operator.metrics.reporter.prom.class"", ""org.apache.flink.metrics.prometheus.PrometheusReporter""}}
{{2022-10-25 18:32:00,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.cancel.enable, false}}
{{2022-10-25 18:32:00,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.target, remote}}
{{2022-10-25 18:32:00,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1024m}}
{{2022-10-25 18:32:00,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.rpc.port, 6122}}
{{2022-10-25 18:32:00,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.internal.cluster-mode, SESSION}}
{{2022-10-25 18:32:00,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: kubernetes.jobmanager.cpu, 0.2}}
{{2022-10-25 18:32:00,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.flink.version, v1_15}}
{{2022-10-25 18:32:00,924 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Reactive mode is configured for an unsupported cluster type. At the moment, reactive mode is only supported by standalone application clusters (bin/standalone-job.sh).}}
{{Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: Reactive mode is configured for an unsupported cluster type. At the moment, reactive mode is only supported by standalone application clusters (bin/standalone-job.sh).}}
{{        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.<init>(ClusterEntrypoint.java:177)}}
{{        at org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.<init>(SessionClusterEntrypoint.java:39)}}
{{        at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.<init>(StandaloneSessionClusterEntrypoint.java:32)}}
{{        at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:56)}}

 

 ","Kubernetes Version 1.22 on EKS.

Flink Operator veresion 1.2.0

Flink Veresion 1.15 (errors in 1.14 too)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 16:17:23 UTC 2022,,,,,,,,,,"0|z1a8eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 18:37;yuvipanda;(Not sure if 'Major' is appropriate, but it was the default so I let it be);;;","25/Oct/22 18:48;yuvipanda;Here is the jobmanager deployment created:

 

{{apiVersion: apps/v1}}
{{kind: Deployment}}
{{metadata:}}
{{  annotations:}}
{{    deployment.kubernetes.io/revision: ""1""}}
{{    flinkdeployment.flink.apache.org/generation: ""1""}}
{{  creationTimestamp: ""2022-10-25T18:31:36Z""}}
{{  generation: 1}}
{{  labels:}}
{{    app: test-flink-cluster}}
{{    component: jobmanager}}
{{    type: flink-standalone-kubernetes}}
{{  name: test-flink-cluster}}
{{  namespace: default}}
{{  resourceVersion: ""14946930""}}
{{  uid: 766cdb2c-5e2d-4b27-8694-cf8ae58ea084}}
{{spec:}}
{{  progressDeadlineSeconds: 600}}
{{  replicas: 1}}
{{  revisionHistoryLimit: 10}}
{{  selector:}}
{{    matchLabels:}}
{{      app: test-flink-cluster}}
{{      component: jobmanager}}
{{      type: flink-standalone-kubernetes}}
{{  strategy:}}
{{    rollingUpdate:}}
{{      maxSurge: 25%}}
{{      maxUnavailable: 25%}}
{{    type: RollingUpdate}}
{{  template:}}
{{    metadata:}}
{{      annotations:}}
{{        flinkdeployment.flink.apache.org/generation: ""1""}}
{{      creationTimestamp: null}}
{{      labels:}}
{{        app: test-flink-cluster}}
{{        component: jobmanager}}
{{        type: flink-standalone-kubernetes}}
{{    spec:}}
{{      containers:}}
{{      - args:}}
{{        - jobmanager}}
{{        command:}}
{{        - /docker-entrypoint.sh}}
{{        env:}}
{{        - name: _POD_IP_ADDRESS}}
{{          valueFrom:}}
{{            fieldRef:}}
{{              apiVersion: v1}}
{{              fieldPath: status.podIP}}
{{        image: flink:1.15}}
{{        imagePullPolicy: IfNotPresent}}
{{        name: flink-main-container}}
{{        ports:}}
{{        - containerPort: 8081}}
{{          name: rest}}
{{          protocol: TCP}}
{{        - containerPort: 6123}}
{{          name: jobmanager-rpc}}
{{          protocol: TCP}}
{{        - containerPort: 6124}}
{{          name: blobserver}}
{{          protocol: TCP}}
{{        resources:}}
{{          limits:}}
{{            cpu: 200m}}
{{            memory: 1Gi}}
{{          requests:}}
{{            cpu: 200m}}
{{            memory: 1Gi}}
{{        terminationMessagePath: /dev/termination-log}}
{{        terminationMessagePolicy: File}}
{{        volumeMounts:}}
{{        - mountPath: /opt/flink/conf}}
{{          name: flink-config-volume}}
{{      dnsPolicy: ClusterFirst}}
{{      restartPolicy: Always}}
{{      schedulerName: default-scheduler}}
{{      securityContext: {}}}
{{      serviceAccount: flink}}
{{      serviceAccountName: flink}}
{{      terminationGracePeriodSeconds: 30}}
{{      volumes:}}
{{      - configMap:}}
{{          defaultMode: 420}}
{{          items:}}
{{          - key: log4j-console.properties}}
{{            path: log4j-console.properties}}
{{          - key: flink-conf.yaml}}
{{            path: flink-conf.yaml}}
{{          name: flink-config-test-flink-cluster}}
{{        name: flink-config-volume}}
{{status:}}
{{  conditions:}}
{{  - lastTransitionTime: ""2022-10-25T18:31:36Z""}}
{{    lastUpdateTime: ""2022-10-25T18:31:39Z""}}
{{    message: ReplicaSet ""test-flink-cluster-58cd584fdd"" has successfully progressed.}}
{{    reason: NewReplicaSetAvailable}}
{{    status: ""True""}}
{{    type: Progressing}}
{{  - lastTransitionTime: ""2022-10-25T18:45:34Z""}}
{{    lastUpdateTime: ""2022-10-25T18:45:34Z""}}
{{    message: Deployment does not have minimum availability.}}
{{    reason: MinimumReplicasUnavailable}}
{{    status: ""False""}}
{{    type: Available}}
{{  observedGeneration: 1}}
{{  replicas: 1}}
{{  unavailableReplicas: 1}}
{{  updatedReplicas: 1}}

 

and the configmap

 

{{apiVersion: v1}}
{{data:}}
{{  flink-conf.yaml: |}}
{{    blob.server.port: 6124}}
{{    kubernetes.jobmanager.annotations: flinkdeployment.flink.apache.org/generation:1}}
{{    kubernetes.jobmanager.replicas: 1}}
{{    scheduler-mode: reactive}}
{{    ""kubernetes.operator.metrics.reporter.prom.port"": ""9999""}}
{{    jobmanager.rpc.address: test-flink-cluster.default}}
{{    kubernetes.taskmanager.cpu: 0.2}}
{{    ""prometheus.io/port"": ""9999""}}
{{    kubernetes.service-account: flink}}
{{    kubernetes.cluster-id: test-flink-cluster}}
{{    kubernetes.container.image: flink:1.15}}
{{    parallelism.default: 2}}
{{    kubernetes.namespace: default}}
{{    taskmanager.numberOfTaskSlots: 2}}
{{    kubernetes.rest-service.exposed.type: ClusterIP}}
{{    ""prometheus.io/scrape"": ""true""}}
{{    taskmanager.memory.process.size: 1024m}}
{{    ""kubernetes.operator.metrics.reporter.prom.class"": ""org.apache.flink.metrics.prometheus.PrometheusReporter""}}
{{    web.cancel.enable: false}}
{{    execution.target: remote}}
{{    jobmanager.memory.process.size: 1024m}}
{{    taskmanager.rpc.port: 6122}}
{{    kubernetes.internal.cluster-mode: SESSION}}
{{    kubernetes.jobmanager.cpu: 0.2}}
{{    $internal.flink.version: v1_15}}
{{  log4j-console.properties: |}}
{{    ################################################################################}}
{{    #  Licensed to the Apache Software Foundation (ASF) under one}}
{{    #  or more contributor license agreements.  See the NOTICE file}}
{{    #  distributed with this work for additional information}}
{{    #  regarding copyright ownership.  The ASF licenses this file}}
{{    #  to you under the Apache License, Version 2.0 (the}}
{{    #  ""License""); you may not use this file except in compliance}}
{{    #  with the License.  You may obtain a copy of the License at}}
{{    #}}
{{    #      http://www.apache.org/licenses/LICENSE-2.0}}
{{    #}}
{{    #  Unless required by applicable law or agreed to in writing, software}}
{{    #  distributed under the License is distributed on an ""AS IS"" BASIS,}}
{{    #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.}}
{{    #  See the License for the specific language governing permissions and}}
{{    # limitations under the License.}}
{{    ################################################################################}}

{{    # This affects logging for both user code and Flink}}
{{    rootLogger.level = INFO}}
{{    rootLogger.appenderRef.console.ref = ConsoleAppender}}
{{    rootLogger.appenderRef.rolling.ref = RollingFileAppender}}

{{    # Uncomment this if you want to _only_ change Flink's logging}}
{{    #logger.flink.name = org.apache.flink}}
{{    #logger.flink.level = INFO}}

{{    # The following lines keep the log level of common libraries/connectors on}}
{{    # log level INFO. The root logger does not override this. You have to manually}}
{{    # change the log levels here.}}
{{    logger.akka.name = akka}}
{{    logger.akka.level = INFO}}
{{    logger.kafka.name= org.apache.kafka}}
{{    logger.kafka.level = INFO}}
{{    logger.hadoop.name = org.apache.hadoop}}
{{    logger.hadoop.level = INFO}}
{{    logger.zookeeper.name = org.apache.zookeeper}}
{{    logger.zookeeper.level = INFO}}

{{    # Log all infos to the console}}
{{    appender.console.name = ConsoleAppender}}
{{    appender.console.type = CONSOLE}}
{{    appender.console.layout.type = PatternLayout}}
{{    appender.console.layout.pattern = %d\{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n}}

{{    # Log all infos in the given rolling file}}
{{    appender.rolling.name = RollingFileAppender}}
{{    appender.rolling.type = RollingFile}}
{{    appender.rolling.append = false}}
{{    appender.rolling.fileName = ${sys:log.file}}}
{{    appender.rolling.filePattern = ${sys:log.file}.%i}}
{{    appender.rolling.layout.type = PatternLayout}}
{{    appender.rolling.layout.pattern = %d\{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n}}
{{    appender.rolling.policies.type = Policies}}
{{    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy}}
{{    appender.rolling.policies.size.size=100MB}}
{{    appender.rolling.strategy.type = DefaultRolloverStrategy}}
{{    appender.rolling.strategy.max = 10}}

{{    # Suppress the irrelevant (wrong) warnings from the Netty channel handler}}
{{    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline}}
{{    logger.netty.level = OFF}}

{{    # Flink Deployment Logging Overrides}}
{{    # rootLogger.level = DEBUG}}
{{kind: ConfigMap}}
{{metadata:}}
{{  creationTimestamp: ""2022-10-25T18:31:36Z""}}
{{  labels:}}
{{    app: test-flink-cluster}}
{{    type: flink-standalone-kubernetes}}
{{  name: flink-config-test-flink-cluster}}
{{  namespace: default}}
{{  ownerReferences:}}
{{  - apiVersion: apps/v1}}
{{    blockOwnerDeletion: true}}
{{    controller: true}}
{{    kind: Deployment}}
{{    name: test-flink-cluster}}
{{    uid: 766cdb2c-5e2d-4b27-8694-cf8ae58ea084}}
{{  resourceVersion: ""14943742""}}
{{  uid: ef1dbf21-8b85-4d0a-95be-519b1f4d8f04}};;;","25/Oct/22 19:51;gyfora;As you can see from the error message: ""reactive mode is only supported by standalone application clusters""

You forgot to specify the job spec in your FlinkDeployment yaml. You need that to create an Application cluster otherwise you get an empty Session cluster .

Only Application clusters support reactive mode;;;","25/Oct/22 19:54;yuvipanda;Oh, I see - I only saw the `standalone` part but not the `application` part. I'm trying to run an Apache Beam pipeline, and I'm not sure if that'll actually work if I had to specify a job during cluster creation.;;;","25/Oct/22 19:55;yuvipanda;So if I understand this correctly, we can't actually use Reactive Mode with Apache Beam on Flink until Standalone Session clusters are also supported?;;;","25/Oct/22 20:26;gyfora;I think you should be able to set up your Beam job as Flink Application. That is a pre-requisite for the reactive mode. I am not aware of any plans for supporting reactive mode for session clusters. Thre problem is if there are multiple jobs they would be competing for resources.;;;","26/Oct/22 16:17;yuvipanda;Ah interesting! I'm using the portable runner with Beam to run Python pipelines, and am not entirely sure how to run it as an application as I'd imagine I'd have to submit a jar. But perhaps that belongs in the beam issue tracker?;;;",,,,,,,,,,,,,,,,
Simplify HadoopModule,FLINK-29761,13491358,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,25/Oct/22 18:27,07/Nov/22 16:09,04/Jun/24 20:41,07/Nov/22 16:09,,,,,,,,,,,Connectors / Common,Deployment / Kubernetes,Deployment / YARN,,0,pull-request-available,security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 16:09:50 UTC 2022,,,,,,,,,,"0|z1a8e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 16:09;mbalassi;[{{575517b}}|https://github.com/apache/flink/commit/575517bbb8de36b21632e54b441b7dcbc4d061c4] in master;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce snapshots metadata table,FLINK-29760,13491313,13490538,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/Oct/22 14:04,27/Oct/22 04:00,04/Jun/24 20:41,27/Oct/22 04:00,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Introduce snapshots metadata table to show snapshot history.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 04:00:55 UTC 2022,,,,,,,,,,"0|z1a848:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 04:00;lzljs3620320;master: a58513e40eb0ffb50fb6eee357c4449a9d8483cd;;;",,,,,,,,,,,,,,,,,,,,,,
Cast type in LEFT JOIN,FLINK-29759,13491298,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,adecuq,adecuq,25/Oct/22 13:11,26/Oct/22 14:46,04/Jun/24 20:41,,1.14.5,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Hello,

I would like to use LEFT JOIN in order to implement a non blocking join without two tables (relationship is optional).

There is a specificity: key on both side has not the same type (STRING vs INT).

Here a snap a code:

Avro input:

 
{code:java}
{
  ""name"": ""CompanyBankAccountMessage"",
  ""type"": ""record"",
  ""namespace"": ""com.kyriba.dataproduct.core.model.input"",
  ""fields"": [ 
    {
      ""name"": ""data"",
      ""type"": {
        ""fields"": [
          {
            ""name"": ""CURRENCY_ID"",
            ""type"": [
              ""null"",
              ""string""
            ],
            ""default"": null,
          },
          ...
        ]
      }
    }
  ]
}{code}
 

Avro output:

 
{code:java}
{
  ""name"": ""CurrencyMessage"",
  ""type"": ""record"",
  ""namespace"": ""com.kyriba.dataproduct.core.model.input"", 
  ""fields"": [
    {
      ""name"": ""data"",
      ""type"": {
        ""fields"": [
          {
            ""name"": ""CURRENCY_ID"",
            ""type"": ""int""
          },
          ...
        ]
      }
    }
  ]
}{code}
 

Sql query:

 
{code:java}
SELECT ...
FROM `my.input.COMPANY_BANK_ACCOUNT.v1.avro` as COMPANY_BANK_ACCOUNT
LEFT JOIN `my.input.CURRENCY.v1.avro` as CURRENCY
ON CAST(COMPANY_BANK_ACCOUNT.CURRENCY_ID as INT) = CURRENCY.CURRENCY_ID{code}
I got this exception:

 

 
{code:java}
Conversion to relational algebra failed to preserve datatypes:
validated type:
  RecordType(BIGINT currencyUid, ...)
converted type: 
  RecordType(BIGINT currencyUid NOT NULL, ...)
rel:
  LogicalProject(currencyUid=[CAST($116.CURRENCY_ID):BIGINT NOT NULL], ...)
    LogicalJoin(condition=[=($11, $117)], joinType=[left])
      LogicalTableScan(table=[[data-platform, core, kyriba.flink-sql-test.core.cdc.COMPANY_BANK_ACCOUNT.v1.avro]])
      LogicalProject(the_port_key=[$0], data=[$1], $f2=[$1.CURRENCY_ID])
        LogicalTableScan(table=[[data-platform, core, kyriba.flink-sql-test.core.cdc.CURRENCY.v1.avro]])
at org.apache.calcite.sql2rel.SqlToRelConverter.checkConvertedType(SqlToRelConverter.java:467){code}
Did I make something wrong or this is a bug?

 

Nota: it works well with this inner join:
{code:java}
FROM `my.input.COMPANY_BANK_ACCOUNT.v1.avro` as COMPANY_BANK_ACCOUNT,
     `my.input.CURRENCY.v1.avro` as CURRENCY
WHERE CAST(COMPANY_BANK_ACCOUNT.CURRENCY_ID as INT) = CURRENCY.CURRENCY_ID {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 14:46:08 UTC 2022,,,,,,,,,,"0|z1a80w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 14:03;martijnvisser;Please verify this with Flink 1.15 since there was a big overhaul in the CAST functionality there, see FLINK-24403;;;","26/Oct/22 01:55;luoyuxia;[~adecuq] Does the key on both side has the same nullabity (STRING vs INT)? If not, you can make them has same nullabity and try again.;;;","26/Oct/22 08:02;adecuq;[~luoyuxia] not the same nullability - on the foreign key side this relationship is optional (STRING) and on primary key side it's mandatory (INT NOT NULL).

By the way, I tried to remove null value for the STRING so I have this on both side:
{code:java}
{
  ""name"": ""CURRENCY_ID"",
  ""type"": ""string""
}{code}
but still the issue.

What I don't understand is why in converted type it became NOT NULL?
{code:java}
validated type:
RecordType(BIGINT currencyUid, ...)
converted type: 
RecordType(BIGINT currencyUid NOT NULL, ...){code};;;","26/Oct/22 08:59;adecuq;[~martijnvisser] I also tried to set STRING type on both side but I still have this issue so pretty sure this is not linked to CAST.

When I replace LEFT JOIN by INNER JOIN it works perfectly: 
{code:java}
FROM `my.input.COMPANY_BANK_ACCOUNT.v1.avro` as COMPANY_BANK_ACCOUNT
  INNER JOIN `my.input.CURRENCY.v1.avro` as CURRENCY
    ON COMPANY_BANK_ACCOUNT.CURRENCY_ID = CURRENCY.CURRENCY_ID {code};;;","26/Oct/22 12:47;martijnvisser;[~adecuq] It could still be related to an implicit cast. To make sure that this bug still exists, please verify this on Flink 1.15 (or the soon-to-be-released 1.16);;;","26/Oct/22 14:46;adecuq;[~martijnvisser] noted we will try with 1.15 and let u updated.;;;",,,,,,,,,,,,,,,,,
Serialization issue with enum,FLINK-29758,13491295,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,adecuq,adecuq,25/Oct/22 12:52,27/Oct/22 07:06,04/Jun/24 20:41,,1.14.5,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / API,,,0,,,,,"Hello,

I am implementing an SQL script between two Kafka topics (Avro).

One field of my input is a STRING, the equivalent in output has been defined as ENUM.

Here snaps of code:

Avro input
{code:java}
{
  ""name"": ""CompanyBankAccountMessage"",
  ""type"": ""record"",
  ""namespace"": ""com.kyriba.dataproduct.core.model.input"",
  ""fields"": [ 
    {
      ""default"": null,
      ""name"": ""ACCOUNT_TYPE"",
      ""type"": [
        ""null"",
        ""int""
      ]
    },
    ...
},{code}
Avro output
{code:java}
{
  ""name"": ""Account"",
  ""type"": ""record"",
  ""namespace"": ""com.kyriba.dataproduct.core.model.output"",
  ""fields"": [
    {
      ""name"": ""type"",
      ""type"": {
        ""type"": ""enum"",
        ""name"": ""Type"",
        ""symbols"": [
          ""All"",
          ""Bank_account"",
          ""Intercompany_account"",
          ""Shared_account"",
          ""Other_account""
        ]
      }
    },
    ...
  ]
}{code}
My SQL looks like
{code:java}
INSERT INTO `my.output.account.v1.avro`
SELECT BANK_ACCOUNT_TYPE type,
       ... 
FROM `my.input.COMPANY_BANK_ACCOUNT.v1.avro`{code}
I got this exception:
{code:java}
Caused by: java.lang.RuntimeException: Failed to serialize row.
  at org.apache.flink.formats.avro.AvroRowDataSerializationSchema.serialize(AvroRowDataSerializationSchema.java:90)
  at com.kyriba.flink.datacatalog.formats.datacatalog.DataCatalogAvroRowDataSerializationSchema.serialize(DataCatalogAvroRowDataSerializationSchema.java:37)
...
Caused by: org.apache.avro.AvroTypeException: value Bank_account (a org.apache.avro.util.Utf8) is not a Type at Account.type
...
Caused by: org.apache.avro.AvroTypeException: value Bank_account (a org.apache.avro.util.Utf8) is not a Type {code}
Did I make something wrong or this is a bug?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 08:22:23 UTC 2022,,,,,,,,,,"0|z1a808:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 14:00;martijnvisser;Per https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/avro/ enum isn't supported as a type in Flink. See also FLINK-29267;;;","26/Oct/22 08:22;adecuq;[~martijnvisser] Ok thanks for your quick answer :) so my idea will be to use an user defined function to check if the string contains a value among a list of values (like an enum).;;;",,,,,,,,,,,,,,,,,,,,,
ContinuousFileSplitEnumerator skip unprocessed splits when the file is splittable,FLINK-29757,13491290,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,OlegPT,OlegPT,OlegPT,25/Oct/22 12:28,15/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,stale-assigned,,,"ContinuousFileSplitEnumerator use a HashSet<Path> to store processed splits. This works fine when process a file as a single split, once the file is splittable it will make unprocessed splits skipped.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:10 UTC 2023,,,,,,,,,,"0|z1a7z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 14:06;martijnvisser;[~OlegPT] I'm not sure I understand because it sounds like you're modifying a file while having a running Flink application. Can you please explain the steps that you would need to take to reproduce this error? ;;;","26/Oct/22 01:49;luoyuxia;[~OlegPT] Thanks for reporting it. I think we may need use <path, offset>  to identify a split instead of single a path. What's you idea for fixing it ?;;;","26/Oct/22 02:59;OlegPT;[~martijnvisser] [~luoyuxia]  I think we should use a combined string of path, offset and length instead of just path to track processed splits. I created a pull request for this, could you have a look at it? Thanks.;;;","26/Oct/22 03:07;luoyuxia;Thanks. I'll find some times to review it. ;;;","07/Nov/22 09:55;清月;[~OlegPT] Can this issue be assigned to me for processing?;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
Support materialized column to improve query performance for complex types,FLINK-29756,13491282,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,nicholasjiang,nicholasjiang,25/Oct/22 11:36,29/Mar/23 01:51,04/Jun/24 20:41,29/Mar/23 01:51,table-store-0.3.0,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"In the world of data warehouse, it is very common to use one or more columns from a complex type such as a map, or to put many subfields into it. These operations can greatly affect query performance because:
 # These operations are very wasteful IO. For example, if we have a field type of Map, which contains dozens of subfields, we need to read the entire column when reading this column. And Spark will traverse the entire map to get the value of the target key.
 # Cannot take advantage of vectorized reads when reading nested type columns.
 # Filter pushdown cannot be used when reading nested columns.

It is necessary to introduce the materialized column feature in Flink Table Store, which transparently solves the above problems of arbitrary columnar storage (not just Parquet).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:51:15 UTC 2023,,,,,,,,,,"0|z1a7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 01:51;lzljs3620320;https://github.com/apache/incubator-paimon/issues/735;;;",,,,,,,,,,,,,,,,,,,,,,
PulsarSourceUnorderedE2ECase.testSavepoint failed because of missing TaskManagers,FLINK-29755,13491265,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Invalid,,mapohl,mapohl,25/Oct/22 10:27,04/Jan/23 02:53,04/Jun/24 20:41,04/Jan/23 02:53,1.16.0,pulsar-3.0.0,,,,,,,,,Connectors / Pulsar,,,,0,test-stability,,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42325&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=13932] failed (not exclusively) due to a problem with {{PulsarSourceUnorderedE2ECase.testSavepoint}}. It seems like there were no TaskManagers spun up which resulted in the test job failing with a {{NoResourceAvailableException}}.
{code}
org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: []. Current slot pool status: Registered TMs: 0, registered slots: 0 free slots: 0
{code}

I didn't raise this one to critical because it looks like a missing TaskManager test environment issue. I attached the e2e test-specific logs to the Jira issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,FLINK-30397,,,FLINK-29836,,,,,,FLINK-29835,"25/Oct/22 10:27;mapohl;PulsarSourceUnorderedE2ECase.testSavepoint.log;https://issues.apache.org/jira/secure/attachment/13051379/PulsarSourceUnorderedE2ECase.testSavepoint.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 02:53:25 UTC 2023,,,,,,,,,,"0|z1a7tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 10:29;mapohl;[~syhily] (selected based on the git history of {{flink-end-to-end-tests-pulsar}}) may you have a look at this?;;;","25/Oct/22 14:52;syhily;[~mapohl] I'll try to figure out the cause tomorrow.;;;","01/Nov/22 08:26;martijnvisser;I'm assuming that https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=16222 is also related to this test, WDYT [~mapohl] [~syhily] ?;;;","01/Nov/22 09:37;mapohl;It doesn't look like that. Build #42680 failed due to some issue with the BlobServer (at least based on the logs). I created FLINK-29835 and linked this issue to it.;;;","14/Nov/22 03:47;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43102&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34;;;","14/Nov/22 03:56;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43104&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34;;;","21/Nov/22 04:15;leonard;PulsarSourceUnorderedE2ECase.testSourceSingleSplit failed on azure may due to same issue
https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/43307/logs/847;;;","21/Nov/22 04:20;leonard;[~syhily] I also found some shade error  log and pulsar internal error log, these error logs really make the troubleshoot harder, could you take a look ?


{noformat}
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 2022-11-19 03:05:19,977 ERROR org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: java.lang.NoClassDefFoundError: org/apache/pulsar/shade/io/netty/util/concurrent/GlobalEventExecutor$2
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.GlobalEventExecutor.startThread(GlobalEventExecutor.java:223) ~[blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.GlobalEventExecutor.execute0(GlobalEventExecutor.java:211) ~[blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.GlobalEventExecutor.execute(GlobalEventExecutor.java:205) ~[blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:499) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:96) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:1057) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.pulsar.shade.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [blob_p-fb94d82f266979b2959919c77d8d46821bf01b74-6789386b595a9ff48b74a062fd69a96e:2.10.2]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: Caused by: java.lang.ClassNotFoundException: org.apache.pulsar.shade.io.netty.util.concurrent.GlobalEventExecutor$2
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_342]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_342]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_342]
03:05:19,978 [docker-java-stream-848840694] INFO  org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment [] - [JobManager] STDOUT: 	... 12 more{noformat}


{noformat}
ERROR org.apache.pulsar.broker.service.ServerCnx - Send response error for END_TXN request 2213444997852929784.
ERROR org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState [] - Authentication failed
{noformat}
;;;","22/Nov/22 07:48;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43368&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678;;;","30/Nov/22 06:54;mapohl;I moved this under the Umbrella ticket for Pulsar stability issues;;;","08/Dec/22 14:41;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43807&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","09/Dec/22 09:11;Sergey Nuyanzin;PulsarSourceUnorderedE2ECase.testSourceSingleSplit failed on azure may due to same issue

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43809&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14255;;;","13/Dec/22 07:18;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43904&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34;;;","19/Dec/22 11:21;rmetzger;Another instability with the Pulsar E2e tests:

{code}
2022-12-18T16:01:17.3859942Z Dec 18 16:01:17 [ERROR] org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.testScaleDown(TestEnvironment, DataStreamSourceExternalContext, CheckpointingMode)[2]  Time elapsed: 124.035 s  <<< FAILURE!

022-12-18T16:01:17.3861087Z Dec 18 16:01:17 Expecting
2022-12-18T16:01:17.3861386Z Dec 18 16:01:17   <CompletableFuture[Incomplete]>
2022-12-18T16:01:17.3861716Z Dec 18 16:01:17 to be completed within 2M.
2022-12-18T16:01:17.3861992Z Dec 18 16:01:17 
2022-12-18T16:01:17.3862369Z Dec 18 16:01:17 exception caught while trying to get the future result: java.util.concurrent.TimeoutException
2022-12-18T16:01:17.3862901Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2022-12-18T16:01:17.3863456Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-12-18T16:01:17.3863987Z Dec 18 16:01:17 	at org.assertj.core.internal.Futures.assertSucceededWithin(Futures.java:109)
2022-12-18T16:01:17.3864610Z Dec 18 16:01:17 	at org.assertj.core.api.AbstractCompletableFutureAssert.internalSucceedsWithin(AbstractCompletableFutureAssert.java:400)
2022-12-18T16:01:17.3865289Z Dec 18 16:01:17 	at org.assertj.core.api.AbstractCompletableFutureAssert.succeedsWithin(AbstractCompletableFutureAssert.java:396)
2022-12-18T16:01:17.3866019Z Dec 18 16:01:17 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.checkResultWithSemantic(UnorderedSourceTestSuiteBase.java:55)
2022-12-18T16:01:17.3866771Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:329)
2022-12-18T16:01:17.3867465Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testScaleDown(SourceTestSuiteBase.java:279)
2022-12-18T16:01:17.3868017Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-12-18T16:01:17.3868513Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-12-18T16:01:17.3869078Z Dec 18 16:01:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-12-18T16:01:17.3869596Z Dec 18 16:01:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-12-18T16:01:17.3870098Z Dec 18 16:01:17 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2022-12-18T16:01:17.3870794Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-12-18T16:01:17.3871458Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-12-18T16:01:17.3872128Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2022-12-18T16:01:17.3872754Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2022-12-18T16:01:17.3873407Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:94)
2022-12-18T16:01:17.3874141Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2022-12-18T16:01:17.3874918Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2022-12-18T16:01:17.3875638Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-12-18T16:01:17.3876515Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-12-18T16:01:17.3877239Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-12-18T16:01:17.3877912Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-12-18T16:01:17.3878578Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2022-12-18T16:01:17.3879243Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2022-12-18T16:01:17.3879939Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2022-12-18T16:01:17.3889139Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.3889972Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2022-12-18T16:01:17.3890666Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2022-12-18T16:01:17.3891311Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2022-12-18T16:01:17.3893070Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.3893722Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.3894372Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.3895063Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.3895674Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.3896322Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.3896956Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.3897568Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.3898303Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.3899178Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-12-18T16:01:17.3899956Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-12-18T16:01:17.3900656Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-12-18T16:01:17.3901327Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
2022-12-18T16:01:17.3902009Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.3902618Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.3903153Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.3903811Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-12-18T16:01:17.3904448Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.3904969Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.3905502Z Dec 18 16:01:17 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-12-18T16:01:17.3906039Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-12-18T16:01:17.3906570Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.3907112Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.3907650Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.3908172Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.3908725Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.3909289Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.3909831Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.3910350Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.3910878Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.3911419Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.3911941Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.3912469Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.3913023Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.3913576Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.3914115Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.3914633Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.3915218Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.3915877Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
2022-12-18T16:01:17.3916539Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.3917185Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.3917844Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.3918448Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.3919031Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.3931092Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.3931775Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4294232Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4295652Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4296857Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.4297881Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.4298766Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.4299553Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4300343Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4302161Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4304074Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4305435Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4306340Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4307244Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4308267Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4309464Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.4311017Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.4312369Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.4314270Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4315623Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4316475Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4317361Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4318297Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4319210Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4320436Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4322032Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4324040Z Dec 18 16:01:17 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-12-18T16:01:17.4324554Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-12-18T16:01:17.4325809Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-12-18T16:01:17.4326453Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-12-18T16:01:17.4326992Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-12-18T16:01:17.4327397Z Dec 18 16:01:17 
2022-12-18T16:01:17.4327909Z Dec 18 16:01:17 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.checkResultWithSemantic(UnorderedSourceTestSuiteBase.java:55)
2022-12-18T16:01:17.4328668Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:329)
2022-12-18T16:01:17.4329363Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testScaleDown(SourceTestSuiteBase.java:279)
2022-12-18T16:01:17.4329910Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-12-18T16:01:17.4330411Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-12-18T16:01:17.4330982Z Dec 18 16:01:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-12-18T16:01:17.4331484Z Dec 18 16:01:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-12-18T16:01:17.4331997Z Dec 18 16:01:17 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2022-12-18T16:01:17.4332575Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-12-18T16:01:17.4333220Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-12-18T16:01:17.4334651Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2022-12-18T16:01:17.4335316Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2022-12-18T16:01:17.4335968Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:94)
2022-12-18T16:01:17.4337206Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2022-12-18T16:01:17.4338014Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2022-12-18T16:01:17.4340260Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-12-18T16:01:17.4361577Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-12-18T16:01:17.4362589Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-12-18T16:01:17.4363905Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-12-18T16:01:17.4364755Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2022-12-18T16:01:17.4369490Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2022-12-18T16:01:17.4372628Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2022-12-18T16:01:17.4373477Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4374141Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2022-12-18T16:01:17.4375038Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2022-12-18T16:01:17.4376734Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2022-12-18T16:01:17.4377822Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.4378654Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4412434Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4413194Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4413831Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4414746Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4415587Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4416208Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4417141Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4418358Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-12-18T16:01:17.4419599Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-12-18T16:01:17.4420831Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-12-18T16:01:17.4422062Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
2022-12-18T16:01:17.4422999Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.4424638Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.4425193Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.4425930Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-12-18T16:01:17.4426470Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.4427015Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.4429413Z Dec 18 16:01:17 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-12-18T16:01:17.4429955Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-12-18T16:01:17.4432782Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.4433414Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.4434473Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.4435022Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.4438260Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.4440639Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.4441366Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.4441896Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.4442415Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.4446470Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.4447499Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.4448035Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.4448589Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.4449157Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.4451082Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.4453761Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.4454430Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.4455090Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
2022-12-18T16:01:17.4458151Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.4458815Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4462737Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4463370Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4463970Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4466949Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4467596Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4468217Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4477096Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4478249Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.4495452Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.4496521Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.4497659Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4505788Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4506420Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4507960Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4508875Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4510269Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4511282Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4512743Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4514325Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.4515856Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.4516792Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.4517578Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4518886Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4520127Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4520875Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4521652Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4522765Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4523513Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4524378Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4527602Z Dec 18 16:01:17 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-12-18T16:01:17.4528144Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-12-18T16:01:17.4528804Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-12-18T16:01:17.4529749Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-12-18T16:01:17.4532285Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-12-18T16:01:17.4532725Z Dec 18 16:01:17 
2022-12-18T16:01:17.4533372Z Dec 18 16:01:17 [ERROR] org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.testSavepoint(TestEnvironment, DataStreamSourceExternalContext, CheckpointingMode)[1]  Time elapsed: 7.746 s  <<< FAILURE!
2022-12-18T16:01:17.4534911Z Dec 18 16:01:17 java.lang.AssertionError: 
2022-12-18T16:01:17.4535253Z Dec 18 16:01:17 
2022-12-18T16:01:17.4535927Z Dec 18 16:01:17 Expecting
2022-12-18T16:01:17.4536301Z Dec 18 16:01:17   <CompletableFuture[Failed with the following stack trace:
2022-12-18T16:01:17.4537076Z Dec 18 16:01:17 java.lang.RuntimeException: Failed to fetch next result
2022-12-18T16:01:17.4537885Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-12-18T16:01:17.4538994Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-12-18T16:01:17.4540413Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.compareWithExactlyOnceSemantic(UnorderedCollectIteratorAssert.java:108)
2022-12-18T16:01:17.4541403Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.matchesRecordsFromSource(UnorderedCollectIteratorAssert.java:79)
2022-12-18T16:01:17.4550324Z Dec 18 16:01:17 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.lambda$checkResultWithSemantic$0(UnorderedSourceTestSuiteBase.java:53)
2022-12-18T16:01:17.4551120Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-12-18T16:01:17.4552980Z Dec 18 16:01:17 	at java.lang.Thread.run(Thread.java:750)
2022-12-18T16:01:17.4553423Z Dec 18 16:01:17 Caused by: java.io.IOException: Failed to fetch job execution result
2022-12-18T16:01:17.4554035Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-12-18T16:01:17.4554921Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-12-18T16:01:17.4556168Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-12-18T16:01:17.4556679Z Dec 18 16:01:17 	... 6 more
2022-12-18T16:01:17.4557451Z Dec 18 16:01:17 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 89bea81c04b959b2a4af661d53e2f8bc)
2022-12-18T16:01:17.4558103Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-12-18T16:01:17.4558635Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-12-18T16:01:17.4559912Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-12-18T16:01:17.4560422Z Dec 18 16:01:17 	... 8 more
2022-12-18T16:01:17.4560871Z Dec 18 16:01:17 Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 89bea81c04b959b2a4af661d53e2f8bc)
2022-12-18T16:01:17.4562021Z Dec 18 16:01:17 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
2022-12-18T16:01:17.4562840Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-12-18T16:01:17.4563553Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-12-18T16:01:17.4564138Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.4594784Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-12-18T16:01:17.4597513Z Dec 18 16:01:17 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301)
2022-12-18T16:01:17.4600130Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-12-18T16:01:17.4607241Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-12-18T16:01:17.4608096Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.4610465Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-12-18T16:01:17.4612782Z Dec 18 16:01:17 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$31(RestClusterClient.java:772)
2022-12-18T16:01:17.4614953Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-12-18T16:01:17.4618136Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-12-18T16:01:17.4631360Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.4632040Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-12-18T16:01:17.4632740Z Dec 18 16:01:17 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301)
2022-12-18T16:01:17.4633518Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-12-18T16:01:17.4634181Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-12-18T16:01:17.4634838Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.4635384Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2022-12-18T16:01:17.4635966Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2022-12-18T16:01:17.4636609Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2022-12-18T16:01:17.4637241Z Dec 18 16:01:17 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-12-18T16:01:17.4637798Z Dec 18 16:01:17 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-12-18T16:01:17.4638272Z Dec 18 16:01:17 	... 1 more
2022-12-18T16:01:17.4638738Z Dec 18 16:01:17 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-12-18T16:01:17.4639424Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-12-18T16:01:17.4640138Z Dec 18 16:01:17 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128)
2022-12-18T16:01:17.4640711Z Dec 18 16:01:17 	... 24 more
2022-12-18T16:01:17.4641207Z Dec 18 16:01:17 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-12-18T16:01:17.4641918Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-12-18T16:01:17.4686605Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-12-18T16:01:17.4687332Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-12-18T16:01:17.4687954Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-12-18T16:01:17.4688574Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-12-18T16:01:17.4689212Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
2022-12-18T16:01:17.4689856Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-12-18T16:01:17.4690585Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-12-18T16:01:17.4691402Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1622)
2022-12-18T16:01:17.4692079Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1137)
2022-12-18T16:01:17.4692642Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1077)
2022-12-18T16:01:17.4693409Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:916)
2022-12-18T16:01:17.4694111Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionOperations.markFailed(DefaultExecutionOperations.java:43)
2022-12-18T16:01:17.4694794Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.handleTaskDeploymentFailure(DefaultExecutionDeployer.java:327)
2022-12-18T16:01:17.4695576Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignAllResourcesAndRegisterProducedPartitions$2(DefaultExecutionDeployer.java:170)
2022-12-18T16:01:17.4699044Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-12-18T16:01:17.4699620Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-12-18T16:01:17.4700186Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.4700772Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-12-18T16:01:17.4701373Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
2022-12-18T16:01:17.4702047Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:185)
2022-12-18T16:01:17.4702793Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408)
2022-12-18T16:01:17.4703539Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:396)
2022-12-18T16:01:17.4704239Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:887)
2022-12-18T16:01:17.4704776Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-12-18T16:01:17.4705272Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-12-18T16:01:17.4705838Z Dec 18 16:01:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-12-18T16:01:17.4706350Z Dec 18 16:01:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-12-18T16:01:17.4706876Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:301)
2022-12-18T16:01:17.4707531Z Dec 18 16:01:17 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-12-18T16:01:17.4708171Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:300)
2022-12-18T16:01:17.4708761Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-12-18T16:01:17.4709360Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-12-18T16:01:17.4709954Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-12-18T16:01:17.4710482Z Dec 18 16:01:17 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-12-18T16:01:17.4710963Z Dec 18 16:01:17 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-12-18T16:01:17.4711445Z Dec 18 16:01:17 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
2022-12-18T16:01:17.4711932Z Dec 18 16:01:17 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
2022-12-18T16:01:17.4712421Z Dec 18 16:01:17 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-12-18T16:01:17.4712932Z Dec 18 16:01:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
2022-12-18T16:01:17.4713440Z Dec 18 16:01:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
2022-12-18T16:01:17.4714028Z Dec 18 16:01:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
2022-12-18T16:01:17.4714548Z Dec 18 16:01:17 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-12-18T16:01:17.4714989Z Dec 18 16:01:17 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-12-18T16:01:17.4715452Z Dec 18 16:01:17 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-12-18T16:01:17.4715947Z Dec 18 16:01:17 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
2022-12-18T16:01:17.4716399Z Dec 18 16:01:17 	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
2022-12-18T16:01:17.4716845Z Dec 18 16:01:17 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-12-18T16:01:17.4717291Z Dec 18 16:01:17 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-12-18T16:01:17.4717713Z Dec 18 16:01:17 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-12-18T16:01:17.4718163Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-12-18T16:01:17.4718690Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-12-18T16:01:17.4719222Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-12-18T16:01:17.4719740Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-12-18T16:01:17.4720470Z Dec 18 16:01:17 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-12-18T16:01:17.4721287Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:227)
2022-12-18T16:01:17.4721786Z Dec 18 16:01:17 	... 39 more
2022-12-18T16:01:17.4722303Z Dec 18 16:01:17 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-12-18T16:01:17.4722981Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-12-18T16:01:17.4723554Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-12-18T16:01:17.4724113Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-12-18T16:01:17.4724658Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-12-18T16:01:17.4725290Z Dec 18 16:01:17 	... 37 more
2022-12-18T16:01:17.4725759Z Dec 18 16:01:17 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-12-18T16:01:17.4726202Z Dec 18 16:01:17 ]>
2022-12-18T16:01:17.4726485Z Dec 18 16:01:17 to be completed within 2M.
2022-12-18T16:01:17.4726755Z Dec 18 16:01:17 
2022-12-18T16:01:17.4727232Z Dec 18 16:01:17 exception caught while trying to get the future result: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Failed to fetch next result
2022-12-18T16:01:17.4727857Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-12-18T16:01:17.4728395Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-12-18T16:01:17.4728919Z Dec 18 16:01:17 	at org.assertj.core.internal.Futures.assertSucceededWithin(Futures.java:109)
2022-12-18T16:01:17.4729534Z Dec 18 16:01:17 	at org.assertj.core.api.AbstractCompletableFutureAssert.internalSucceedsWithin(AbstractCompletableFutureAssert.java:400)
2022-12-18T16:01:17.4730213Z Dec 18 16:01:17 	at org.assertj.core.api.AbstractCompletableFutureAssert.succeedsWithin(AbstractCompletableFutureAssert.java:396)
2022-12-18T16:01:17.4730947Z Dec 18 16:01:17 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.checkResultWithSemantic(UnorderedSourceTestSuiteBase.java:55)
2022-12-18T16:01:17.4731768Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:329)
2022-12-18T16:01:17.4732522Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testSavepoint(SourceTestSuiteBase.java:235)
2022-12-18T16:01:17.4733067Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-12-18T16:01:17.4733558Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-12-18T16:01:17.4734121Z Dec 18 16:01:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-12-18T16:01:17.4734627Z Dec 18 16:01:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-12-18T16:01:17.4735136Z Dec 18 16:01:17 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2022-12-18T16:01:17.4735719Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-12-18T16:01:17.4736375Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-12-18T16:01:17.4737031Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2022-12-18T16:01:17.4737646Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2022-12-18T16:01:17.4738294Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:94)
2022-12-18T16:01:17.4739026Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2022-12-18T16:01:17.4739794Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2022-12-18T16:01:17.4740517Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-12-18T16:01:17.4741208Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-12-18T16:01:17.4741873Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-12-18T16:01:17.4742530Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-12-18T16:01:17.4743417Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2022-12-18T16:01:17.4744225Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2022-12-18T16:01:17.4744926Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2022-12-18T16:01:17.4745607Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4746255Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2022-12-18T16:01:17.4746914Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2022-12-18T16:01:17.4747567Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2022-12-18T16:01:17.4748222Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.4748859Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4749611Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4750212Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4750810Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4751458Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4752088Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4752688Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4753426Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4754285Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-12-18T16:01:17.4755048Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-12-18T16:01:17.4755724Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-12-18T16:01:17.4756401Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
2022-12-18T16:01:17.4757083Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.4757692Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.4758231Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.4758772Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-12-18T16:01:17.4759289Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.4759819Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.4760350Z Dec 18 16:01:17 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-12-18T16:01:17.4760888Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-12-18T16:01:17.4761413Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.4761970Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.4762513Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.4763033Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.4763582Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.4764140Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.4764718Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.4765681Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.4766214Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.4766749Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.4767358Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.4767945Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.4768480Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.4769040Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.4769583Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.4770096Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.4770690Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.4771353Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
2022-12-18T16:01:17.4772013Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.4772670Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4773319Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4773919Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4774508Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4775165Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4775800Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4776409Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4777152Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4778046Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.4778913Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.4779659Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.4780316Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4780969Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4781563Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4782163Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4782815Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4783434Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4784047Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4784839Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4785784Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.4786645Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.4787396Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.4788048Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4788701Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.4789293Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.4789893Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.4790543Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.4791165Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.4791772Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.4792512Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.4793201Z Dec 18 16:01:17 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-12-18T16:01:17.4793708Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-12-18T16:01:17.4794234Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-12-18T16:01:17.4794752Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-12-18T16:01:17.4795281Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-12-18T16:01:17.4795767Z Dec 18 16:01:17 Caused by: java.lang.RuntimeException: Failed to fetch next result
2022-12-18T16:01:17.4796341Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-12-18T16:01:17.4797041Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-12-18T16:01:17.4797779Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.compareWithExactlyOnceSemantic(UnorderedCollectIteratorAssert.java:108)
2022-12-18T16:01:17.4798578Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.utils.UnorderedCollectIteratorAssert.matchesRecordsFromSource(UnorderedCollectIteratorAssert.java:79)
2022-12-18T16:01:17.4799368Z Dec 18 16:01:17 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.lambda$checkResultWithSemantic$0(UnorderedSourceTestSuiteBase.java:53)
2022-12-18T16:01:17.4800064Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-12-18T16:01:17.4800543Z Dec 18 16:01:17 	at java.lang.Thread.run(Thread.java:750)
2022-12-18T16:01:17.4800956Z Dec 18 16:01:17 Caused by: java.io.IOException: Failed to fetch job execution result
2022-12-18T16:01:17.4801589Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-12-18T16:01:17.4802336Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-12-18T16:01:17.4803024Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-12-18T16:01:17.4803511Z Dec 18 16:01:17 	... 6 more
2022-12-18T16:01:17.4807857Z Dec 18 16:01:17 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 89bea81c04b959b2a4af661d53e2f8bc)
2022-12-18T16:01:17.4844052Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-12-18T16:01:17.4847380Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-12-18T16:01:17.4852117Z Dec 18 16:01:17 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-12-18T16:01:17.4876086Z Dec 18 16:01:17 	... 8 more
2022-12-18T16:01:17.4876605Z Dec 18 16:01:17 Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 89bea81c04b959b2a4af661d53e2f8bc)
2022-12-18T16:01:17.4887443Z Dec 18 16:01:17 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
2022-12-18T16:01:17.4893031Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-12-18T16:01:17.4894919Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-12-18T16:01:17.4900794Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.4911188Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-12-18T16:01:17.4912089Z Dec 18 16:01:17 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301)
2022-12-18T16:01:17.4917943Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-12-18T16:01:17.5126335Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-12-18T16:01:17.5267191Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.5268242Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-12-18T16:01:17.5276080Z Dec 18 16:01:17 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$31(RestClusterClient.java:772)
2022-12-18T16:01:17.5281824Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-12-18T16:01:17.5282613Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-12-18T16:01:17.5290340Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.5291214Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-12-18T16:01:17.5292139Z Dec 18 16:01:17 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301)
2022-12-18T16:01:17.5298621Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-12-18T16:01:17.5376226Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-12-18T16:01:17.5376873Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.5409485Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2022-12-18T16:01:17.5416309Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2022-12-18T16:01:17.5417032Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2022-12-18T16:01:17.5426365Z Dec 18 16:01:17 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-12-18T16:01:17.5426986Z Dec 18 16:01:17 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-12-18T16:01:17.5427440Z Dec 18 16:01:17 	... 1 more
2022-12-18T16:01:17.5427825Z Dec 18 16:01:17 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-12-18T16:01:17.5428415Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-12-18T16:01:17.5434016Z Dec 18 16:01:17 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128)
2022-12-18T16:01:17.5434606Z Dec 18 16:01:17 	... 24 more
2022-12-18T16:01:17.5435065Z Dec 18 16:01:17 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-12-18T16:01:17.5435760Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-12-18T16:01:17.5436518Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-12-18T16:01:17.5437262Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-12-18T16:01:17.5446009Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-12-18T16:01:17.5446661Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-12-18T16:01:17.5447337Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
2022-12-18T16:01:17.5448018Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-12-18T16:01:17.5448738Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-12-18T16:01:17.5449584Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1622)
2022-12-18T16:01:17.5466185Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1137)
2022-12-18T16:01:17.5466820Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1077)
2022-12-18T16:01:17.5467375Z Dec 18 16:01:17 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:916)
2022-12-18T16:01:17.5468039Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionOperations.markFailed(DefaultExecutionOperations.java:43)
2022-12-18T16:01:17.5468781Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.handleTaskDeploymentFailure(DefaultExecutionDeployer.java:327)
2022-12-18T16:01:17.5469546Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignAllResourcesAndRegisterProducedPartitions$2(DefaultExecutionDeployer.java:170)
2022-12-18T16:01:17.5484448Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-12-18T16:01:17.5485211Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-12-18T16:01:17.5485831Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-12-18T16:01:17.5486402Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-12-18T16:01:17.5487225Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
2022-12-18T16:01:17.5541164Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:185)
2022-12-18T16:01:17.5589909Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408)
2022-12-18T16:01:17.5590821Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:396)
2022-12-18T16:01:17.5593898Z Dec 18 16:01:17 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:887)
2022-12-18T16:01:17.5594446Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-12-18T16:01:17.5595009Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-12-18T16:01:17.5607573Z Dec 18 16:01:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-12-18T16:01:17.5608134Z Dec 18 16:01:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-12-18T16:01:17.5608720Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:301)
2022-12-18T16:01:17.5609416Z Dec 18 16:01:17 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-12-18T16:01:17.5610172Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:300)
2022-12-18T16:01:17.5610757Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-12-18T16:01:17.5645741Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-12-18T16:01:17.5646467Z Dec 18 16:01:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-12-18T16:01:17.5647041Z Dec 18 16:01:17 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-12-18T16:01:17.5647584Z Dec 18 16:01:17 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-12-18T16:01:17.5648113Z Dec 18 16:01:17 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
2022-12-18T16:01:17.5648593Z Dec 18 16:01:17 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
2022-12-18T16:01:17.5665746Z Dec 18 16:01:17 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-12-18T16:01:17.5666345Z Dec 18 16:01:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
2022-12-18T16:01:17.5666851Z Dec 18 16:01:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
2022-12-18T16:01:17.5667398Z Dec 18 16:01:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
2022-12-18T16:01:17.5667930Z Dec 18 16:01:17 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-12-18T16:01:17.5668372Z Dec 18 16:01:17 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-12-18T16:01:17.5671717Z Dec 18 16:01:17 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-12-18T16:01:17.5672259Z Dec 18 16:01:17 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
2022-12-18T16:01:17.5672712Z Dec 18 16:01:17 	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
2022-12-18T16:01:17.5673203Z Dec 18 16:01:17 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-12-18T16:01:17.5675413Z Dec 18 16:01:17 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-12-18T16:01:17.5704138Z Dec 18 16:01:17 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-12-18T16:01:17.5707108Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-12-18T16:01:17.5707984Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-12-18T16:01:17.5711416Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-12-18T16:01:17.5712096Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-12-18T16:01:17.5715407Z Dec 18 16:01:17 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-12-18T16:01:17.5725703Z Dec 18 16:01:17 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:227)
2022-12-18T16:01:17.5726418Z Dec 18 16:01:17 	... 39 more
2022-12-18T16:01:17.5728525Z Dec 18 16:01:17 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-12-18T16:01:17.5729444Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-12-18T16:01:17.5732883Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-12-18T16:01:17.5733508Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-12-18T16:01:17.5734065Z Dec 18 16:01:17 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-12-18T16:01:17.5734532Z Dec 18 16:01:17 	... 37 more
2022-12-18T16:01:17.5737670Z Dec 18 16:01:17 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-12-18T16:01:17.5738151Z Dec 18 16:01:17 
2022-12-18T16:01:17.5738718Z Dec 18 16:01:17 	at org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.checkResultWithSemantic(UnorderedSourceTestSuiteBase.java:55)
2022-12-18T16:01:17.5739519Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:329)
2022-12-18T16:01:17.5740217Z Dec 18 16:01:17 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testSavepoint(SourceTestSuiteBase.java:235)
2022-12-18T16:01:17.5740814Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-12-18T16:01:17.5743788Z Dec 18 16:01:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-12-18T16:01:17.5744368Z Dec 18 16:01:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-12-18T16:01:17.5744923Z Dec 18 16:01:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-12-18T16:01:17.5745475Z Dec 18 16:01:17 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2022-12-18T16:01:17.5746097Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-12-18T16:01:17.5746755Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-12-18T16:01:17.5750416Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2022-12-18T16:01:17.5751142Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2022-12-18T16:01:17.5751978Z Dec 18 16:01:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:94)
2022-12-18T16:01:17.5752861Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2022-12-18T16:01:17.5753709Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2022-12-18T16:01:17.5758014Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-12-18T16:01:17.5758910Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-12-18T16:01:17.5760792Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-12-18T16:01:17.5761606Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-12-18T16:01:17.5762370Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2022-12-18T16:01:17.5763112Z Dec 18 16:01:17 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2022-12-18T16:01:17.5769118Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2022-12-18T16:01:17.5769877Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5770641Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2022-12-18T16:01:17.5771429Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2022-12-18T16:01:17.5772124Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2022-12-18T16:01:17.5775778Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.5780089Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5780867Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.5782542Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.5786991Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.5787767Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5788501Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.5790549Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.5791401Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.5795013Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-12-18T16:01:17.5795865Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-12-18T16:01:17.5798384Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-12-18T16:01:17.5799183Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
2022-12-18T16:01:17.5805898Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.5810158Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.5810955Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.5818304Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-12-18T16:01:17.5831061Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-12-18T16:01:17.5846994Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-12-18T16:01:17.5847532Z Dec 18 16:01:17 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-12-18T16:01:17.5851878Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-12-18T16:01:17.5854465Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.5855039Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.5855698Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.5858114Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.5863006Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.5864095Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.5866455Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.5867385Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.5870494Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-12-18T16:01:17.5871159Z Dec 18 16:01:17 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-12-18T16:01:17.5873380Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-12-18T16:01:17.5874214Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-12-18T16:01:17.5874827Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-12-18T16:01:17.5876927Z Dec 18 16:01:17 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-12-18T16:01:17.5881873Z Dec 18 16:01:17 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-12-18T16:01:17.5883444Z Dec 18 16:01:17 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-12-18T16:01:17.5884079Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
2022-12-18T16:01:17.5884815Z Dec 18 16:01:17 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
2022-12-18T16:01:17.5885873Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-12-18T16:01:17.5890797Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5891506Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.5892305Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.5892976Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.5895690Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5896418Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.5899616Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.5901162Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.5902108Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.5903046Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.5907124Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.5908401Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5909105Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.5909783Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.5911367Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.5914696Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5915500Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.5916118Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.5916903Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.5919986Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-12-18T16:01:17.5920915Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-12-18T16:01:17.5921662Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-12-18T16:01:17.5922357Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5923042Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-12-18T16:01:17.5923648Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-12-18T16:01:17.5926362Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-12-18T16:01:17.5927074Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-12-18T16:01:17.5927705Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-12-18T16:01:17.5928360Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-12-18T16:01:17.5929180Z Dec 18 16:01:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-12-18T16:01:17.5932489Z Dec 18 16:01:17 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-12-18T16:01:17.5933287Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-12-18T16:01:17.5933965Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-12-18T16:01:17.5934572Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-12-18T16:01:17.5935138Z Dec 18 16:01:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-12-18T16:01:17.5935601Z Dec 18 16:01:17 
{code}

in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44045&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Jan/23 10:16;mapohl;I updated the affected versions. FLINK-30397 removed Pulsar-related code from {{master}}. Therefore, it's only an issue on {{release-1.16}}  where the test disabled (see FLINK-30351). {{pulsar-3.0.0}} and {{pulsar-4.0.0}} were added as affected versions.;;;","03/Jan/23 15:30;syhily;[~mapohl] We have removed these tests in {{pulsar-4.0.0}}.;;;","03/Jan/23 17:28;mapohl;Thanks for letting me know.~ Feel free to update the Jira issue next time to keep the Jira issue consistent. I'm gonna remove {{pulsar-4.0.0}} from the affected versions list.;;;","04/Jan/23 02:53;tison;After FLINK-30413 dropped the related support, this test was dropped also. Invalid now.;;;",,,,,
HadoopConfigLoader should consider Hadoop configuration files,FLINK-29754,13491238,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pvary,pvary,25/Oct/22 08:21,25/Oct/22 14:48,04/Jun/24 20:41,,,,,,,,,,,,FileSystems,,,,0,,,,,"Currently [HadoopConfigLoader|https://github.com/apache/flink/blob/master/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/util/HadoopConfigLoader.java] considers Hadoop configurations on the classpath, but does not consider Hadoop configuration files which are set in another way.

So if the Hadoop configuration is set through the {{HADOOP_CONF_DIR}} environment variable, then the configuration loaded by the HadoopConfigLoader will not contain the values set there.

This can cause unexpected behaviour when setting checkpoint / savepoint dirs on S3, and the specific S3 configurations are set in the Hadoop configuration files",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 14:48:23 UTC 2022,,,,,,,,,,"0|z1a7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 08:33;gaborgsomogyi;> if the Hadoop configuration is set through the {{HADOOP_CONF_DIR}} environment variable, then the configuration loaded by the HadoopConfigLoader will not contain the values set there

Set where exactly? What is missing?
;;;","25/Oct/22 09:30;pvary;I create a configuration file like this:
{code:java}
<?xml version=""1.0""?>
<!-- core-site.xml -->
<configuration>
    <property>
        <name>hadoop.fs.s3a.buffer.dir</name>
        <value>/flink-data</value>
    </property>
    <property>
        <name>fs.s3a.bucket.probe</name>
        <value>0</value>
    </property>
[..]
</configuration> {code}

The configuration values set in the configuration files are available, and used when accessing S3 in the case when this configuration file is on the classpath (packaged in the jar). OTOH, if I create a HADOO_CONF_DIR and put the config files there, then the configuration values are not available and not used when accessing S3.;;;","25/Oct/22 10:21;gaborgsomogyi;I see the intention. Now it really hits-in that we have 6 different ways how we load Hadoop config. 2 from 6 are in the `flink-hadoop-fs` module, please see [this|https://github.com/apache/flink/blob/0e612856772d5f469c7d4a4fff90a58b6e0f5578/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/util/HadoopUtils.java#L53-L54].;;;","25/Oct/22 14:48;pvary;[This|https://github.com/apache/flink/blob/0e612856772d5f469c7d4a4fff90a58b6e0f5578/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/util/HadoopUtils.java#L59] is the line which causes the issue.

When you try to instantiate the {{HdfsConfiguration}} you need the {{hadoop-hdfs}} on the classpath.;;;",,,,,,,,,,,,,,,,,,,
FileSource throws exception reading file with name that ends with xz ,FLINK-29753,13491237,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,xuannan,xuannan,25/Oct/22 08:18,25/Oct/22 14:22,04/Jun/24 20:41,25/Oct/22 14:22,1.15.2,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"FileSource throws the following exception reading file with a name that ends with xz
{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoClassDefFoundError: org/tukaani/xz/XZInputStream
    at org.apache.flink.api.common.io.compression.XZInputStreamFactory.create(XZInputStreamFactory.java:42)
    at org.apache.flink.api.common.io.compression.XZInputStreamFactory.create(XZInputStreamFactory.java:31)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.lambda$openStream$3(StreamFormatAdapter.java:178)
    at org.apache.flink.connector.file.src.util.Utils.doWithCleanupOnException(Utils.java:45)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.openStream(StreamFormatAdapter.java:172)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.createReader(StreamFormatAdapter.java:70)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:112)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:65)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.lang.ClassNotFoundException: org.tukaani.xz.XZInputStream
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    ... 16 more {code}
The code to reproduce the error:
{code:java}
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    final FileSource<String> source =
            FileSource.forRecordStreamFormat(new TextLineInputFormat(), new Path(""/tmp/abcxz""))
                    .build();

    env.fromSource(source, WatermarkStrategy.noWatermarks(), ""source"").print();
    env.execute();
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 14:22:41 UTC 2022,,,,,,,,,,"0|z1a7nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 14:22;martijnvisser;This works fine for me. The error {{Caused by: java.lang.NoClassDefFoundError: org/tukaani/xz/XZInputStream}} makes it more plausible that you're not packaging your JAR correctly or, if you run this from IntelliJ, you're not marking to add the dependencies with 'Provided' scope to the classpath. ;;;",,,,,,,,,,,,,,,,,,,,,,
Modify Flink Table Store connector to trigger full compaction constantly when full changelog is needed,FLINK-29752,13491223,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Oct/22 07:18,10/Nov/22 02:35,04/Jun/24 20:41,10/Nov/22 02:35,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"The last step to produce full compaction changelog is to modify Flink Table Store connector, so that full compaction will be triggered once in a while. If not, changelog files are not guaranteed to be produced, and the last few records for a partition may not appear in changelog.",,,,,,,,,,,,FLINK-29842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 02:35:53 UTC 2022,,,,,,,,,,"0|z1a7k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 02:35;TsReaper;master: 0bce24e7e2430430f01344c4355bae5387b91e50;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce Client Result to wrap the ResultSet returned by SQL Gateway,FLINK-29751,13491214,13500273,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,fsk119,yzl,yzl,25/Oct/22 06:12,29/Jan/23 03:58,04/Jun/24 20:41,29/Jan/23 03:58,1.17.0,,,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,,"The embedded client uses the internal interface ‘TableResultInternal’ directly and builds many util classes based on the  ‘TableResultInternal’. But the gateway’s return is wrapped into ResultSet. If we refactor the existing classes to use ResultSet, it will be a big burden. So it's better to wrap the ResultSet to a format that can be used by the Client.

(This issue is for migrating SQL client Embedded Mode before, and there is many subtasks, so it is divided into some subtasks and this issue is changed to one of the subtasks under the parent issue FLINK-29941)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-25 06:12:03.0,,,,,,,,,,"0|z1a7i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve PostgresCatalog#listTables() by reusing resources,FLINK-29750,13491213,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuml07,liuml07,liuml07,25/Oct/22 06:09,26/May/23 09:38,04/Jun/24 20:41,26/May/23 09:38,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,Table SQL / Ecosystem,,,0,pull-request-available,,,,Currently the {{PostgresCatalog#listTables()}} creates a new connection and prepared statement for every schema and table when listing tables. This can be optimized by reusing those resources.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 09:38:42 UTC 2023,,,,,,,,,,"0|z1a7i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 09:38;martijnvisser;Fixed in 
apache/flink-connector-jdbc:main 7fc202be3dfcdb6510f9855a6943dd97fa2bd3af;;;",,,,,,,,,,,,,,,,,,,,,,
flink info command support dynamic properties,FLINK-29749,13491210,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,25/Oct/22 05:47,28/Oct/22 01:53,04/Jun/24 20:41,28/Oct/22 01:51,1.17.0,,,,,,1.15.3,1.16.1,1.17.0,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27579,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 01:53:28 UTC 2022,,,,,,,,,,"0|z1a7hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 01:51;wangyang0918;Fixed via:

master: f8c6a668cd2b887f33a0cf4608de2d6b95c71f03

release-1.16: 38e90428bf7e603fdd353243f1edeba3553af2a3

release-1.15: 1d29f540a0692540a01b951033a8dc04fdb74d4f;;;","28/Oct/22 01:53;wangyang0918;Thanks [~jackylau] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,
Expose the optimize phase in the connector context,FLINK-29748,13491060,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,aitozi,aitozi,25/Oct/22 03:30,04/Nov/22 03:32,04/Jun/24 20:41,04/Nov/22 03:32,,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,0,,,,,"Currently, in the connector it can not know whether the whole optimize is finished.
When the optimize finished, the all information is static, eg: the reading partitions. If I want to validate the final optimized result (like whether the reading partition is too much or empty), it needs the context of what is the current phase. I think the {{ScanContext}} is ok to expose this information. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 02:44:16 UTC 2022,,,,,,,,,,"0|z1a6k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 02:47;fsk119;If your connector implements `SupportsPartitionPushDown`,  the planner will tell you which partition needs to be read.;;;","26/Oct/22 02:57;aitozi;My connector has implemented the \{{SupportsPartitionPushDown}}. But, due to the \{{PartitionPushDown}} optimization, the effective partition count can be different from the original source table. If I want to validate the source do not consume too much partition, from the connector's perspective, It do not know whether the optimization is finished, so it do not know when to apply the validation on the final optimization results.;;;","28/Oct/22 02:31;fsk119;I think you can validate  when StreamExecSource invokes getScanRuntimeProvider. When execnode -> transformat, it means the optimization finishes.;;;","28/Oct/22 05:42;aitozi;As I know, the \{{getScanRuntimeProvider}} will be invoked serval times during the optimize, eg:
 * org.apache.flink.table.planner.connectors.DynamicSourceUtils#validateScanSource
 * org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan#translateToPlanInternal

So, to the connector, it do not know whether the optimization is finished.;;;","31/Oct/22 01:57;fsk119;Yes. I think you are right. 

But I am confused about 

> If I want to validate the source do not consume too much partition, from the connector's perspective, It do not know whether the optimization is finished, so it do not know when to apply the validation on the final optimization results.

The rule will only apply once during the optimization because the rule uses hep optimizer which is not based on the cost.
;;;","31/Oct/22 02:44;aitozi;
{noformat}
The rule will only apply once during the optimization because the rule uses hep optimizer which is not based on the cost.
{noformat}

Yes you are right. My {{optimization}} words mainly point to the whole planner phase, so the connector do not know whether the optimize is finished or not.
As a workaround, I think we can add the callback interface eg: {{onOptimizeFinished}} or {{onTranslateExecNodeStart}} on the {{ScanTableSource}} . All the external validation work on the final results can perform on that stage. what do you think about it ?;;;",,,,,,,,,,,,,,,,,
[UI] Refactor runtime web from module-based to standalone components,FLINK-29747,13491020,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,junhan,junhan,junhan,25/Oct/22 02:46,13/Apr/23 05:59,04/Jun/24 20:41,27/Oct/22 02:10,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"From v14 onwards, Angular provides a capability of standalone components that can be independently bootstrapping. This is a powerful feature in terms of refactoring the application to be less-heavy and structurally clean. It also enables the component-level lazy loading in routes, improving the web performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31792,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 02:10:15 UTC 2022,,,,,,,,,,"0|z1a6b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 02:10;junhan;master: 77204ff7b92da132c1033a356344d8ccd6b443f2;;;",,,,,,,,,,,,,,,,,,,,,,
Add workflow in github for micro benchmarks,FLINK-29746,13490976,13486239,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,25/Oct/22 01:26,19/Mar/23 05:49,04/Jun/24 20:41,19/Mar/23 05:49,table-store-0.2.2,,,,,,,,,,Table Store,,,,0,,,,,Add workflow in github for micro benchmarks project,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-25 01:26:22.0,,,,,,,,,,"0|z1a61c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split reader/writer factory for compaction in MergeTreeTest,FLINK-29745,13490975,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,25/Oct/22 01:24,25/Oct/22 03:41,04/Jun/24 20:41,25/Oct/22 03:41,table-store-0.2.2,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 03:41:01 UTC 2022,,,,,,,,,,"0|z1a614:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 03:41;lzljs3620320;master: 537e8cf4b3733d93720381c65b30024870ece533;;;",,,,,,,,,,,,,,,,,,,,,,
Throw DeploymentFailedException on ImagePullBackOff,FLINK-29744,13490693,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,24/Oct/22 22:56,13/Dec/22 01:38,04/Jun/24 20:41,25/Oct/22 16:08,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30315,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 16:08:01 UTC 2022,,,,,,,,,,"0|z1a4ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 16:08;gyfora;merged to main ddd7c927481b52fc3422ef6534760d8d436428e7;;;",,,,,,,,,,,,,,,,,,,,,,
CatalogPropertiesUtil supports de/serializing column comment,FLINK-29743,13490592,13478113,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,qingyue,qingyue,24/Oct/22 11:49,11/Mar/23 14:29,04/Jun/24 20:41,11/Mar/23 14:29,,,,,,,1.18.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"We should consider adding {{schema.${i}.comment}} to {{CatalogPropertiesUtil, in order to support comment's de/serialization.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 14:29:05 UTC 2023,,,,,,,,,,"0|z1a3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/23 14:29;jark;Fixed in master: e22d164ff710571ff2708568a90a21dee194116a;;;",,,,,,,,,,,,,,,,,,,,,,
Support completing statement in SqlGatewayService.,FLINK-29742,13490564,13478114,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,yzl,yzl,24/Oct/22 09:36,12/Dec/22 02:00,04/Jun/24 20:41,12/Dec/22 02:00,,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,,"Implement SqlGatewayService#completeStatement described in [FLIP-91|https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Gateway].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28796,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 02:00:21 UTC 2022,,,,,,,,,,"0|z1a3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 02:00;fsk119;Merged into master: dc3ea827941f9c0a28b593f6fc628d742a8562a2;;;",,,,,,,,,,,,,,,,,,,,,,
[FLIP-265] Remove all Scala APIs,FLINK-29741,13490561,13490559,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,24/Oct/22 09:29,01/Dec/23 02:40,04/Jun/24 20:41,,,,,,,,2.0.0,,,,API / Scala,,,,0,2.0-related,,,,"- Remove all @Public, @PublicEvolving and @Experimental Scala APIs (which should have been marked as @Deprecated in FLINK-29740) 
- Remove all Scala API documentation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-24 09:29:09.0,,,,,,,,,,"0|z1a3hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-265] Deprecate all customer-facing Scala APIs,FLINK-29740,13490560,13490559,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,24/Oct/22 09:26,05/Dec/23 06:17,04/Jun/24 20:41,08/Nov/22 13:33,,,,,,,1.17.0,,,,,,,,0,2.0-related,pull-request-available,,,"- Annotate all @Public, @PublicEvolving and @Experimental Scala APIs as @Deprecated. 
- Remove all Scala API code examples from the codebase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33383,FLINK-32560,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 13:33:32 UTC 2022,,,,,,,,,,"0|z1a3h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 13:33;martijnvisser;Fixed in master: 5ac290a518a57030bf5b773ca69fbbdda4e0088f;;;",,,,,,,,,,,,,,,,,,,,,,
[FLIP-265] Deprecate and remove Scala API support,FLINK-29739,13490559,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,24/Oct/22 09:25,05/Dec/23 06:17,04/Jun/24 20:41,,,,,,,,,,,,API / Scala,,,,0,2.0-related,,,,"FLIP: https://cwiki.apache.org/confluence/display/FLINK/FLIP-265+Deprecate+and+remove+Scala+API+support
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-9667,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 09:01:31 UTC 2022,,,,,,,,,,"0|z1a3gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 09:01;twalthr;Sorry, to jump into this topic so late. But I just wanted to express my personal opinion that we might be a bit too strict here. For DataStream API, it makes absolutely sense to drop the stack due to lack of macro support, state incompatibilities, etc. However, for Table API, we have put in significant effort to support Scala, Kotlin, etc. long-term without larger maintenance overhead. They only add some cherries on top of the cake with minimal code. Mostly adding some implicits to the otherwise full Java implementation. Given that Table API is the API with the highest abstraction, I would still vote for keeping the Scala Table API. Of course I'm biased emotionally here, but also from a technical point of view, all mentioned arguments mostly refer to DataStream API, not Table API which has been designed from the mistakes of the past.;;;",,,,,,,,,,,,,,,,,,,,,,
Allow UDT codec registration for CassandraSinkBuilder,FLINK-29738,13490554,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bill G,Bill G,24/Oct/22 09:09,24/Oct/22 09:28,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Connectors / Cassandra,,,,0,,,,,"When streaming POJO types, the codec is registered automatically. However, when streaming a tuples containing a UDT, the cassandra driver can't serialize the type.

Motivating Example: If we have a table containing a collection of UDTs, then the only way to append is through a tuple stream.
{code:java}
create type link (
  title text,
  url text
);

create table users (
  id int primary key,
  links set<frozen<link>>
);
{code}
If we were to use a POJO stream, the field containing the collection would be overwritten with a new collection on each upsert. If we set the query in a tuple stream:
{code:java}
DataStream<Tuple2<Set<Link>, Integer>> linkStream = ...
CassandraSink.addSink(linkStream)
  .setQuery(""update users set links = links + ? where id = ?"")
  ...
  .build();
{code}
We will get a {{{}CodecNotFoundException{}}}. Using the datastax java driver outside of the Flink framework, it is easy to register a codec:
{code:java}
Session session = cluster.connect();
new MappingManager(session).udtCodec(Link.class);
{code}
However, this requires access to a session, which {{ClusterBuilder}} does not expose in any way.

Potential solutions: expose {{Session}} or {{MapperManager}} in some way to the {{ClusterBuilder}} class or create some method such as {{registerUDT}} on {{{}CassandraSinkBuilder{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-24 09:09:51.0,,,,,,,,,,"0|z1a3fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support DataGen on waveform function,FLINK-29737,13490542,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,chenzihao,chenzihao,24/Oct/22 08:11,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Table SQL / API,,,,0,auto-deprioritized-major,pull-request-available,,,"In some scenarios, we need to simulate flow changes in the production environment. The current DATAGEN feature only supports data generation at a constant rate. We try to simulate increments of flow using batch jobs, but the production rate is not smooth, so I suggest that we can support sin-based data generation in order to get smooth changes. 
1. add another batch job to simulate increments of flow.

!image-2022-10-24-16-09-52-410.png!
2. sin-based.

!image-2022-10-24-16-09-47-386.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/22 08:09;chenzihao;image-2022-10-24-16-09-47-386.png;https://issues.apache.org/jira/secure/attachment/13051327/image-2022-10-24-16-09-47-386.png","24/Oct/22 08:09;chenzihao;image-2022-10-24-16-09-52-410.png;https://issues.apache.org/jira/secure/attachment/13051326/image-2022-10-24-16-09-52-410.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:17 UTC 2023,,,,,,,,,,"0|z1a3d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/22 08:22;chenzihao;I have simply extended the functionality of DATAGEN to support the smooth change of data generation rate. And support adjusting the data change period and the peak value. If this small improvement is useful for Flink, I am willing to open a PR. [~lzljs3620320] Please help to review this, thanks a lot.;;;","25/Oct/22 01:49;fsk119;Thanks for your idea. Could you share the PR link here?;;;","25/Oct/22 03:57;chenzihao;[~fsk119] Thanks for your reply. I have opened the PR, please help to review it, thanks a lot.;;;","26/Oct/22 02:12;fsk119;Sure. I have added this in my todo list. I will take a look when I am free.;;;","26/Oct/22 11:18;yzl;Thanks for your idea. I have some points about this feature.
 # As far as I'm concerned, the `datagen` connector is usually used in test cases, so how the data are generated is usually ignored. I wonder whether you can share more details of `wave form generation rate` usage scenarios? I think maybe I need to discuss with [~fsk119] and other community members to estimate whether to support this feature.
 # To add support for sin-based waveform generation rate  is not scalable enough. In my opinion, a better way is to introduce an option like `RateShape`[1], and mark current mode as `square`. It backs to point 1 that whether it's generic enough to introduce this new feature.

Anyway, thanks again. [~fsk119] Please take a look.

 

[1][`RateShape` example|https://github.com/nexmark/nexmark/blob/54974ef36a0d01ef8ebc0b4ba39cfc50136af0f6/nexmark-flink/src/main/java/com/github/nexmark/flink/utils/NexmarkUtils.java#L53];;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,
Abstract a table interface for both data and metadata tables,FLINK-29736,13490539,13490538,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Oct/22 08:03,25/Oct/22 13:54,04/Jun/24 20:41,25/Oct/22 13:54,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently, FileStoreTable is only for data tables, we can create a new interface Table for all tables.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 13:54:33 UTC 2022,,,,,,,,,,"0|z1a3cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 13:54;lzljs3620320;master: b9527abac18a7c1f4eba13dadfb07227ba23f4b8;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce Metadata tables for table store,FLINK-29735,13490538,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Oct/22 08:01,03/Feb/23 05:21,04/Jun/24 20:41,03/Feb/23 05:21,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"You can query the related metadata of the table through SQL, for example, query the historical version information of table ""T"" through the following SQL:

SELECT * FROM T$history;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-24 08:01:28.0,,,,,,,,,,"0|z1a3c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support authentication in the Flink SQL Gateway,FLINK-29734,13490504,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,24/Oct/22 06:31,09/Feb/23 06:01,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Table SQL / Gateway,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 06:01:18 UTC 2023,,,,,,,,,,"0|z1a34o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 06:01;melin;(?);;;",,,,,,,,,,,,,,,,,,,,,,
Error Flink connector hive Test failing,FLINK-29733,13490500,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,samrat007,samrat007,24/Oct/22 06:30,31/Oct/22 08:22,04/Jun/24 20:41,31/Oct/22 08:18,1.17.0,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f]

This is caused by FLINK-29478

reported by [~hxbks2ks] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 08:18:47 UTC 2022,,,,,,,,,,"0|z1a33s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 11:50;samrat007;Hi [~hxbks2ks] 

please review the changes in free time ;;;","31/Oct/22 08:18;hxbks2ks;Merged into master via 759d954960cbec28437955e5f482788902342035;;;",,,,,,,,,,,,,,,,,,,,,
Support configuring session with SQL statement,FLINK-29732,13490097,13478114,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,24/Oct/22 02:56,08/Dec/22 02:27,04/Jun/24 20:41,08/Dec/22 02:27,,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,,"Implement SqlGatewayService#configureSession described in [FLIP-91|https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Gateway].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29947,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 02:27:49 UTC 2022,,,,,,,,,,"0|z1a0m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 02:27;fsk119;Merged into master at: e6215d4f7ede96f1225921f2b0f322c87a5040cd;;;",,,,,,,,,,,,,,,,,,,,,,
Protobuf-confluent format,FLINK-29731,13489478,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,krisnaru,krisnaru,krisnaru,23/Oct/22 13:07,11/Feb/23 21:13,04/Jun/24 20:41,,1.17.0,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,2,,,,,"Flink needs to integrate with Confluent for protobuf format, to able to leverage schema registry for schema versions.",Confluent kafka with schema registry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 24 17:54:23 UTC 2022,,,,,,,,,,"0|z19wso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/22 01:56;fsk119;Is it same to the confluent avro format[1]? If so, I think it should belong to the format component. 

I am not familar with protobuf, cc [~libenchao]

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/avro-confluent/;;;","24/Oct/22 17:54;gang ye;Is this for adding ConfluentSchemaRegistryCoder for Protobuf (like avro https://github.com/apache/flink/blob/master/flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentSchemaRegistryCoder.java#L36) ;;;",,,,,,,,,,,,,,,,,,,,,
Do not support concurrent unaligned checkpoints in the ChannelStateWriteRequestDispatcherImpl,FLINK-29730,13489144,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,22/Oct/22 13:35,04/Nov/22 10:01,04/Jun/24 20:41,04/Nov/22 08:13,1.17.0,,,,,,1.17.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"The _`Map<Long, ChannelStateCheckpointWriter> writers;`_ can be simplified to {_}`long ongoingCheckpointId{_}` and `{_}ChannelStateCheckpointWriter writer`{_} due to not supported concurrent unaligned checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 10:01:17 UTC 2022,,,,,,,,,,"0|z19urs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 08:13;pnowojski;merged commit 00a2580 into apache:master now;;;","04/Nov/22 10:01;fanrui;[~pnowojski]  Thanks for your review :);;;",,,,,,,,,,,,,,,,,,,,,
Fix credential info configured in flink-conf.yaml is lost during creating ParquetReader,FLINK-29729,13489056,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,stayrascal,stayrascal,22/Oct/22 09:56,08/Mar/23 17:54,04/Jun/24 20:41,02/Mar/23 02:07,,,,,,,1.15.4,1.16.2,1.17.0,1.18.0,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"Hi, I'm thinking if we can include the configured properties from flink-conf.yaml during create ParquetReader in `ParquetVectorizedInputformat` besides hadoop configuration.

 

I meet a use case that I want to query a table from S3 bucket with parquet format via filesystem connector, and I configured the AWS credential info in the `flink-conf.yaml`, e.g. fs.s3a.access.key, fs.s3a.secret.key, etc. 

 

The JobManager(SourceCoordinator) works well about ""getFileStatus"" of S3 objects and generate splits, but TaskManager(SourceOperator -> ParquetVectorizedInputFormat -> ParquetReader) doesn't work since missing AWS credential info.

 

After taking a deep analysis at the source code about creating ParquetReader to reader footer, I found that the AWS credential info is not passed during create & initialize S3AFileSystem, the detail info as showing in the bellow snapshot.  !image-2022-10-22-17-41-38-084.png!

 

The `hadoopConfig` only contains the properties from table format options and default hadoop properties from core-site.xml, hdfs-site.xml and etc. Because the `hadoopConfig` is injected by `ParquetFileFormatFactory#createRuntimeDecoder` -> `ParquetColumnarRowInputFormat.createPartitionedFormat` -> `ParquetFileFormatFactory.generateParquetConfiguration`

 
{code:java}
@Override
public BulkFormat<RowData, FileSourceSplit> createRuntimeDecoder(
DynamicTableSource.Context sourceContext,
DataType producedDataType,
int[][] projections) {

return ParquetColumnarRowInputFormat.createPartitionedFormat(
getParquetConfiguration(formatOptions),
(RowType) Projection.of(projections).project(producedDataType).getLogicalType(),
sourceContext.createTypeInformation(producedDataType),
Collections.emptyList(),
null,
VectorizedColumnBatch.DEFAULT_SIZE,
formatOptions.get(UTC_TIMEZONE),
true);
}

 

private static Configuration getParquetConfiguration(ReadableConfig options) {
Configuration conf = new Configuration();
Properties properties = new Properties();
((org.apache.flink.configuration.Configuration) options).addAllToProperties(properties);
properties.forEach((k, v) -> conf.set(IDENTIFIER + ""."" + k, v.toString()));
return conf;
}

{code}
 

I know that I can add the AWS credential info into core-site.xml or hdfs-site.xml, so that the `ParquetReader` can get the credential, but I think it might not a good practice, especially different flink jobs will use different AWS credential, so I'm thinking if we can combine the default hadoop configuration(static) and the properties from `flink-conf.yaml`(dynamic) during create `ParquetReader`. 

For example, just like how this PR doing? https://github.com/apache/flink/pull/21130

 

BTW,  I'm using Flink 1.15.1 in a standalone cluster to validate the whole process, but I think not only 1.15.1 version meet this problem, and not only access the objects/files from AWS S3 bucket, any other cloud object storage might also meet this problem.

 

Besides change the code, is there any other solution can help me to handle this problem? thanks. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/22 09:41;stayrascal;image-2022-10-22-17-41-38-084.png;https://issues.apache.org/jira/secure/attachment/13051289/image-2022-10-22-17-41-38-084.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 14:03:49 UTC 2023,,,,,,,,,,"0|z19u88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/22 13:54;lsy;Thanks for opening this ticket, I will take a look.;;;","24/Oct/22 02:12;lsy;Do we can configure the aws credential info at the job level by providing format options? If different jobs run in the same cluster need to use different credential info, the change you propose maybe also can not work.;;;","24/Oct/22 05:24;stayrascal;Hi [~lsy], thanks for looking at this case.

From my understanding, if we want to configure the aws credential info at job level by format options, the filesystem connector format should support customized properties at first, right? And the cloud IAM credential info(AK/SK) should not couple with a generic filesystem connector options.

 

And regarding run different jobs in the same cluster with different credential info, yeah, it might be a problem in standalone cluster, but should be works in pe-job or application mode. Since it seems that the `config` of `createReader()` method is also a static configuration from the environment of taskmanager.

 

I also considered enhance the `hadoopConfiguration` during createRunDecoder -> getParquetConfiguration method, but cannot retrieve the configured properties from `sourceContext`
{code:java}
@Override
public BulkFormat<RowData, FileSourceSplit> createRuntimeDecoder(
    DynamicTableSource.Context sourceContext,
    DataType producedDataType,
    int[][] projections) {

  return ParquetColumnarRowInputFormat.createPartitionedFormat(
    getParquetConfiguration(formatOptions),
    (RowType) Projection.of(projections).project(producedDataType).getLogicalType(),
    sourceContext.createTypeInformation(producedDataType),
    Collections.emptyList(),
    null,
    VectorizedColumnBatch.DEFAULT_SIZE,
    formatOptions.get(UTC_TIMEZONE),
    true);
}

 

private static Configuration getParquetConfiguration(ReadableConfig options) {
  Configuration conf = new Configuration();
  Properties properties = new Properties();
  ((org.apache.flink.configuration.Configuration) options).addAllToProperties(properties);
  properties.forEach((k, v) -> conf.set(IDENTIFIER + ""."" + k, v.toString()));
  return conf;
}

{code};;;","25/Nov/22 08:51;qinjunjerry;To pass S3 credentials, there is an alternative:  pass them via env var:
{code:java}
AWS_SECRET_KEY
AWS_ACCESS_KEY
{code}

However, if an on-premise S3 is used, one would need to pass {{fs.s3a.endpoint}}. I do not see this is possible via env var. This makes Flink SQL not possible in this case, as one would need to pass the config in flink-conf.yaml when creating ParquetReader. So, a solution is still needed here.

[~stayrascal]'s PR seems to be ok, but I am not an expert in this area. 
[~lsy] Are you still handling this?;;;","30/Nov/22 03:07;lsy;[~stayrascal] [~qinjunjerry] Sorry for the late response, I think we should support configuring the ak/sk in flink-conf.yaml level and job level, the job level has better priority. We read the ak/sk from table option first, if it does not configure, then read from flink-conf.yaml. By this way, I think we can support all cases of sk/ak requirements.

> From my understanding, if we want to configure the aws credential info at job level by format options, the filesystem connector format should support customized properties at first, right? And the cloud IAM credential info(AK/SK) should not couple with a generic filesystem connector options.

Yes, I think we can try to support it by customized format options in filesystem connector, can you help to do a poc to verify whether is it feasible?

The hbase connector also supports customed options that start with 'properties.', more detail for [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/hbase/#connector-options].;;;","30/Nov/22 08:15;qinjunjerry;Thanks [~lsy]!  Just to make it clear, it is not only ak/sk ( I assume you mean access key and secret key by this), but also other options like {{{}fs.s3a.endpoint{}}}.

{{properties.*}} sounds like a good idea. The Flink Kafka connector has the same: [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kafka/#properties];;;","30/Nov/22 08:47;lsy;[~qinjunjerry] Yes, I just cleared ak/sk here as an example. The `{{{}properties.*{}}}` solution can also support other customed options requirements, we can try it.;;;","17/Jan/23 15:53;jark;I think it's reasonable for the parquet to be aware of {{flink-conf.yaml}}. It makes the filesystem connector (with parquet format) to be able to configure the authentication credentials, but with a limitation that a cluster can only have one credential for a filesystem scheme (including the checkpoint configuration). 

In addition, I'm afraid a big change is required if we cant to support format/connector-level authentication properties. Because Flink filesystem mechanism doesn't support multiple credentials for a scheme in a cluster. For example, a Flink can't read from S3 and write into S3 with different credentials. Thus, I suggest creating another JIRA issue to track the long effort. ;;;","20/Feb/23 03:54;lsy;Sorry for responding later, after taking a deep analysis of the Flink filesystem mechanism, I think we can't support configuring authentication credentials at the job level currently, the filesystem is cached at the process level. So we only can configure the authentication credentials in flink-conf.yaml or Hadoop core-site.xml, other stores such as Hudi currently only support configuring authentication information in the hadoop core-site.xml file. 

_As the issue description, we can't get Hadoop-related authentication information from flink-conf.yaml when create {{ParquetReader}} using S3AFileSystem in TM, but in JM, {{SourceCoordinator}} works well when getFileStatus from s3a. After taking a deep analysis of the source code, I found that on the JM side, we use Flink's own FileSystem to get {{{}FileStatus{}}}, which can get the authentication information from flink-conf.yaml; but on the TM side, we use Hadoop's own FileSystem to create footer, which can't get the authentication information, which is an inconsistent behavior and causes the problem of the issue description. I think this is bug, so we should use Flink's FileSystem on both JM side and TM side._;;;","01/Mar/23 14:03;jark;Fixed in:
 - master: 49b8726850cb0da5f3a888c836f31bfa69be739d
 - release-1.17: 3e02b3002e07b704073bffdae7d41c4d7b462a2e
 - release-1.16: 5bb44a1565b87189338226f223bf0a8bc5ce356e
 - release-1.15:  25603b00678841e98f8e02c3d7aac35aafbdb34d;;;",,,,,,,,,,,,,
TablePlanner prevents Flink from starting is working directory is a symbolic link,FLINK-29728,13489026,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,angelok,angelok,22/Oct/22 03:04,14/Nov/22 14:41,04/Jun/24 20:41,31/Oct/22 01:37,1.15.2,,,,,,1.16.1,1.17.0,,,Table SQL / Planner,,,,0,,,,,"The Flink runtime throws an exception when using the table API if the working directory is a symbolic link. This is the case when run on AWS EMR with Yarn. There is a similar issue [here|https://issues.apache.org/jira/browse/FLINK-20267] and I believe the same fix applied there would work.

 

 
{code:java}
Caused by: org.apache.flink.table.api.TableException: Could not initialize the table planner components loader.
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:123) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:52) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule$PlannerComponentsHolder.<clinit>(PlannerModule.java:131) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.getInstance(PlannerModule.java:135) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.DelegateExecutorFactory.<init>(DelegateExecutorFactory.java:34) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_342]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_342]
    at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_342]
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_342]
    at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:798) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:517) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:276) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at com.ballista.Hermes.BCSE$.useLocalCatalog(BCSE.scala:210) ~[?:?]
    at com.ballista.Hermes.BCSE$.main(BCSE.scala:114) ~[?:?]
    at com.ballista.Hermes.BCSE.main(BCSE.scala) ~[?:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.15.1.jar:1.15.1]
    ... 7 more
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) ~[?:1.8.0_342]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_342]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_342]
    at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_342]
    at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_342]
    at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) ~[?:1.8.0_342]
    at java.nio.file.Files.createDirectories(Files.java:727) ~[?:1.8.0_342]
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:96) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:52) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule$PlannerComponentsHolder.<clinit>(PlannerModule.java:131) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.getInstance(PlannerModule.java:135) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.DelegateExecutorFactory.<init>(DelegateExecutorFactory.java:34) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_342]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_342]
    at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_342]
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_342]
    at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:798) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:517) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:276) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at com.ballista.Hermes.BCSE$.useLocalCatalog(BCSE.scala:210) ~[?:?]
    at com.ballista.Hermes.BCSE$.main(BCSE.scala:114) ~[?:?]
    at com.ballista.Hermes.BCSE.main(BCSE.scala) ~[?:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.15.1.jar:1.15.1] {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30001,,,,,,,,FLINK-28102,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 01:37:18 UTC 2022,,,,,,,,,,"0|z19u1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/22 04:51;Weijie Guo;[~angelok] Thanks for reporting this, the code that will cause problems is the same as FLINK-28102, I will fix it together in that ticket. To facilitate tracking issue, I suggest closing this ticket.;;;","31/Oct/22 01:37;xtsong;- master (1.17): e8e9db37e17110ff04175d2720484b34f5c4d5ba
- release-1.16: 8fd9aa63a30a6037fcad752ab74fbdd6649ca3f0;;;",,,,,,,,,,,,,,,,,,,,,
Fall back to flink-conf.yaml if no JOB_MANAGER_RPC_ADDRESS is specified,FLINK-29727,13488657,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,21/Oct/22 14:03,21/Oct/22 14:05,04/Jun/24 20:41,,,,,,,,,,,,flink-docker,,,,1,,,,,"Currently {{docker-enterypoint.sh}} always overrides {{jobmanager.rpc.address}} in {{flink-conf.yaml}} either with an environment variable {{JOB_MANAGER_RPC_ADDRESS}} or with a {{hosname}} : [link|https://github.com/apache/flink-docker/blob/3c259f46231b97202925a111a8205193c15bbf78/1.15/scala_2.12-java8-ubuntu/docker-entrypoint.sh#L25] . This causes, for instance, jobmanager address configured in {{FlinkContainers}} that are based on an existing image (contains {{docker-entrypoint.sh}} in contrast to an image that is built on the flight from {{{}flink-dist{}}}) to be overridden by the hostname. TMs then fail for connect to the JM. A workaround is to use {{TestContainersSettings}} to set {{{}JOB_MANAGER_RPC_ADDRESS{}}}, which is suboptimal from the user's perspective.

Configuration in flink-conf.yaml should instead be kept if JOB_MANAGER_RPC_ADDRESS is not passed explicitly and only overridden by the {{hostname}} if nothing was specified in the config. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-21 14:03:48.0,,,,,,,,,,"0|z19rrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports hive stddev_samp function by native implementation,FLINK-29726,13488631,13488604,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,21/Oct/22 12:14,21/Oct/22 12:14,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-21 12:14:16.0,,,,,,,,,,"0|z19rls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports hive std function by native implementation,FLINK-29725,13488630,13488604,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,21/Oct/22 12:13,21/Oct/22 12:13,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-21 12:13:43.0,,,,,,,,,,"0|z19rlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports hive last_value function by native implementation,FLINK-29724,13488629,13488604,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,21/Oct/22 12:09,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,1.20.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-21 12:09:19.0,,,,,,,,,,"0|z19rlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports hive first_value function by native implementation,FLINK-29723,13488628,13488604,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,21/Oct/22 12:08,11/Mar/24 12:43,04/Jun/24 20:41,,,,,,,,1.20.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-21 12:08:48.0,,,,,,,,,,"0|z19rl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports hive max function by native implementation,FLINK-29722,13488626,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,21/Oct/22 12:07,19/Jan/23 03:35,04/Jun/24 20:41,19/Jan/23 03:35,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 03:35:28 UTC 2023,,,,,,,,,,"0|z19rko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 03:35;godfrey;Fixed in 1.17.0: 74c7188ae9898b492c94a472d9d407bf4f8e0876;;;",,,,,,,,,,,,,,,,,,,,,,
Supports hive min function by native implementation,FLINK-29721,13488625,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,lsy,lsy,21/Oct/22 12:06,18/Jan/23 02:37,04/Jun/24 20:41,18/Jan/23 02:37,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 02:37:52 UTC 2023,,,,,,,,,,"0|z19rkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 02:37;godfrey;Fixed in 1.17.0: 8a1f66ca827163b32387e0043f4362921f6c11a9;;;",,,,,,,,,,,,,,,,,,,,,,
Supports hive average function by native implemetatoin,FLINK-29720,13488624,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,21/Oct/22 12:04,30/Jan/23 09:27,04/Jun/24 20:41,30/Jan/23 09:27,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 09:27:23 UTC 2023,,,,,,,,,,"0|z19rk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 09:27;godfrey;Fixed in 1.17.0: ec3243e36eebb706c624068c8a7622cf308df95e;;;",,,,,,,,,,,,,,,,,,,,,,
Supports hive count function by native implementation,FLINK-29719,13488623,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,21/Oct/22 12:03,18/Jan/23 14:15,04/Jun/24 20:41,18/Jan/23 14:15,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 14:15:31 UTC 2023,,,,,,,,,,"0|z19rk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 14:15;godfrey;Fixed in 1.17.0: 606f297198acd74a5c1a39700bd84ad9e26e7b82;;;",,,,,,,,,,,,,,,,,,,,,,
Supports hive sum function by native implementation,FLINK-29718,13488622,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,21/Oct/22 12:03,30/Dec/22 04:23,04/Jun/24 20:41,30/Dec/22 03:20,,,,,,,1.17.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 30 03:20:19 UTC 2022,,,,,,,,,,"0|z19rjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 03:20;jingzhang;Merged in master: 4c0f5e63775074c87f3a834b22d6c9eb296721e6;;;",,,,,,,,,,,,,,,,,,,,,,
Supports hive udaf  such as sum/count by native implementation ,FLINK-29717,13488604,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,21/Oct/22 10:22,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Connectors / Hive,Table SQL / Runtime,,,0,,,,,"The current Flink side of the Hive UDAF has a unified encapsulation HiveGenericUDAF, and the intermediate result type of the aggregation function is encapsulated as RAW type, which is a variable-length data type that is serialized and deserialized by default using the Kryo serializer, so BinaryRowData does not support in-place updates to this type, which also leads to aggregation function that uses the RAW type as an intermediate aggregation buffer and cannot use hash-based aggregation strategies. Since the intermediate state type of Hive's UDAF is RAW type, it also cannot use hash-based aggregation strategy, and the overall performance in TPC-DS scenario is more than 2 times than the performance of Flink's built-in function. After some research, here we propose implementing this common udaf in a native way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25641,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-21 10:22:51.0,,,,,,,,,,"0|z19rfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Separate slf4j jar in the lib folder from the distribution,FLINK-29716,13488511,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,asardaes,asardaes,21/Oct/22 08:38,27/Oct/22 21:07,04/Jun/24 20:41,27/Oct/22 21:07,1.15.2,,,,,,,,,,,,,,0,,,,,"Flink's binary distribution includes several jars under the {{lib}} folder, which has individual jars for all log4j artifacts. This makes it relatively easy to swap out those logging jars when necessary, for example when critical vulnerabilities are found (as was recently the case).

With SLF4J 2.+, some breaking changes mean that many implementations are not directly backwards compatible, see for example the [notes for log4j2|https://logging.apache.org/log4j/2.x/log4j-slf4j-impl/index.html]. This means that, in the future, if swapping logging jars were necessary, the SLF4J jar might have to be changed as well.

Right now the SLF4J jar is not included separately in the distribution, I believe it's packed inside the {{flink-dist}} jar, although I'm not sure. It would be better to separate that as it is done for the default log4j2 jars.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 21:07:21 UTC 2022,,,,,,,,,,"0|z19qv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 14:42;chesnay;The log4j jars are bundled separately to allow users to switch logging backends or even go back to log4j1.

There's no such requirement for slf4j.
Should we upgrade to slf4j 2.x then that is just what Flink will require like any other direct dependency.
Given that it is a compile dependency (unlike log4j) replacing it isn't as trivially safe as log4j is; for example if we were to start using the new fluent logging API then replacing it with slf4j v1 is just not an option.;;;","27/Oct/22 21:07;asardaes;I see, all right.;;;",,,,,,,,,,,,,,,,,,,,,
Expose max_parallelism in JSON plan,FLINK-29715,13488509,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,gyfora,gyfora,gyfora,21/Oct/22 08:28,21/Oct/22 14:14,04/Jun/24 20:41,21/Oct/22 14:14,,,,,,,,,,,Runtime / REST,,,,0,,,,,The JobGraph json plan currently only contains vertex parallelism but not the max_parallelism. This could be very useful information to also show on the UI for debugging data skew/performance issues or for any tooling that relies on the jobgraph information gathered from the rest endpoint.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 14:14:23 UTC 2022,,,,,,,,,,"0|z19quo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 14:14;gyfora;Turns out this is already part of the JobDetailsInfo;;;",,,,,,,,,,,,,,,,,,,,,,
Merge TableWrite and TableCompact into one interface,FLINK-29714,13488504,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,21/Oct/22 08:08,25/Oct/22 08:03,04/Jun/24 20:41,25/Oct/22 08:03,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"To make sure that full compaction is triggered constantly for every written bucket regardless of failure, we need to add {{compact}} interface to {{TableWrite}} so that Flink sink operators can trigger compaction when needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 08:03:40 UTC 2022,,,,,,,,,,"0|z19qtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 08:03;lzljs3620320;master: 9c9f5f823ce52a20eb417444b59baac3e96d6a75;;;",,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator should restart failed jobs,FLINK-29713,13488503,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,21/Oct/22 08:03,03/Nov/22 13:00,04/Jun/24 20:41,27/Oct/22 15:41,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"It would be good to have the possibility to restart the Flink Application if it goes to {{FAILED}} state.
This could be used to restart, and reconfigure the job dynamically in the application {{main}} method if the current application can not handle the incoming data",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 15:41:37 UTC 2022,,,,,,,,,,"0|z19qtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 17:57;dannycranmer;Did you try using the job restart strategy? [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#restart-strategy-type]

 

Does not cover the reconfiguration but will restart failed jobs;;;","22/Oct/22 06:42;pvary;The main goal is to have a possibility to add a new sink, when a new type of data has arrived.

Imagine a job where the incoming data defines the sinks, like a Kafka topics or a database tables. We start with a set of known data types. The main task will create the appropriate topics and sinks based on the current known data types. Later a new record arrives with a new type which needs a new sink. The job needs to be reconfigured and a new sink needs to be added.

This way the Flink job can dynamically adjust itself to handle the incoming data.;;;","27/Oct/22 15:41;gyfora;merged to main 5c2f5bfe4108dad57ecfe5db5dc5de092529d5ac;;;",,,,,,,,,,,,,,,,,,,,
"The same batch task works fine in 1.15.2 and 1.16.0-rc1, but fails in 1.16.0-rc2",FLINK-29712,13488218,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,macdoor615,macdoor615,21/Oct/22 05:08,02/Nov/22 09:04,04/Jun/24 20:41,21/Oct/22 08:00,1.16.0,,,,,,,,,,Table SQL / Client,,,,0,,,,,"All my batch jobs have failed with same error. All streaming jobs work fine.
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=60000)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getGlobalFailureHandlingResult(ExecutionFailureHandler.java:102)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleGlobalFailure(DefaultScheduler.java:299)
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.lambda$failJob$0(OperatorCoordinatorHolder.java:635)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: p_hswtv[4] -> Calc[5]' (operator 6cdc5bb954874d922eaee11a8e7b5dd5).
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:617)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$QuiesceableContext.failJob(RecreateOnResetOperatorCoordinator.java:237)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.failJob(SourceCoordinatorContext.java:360)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:217)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:315)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.start(RecreateOnResetOperatorCoordinator.java:70)
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:198)
    at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:165)
    at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:82)
    at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:605)
    at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:1046)
    at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:963)
    at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:422)
    at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:198)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:622)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:621)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:190)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    ... 13 more
Caused by: java.lang.NumberFormatException: null
    at java.lang.Integer.parseInt(Integer.java:542)
    at java.lang.Integer.parseInt(Integer.java:615)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.calculateFilesSizeWithOpenCost(HiveSourceFileEnumerator.java:157)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.setSplitMaxSize(HiveSourceFileEnumerator.java:135)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:89)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.enumerateSplits(HiveSourceFileEnumerator.java:67)
    at org.apache.flink.connector.file.src.AbstractFileSource.createEnumerator(AbstractFileSource.java:141)
    at org.apache.flink.connectors.hive.HiveSource.createEnumerator(HiveSource.java:129)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:213)
    ... 33 more
 
 
{code}
 ","Flink 1.16.0-rc2

Hive 3.1.3

Hadoop 3.3.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/22 05:13;macdoor615;flink-conf.yaml;https://issues.apache.org/jira/secure/attachment/13051265/flink-conf.yaml",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 08:00:27 UTC 2022,,,,,,,,,,"0|z19p20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 05:14;macdoor615;job parameters 

 
{code:java}
SET table.dml-sync = true;
SET execution.runtime-mode = batch;
SET parallelism.default = -1;
SET table.exec.hive.infer-source-parallelism = true;
SET table.exec.hive.infer-source-parallelism.max = 16;
SET taskmanager.network.memory.buffers-per-channel = 0;
SET jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task = 4kb;
SET table.dynamic-table-options.enabled = true;
SET taskmanager.network.memory.buffer-debloat.enabled = true;
SET table.exec.legacy-cast-behaviour=enabled;
{code}
 ;;;","21/Oct/22 06:18;Weijie Guo;cc [~luoyuxia];;;","21/Oct/22 06:56;luoyuxia;[~macdoor615] Which mode do you run Flink? Application, Session mode or others?;;;","21/Oct/22 07:05;macdoor615;[~luoyuxia] session mode;;;","21/Oct/22 07:35;luoyuxia;[~macdoor615] Hi, thanks for raising it. But I can't reproduce it in my local env. I test with hive 2.9 and hadoop2 since I have no hive3, but i think the version shouldn't make difference.

I try with the following steps:

1: build flink

2: start flink cluster

 
{code:java}
./bin/start-cluster {code}
3: start sql-client

 

 
{code:java}
./bin/sql-client
{code}
4: create a hive catalog  which has contains a table `t2` with orc format.

 

 
{code:java}
create catalog hive_catalog with ('type' ='hive', 'hive-conf-dir' = 'xxx'); 

use catalog hive_catalog;{code}
5:  run a query

 

 
{code:java}
select * from t2;
{code}
 

[~macdoor615] Since you use session mode, have you also updated the client's version to the rc2?;;;","21/Oct/22 07:59;macdoor615;@luoyuxia I upgraded client to Flink 1.16.0-rc2 from 1.16.0-rc1. Problem solved. Thx;;;","21/Oct/22 08:00;macdoor615;upgraded client to Flink 1.16.0-rc2 from 1.16.0-rc1. Problem solved;;;",,,,,,,,,,,,,,,,
Topic notification not present in metadata after 60000 ms.,FLINK-29711,13487859,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,durgeshkumarmishragfx,durgeshkumarmishragfx,21/Oct/22 02:26,24/Nov/22 08:04,04/Jun/24 20:41,24/Nov/22 06:51,1.14.4,1.14.6,,,,,,,,,Connectors / Kafka,,,,0,,,,,"Failed to send data to Kafka null with FlinkKafkaInternalProducer\{transactionalId='null', inTransaction=false, closed=false}
at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.throwException(KafkaWriter.java:405)
at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.lambda$onCompletion$0(KafkaWriter.java:391)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
at java.base/java.lang.Thread.run(Unknown Source)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/22 03:11;durgeshkumarmishragfx;image-2022-11-01-08-41-21-836.png;https://issues.apache.org/jira/secure/attachment/13051658/image-2022-11-01-08-41-21-836.png","02/Nov/22 04:49;durgeshkumarmishragfx;image-2022-11-02-10-19-01-824.png;https://issues.apache.org/jira/secure/attachment/13051695/image-2022-11-02-10-19-01-824.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 06:49:59 UTC 2022,,,,,,,,,,"0|z19mu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 08:16;martijnvisser;Please add more information to the ticket next to the stacktrace. How did you end up with this error, what steps can be taken to reproduce the issue?;;;","21/Oct/22 08:35;durgeshkumarmishragfx;Hello [~martijnvisser]  We created one flink job and one Azure eventhub. Flink is processing real time data which published to Azure event hub and after continually running of flink job for about 10 hours  above  exception occurs.

 

 Used  following configuration.
 # Checkpoints configurations

checkpoints.interval= 240000

checkpoints.minPauseBetweenCheckpoints= 120000

checkpoints.timeout= 110000

 
 # Common Flink-Kafka-Connector(Source and Sink) configurations

allow.auto.create.topics=false

auto.offset.reset=latest

request.timeout.ms=60000

transaction.timeout.ms=60000

kafka.semantic=1

kafka.internalProducerPoolSize=5
 # For reducing the kafka timeout

max.block.ms=5000
 # For increasing the metadata fetch time

metadata.max.idle.ms= 180000

 ;;;","21/Oct/22 08:53;martijnvisser;Which Kafka sink are you using, FlinkKafkaProducer or KafkaSink? 
Are you using Exactly Once or a different guarantee? 
I know that Azure Eventhub can emulate Kafka, but we've also seen that these emulators can't exactly mirror everything that Kafka is doing (we've seen the same with RedPanda). My suspicion would be that there's something at the Azure Eventhub side of things that doesn't work exactly the same as in Kafka. ;;;","21/Oct/22 10:42;durgeshkumarmishragfx;[~martijnvisser] I am using the KafkaDink and using the AtleastOnce gurantee. 

We are using Azure eventhub in other projects as well, but there we are not facing this kind of issue.;;;","25/Oct/22 05:23;durgeshkumarmishragfx;[~martijnvisser]  and [~mason6345]  Can you guys help us here to fix this problem ?;;;","25/Oct/22 06:50;mason6345;Hi, I did a quick pass over the source code. Can you double check that you are not producing a null record? For example, you can add a log to your KafkaRecordSerializationSchema to verify.;;;","26/Oct/22 13:25;durgeshkumarmishragfx;[~mason6345]  I added the logs but didn't got any null values.;;;","26/Oct/22 14:21;martijnvisser;Is this something that you can make reproducible for us? ;;;","26/Oct/22 16:35;mason6345;Can you provide exact details where you added logs? I can reproduce this error message in unit test.;;;","27/Oct/22 06:34;durgeshkumarmishragfx;[~mason6345]  I added logs in class which implements KafkaRecordSerializationSchema.;;;","28/Oct/22 14:33;durgeshkumarmishragfx;[~martijnvisser]  I checked with microsoft  Azure Eventhub side of things, but sadly no luck they showed me everything working well on their side.

 

Do you guys can suggest something on this ? If possible we can have call to discuss this problem sometime. What's your thought ?;;;","29/Oct/22 19:30;mason6345;[~durgeshkumarmishragfx] can you post a minimal example of your KafkaSink setup?;;;","01/Nov/22 03:13;durgeshkumarmishragfx;[~mason6345]  Please find below example for kafka sink setup. Kindly let me know if you need anything else.

!image-2022-11-01-08-41-21-836.png|width=521,height=207!;;;","01/Nov/22 03:33;mason6345;Yes, I'm wondering about the TripSerializer implementation. Is it possible for it to return null?

I'm pretty sure this improvement will resolve your issue that you are experiencing:
https://issues.apache.org/jira/browse/FLINK-29480;;;","01/Nov/22 03:43;durgeshkumarmishragfx;[~mason6345]  Yes TripSerializer can return some entity which contains null values but object will never be null.

As i see the pull request we just need to @Nullable annotation which should resolved issue. Correct ?;;;","01/Nov/22 03:50;mason6345;No, the fix is included in 1.17.

That's your issue then–the API breaks when null is returned by TripSerializer.

You need to filter out inputs that result in `null` before the KafkaSink and before these values are serialized into the `null` value.;;;","01/Nov/22 03:58;durgeshkumarmishragfx;[~mason6345]  Not sure that will be issue, Because as a whole json object which we collecting is never null.

Object contains some entity which might be null.

Code which we are using inside the TripSerializer. Null records will only send in case of exception.

[~mason6345]  Now can you help me out here ? I understand we need to add null check before sending out the data. But apart from that, Is anything else we can try ?

!image-2022-11-02-10-19-01-824.png|width=354,height=133!;;;","23/Nov/22 08:26;martijnvisser;[~durgeshkumarmishragfx] I've verified with other services that this issue no longer occurs with Flink 1.16, most likely due to the upgrade to a newer version of the Kafka Client. Can you also check on your end with this version?;;;","24/Nov/22 06:49;durgeshkumarmishragfx;[~martijnvisser]  Yes you are correct we upgrade to 1.16 and the issue gone off after changing some request.timeout.ms to 30 sec.

 

So we are good to close this issue.;;;",,,,
Upgrade the minimal supported hadoop version to 2.10.2,FLINK-29710,13487365,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,20/Oct/22 20:07,30/Jan/23 08:28,04/Jun/24 20:41,10/Jan/23 00:04,,,,,,,1.17.0,,,,FileSystems,,,,0,pull-request-available,,,,"Hadoop 2.8.5 is vulnerable for multiple CVEs such as https://nvd.nist.gov/vuln/detail/CVE-2022-25168 and https://nvd.nist.gov/vuln/detail/CVE-2022-26612 which are classified as Critical. While Flink is not directly impacted by those, we do see vulnerability scanners flag Flink as being vulnerable. We could easily mitigate that by bumping the minimal supported version of Hadoop to 2.10.2.

Please note that this doesn't break the binary protocol compatibility, which means that 2.10.2 client can still talk to older servers.

Discussion thread: https://lists.apache.org/thread/tgw2dmnoxm7sdwyjohskmvpk3pdd3qvm",,,,,,,,,,,,,,,,,,,,,FLINK-15534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 00:04:37 UTC 2023,,,,,,,,,,"0|z19jsg:",9223372036854775807,"The minimum Hadoop version supported by Apache Flink has been updated to version 2.10.2. For Hadoop 3, the minimum Hadoop version that is now supported is version 3.2.3.",,,,,,,,,,,,,,,,,,,"10/Jan/23 00:04;martijnvisser;Fixed in master:

[FLINK-29710][Filesystem] Bump minimum supported Hadoop version to 2.10.2:
573ed922346c791760d27653543c2b8df56f51f7

[FLINK-29710][Hadoop/Hive] Exclude Reload4J from all Hadoop and Hive dependencies:
a9151c42100ec09388d8052c7aa9f77f82efe469

[hotfix] Sync English version of gcs.md to Chinese version due to being out-of-sync: 627d293b6938f9f9e6ceca6dfef3a3ff42b9de39

;;;",,,,,,,,,,,,,,,,,,,,,,
Bump Pulsar to 2.10.2,FLINK-29709,13487364,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,syhily,syhily,syhily,20/Oct/22 20:01,07/Nov/22 03:44,04/Jun/24 20:41,07/Nov/22 03:44,1.17.0,,,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,Pulsar released a new version 2.10.2 which contains a lot of bugfix.,,,,,,,,,,,,,,,,,,,,,FLINK-26980,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 03:44:43 UTC 2022,,,,,,,,,,"0|z19js8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 03:44;tison;master via b5a4d485e7041c57124d4188bce63c4a2066ce22;;;",,,,,,,,,,,,,,,,,,,,,,
Enrich Flink Kubernetes Operator CRD error field,FLINK-29708,13487309,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,darenwkt,darenwkt,20/Oct/22 17:59,27/Oct/22 13:59,04/Jun/24 20:41,27/Oct/22 13:59,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"h1. Problem Statement:

FlinkDeployment and FlinkSessionJob CRD has a CommonStatus error field of String type. Currently, this field stores various errors such as:
 * CR validation error
 * Missing SessionJob error/ Missing JobManager deployment error
 * Unknown Job error
 * DeploymentFailedException
 * ReconciliationError such as RestClientException from Flink Internal such as FlinkRest and FlinkRuntime

It is insufficient to store each error simply as string only. We need to include some exception metadata to help operator handle this error accordingly. For example, it is very useful to know the HttpResponseStatus code from RestClientException.
h1. Proposed Solution:
 * The error field should store a JSON with exception metadata. For example:

{code:java}
{    
  ""type"": ""JobManagerNotFoundException"",    
  ""message"": ""JobManager with leadership ID: 1234 was not found"",    
  ""stackTrace"": ""JobManager lost connection at ...."",
  ""additionalMetadata"": {     
    ""httpResponseCode"": ""400""
  },
  ""throwableList"": [
    {
      ""type"": ""FlinkRuntimeException"",
      ""message"": ""other exception""
    },
    ....
  ]
} {code}
 * The stackTrace field can be enabled or disabled via spec change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 13:59:54 UTC 2022,,,,,,,,,,"0|z19jg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 19:00;gyfora;Looks good!
Maybe we could shorten operatorErrorType -> type

I am a bit torn about httpResponseCode, we should probably only include it for specific error types. ;;;","23/Oct/22 23:22;darenwkt;Hi Gyula, thanks for the feedback, have updated the Jira and PR;;;","27/Oct/22 13:59;gyfora;merged to main 3c63653820742993926ff8901cfaa618a2f855cb;;;",,,,,,,,,,,,,,,,,,,,
"Fix possible comparator violation for ""flink list""",FLINK-29707,13487279,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,20/Oct/22 14:34,26/Oct/22 14:28,04/Jun/24 20:41,26/Oct/22 14:28,1.16.0,,,,,,,,,,Command Line Client,,,,0,pull-request-available,,,,"For the {{list}} CLI option, the code that prints the jobs, there is a {{startTimeComparator}} definition, which orders the jobs and it is done this way:
{code:java}
Comparator<JobStatusMessage> startTimeComparator =
                (o1, o2) -> (int) (o1.getStartTime() - o2.getStartTime());
{code}
In some rare situation this can lead to this:
{code:java}
2022-10-19 09:58:11,690 ERROR org.apache.flink.client.cli.CliFrontend                      [] - Error while running the command.
java.lang.IllegalArgumentException: Comparison method violates its general contract!
	at java.util.TimSort.mergeLo(TimSort.java:777) ~[?:1.8.0_312]
	at java.util.TimSort.mergeAt(TimSort.java:514) ~[?:1.8.0_312]
	at java.util.TimSort.mergeForceCollapse(TimSort.java:457) ~[?:1.8.0_312]
	at java.util.TimSort.sort(TimSort.java:254) ~[?:1.8.0_312]
	at java.util.Arrays.sort(Arrays.java:1512) ~[?:1.8.0_312]
	at java.util.ArrayList.sort(ArrayList.java:1464) ~[?:1.8.0_312]
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:392) ~[?:1.8.0_312]
	at java.util.stream.Sink$ChainedReference.end(Sink.java:258) ~[?:1.8.0_312]
	at java.util.stream.Sink$ChainedReference.end(Sink.java:258) ~[?:1.8.0_312]
	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:363) ~[?:1.8.0_312]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483) ~[?:1.8.0_312]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_312]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_312]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_312]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_312]
	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ~[?:1.8.0_312]
	at org.apache.flink.client.cli.CliFrontend.printJobStatusMessages(CliFrontend.java:574)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 14:28:28 UTC 2022,,,,,,,,,,"0|z19j9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 15:13;ferenc-csaky;Just opened a PR with the proposed fix.;;;","26/Oct/22 14:28;mbalassi;[5176fb2|https://github.com/apache/flink/commit/5176fb2c334edceb828f92ed90606401b8381d3f] in master;;;",,,,,,,,,,,,,,,,,,,,,
Remove japicmp dependency bumps,FLINK-29706,13487278,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,20/Oct/22 14:19,21/Oct/22 08:49,04/Jun/24 20:41,21/Oct/22 08:49,,,,,,,1.17.0,,,,Build System,,,,0,pull-request-available,,,,"Way back when we worked on Java 11 support we bumped several dependencies from japicmp.
These are no longer required for the latest version that we're using.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 08:49:05 UTC 2022,,,,,,,,,,"0|z19j94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 08:49;chesnay;master: b8672a230a3d34e09fddc7f506e090910e2d202e;;;",,,,,,,,,,,,,,,,,,,,,,
Document the least access with RBAC setting for native K8s integration,FLINK-29705,13487259,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ouyangwuli,wangyang0918,wangyang0918,20/Oct/22 11:45,15/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,Deployment / Kubernetes,Documentation,,,0,pull-request-available,stale-assigned,,,"We should document the least access with RBAC settings[1]. And the operator docs could be taken as a reference[2].

 

[1]. [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/native_kubernetes/#rbac]

[2]. [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/rbac/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:11 UTC 2023,,,,,,,,,,"0|z19j4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 07:20;ouyangwuli;[~wangyang0918]  Can assign this ticket for me ?;;;","14/Mar/23 09:04;Wencong Liu;Hello [~ouyangwuli] . Are you still working on this?;;;","16/Mar/23 11:48;ouyangwuli;apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    system: taskmanager-serviceaccount
  name: taskmanager-serviceaccount

—
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: taskmanager-serviceaccount
rules:
  - apiGroups: [""""]
    resources: [""configmaps""]
    verbs: [""get"", ""list"", ""watch""]

—
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: taskmanager-serviceaccount
subjects:
  - kind: ServiceAccount
    name: taskmanager-serviceaccount
roleRef:
  kind: Role
  name: taskmanager-serviceaccount
  apiGroup: rbac.authorization.k8s.io

 

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    system: jobmanager-serviceaccount
  name: jobmanager-serviceaccount

—
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: jobmanager-serviceaccount
rules:
  - apiGroups: [""""]
    resources: [""pods"",""configmaps""]
    verbs: [""get"", ""list"", ""watch"", ""create"", ""update"", ""patch"", ""delete""]
  - apiGroups: [""apps""]
    resources: [""deployments""]
    verbs: [""get"", ""list"", ""create"", ""update"", ""patch"", ""delete""]  

—
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: jobmanager-serviceaccount
subjects:
  - kind: ServiceAccount
    name: jobmanager-serviceaccount
roleRef:
  kind: Role
  name: jobmanager-serviceaccount
  apiGroup: rbac.authorization.k8s.io

 [~Wencong Liu]  can you help review this rbac config ?;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
E2E test for delegation token framework,FLINK-29704,13487250,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,mbalassi,mbalassi,20/Oct/22 11:29,16/Nov/22 10:33,04/Jun/24 20:41,16/Nov/22 10:33,,,,,,,1.17.0,,,,Runtime / Coordination,Tests,,,0,pull-request-available,security,,,"We should add an end-to-end test verifying that the delegation token framework can be used to authenticate taskmanagers to a secure service, for example Kerberos enabled HDFS.

I would suggest to rely on the existing tooling here: https://github.com/apache/flink/tree/master/flink-end-to-end-tests/test-scripts",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 10:33:01 UTC 2022,,,,,,,,,,"0|z19j2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 11:32;gaborgsomogyi;Thanks for creating the jira. I'm intended to pick this up when reaching it.;;;","16/Nov/22 10:33;mbalassi;[{{4a0f283}}|https://github.com/apache/flink/commit/4a0f283a73f3022390070637647df3fc5abba0a8] in master;;;",,,,,,,,,,,,,,,,,,,,,
Fail to call unix_timestamp  in runtime in Hive dialect ,FLINK-29703,13487242,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,20/Oct/22 10:21,21/Oct/22 03:57,04/Jun/24 20:41,21/Oct/22 03:57,,,,,,,,,,,Connectors / Hive,,,,0,,,,,"Can be reproduced by the following sql with Hive dialect:
{code:java}
select unix_timestamp();{code}",,,,,,,,,,,,,,,,,,,,FLINK-26474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 03:12:08 UTC 2022,,,,,,,,,,"0|z19j14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 03:12;luoyuxia;The reason is the hive function unix_timestamp will call `SessionState.get().getQueryCurrentTimestamp()` in runtime.

But the SessionState.get() will return null since the SessionState will be closed in Flink.

Considering `SessionState.get().getQueryCurrentTimestamp()` is actually a fixed value set in query parse phase.

To fix it, we need to convert the function call `unix_timestamp` to literal instead of evaluting in runtime siince the value has no difference between convert it to a literal and evaluating it in runtime. ;;;",,,,,,,,,,,,,,,,,,,,,,
Add merge tree reader and writer micro benchmarks,FLINK-29702,13487226,13486239,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,20/Oct/22 08:25,26/Oct/22 11:34,04/Jun/24 20:41,26/Oct/22 11:34,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Add merge tree reader and writer micro benchmarks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 11:34:33 UTC 2022,,,,,,,,,,"0|z19ixk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 11:34;lzljs3620320;master: 8af95e62a9286e78f135b2ad24f2d61b54d6f7e9;;;",,,,,,,,,,,,,,,,,,,,,,
Refactor flink-table-store-benchmark and create micro benchmarks module,FLINK-29701,13487225,13486239,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,20/Oct/22 08:24,21/Oct/22 00:45,04/Jun/24 20:41,21/Oct/22 00:45,table-store-0.2.2,table-store-0.3.0,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Refactor the `flink-table-store-benchmark` to `flink-table-store-cluster-benchmark`,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 00:45:43 UTC 2022,,,,,,,,,,"0|z19ixc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 00:45;lzljs3620320;master: cc6d29a464a8790324ef961722e2d822dea6dd33;;;",,,,,,,,,,,,,,,,,,,,,,
Serializer to BinaryInMemorySortBuffer is wrong,FLINK-29700,13487218,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,20/Oct/22 08:04,21/Oct/22 06:52,04/Jun/24 20:41,21/Oct/22 06:52,,,,,,,table-store-0.2.2,,,,Table Store,,,,0,pull-request-available,,,,"In SortBufferMemTable, it will use `BinaryInMemorySortBuffer.createBuffer(BinaryRowDataSerializer serializer)`, the serializer is for full row, not just sort key fields.

Problems may occur when there are many fields.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 06:52:29 UTC 2022,,,,,,,,,,"0|z19ivs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 06:52;lzljs3620320;release-0.2: aa6846089afc61e6715a7d50ae40e6bb9d8efc0f;;;",,,,,,,,,,,,,,,,,,,,,,
Json format parsing supports converting strings  at the end with Z and numbers to timestamp,FLINK-29699,13487212,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,waywtdcc,waywtdcc,waywtdcc,20/Oct/22 07:27,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,1.16.1,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,stale-assigned,,,"When I use flink cdc to read oracle, the time type data returned by cdc is a long type timestamp. I want to convert it to timestamp type, but it is not supported.

 

1. JSON parsing supports converting long timestamps into flink timestamp types, for example, supporting JSON parsing of 1666255300000 numbers into timestamp
2. JSON analysis supports the conversion of WITH_LOCAL_TIMEZONE string data into flink timestamp type, for example, it supports 1990-10-14T12:12:43.123456789Z into timestamp type",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:11 UTC 2023,,,,,,,,,,"0|z19iug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:32;martijnvisser;[~waywtdcc] Thanks for the ticket, but what is the feature that you propose for Flink?;;;","20/Oct/22 07:41;waywtdcc;[~martijnvisser] Thanks you .update now;;;","20/Oct/22 07:57;martijnvisser;I still don't understand what is missing in Flink. Please elaborate ;;;","20/Oct/22 08:01;martijnvisser;[~waywtdcc] New features can't be added to patch releases: it can only go to 1.17. ;;;","17/Feb/23 03:24;waywtdcc;I have already edited it;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
Job Manager fails with FileAlreadyExistsException if java.io.tmpdir points to a symbolic link,FLINK-29698,13487198,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,TsReaper,TsReaper,20/Oct/22 04:29,20/Oct/22 07:09,04/Jun/24 20:41,20/Oct/22 07:09,1.15.2,1.16.0,1.17.0,,,,,,,,Runtime / RPC,,,,0,,,,,"The {{/tmp}} directory on my PC is a symbolic link.

{code}
ls -all /
(result)
lrwxr-xr-x    1 root  wheel      11  3 25  2019 tmp -> private/tmp
{code}

In {{flink-conf.yaml}} if I add

{code}
env.java.opts: -Djava.io.tmpdir=/tmp
{code}

then job manager fails to start. The exception stack is

{code}
2022-10-20 11:34:34,627 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics org.apache.flink.runtime.rpc.exceptions.RpcLoaderException: Could not load RpcSystem.
	at org.apache.flink.runtime.rpc.RpcSystem.load(RpcSystem.java:106)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:355)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711)
	at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:59)
Caused by: java.lang.RuntimeException: Could not initialize RPC system.
	at org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.loadRpcSystem(AkkaRpcSystemLoader.java:85)
	at org.apache.flink.runtime.rpc.RpcSystem.load(RpcSystem.java:101)
	... 7 more
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:727)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.loadRpcSystem(AkkaRpcSystemLoader.java:58)
	... 8 more
{code}

There are similar issues in {{PlannerModule}} class of {{flink-table-planner-loader}} module and in {{CodeGenLoader}} class of table store.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28102,,,,,FLINK-20267,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 07:09:01 UTC 2022,,,,,,,,,,"0|z19irc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 06:40;Weijie Guo;Maybe this is duplicated with FLINK-28102;;;","20/Oct/22 06:44;TsReaper;[~Weijie Guo] Yes it is, sorry for the duplicate. However I insist to make this issue as a blocker because the {{/tmp}} directory of an Amazon EMR is a symlink. Amazon EMR users will not be able to use Flink directly out of the box.;;;","20/Oct/22 06:57;Weijie Guo;Yes, I agree with you that this is a serious problem.

However, this  should already exist in 1.15, perhaps it should not be considered as a blocker of 1.16? By the way, I am not very clear about the definition of blocker issue, please feel free to point out my mistakes.

Let's first track the problem in only one ticket(this one or that earlier one is also fine), and then try to find a solution.;;;","20/Oct/22 07:09;xtsong;Closing this for duplication. I also upgraded FLINK-28102 to Critical.;;;",,,,,,,,,,,,,,,,,,,
Flaky test failure KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecoveryEventTime,FLINK-29697,13487196,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,samrat007,samrat007,20/Oct/22 03:50,20/Oct/22 09:56,04/Jun/24 20:41,20/Oct/22 09:55,,,,,,,,,,,Tests,,,,0,,,,,"{code:java}
org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecovery(KafkaShuffleExactlyOnceITCase.java:158) 2022-10-19T16:06:46.5083193Z Oct 19 16:06:46 at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecoveryEventTime(KafkaShuffleExactlyOnceITCase.java:101) 2022-10-19T16:06:46.5084039Z Oct 19 16:06:46 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2022-10-19T16:06:46.5084660Z Oct 19 16:06:46 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2022-10-19T16:06:46.5085360Z Oct 19 16:06:46 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2022-10-19T16:06:46.5085992Z Oct 19 16:06:46 at java.lang.reflect.Method.invoke(Method.java:498) 2022-10-19T16:06:46.5086643Z Oct 19 16:06:46 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 2022-10-19T16:06:46.5087359Z Oct 19 16:06:46 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 2022-10-19T16:06:46.5088069Z Oct 19 16:06:46 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 2022-10-19T16:06:46.5088764Z Oct 19 16:06:46 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 2022-10-19T16:06:46.5089451Z Oct 19 16:06:46 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 2022-10-19T16:06:46.5090170Z Oct 19 16:06:46 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 2022-10-19T16:06:46.5090953Z Oct 19 16:06:46 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 2022-10-19T16:06:46.5091773Z Oct 19 16:06:46 at java.util.concurrent.FutureTask.run(FutureTask.java:266) 2022-10-19T16:06:46.5092308Z Oct 19 16:06:46 at java.lang.Thread.run(Thread.java:748) 2022-10-19T16:06:46.5092707Z Oct 19 16:06:46 2022-10-19T16:06:47.2228761Z Oct 19 16:06:47 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError 2022-10-19T16:06:48.3265710Z Oct 19 16:06:48 [INFO] Running org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42229&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24119,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 09:56:28 UTC 2022,,,,,,,,,,"0|z19iqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:36;martijnvisser;[~samrat007] Thanks for filing the ticket, but the Azure run that you've linked it to doesn't even compile properly. When that's the case, it could very well be that the error run is related to that failing compilation.;;;","20/Oct/22 07:41;samrat007;Oh ! sorry ! [~martijnvisser] ! 
I have attached wrong link .
updated it now!  

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42229&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37456;;;","20/Oct/22 09:56;martijnvisser;[~samrat007] Thanks! The test instability was already reported previously, but closed because we thought it was fixed. I've reopened that ticket and added your example to it. I'm closing this to being a duplicate of the original ticket. ;;;",,,,,,,,,,,,,,,,,,,,
[Doc] Operator helm install command points to wrong repo,FLINK-29696,13487188,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,affe,affe,20/Oct/22 01:54,23/Feb/24 16:17,04/Jun/24 20:41,21/Oct/22 15:24,,,,,,,,,,,Documentation,Kubernetes Operator,,,0,,,,,"In the operator documentation, the repo is added via:

`helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-<OPERATOR-VERSION>/`

 

But later in the Operation-> Helm, the code instruct us to use 

 

`{{{}helm install flink-kubernetes-operator helm/flink-kubernetes-operator`{}}}

{{}}

Here we won't be able to download the helm chart since we are not using the right repo.

 

You can assign this Jira to me and I can submit a PR to fix it~ ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 16:17:35 UTC 2024,,,,,,,,,,"0|z19ip4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 05:08;gyfora;Those two docs refer to two different ways of installing the helm chart. One is from remotely, the other is from your local operator source (git repo). 

I think both are valuable under Operation/Helm but we should be specific about the requirements and maybe also add the remote version there too :) ;;;","21/Oct/22 15:24;mbalassi;This is intentional. One references the released helm chart, the other the local dev one.;;;","23/Feb/24 02:11;domenicbove;Hi, I think these helm docs: [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/helm/] should mention the helm chart repo: https://downloads.apache.org/flink/flink-kubernetes-operator-<OPERATOR-VERSION>/

If you miss the quickstart, IDK how you will find the chart repo

 ;;;","23/Feb/24 07:13;gyfora;[~domenicbove] feel free to open a doc improvement PR if you think it's better that way :) ;;;","23/Feb/24 16:17;domenicbove;Great idea [~gyfora] - here's my PR if you can review: [https://github.com/apache/flink-kubernetes-operator/pull/784]

 ;;;",,,,,,,,,,,,,,,,,,
Create a utility to report the status of the last savepoint,FLINK-29695,13487129,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,claraxiong,claraxiong,claraxiong,19/Oct/22 16:33,21/Nov/22 17:37,04/Jun/24 20:41,21/Oct/22 08:57,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Users want to know the status of last savepoint, especially for manually triggered ones, to manage savepoints.

Currently, users can infer the status of the last savepoint (PENDING, SUCCEEDED and ABANDONED) from jobStatus.triggerId, lastSavepoint.triggerNonce, spec.job.savepointTriggerNonce and savepointTriggerNonce from last reconciliation. If the last savepoint is not manually triggered, there is no ABANDONED status, only PENDING or SUCCEEDED.

Creating a utility will encapsulate the internal logic of Flink operator guard against regression by any future version changes.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30047,FLINK-30119,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 08:57:56 UTC 2022,,,,,,,,,,"0|z19ic8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 08:57;gyfora;merged to main 67d4cef5aadf05bb043c4df0a127c252666b31c1;;;",,,,,,,,,,,,,,,,,,,,,,
Support tolerations in helm template for flink operator deployment,FLINK-29694,13487108,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,meetyourmaker,meetyourmaker,meetyourmaker,19/Oct/22 15:08,21/Oct/22 14:25,04/Jun/24 20:41,21/Oct/22 14:25,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,1,pull-request-available,,,,"The Operator's deployment should allow specifying tolerations. There are cases where we want the operator to not be scheduled onto inappropriate nodes. In such cases, it will be great if we can support this via the Helm Chart.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 14:25:51 UTC 2022,,,,,,,,,,"0|z19i7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 15:14;gyfora;[~meetyourmaker] would you like to work on this ticket?

 ;;;","19/Oct/22 15:18;meetyourmaker;[~gyfora] Yes please, I will make a PR on github :);;;","21/Oct/22 14:25;gyfora;merged to main f2c34abc466c9e33745a9b2b62e4ac2ad640ef39;;;",,,,,,,,,,,,,,,,,,,,
MiniClusterExtension should respect DEFAULT_PARALLELISM if set,FLINK-29693,13487100,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,19/Oct/22 14:35,01/Nov/22 09:38,04/Jun/24 20:41,20/Oct/22 09:54,,,,,,,1.16.1,1.17.0,,,Tests,,,,0,pull-request-available,,,,"MiniClusterExtension#registerEnv sets the default parallelism of the environment to the number of the slots the cluster has.

This effectively prevents multiple jobs from running on the same MiniCluster unless they specify a parallelism via the API.
This isn't ideal since it means you can't easily mix workloads during testing.

It would be better if the cluster would check the config for whether {{DEFAULT_PARALLELISM}} was set.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29691,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 09:54:30 UTC 2022,,,,,,,,,,"0|z19i5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 14:43;Sergey Nuyanzin;is not it the same https://issues.apache.org/jira/browse/FLINK-29691 ?;;;","20/Oct/22 09:54;chesnay;master: a6db6ee5d0d6e9b50c6d110793e2efbd0d57cc38
1.16: 1ed2494cf570e6e137f870b7d4a13ef18a9c381c;;;",,,,,,,,,,,,,,,,,,,,,
Support early/late fires for Windowing TVFs,FLINK-29692,13487097,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,canopenerda,canopenerda,19/Oct/22 14:17,21/May/24 07:49,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"I have cases where I need to 1) output data as soon as possible and 2) handle late arriving data to achieve eventual correctness. In the logic, I need to do window deduplication which is based on Windowing TVFs and according to source code, early/late fires are not supported yet in Windowing TVFs.

Actually 1) contradicts with 2). Without early/late fires, we had to compromise, either live with fresh incorrect data or tolerate excess latency for correctness.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 17 09:10:54 UTC 2023,,,,,,,,,,"0|z19i54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 09:51;martijnvisser;[~godfrey] [~jark] [~jingzhang] WDYT?;;;","20/Oct/22 14:48;jark;I'm fine with supporting this feature. But we have supported early/late fires for the legacy group window aggregation. You can use {{last_value(col)}} agg function to get window deduplication result. For example:

{code:sql}
-- output window result for every 5 seconds before window fire
SET table.exec.emit.early-fire.enabled = true;
SET table.exec.emit.early-fire.delay = 5s;
-- emit window result for every late arriving record
SET table.exec.emit.late-fire.enabled = true;
SET table.exec.emit.late-fire.delay = 0; 

SELECT TUMBLE_START(rowtime, INTERVAL '5' MINUTES), last_value(col_a), last_value(col_b)
FROM my_source_table
GROUP BY mykey, TUMBLE(rowtime, INTERVAL '5' MINUTES);
{code};;;","21/Oct/22 01:48;canopenerda;Thanks [~jark] for suggesting the legacy group window agg. So last value in your example is based on rowtime or proctime? Another problem in my case is that we require time precision to be microsecond, but time attribute only supports millisecond so far.;;;","21/Oct/22 12:23;jjpar;It would be really nice if Windowing TVF's supported early/late fires.

For us early fire is a must, so we are forced to use legacy group window agg, which lack support for several optimizations: Split Distinct Aggregation and Local-Global Aggregation.;;;","24/Oct/22 04:20;jingzhang;Sorry for late response.

I'm still wondering whether we really need early fire/late fire.

As [~jark] said, eary fire/late fires is supported in group window aggregation. when users enable early fire/ late fires of window aggregate, he/she would allow:
 # the emitted result would has no restrict mapping with time
 # the intermediate result could be allowed to emitted
 # the window result might be retracted later

If a user could allows the above conditions, why not use unbounded aggregate directly? Otherwise, window aggregate would be the best options. 

Could you describe the demand in detail [~canopenerda] [~jjpar] , thanks a lot. 

Let's see if there are other better plans which could satisfy your demands, such as [cumulate window tvf?|https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/table/sql/queries/window-tvf/#cumulate];;;","24/Oct/22 04:58;libenchao;Regarding ""early-fire with window"" versus ""unbounded aggregate"", I could share a point. Usually they could do the same thing, but when come to the state retiring, they will be a little different. 
 # window could retire state using watermark, and drop late event
 # for unbounded aggregate, we could only set a state ttl, and there is no way to drop late event

This is the most important reason why some use cases still prefer to use ""early-fire with window"".;;;","24/Oct/22 08:47;canopenerda;[~jingzhang] Let me elaborate my case a little bit.

First of all, it's a case in high frequency trading. We deployed multiple agents globally to individually collect data  from exchanges, so inherently there are duplicates and each data entry has a field that uniquely identifies the data from biz perspective, and an event time column in microsecond. The event time column may vary for entries with the same unique column value. So we would like to get the entry with the earliest event time for each unique column value.;;;","24/Oct/22 12:08;jingzhang;[~libenchao] Thanks for sharing.

'Early fire' would periodically sent intermediate result to downstream, the frequency to send intermediate result is based on processing time, even thought the window itself based on event time. It mixed up processing time semantics with the time attribute of Window.

If you need to retire window state and send intermediate result, how about trying ""[cumulate window tvf|https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/table/sql/queries/window-tvf/#cumulate]"" instead of 'early fire'?;;;","24/Oct/22 12:43;jingzhang;[~canopenerda] Thanks for sharing. 

The demand is a [deduplication|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/deduplication] on event time, get first row. But now time attribute is on millisecond unit instead of microsecond. I guess it's also the reason why you choose early-fire window.

How about use [topN|https://issues.apache.org/jira/browse/FLINK-23107]? top1 almost has well performance as good as [deduplication|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/deduplication] because of [FLINK-23107|https://issues.apache.org/jira/browse/FLINK-23107].;;;","25/Oct/22 01:25;libenchao;{quote}'Early fire' would periodically sent intermediate result to downstream, the frequency to send intermediate result is based on processing time, even thought the window itself based on event time. It mixed up processing time semantics with the time attribute of Window.
{quote}
I agree, it's not perfect.
{quote}If you need to retire window state and send intermediate result, how about trying ""[cumulate window tvf|https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/table/sql/queries/window-tvf/#cumulate]"" instead of 'early fire'?
{quote}
Cumulate window could cover most cases, except that it's semantic is different from others (unbounded aggregate and window with early-fire's output semantic is changelog for the same grouping key).;;;","25/Oct/22 07:04;jjpar;{quote}Could you describe the demand in detail [~canopenerda] [~jjpar] , thanks a lot. 

Let's see if there are other better plans which could satisfy your demands, such as [cumulate window tvf?|https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/table/sql/queries/window-tvf/#cumulate]
{quote}
In my case I have a source, that has event time and watermark time in different columns. Watermark column guarantees, that all events with event time <=  watermark time had arrived. But this watermark has greater delay, and we are working with minute-sized windows. Therefore we use early fire to emit intermediate results, without waiting for watermark.;;;","13/Apr/23 16:01;charles-tan;Hi all, I notice this thread has gotten a little bit stale and was hoping to continue this conversation. I also have a use case that would benefit from the `emit.early-fire.enabled` configuration working with window TVF. Using cumulate window helps our use case, but cumulate window can be quite expensive if you have a small step size. Are there any plans for this issue?;;;","13/Apr/23 17:24;martijnvisser;[~charles-tan] I don’t think there’s consensus yet how this would/should work properly;;;","17/Apr/23 07:57;martijnvisser;[~charles-tan] Could you also elaborate on your use case for this?;;;","18/Apr/23 19:11;charles-tan;hi [~martijnvisser], there are many use cases that I believe can benefit from a feature like this. One use case example would be fraud detection. Let's say you want to be notified if a withdrawal from a bank account happens 3 times in an hour. With a TUMBLING window of 1 hour, it isn't acceptable that you should need to wait an hour for the window to end before realizing that a potential fraud has happened. Of course CUMULATE window can reduce this delay, but like I mentioned above we would be opening many more windows depending on how often we want to emit an intermediate result which can be resource intensive. I think supporting an early fire configuration, as was supported with older windowing functions, for the newer TVFs would be a more streamlined way of supporting use cases like these.

A secondary point is that windows in ksqldb have early fire by design (they struggle in the opposite way here in that they don't easily support emitting one correct result per window). Supporting an early fire configuration makes it much easier for users who are trying to migrate their use cases from ksqldb to Flink.;;;","21/Apr/23 04:09;aitozi;hi, sorry for jumping into this discussion. I want to share two thoughts about this feature.
 * As [~jark] mentioned, in the group window aggregation the early/late fire has been supported. Window TVF as a feature-rich version of group window aggregation, so I think this ability should be aligned.
 * There is difference between the early/late fire and cumulate window tvf. early/late fire is something relates to the window trigger. But cumulative window relates to window assigner. We could use cumulative window to simulate the same functionality, but it may bring overhead as [~charles-tan] said.

So, +1 from my side to support emit strategy(early/late fire) in window TVF.;;;","21/Apr/23 12:52;jark;Hi [~charles-tan], thank you for sharing your use case. I'm just curious that is it possible to support your use case by using Group Aggregate instead of Window Aggregate? For example: 

{code}
SELECT user, COUNT(*) as cnt
FROM withdrawal
GROUP BY 
   user, 
   DATE_FORMAT(withdrawal_timestamp, ""yyyy-MM-dd HH:00"") -- trim into hour
HAVING cnt >= 3
{code}

IIUC, this can also archive that ""notified if a withdrawal from a bank account happens 3 times in an hour"" ASAP. And you may get better performance from the tuning[1]. 


[1]: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tuning/;;;","21/Apr/23 17:11;charles-tan;re [~jark]: Thanks for the response. The example query you provided would yield the correct results, but from my understanding with window TVF, after the window expires then the state associated with that window will be cleared. With the example you provided, is it true that the state will just accumulate over time? Also, the approach with the query won't work by adjusting the use case slightly. For example, if instead of 1 hour tumbling windows they were 2 hour windows or if instead of using tumbling windows we had hopping windows with length 1 hour every 30 minutes.;;;","24/Apr/23 06:55;jark;Hi [~charles-tan], yes, they have some differences. Group Aggregate doesn't support expire state like the window does, but you can enable state TTL to expire states. Regarding the hopping windows, you can implement a UDTF to split a record into multiple records associated with different windows and apply group aggregate on the windows. ;;;","24/Apr/23 07:05;jark;I think early fire / late arrival is a great feature, but I just want to explore if we have other or better solutions for the use case because supporting early fire / late arrival on the new window TVF might not be easy. ;;;","02/May/23 20:07;charles-tan;[~jark] your point makes sense. However, it can be cumbersome to come up with workarounds for every use case that would benefit from an early fire feature – imagine if you have 100 ksqlDB workloads that you are trying to migrate to Flink. While many use cases can be supported through Flink's existing features, it would still be nice for Flink to support early fire windows with window TVF.

Out of curiosity, what are the difficult parts of supporting early fire on window TVF? From what I understand, Flink already supports window triggers in the DataStream API and the old windowing functions already supported this configuration. I am not very familiar with the implementation details of SQL operators in Flink, but if there was a list of tasks relevant to supporting this feature maybe I could lend a hand in taking a few of those items on.;;;","17/Oct/23 09:10;tanjialiang;Hi everyone, I notice that there is no solution yet. I want to share my thoughts about this feature. Maybe it can help.
 
I think support early-fire may not be the best solution to the current window function. Because every window triggers are expensive, and also the early-fire is not the realtime trigger.

For example
{code:sql}
SET table.exec.emit.early-fire.enabled = true;
SET table.exec.emit.early-fire.delay = 1min;

SELECT user_id,       
       COUNT(*) AS total,       
       HOP_START(rowtime, INTERVAL '24' HOUR, INTERVAL '48' HOUR) AS window_start,
       HOP_END(rowtime, rowtime, INTERVAL '24' HOUR, INTERVAL '48' HOUR) AS window_end,
FROM user_click
GROUP BY user_id, HOP(rowtime, INTERVAL '24' HOUR, INTERVAL '48' HOUR); {code}
1. whether HOP/TUMBLE/CUMULATE window or enable early-fire, there are having a time delay, which are not realtime enough.

2. when the cardinal of user_id is large, everytime to trigger window is very expensive, which would make job instability, easy to make checkpoint timeout.

3. everytime early-fire would trigger all user_id's windows, but maybe only a small part of the data actually changed in this early-fire trigger interval, which maybe cause write pressure to the sink.

 
In my company, I've added a window TVF function for this case, named HOPv2/TUMBLEv2 (maybe the name is not fit for the community).
{code:sql}
select user_id,       
       COUNT(*) AS total, 
       window_start,
       window_time, -- the record rowtime
       window_end
FROM TABLE(    
    HOPV2(
        DATA => TABLE user_click,
        TIMECOL => DESCRIPTOR(rowtime),
        SLIDE => INTERVAL '24' HOUR
        SIZE => INTERVAL '48' HOUR,
        ALLOWED_LATENESS => true))
GROUP BY user_id, window_start, window_time, window_end; {code}
1. similar to OVER window，we accumulate and output the result when record comming (actually is on timer trigger), which is in realtime trigger and also there is not a lot of write pressure for sink.
2. the window_time is the record rowtime, which is represents the current progress.
3. similar to HOP window, we fire and purge when window_end come.
4. support allowedLateness option, when process the late event, if its window have not been purge, allow acuumulate without emit.

 
I would like to contribute it but maybe need more discussion and help because i am still a novice for flink contribution.
 ;;;",
Adjust org.apache.flink.test.junit5.MiniClusterExtension#registerEnv to honor DEFAULT_PARALLELISM if set ,FLINK-29691,13487088,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,19/Oct/22 13:32,20/Oct/22 09:54,04/Jun/24 20:41,20/Oct/22 09:54,,,,,,,,,,,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29693,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-19 13:32:39.0,,,,,,,,,,"0|z19i34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
listTables in HiveCatalog should only return table store tables,FLINK-29690,13487081,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,19/Oct/22 13:00,20/Oct/22 03:52,04/Jun/24 20:41,20/Oct/22 03:52,,,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 03:52:22 UTC 2022,,,,,,,,,,"0|z19i1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 03:52;lzljs3620320;master: 997682a9507ccf9165e7d2ff4e95aca8c56df113
release-0.2: 159b1446d458f84a85c452dd5ef297458e1a673b;;;",,,,,,,,,,,,,,,,,,,,,,
NIFI Performance issue - ConvertExcelToCSVProcessor to handle more data,FLINK-29689,13487074,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,vrindapalod,vrindapalod,19/Oct/22 12:14,19/Oct/22 12:36,04/Jun/24 20:41,19/Oct/22 12:36,,,,,,,,,,,API / DataSet,,,,0,CSV,excel,nifi,poi,"Currently , when we try to convert an excel with records more than 100,000,000 record type, it throws error and doesnt convert the file to csv format.

 

This can be fixed by adding the below line before reading the excel in the code. 

IOUtils.setByteArrayMaxOverride(2147483647);

 

The value hardcoded is the max length of int array above which we cant handle for now. 

 

!image-2022-10-19-18-04-33-092.png!",,,,,,,,,,,,,,,,,,,FLINK-10684,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/22 12:33;vrindapalod;image-2022-10-19-18-03-13-983.png;https://issues.apache.org/jira/secure/attachment/13051183/image-2022-10-19-18-03-13-983.png","19/Oct/22 12:34;vrindapalod;image-2022-10-19-18-04-33-092.png;https://issues.apache.org/jira/secure/attachment/13051184/image-2022-10-19-18-04-33-092.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 12:36:28 UTC 2022,,,,,,,,,,"0|z19i00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 12:36;martijnvisser;This is not the Apache NiFi jira, but this is the Apache Flink Jira;;;",,,,,,,,,,,,,,,,,,,,,,
Build time compatibility check for DynamoDB SDK,FLINK-29688,13487054,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,19/Oct/22 11:12,14/Nov/22 14:44,04/Jun/24 20:41,14/Nov/22 14:43,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"The DynamoDB connector exposes SDK classes to the end user code, and also is responsible for de/serialization of these classes. Add a build time check to ensure the client model is binary equivalent of a known good version. This will prevent us updating the SDK and unexpectedly breaking the de/serialization.

We use {{japicmp-maven-plugin}} to do something similar for Flink, we can potentially reuse this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 14:43:46 UTC 2022,,,,,,,,,,"0|z19hvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 14:43;dannycranmer;Merged commit [{{4427bcd}}|https://github.com/apache/flink-connector-aws/commit/4427bcdcb91505a2e1155e20b149e2f6e89d4b96] into main;;;",,,,,,,,,,,,,,,,,,,,,,
Do not throw exception when Lookup table is empty,FLINK-29687,13487039,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,19/Oct/22 09:42,21/Oct/22 03:26,04/Jun/24 20:41,21/Oct/22 03:26,table-store-0.2.1,table-store-0.3.0,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"!image-2022-10-19-17-44-10-062.png|width=724,height=431!

When scanning the Lookup table, it is likely that the snapshot does not be committed at that moment. So it's better to wait for the commit other than throwing an exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/22 09:44;qingyue;image-2022-10-19-17-44-10-062.png;https://issues.apache.org/jira/secure/attachment/13051172/image-2022-10-19-17-44-10-062.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 03:26:34 UTC 2022,,,,,,,,,,"0|z19hs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 03:26;lzljs3620320;master: b54c474e2596271fedfffaec5eddc895e0bb5455
release-0.2: 56c952e059686a5096163356f563548bb1dc3f2e;;;",,,,,,,,,,,,,,,,,,,,,,
There are bugs in Flink-SQL using Hive dialect,FLINK-29686,13487014,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,Vincent Long,Vincent Long,19/Oct/22 07:38,20/Oct/22 07:20,04/Jun/24 20:41,19/Oct/22 08:31,1.14.4,,,,,,,,,,Connectors / Hive,Table SQL / Client,,,0,,,,,"When I submit tasks to the session cluster using SQL-CLI, the following error occurred when I executed sql code using hive dialect through flink-sql-connector-hive-2.2.0_2.12-1.14.4.jar :

 

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: {color:#ff0000}Unexpected exception. This is a bug. Please consider filing an issue.{color}
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
{color:#ff0000}Caused by: java.lang.ExceptionInInitializerError{color}
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.hive.common.util.ReflectionUtil.newInstance(ReflectionUtil.java:83)
    at org.apache.hadoop.hive.ql.exec.Registry.registerUDAF(Registry.java:238)
    at org.apache.hadoop.hive.ql.exec.Registry.registerUDAF(Registry.java:231)
    at org.apache.hadoop.hive.ql.exec.FunctionRegistry.<clinit>(FunctionRegistry.java:430)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.table.catalog.hive.client.HiveShimV120.registerTemporaryFunction(HiveShimV120.java:262)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:207)
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$parseStatement$1(LocalExecutor.java:172)
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
    at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172)
    at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:396)
    at org.apache.flink.table.client.cli.CliClient.executeStatement(CliClient.java:324)
    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:297)
    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:221)
    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
    ... 1 more
{color:#ff0000}Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unrecognized Hadoop major version number: 3.0.0-cdh6.3.2{color}
    at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:102)
    at org.apache.hadoop.hive.ql.udf.UDAFPercentile.<clinit>(UDAFPercentile.java:51)
    ... 25 more
Caused by: java.lang.IllegalArgumentException: Unrecognized Hadoop major version number: 3.0.0-cdh6.3.2
    at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:177)
    at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:144)
    at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:99)
    ... 26 more

Shutting down the session...
done.","Flink-ver : 1.14.4-on-cdh6.3.2

Flink-sql-cli : 1.14.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Oct 20 07:20:59 UTC 2022,,,,,,,,,,"0|z19hmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 07:41;dwysakowicz;Please use English in the open-source JIRA. Feel free to translate the ticket to English and reopen the ticket.;;;","19/Oct/22 07:56;Vincent Long;Ok, it has been corrected. Thanks for reminding me.;;;","19/Oct/22 08:30;martijnvisser;Please also provide a way to reproduce this issue. Only the stacktrace doesn't give us the information what steps you took that resulted in that stacktrace;;;","19/Oct/22 08:31;martijnvisser;While looking at the stacktrace I noticed that you're using CDH and that the stacktrace mentions that the version isn't recognized. This ticket shouldn't be filed with the Flink community, but at CDH.;;;","19/Oct/22 08:54;luoyuxia;[~Vincent Long] Thanks for raising it. The reason is that you use  hive connector-2.2, but the hadoop version is 3.x.  Since Hive 2.3, hadoop 3 is also supported after  HIVE-16081. 

To fix it, you can upgrade your hive connector to hive-connector-2.3.x or use the haddop 2.;;;","20/Oct/22 07:20;Vincent Long;Ok, thank you very much for your opinion, I will test this scheme ~;;;",,,,,,,,,,,,,,,,,
Add JM info into ExecutionGraph,FLINK-29685,13487011,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,dangshazi,dangshazi,19/Oct/22 07:27,24/Nov/22 08:43,04/Jun/24 20:41,24/Nov/22 08:43,,,,,,,,,,,Runtime / Configuration,,,,0,execution,runtime,,,"h1. Backgroud

Web UI didn't show all JM info if enable JM HA now.
h1. Example

JM will restart after failed if enable [yarn.application-attempts|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#yarn-application-attempts]. We need to track all JM address for batch jobs after job finished or failed.
h1. Suggestion

So, I suggest add multi JM info into ExecutionGraph and archive it into ArchivedExecutionGraph when job finished.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:42:59 UTC 2022,,,,,,,,,,"0|z19hm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 08:35;martijnvisser;I don't see why this should be added to the ExecutionGraph. The information is available from logging (which is also where the reason for the failure is sent to), so why not use it from there? ;;;","25/Oct/22 10:23;dangshazi;Logging is also OK. But it is not convenient for users

 

We also have a log system which provide logs of last 2days. 

 

But data of log  is too huge to maintain. So we also archive this logs into a distribute data system. So we need the JM IPs or container Ids info to search log from diff JM

 ;;;","25/Oct/22 10:34;martijnvisser;You could choose to filter your logs to only use the relevant data. I still don't see a real value for Flink to add this feature. ;;;","24/Nov/22 08:42;xtsong;Agree with [~martijnvisser], I think this should not be added to Flink.

I think there're better ways to manage the logs in your log system rather than relying on Flink to provide JM IPs. E.g., grouping logs by Yarn application-id / application-attempt-id.

Closing this ticket for now. Please reopen if there's other opinions.;;;",,,,,,,,,,,,,,,,,,,
[UI] Upgrade runtime web Angular framework and associated deps to v14,FLINK-29684,13487004,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,junhan,junhan,junhan,19/Oct/22 06:52,24/Oct/22 05:45,04/Jun/24 20:41,21/Oct/22 03:33,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,Angular framework and NG-ZORRO both have released their stable v14 versions. It is a good time to bump them to the latest.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 03:32:43 UTC 2022,,,,,,,,,,"0|z19hkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 03:32;junhan;master: 97bc331f84285117dc4c30bc583cc2df45196356;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce config parser for AWS BackoffStrategy,FLINK-29683,13486906,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,18/Oct/22 20:10,18/Oct/22 20:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,,"*What*

Different connectors use AWS SDK, and there are some de-duplicable logic here.
 # Parsing retry backoff strategy configuration from configs.
 # Converting retry backoff strategy into the AWS SDK Java objects.

We want to introduce support for this in the `flink-connector-aws-base` so that other connectors can just use this config parser to retrieve the BackoffStrategy.

 

See [here|https://github.com/apache/flink-connector-dynamodb/pull/1/files#diff-d383895c9604d623476abe5740a12bf4db5ec17110d6d7d4df748bcf634dd776] for some inspiration",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-18 20:10:26.0,,,,,,,,,,"0|z19h0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test failure in test_finegrained_resourcemanagement,FLINK-29682,13486901,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,samrat007,samrat007,18/Oct/22 19:39,19/Oct/22 08:37,04/Jun/24 20:41,19/Oct/22 08:37,,,,,,,,,,,Tests,,,,0,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42113&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7] 

[https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/42113/logs/175] 

[^finegrained_test_fail.log]",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29419,,,,,,,,,,,,,,,,,"18/Oct/22 19:38;samrat007;finegrained_test_fail.log;https://issues.apache.org/jira/secure/attachment/13051097/finegrained_test_fail.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 02:49:27 UTC 2022,,,,,,,,,,"0|z19gzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 02:49;Weijie Guo;[~samrat007] We have tracked this issue in FLINK-29419, I think this ticket can be closed.;;;",,,,,,,,,,,,,,,,,,,,,,
Python side-output operator not generated in some cases,FLINK-29681,13486876,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,18/Oct/22 16:36,17/Feb/23 07:45,04/Jun/24 20:41,19/Oct/22 06:15,1.16.0,,,,,,1.16.0,,,,API / Python,,,,0,pull-request-available,,,,"If a SideOutputDataStream is used in `execute_and_collect`, `from_data_stream`, `create_temporary_view`, the side-outputing operator will not be properly configured, since we rely on the bottom-up scan of transformations while there's no solid transformation after the SideOutputTransformation in these cases. Thus, an error, ""tuple object has no attribute get_fields_by_names"", will be raised.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 06:15:40 UTC 2022,,,,,,,,,,"0|z19gug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 06:15;hxb;Merged into master via 93a9c504b882356fdf65ca962c4169ecb1bf66e5

Merged into release-1.16 via 06309528603333153dcabe1bd0e74bcde19cf6f5;;;",,,,,,,,,,,,,,,,,,,,,,
"Create Database with DatabaseName ""default"" fails",FLINK-29680,13486843,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,prabhujoseph,prabhujoseph,18/Oct/22 12:59,18/Oct/22 13:28,04/Jun/24 20:41,18/Oct/22 13:16,1.15.1,,,,,,,,,,Table SQL / Client,,,,0,,,,,"Create Database with DatabaseName ""default"" fails in Default Dialect.

*Exception:*
{code:java}
Caused by: org.apache.flink.sql.parser.impl.ParseException: Encountered ""default"" at line 1, column 17.
Was expecting one of:
<BRACKET_QUOTED_IDENTIFIER> ...
<QUOTED_IDENTIFIER> ...
<BACK_QUOTED_IDENTIFIER> ...
<HYPHENATED_IDENTIFIER> ...
<IDENTIFIER> ...
<UNICODE_QUOTED_IDENTIFIER> ...

at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:42459) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:42270) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.IdentifierSegment(FlinkSqlParserImpl.java:26389) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.CompoundIdentifier(FlinkSqlParserImpl.java:26869) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreateDatabase(FlinkSqlParserImpl.java:4188) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreateExtended(FlinkSqlParserImpl.java:6583) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreate(FlinkSqlParserImpl.java:23158) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3507) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:2911) ~[?:?]
at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:287) ~[?:?]
at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:193) ~[?:?]
at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:77) ~[?:?]
at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101) ~[?:?]
at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$parseStatement$1(LocalExecutor.java:172) ~[flink-sql-client-1.15.1.jar:1.15.1]
at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.15.1.jar:1.15.1]
... 11 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 13:28:25 UTC 2022,,,,,,,,,,"0|z19gn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 13:16;martijnvisser;{{default}} is a reserved keyword per https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/overview/#reserved-keywords - Have you tried using backticks ` to escape the keyword?;;;","18/Oct/22 13:28;prabhujoseph;It worked with backtick. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,
Migrate to new schema framework & show column comment,FLINK-29679,13486837,13478113,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,18/Oct/22 12:42,10/Mar/23 15:04,04/Jun/24 20:41,14/Dec/22 03:39,1.17.0,,,,,,1.17.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"comment is very helpful to make table schema user-friendly, many data analysers rely on such message to write sql adaptive to corresponding business logics.",,,,,,,,,,,,,,,,,,,,,FLINK-30848,,,,,,,FLINK-18958,,,,,FLINK-30435,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 03:39:35 UTC 2022,,,,,,,,,,"0|z19gls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 12:53;liyubin117;`buildDescribeResult()` has arg `ResolvedSchema`, and we can use it to implement the feature.

 [~jark] [~lzljs3620320] WDYT? Could you please assign this to me? looking forward to your reply!;;;","19/Oct/22 01:21;luoyuxia;[~liyubin117]  Is it for table's comment or column comment?;;;","19/Oct/22 02:03;liyubin117;[~luoyuxia] table's comment is a table level prop, we could use `show create table <table_name>` to display table's comment as follow:

```

CREATE TABLE `default_catalog`.`default_database`.`t2` (
  `id` INT
) COMMENT 'hello2'
WITH (
  'connector' = 'print'
)

```

`DESCRIBE` statement shows column level props, such as column's comment.;;;","19/Oct/22 08:39;martijnvisser;Does this still fit in the SQL standard? I would argue that an extension to the SQL dialect should be done via FLIP, since it's an interface we would expose to users. ;;;","19/Oct/22 09:06;liyubin117;[~martijnvisser] Thanks for your reminds, the feature doesn't change sql syntax exposed to users, just print more useful contents for users, and most analysts need such messages. furthermore, in many popular data engines like sparksql, `DESCRIBE` also shows column comment.
{code:java}
DESCRIBE TABLE customer;
+-----------------------+---------+----------+
|               col_name|data_type|   comment|
+-----------------------+---------+----------+
|                cust_id|      int|      null|
|                   name|   string|Short name|
|                  state|   string|      null|
|# Partition Information|         |          |
|             # col_name|data_type|   comment|
|                  state|   string|      null|
+-----------------------+---------+----------+
 {code};;;","09/Dec/22 08:43;martijnvisser;Great :) Looking forward, thanks for the clarification too. ;;;","14/Dec/22 03:39;lzljs3620320;master: b30b12d97a00ee5a338b2bd4233df1e2b38ce87f;;;",,,,,,,,,,,,,,,,
Data may loss when sink bounded stream into filesystem with auto compact enabled in streaming mode ,FLINK-29678,13486819,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,18/Oct/22 10:04,18/Oct/22 10:27,04/Jun/24 20:41,,1.15.0,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"In stream mode, when writing bounded data stream into filesystem with auto compactation enabel, the data may loss.

We can reproduce it by adding one code line `'auto-compaction'='true' ` in `FileSystemITCaseBase#open` to enable auto compact.
{code:java}
tableEnv.executeSql(
  s""""""
     |create table partitionedTable (
     |  x string,
     |  y int,
     |  a int,
     |  b bigint,
     |  c as b + 1
     |) partitioned by (a, b) with (
     |  'connector' = 'filesystem',
     |  'auto-compaction'='true', // added line to enable auto compaction.
     |  'path' = '$getScheme://$resultPath',
     |  ${formatProperties().mkString("",\n"")}
     |)
   """""".stripMargin
) {code}
Then the test `StreamFileSystemTestCsvITCase#testPartialDynamicPartition` will fail with the assert failure:
{code:java}
java.lang.AssertionError: 
Expected :List(x18,18)
Actual   :List() {code}
There is no data has been written into the table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 10:27:43 UTC 2022,,,,,,,,,,"0|z19ghs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 10:13;martijnvisser;[~luoyuxia] -Could you elaborate a bit on this? Is this a mistake in the file system implementation, in the Sink interfaces, etc? -

Ah, you've updated it later, thanks :);;;","18/Oct/22 10:17;martijnvisser;[~luoyuxia] Isn't that because checkpointing isn't enabled, which is required as documented at https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/filesystem/#file-sink ? ;;;","18/Oct/22 10:18;luoyuxia;In stream mode, with auto compaction, the pipeline  for  writing is  CompactFileWriter , CompactCoordinator, CompactOperator, PartitionCommitter.

 

if the datastream is bounded, CompactFileWriter write file1, file2, then call method endInput. `CompactCoordinator`  is expected to pack fiel1, file2 to downstream's CompactOperator. But `CompactCoordinator`  won't do that since it has no `endInput` method. So that, file1, file2 will never be compacted which is always Invisible to user. So, the data in file1, file2 will loss.

 

To fix it, we need to add a method `endInput` in `CompactCoordinator` to pack the remain files which are written between last snapshot and endinput. ;;;","18/Oct/22 10:27;luoyuxia;[~martijnvisser] Thanks for reminder. Sorry for that I forget to refer to enable checkpoint in the description.

I'm afriad of that the data loss has no deal with checkpointing.  The test will still fail even though I enable checkpointing in `

StreamingTestBase#before` by `env.enableCheckpointing(100)`;;;",,,,,,,,,,,,,,,,,,,
Prevent dropping the current catalog,FLINK-29677,13486797,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,18/Oct/22 09:06,24/Nov/22 19:28,04/Jun/24 20:41,24/Oct/22 07:00,1.16.0,1.17.0,,,,,1.16.1,1.17.0,,,Table SQL / API,,,,0,pull-request-available,,,,"h3. Issue Description

Currently, the drop catalog statement

 
{code:java}
DROP CATALOG my_cat{code}
 

does not reset the current catalog. As a result, if dropping a catalog in use, then the following statements will yield different results.

 
{code:java}
SHOW CURRENT CATALOG
SHOW CATALOGS
{code}
 
h3. How to Reproduce

!image-2022-10-18-16-55-38-525.png|width=444,height=421!

 
h3. Proposed Fix Plan

The root cause is that `CatalogManager#unregisterCatalog` does not reset `currentCatalogName`. 

Regarding this issue, I checked MySQL and PG's behavior.

For MySQL, it is allowed to drop a database current-in-use and set the current database to NULL.

!image-2022-10-18-17-02-30-318.png|width=288,height=435!

 

For PG, it is not allowed to drop the database currently in use.

 

I think both behaviors are reasonable, while for simplicity I suggest adhering to PG, that throw an Exception when dropping the current catalog.

cc [~jark]  [~fsk119]  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17357,,,,,,,,,,"18/Oct/22 08:55;qingyue;image-2022-10-18-16-55-38-525.png;https://issues.apache.org/jira/secure/attachment/13051072/image-2022-10-18-16-55-38-525.png","18/Oct/22 09:02;qingyue;image-2022-10-18-17-02-30-318.png;https://issues.apache.org/jira/secure/attachment/13051071/image-2022-10-18-17-02-30-318.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 22 14:41:25 UTC 2022,,,,,,,,,,"0|z19gcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 09:09;qingyue;I'd like to resolve this issue, and please assign this ticket to me. Thanks.;;;","18/Oct/22 09:27;fsk119;+1 to align with the PG behaviour.;;;","22/Oct/22 14:41;fsk119;Merged into the master: 271ef7b7dfac195b43abced8b81c38998855dabf
Merged into the release-1.16: cc17d700dd6ec1708ed6b32e6051f0b00371669f;;;",,,,,,,,,,,,,,,,,,,,
unexpected correlate variable $cor262 in the plan,FLINK-29676,13486777,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luochg,luochg,18/Oct/22 07:43,18/Oct/22 10:19,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,,"create two tables

{code:java}

drop table if exists TABLE_1;
CREATE TABLE TABLE_1 (
    `indexcalno` STRING,
    `indextype` STRING,
    `agentcode` STRING,
    `managecom` STRING not null,
    `t1` DECIMAL(12,2),  
    PRIMARY KEY (indexcalno,indextype,agentcode) NOT ENFORCED
    ) 
WITH (  
'connector' = 'jdbc',  
'url' = 'jdbc:mysql://IP:Port/db?useUnicode=true&characterEncoding=utf8&useSSL=false&useLegacyDatetimeCode=false&serverTimezone=UTC&allowPublicKeyRetrieval=true',  
'username'='username', 
'password'='password', 
'table-name' = 'TABLE_1'
);

drop table if exists TABLE_2;
CREATE TABLE TABLE_2 (
`id` BIGINT,
`agentcode` STRING,
`managecom` STRING,
`fyc` DECIMAL(12,2),
PRIMARY KEY (id) NOT ENFORCED
) WITH (  
'connector' = 'jdbc',  
'url' = 'jdbc:mysql://IP:PORT/db?useUnicode=true&characterEncoding=utf8&useSSL=false&useLegacyDatetimeCode=false&serverTimezone=UTC&allowPublicKeyRetrieval=true',  
'username'='username',  
'password'='password',  
'table-name' = 'TABLE_2'
);
{code}

exe sql:

{code:java}
SELECT 
(
  select sum(a.fyc) 
  from TABLE_2  a
  where a.managecom = _t.managecom 
  and a.agentcode=_t.agentcode  
)
from TABLE_1  _t 
{code}

error info:

{code:java}
Fail to run sql command: SELECT 
(
  select sum(a.fyc) 
  from TABLE_2 a
  where a.managecom = _t.managecom 
  and a.agentcode=_t.agentcode  
)
from TABLE_1 _t
org.apache.flink.table.api.TableException: unexpected correlate variable $cor262 in the plan
	at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.checkCorrelVariableExists(FlinkDecorrelateProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.optimize(FlinkDecorrelateProgram.scala:42)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:85)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:56)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:44)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:44)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:44)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:300)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:183)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:805)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
	at org.apache.zeppelin.flink.Flink114Shims.collectToList(Flink114Shims.java:223)
	at org.apache.zeppelin.flink.FlinkZeppelinContext.showData(FlinkZeppelinContext.scala:110)
	at org.apache.zeppelin.interpreter.ZeppelinContext.showData(ZeppelinContext.java:67)
	at org.apache.zeppelin.flink.FlinkBatchSqlInterpreter.callInnerSelect(FlinkBatchSqlInterpreter.java:60)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.callSelect(FlinkSqlInterpreter.java:494)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.callCommand(FlinkSqlInterpreter.java:257)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.runSqlList(FlinkSqlInterpreter.java:151)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.internalInterpret(FlinkSqlInterpreter.java:109)
	at org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:55)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:860)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
	at org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 09:16:09 UTC 2022,,,,,,,,,,"0|z19g8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 09:16;martijnvisser;[~luochg] Which Flink version are you using?;;;",,,,,,,,,,,,,,,,,,,,,,
unexpected correlate variable $cor262 in the plan,FLINK-29675,13486776,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,luochg,luochg,18/Oct/22 07:41,18/Oct/22 07:44,04/Jun/24 20:41,18/Oct/22 07:44,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-18 07:41:39.0,,,,,,,,,,"0|z19g88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Kafka Connector‘s “ setBounded” not valid,FLINK-29674,13486736,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jiagnwei,jiagnwei,18/Oct/22 01:56,11/Nov/22 08:32,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"When I'm using the Kafka connector, and to set kafka's consumption boundary ("" setBounded "") 。when my job runs normally (with no fail), the bounds are valid, and my job will finish. However, when my job fails and I restore it to the checkpoint used during the failure, I find that my job cannot be completed normally and is always running. However, I can see in the log that data has been consumed to the boundary set by me. I don't know if there is a problem with my usage, here is part of my code：

 
{code:java}
//代码占位符
String topicName = ""jw-test-kafka-w-offset-002"";
Map<TopicPartition, Long> offsets = new HashMap<TopicPartition, Long>();
offsets.put(new TopicPartition(topicName,0), 6L);
KafkaSource<String> source = KafkaSource.<String>builder()
        .setBootstrapServers(""xxx:9092"")
        .setProperties(properties)
        .setTopics(topicName)
        .setGroupId(""my-group"")
        .setStartingOffsets(OffsetsInitializer.earliest())
        .setValueOnlyDeserializer(new SimpleStringSchema())
        .setBounded(OffsetsInitializer.offsets(offsets))
        .build(); {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/22 12:38;jiagnwei;image-2022-10-18-20-38-34-515.png;https://issues.apache.org/jira/secure/attachment/13051090/image-2022-10-18-20-38-34-515.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Oct 26 08:13:23 UTC 2022,,,,,,,,,,"0|z19fzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 07:31;martijnvisser;[~jiagnwei] Which Flink version are you using? ;;;","18/Oct/22 07:35;jiagnwei;Hi [~martijnvisser] i'm using Flink1.15.0;;;","18/Oct/22 07:51;martijnvisser;Can you please verify this with the latest Flink 1.15?;;;","18/Oct/22 07:55;jiagnwei;OK，i’m try to verify this with the current 1.15.2;;;","18/Oct/22 12:39;jiagnwei;Hi [~martijnvisser] ,I'm try in 1.15.2 version，but There are still problem.;;;","19/Oct/22 09:01;martijnvisser;[~renqs] Any idea?;;;","20/Oct/22 14:56;coderap;Cloud you paste your test source code ? [~jiagnwei] ;;;","21/Oct/22 01:14;jiagnwei;[~coderap] this is my test source code:

 
{code:java}
package test;

import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.CheckpointConfig.ExternalizedCheckpointCleanup;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.TopicPartition;
import com.alibaba.fastjson.JSONObject;/** 
 * 
 * User: jiangwei
 * Date: Oct 12, 2022
 * Time: 10:22:20 AM
 */
public class KafkaWindowTest {

    @SuppressWarnings(""serial"")
    public static void main(String[] args) {
        
        Properties properties = new Properties();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, ""xxx:9092"");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                ""org.apache.kafka.common.serialization.StringSerializer"");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                ""org.apache.kafka.common.serialization.StringSerializer"");
        
        String jaasTemplate = ""org.apache.kafka.common.security.plain.PlainLoginModule required username=\""%s\"" password=\""%s\"";"";
        String jaasCfg = String.format(jaasTemplate, ""xxx"", ""xxx"");
        properties.put(""sasl.jaas.config"", jaasCfg);
        properties.put(""security.protocol"", ""SASL_PLAINTEXT"");
        properties.put(""sasl.mechanism"", ""PLAIN"");
        
        Map<TopicPartition, Long> offsets = new HashMap<TopicPartition, Long>();
        String topic = ""jw-test-kafka-w-offset-002"";
        offsets.put(new TopicPartition(topic,0), 6L);
        
        KafkaSource<String> source = KafkaSource.<String>builder()
                .setBootstrapServers(""xxx:9092"")
                .setProperties(properties)
//                .setProperty(""commit.offsets.on.checkpoint"", ""false"")
                .setTopics(topic)
//                .setTopicPattern(java.util.regex.Pattern.compile(topic+"".*""))
                .setGroupId(""my-group"")
                .setStartingOffsets(OffsetsInitializer.earliest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .setBounded(OffsetsInitializer.offsets(offsets))
                .build();
        try {
            final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
            //env.setParallelism(1);
            env.enableCheckpointing(5000);
            env.getCheckpointConfig()
                    .setExternalizedCheckpointCleanup(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

            WatermarkStrategy<String> wk = org.apache.flink.api.common.eventtime.WatermarkStrategy
                    .<String>forBoundedOutOfOrderness(Duration.ZERO)
                    .withTimestampAssigner(new SerializableTimestampAssigner<String>() {
                        @Override
                        public long extractTimestamp(String element, long recordTimestamp) {
                            return JSONObject.parseObject(element).getInteger(""time"") * 1000;
                        }
                    });             SingleOutputStreamOperator<String> fromSource = env
                    .fromSource(source, wk, ""Kafka Source"");
            fromSource.print(""first"");
            fromSource.keyBy(data -> JSONObject.parseObject(data).getString(""id""))
                    .window(TumblingEventTimeWindows.of(Time.seconds(3)))
                    .process(new ProcessWindowFunction<String, Iterable<String>, String, TimeWindow>() {
                        @Override
                        public void process(String arg0,
                               ProcessWindowFunction<String, Iterable<String>, String, TimeWindow>.Context arg1,
                                Iterable<String> datas, Collector<Iterable<String>> out) throws Exception {
                            out.collect(datas);
                        }
                    }).print(""window-data"");
            env.execute();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

{code}
 

this is my test data:

 
{code:java}
""{'id':'1','kpi':11,'kpi1':90,'time':1}"", 
""{'id':'1','kpi':11,'kpi1':90,'time':2}"",
""{'id':'1','kpi':11,'kpi1':90,'time':3}"",
""{'id':'1','kpi':11,'kpi1':90,'time':4}"",
""{'id':'1','kpi':11,'kpi1':90,'time':5}"",
""{'id':'1','kpi':11,'kpi1':90,'time':6}"", 
""end""{code}
 

 

this is success result

 
{code:java}
first> {""kpi"":11,""id"":""1"",""time"":1,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":2,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":3,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":4,""kpi1"":90}
window-data> [{""kpi"":11,""id"":""1"",""time"":1,""kpi1"":90}, {""kpi"":11,""id"":""1"",""time"":2,""kpi1"":90}]
first> {""kpi"":11,""id"":""1"",""time"":5,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":6,""kpi1"":90}
window-data> [{""kpi"":11,""id"":""1"",""time"":3,""kpi1"":90}, {""kpi"":11,""id"":""1"",""time"":4,""kpi1"":90}, {""kpi"":11,""id"":""1"",""time"":5,""kpi1"":90}]
window-data> [{""kpi"":11,""id"":""1"",""time"":6,""kpi1"":90}] {code}
 

 

this is use checkpoint result，the last ""window-data"" not print ,and the job always running：
{code:java}
first> {""kpi"":11,""id"":""1"",""time"":1,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":2,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":3,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":4,""kpi1"":90}
window-data> [{""kpi"":11,""id"":""1"",""time"":1,""kpi1"":90}, {""kpi"":11,""id"":""1"",""time"":2,""kpi1"":90}]
first> {""kpi"":11,""id"":""1"",""time"":5,""kpi1"":90}
first> {""kpi"":11,""id"":""1"",""time"":6,""kpi1"":90}
window-data> [{""kpi"":11,""id"":""1"",""time"":3,""kpi1"":90}, {""kpi"":11,""id"":""1"",""time"":4,""kpi1"":90}, {""kpi"":11,""id"":""1"",""time"":5,""kpi1"":90}]{code}
 

 ;;;","26/Oct/22 08:13;jiagnwei;[~martijnvisser] [~coderap] I looked at the part of the source code, I found in, but in ""KafkaSourceEnumerator. GetPartitionChange"" method of ""dedupOrMarkAsRemoved"" problems of the implementation of the object, ""removedPartitions"" has not been added to ""removedpartitions"" after removing the element. I have modified the judgement here and and is now normal

 

 
{code:java}
 @VisibleForTesting
    PartitionChange getPartitionChange(Set<TopicPartition> fetchedPartitions) {
        final Set<TopicPartition> removedPartitions = new HashSet<>();
        Consumer<TopicPartition> dedupOrMarkAsRemoved =
                (tp) -> {
                    if (!fetchedPartitions.remove(tp)) {
                        removedPartitions.add(tp);
                    }
                };
.... {code}
 

 

modified code

 
{code:java}
@VisibleForTesting
    PartitionChange getPartitionChange(Set<TopicPartition> fetchedPartitions) {
        final Set<TopicPartition> removedPartitions = new HashSet<>();
        Consumer<TopicPartition> dedupOrMarkAsRemoved =
                (tp) -> {
                    if (fetchedPartitions.remove(tp)) {
                        removedPartitions.add(tp);
                    }
                };
....{code}
 

so this is bug？

 ;;;",,,,,,,,,,,,,,
Support sqlserver catalog,FLINK-29673,13486733,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,waywtdcc,waywtdcc,waywtdcc,18/Oct/22 01:21,04/Nov/22 01:29,04/Jun/24 20:41,04/Nov/22 01:29,1.16.0,,,,,,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 01:22:06 UTC 2022,,,,,,,,,,"0|z19fyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 01:22;waywtdcc;please assign this to me.;;;",,,,,,,,,,,,,,,,,,,,,,
Support oracle catalog ,FLINK-29672,13486730,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,waywtdcc,waywtdcc,18/Oct/22 01:06,11/Apr/23 15:53,04/Jun/24 20:41,11/Apr/23 15:53,1.16.0,,,,,,,,,,Table SQL / API,,,,0,,,,,Support oracle catalog ,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31530,FLINK-17508,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 10 10:32:59 UTC 2023,,,,,,,,,,"0|z19fy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 01:08;waywtdcc;please assign this to me;;;","10/Apr/23 10:32;RocMarshal;It looks like a duplicated Jira as https://issues.apache.org/jira/browse/FLINK-17508;;;",,,,,,,,,,,,,,,,,,,,,
Kubernetes e2e test fails during test initialization,FLINK-29671,13486663,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,17/Oct/22 15:50,21/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,Deployment / Kubernetes,Test Infrastructure,,,0,auto-deprioritized-critical,starter,test-stability,,"There are two build failures ([branch based on release-1.16|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42038&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5377] and [a release-1.15 build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42073&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4684]) that are (not exclusively) caused by the e2e test {{Kubernetes test}} failing in the initialization phase (i.e. when installing all artifacts):

See the attached logs from the {{release-1.15}} build's CI output. No logs are present as artifacts.

*UPDATE*: The error we're seeing is actually FLINK-28269 because we failed to download {{crictl}}. Looks like we reached some download limit:
{code}
2022-10-16T05:58:33.0821044Z --2022-10-16 05:58:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/80172100/7186c302-3766-4ed5-920a-f85c9d6334ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221016%2Fus-east-1%2Fs3%2Faws4
_request&X-Amz-Date=20221016T055833Z&X-Amz-Expires=300&X-Amz-Signature=5c24f88d9891e793000a904879be5e8913d643cbff1bfdfb8d0cf3c6e7a7908b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=80172100&response-content-disposition=attachment%3B%20filename%3Dcrictl-v1.24.2-linux-amd64.t
ar.gz&response-content-type=application%2Foctet-stream
2022-10-16T05:58:33.1021704Z Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...
2022-10-16T05:58:33.1128247Z Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.
2022-10-16T05:58:38.5605126Z HTTP request sent, awaiting response... 503 Egress is over the account limit.
2022-10-16T05:58:38.5606342Z 2022-10-16 05:58:38 ERROR 503: Egress is over the account limit..
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30178,,,,,,,FLINK-28269,FLINK-32107,,FLINK-30881,FLINK-30893,,,,,,"17/Oct/22 15:50;mapohl;kubernetes_test_failure.log;https://issues.apache.org/jira/secure/attachment/13051036/kubernetes_test_failure.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 22:35:24 UTC 2023,,,,,,,,,,"0|z19fj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 15:52;mapohl;It doesn't seem to happen regularly. Other more recent builds didn't fail with that error (e.g. the [most recent release-1.15 nightly build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42076&view=results]). That's why I keep the issue priority at {{CRITICAL}} for now.;;;","17/Oct/22 15:53;mapohl;FLINK-28269 had a similar error message;;;","18/Oct/22 06:16;mapohl;I'm lowering the priority to {{Major}} because it seem to have been a temporary issue. Both build failures happened around the same time on Oct 16, 2022 at roughly 6am;;;","18/Oct/22 06:25;mapohl;This [Github community issue #8535|https://github.com/orgs/community/discussions/8535] is related supporting the fact that it's an infrastructure issue. We might want to add some timed retry to the downloads from github.;;;","19/Oct/22 06:51;wangyang0918;+1 to count the failed e2e tests as infrastructure issues. We could find ""ERROR 503: Egress is over the account limit"" in the logs.

Adding a retry mechanism might help if it is just a temporary issue.;;;","23/Jan/23 08:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45143&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62

This time, it's a 403 (Forbidden) HTTP status code:
{code}
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.
HTTP request sent, awaiting response... 403 Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
2023-01-22 01:53:30 ERROR 403: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature..
{code};;;","23/Jan/23 08:25;mapohl;I'm leaving it in priority Major for now  because it only appeared twice so far.;;;","01/Feb/23 13:07;mapohl;Another similar failure with {{{}crictl{}}}. There is no account limit error, though:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4972]
{code:java}
Feb 01 11:00:45 Starting minikube ...
Feb 01 11:00:45 * minikube v1.29.0 on Ubuntu 20.04
Feb 01 11:00:45 * Using the none driver based on existing profile
Feb 01 11:00:45 * Starting control plane node minikube in cluster minikube
Feb 01 11:00:45 * Restarting existing none bare metal machine for ""minikube"" ...
Feb 01 11:00:45 * OS release is Ubuntu 20.04.5 LTS
Feb 01 11:01:22 
X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo /usr/local/bin/crictl version: exit status 1
stdout:
[...] {code};;;","02/Feb/23 09:00;mapohl;permissioned denied issues multiple times in same run:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4900]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5597;;;","02/Feb/23 09:09;mapohl;Also permission issues. Build #45588 and #45587 ran at almost the same time
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4818
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=5852;;;","02/Feb/23 09:10;mapohl;Same here: permission issue
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45591&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4915;;;","02/Feb/23 09:11;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45598&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4921;;;","02/Feb/23 10:23;wangyang0918;It seems that the minikube in the azure pipeline upgraded from v1.28.0 to v1.29.0. So it might be with incompatible crictl@v1.24.2.;;;","02/Feb/23 11:05;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4991;;;","02/Feb/23 11:06;mapohl;{quote}It seems that the minikube in the azure pipeline upgraded from v1.28.0 to v1.29.0. So it might be with incompatible crictl@v1.24.2.
{quote}
-Thanks for looking into it, Yang. I assigned the Jira issue to you.-

Correction: I unassigned you from the ticket again and created FLINK-30881. The initial cause of FLINK-29671 is still valid and we shouldn't mix these things up.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,
Add CRD compatibility check against 1.2.0,FLINK-29670,13486652,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sriramgr,gyfora,gyfora,17/Oct/22 15:12,25/Oct/22 07:10,04/Jun/24 20:41,25/Oct/22 07:10,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,,,,,We must extend the compatibility check against 1.2.0 too in the flink-kubernetes-operator module,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 07:10:44 UTC 2022,,,,,,,,,,"0|z19fgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 14:51;sriramgr;[~gyfora]  - Please assign this to me.;;;","24/Oct/22 07:18;sriramgr;[https://github.com/apache/flink-kubernetes-operator/pull/410.]  

[~gyfora] - How many releases will be maintained in the Kubernetes operator pom.xml for CRD compatibility check?;;;","25/Oct/22 07:10;mbalassi;[{{3b649bd}}|https://github.com/apache/flink-kubernetes-operator/commit/3b649bdf422abb93933162c31888798209df9494] in main;;;",,,,,,,,,,,,,,,,,,,,
Remove HCatalog,FLINK-29669,13486632,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Oct/22 13:34,18/Oct/22 13:27,04/Jun/24 20:41,18/Oct/22 13:27,,,,,,,1.17.0,,,,Connectors / Common,,,,0,pull-request-available,,,,Remove HCatalog from the codebase as voted on in https://lists.apache.org/thread/w3jfgdk6lq846oh356qnwczydm9oszp9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 13:27:29 UTC 2022,,,,,,,,,,"0|z19fc8:",9223372036854775807,The HCatalog connector has been removed from Flink. You can use the Hive connector as a replacement. ,,,,,,,,,,,,,,,,,,,"18/Oct/22 13:27;martijnvisser;Fixed in master: a90d9f968725cc9622669159ebf2300c4d9d38d2;;;",,,,,,,,,,,,,,,,,,,,,,
Remove Gelly,FLINK-29668,13486630,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Oct/22 13:31,14/Nov/22 08:48,04/Jun/24 20:41,14/Nov/22 08:48,,,,,,,1.17.0,,,,Library / Graph Processing,,,,0,pull-request-available,,,,"- Remove all Gelly code from {{master}}
- Remove Gelly documentation
- Update feature radar to include remark that as a successor of Gelly the community wants to deliver a replacement based on the new iteration framework from Flink-ML",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 08:48:17 UTC 2022,,,,,,,,,,"0|z19fbs:",9223372036854775807,"Gelly has been removed from Flink. Current users of Gelly should not upgrade to Flink 1.17 but stay on an older version. If you're looking for iterations support, you could investigate [Flink ML Iteration's](https://nightlies.apache.org/flink/flink-ml-docs-stable/docs/development/iteration/) as a potential successor. ",,,,,,,,,,,,,,,,,,,"19/Oct/22 07:30;martijnvisser;Fixed in master: a215d9424af06aba9bddaa985a74cd0d27f6f37b;;;","14/Nov/22 08:48;martijnvisser;Fixed in asf-site: 66b16b429bb3d9a14ee54a9a4d47fc7d276471c0;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade DynamoDB Connector dependencies from SNAPSHOT to release 1.16,FLINK-29667,13486617,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,darenwkt,darenwkt,17/Oct/22 12:41,14/Nov/22 15:27,04/Jun/24 20:41,14/Nov/22 15:27,aws-connector-3.0.0,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,,,,,"DynamoDB connector currently depends on flink-ci-tools and flink-test-utils for licenseCheck and PackagingTest respectively. The working version for both are only available for SNAPSHOT and not release.

This task is to upgrade the dependencies to release 1.16 version when available",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 15:27:32 UTC 2022,,,,,,,,,,"0|z19f8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 15:27;dannycranmer;Merged commit [{{dc09993}}|https://github.com/apache/flink-connector-aws/commit/dc099938054398045cfceb87baaa95f853e5f1c8] into main;;;",,,,,,,,,,,,,,,,,,,,,,
Let adaptive batch scheduler divide subpartition range according to amount of data,FLINK-29666,13486608,13486602,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,17/Oct/22 11:49,10/Jan/23 14:50,04/Jun/24 20:41,10/Jan/23 14:50,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 14:50:24 UTC 2023,,,,,,,,,,"0|z19f6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 14:50;wanglijie;Done via master 75fff37...ca18dd7;;;",,,,,,,,,,,,,,,,,,,,,,
Support flexible subpartition range divisions,FLINK-29665,13486606,13486602,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,17/Oct/22 11:48,29/Dec/22 13:02,04/Jun/24 20:41,29/Dec/22 13:02,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"Currently, the subpartition range division algorithm is fixed (data-independent), and the *TaskDeploymentDescriptor(TDD)* can be generated according to the number of subpartitions and downstream parallelism. In order to support dividing according to the amount of data in the subpartition range, the scheduler needs to support flexible subpartition range divisions. The scheduler will be responsible for deciding the subpartition range division infos for each job vertex, and then pass the division infos to job vertices when initializing them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 13:01:57 UTC 2022,,,,,,,,,,"0|z19f6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 13:01;wanglijie;Done via master:

3881e53003adce22fd0a0fbd9a229d70d27a9cce

1d433fb4b72d74b1aa0cfa8ac7a9f7fed22bce32;;;",,,,,,,,,,,,,,,,,,,,,,
Collect subpartition sizes of BLOCKING result partitions,FLINK-29664,13486605,13486602,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,17/Oct/22 11:47,20/Dec/22 07:13,04/Jun/24 20:41,20/Dec/22 07:13,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"In order to divide subpartition range according to the amount of data, the scheduler needs to collect the size of each subpartition produced by upstream tasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 07:13:31 UTC 2022,,,,,,,,,,"0|z19f68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 07:13;wanglijie;Done via 963333795ae5bf0b6d6c8b1fa793be742a7c1af3;;;",,,,,,,,,,,,,,,,,,,,,,
Further improvements of adaptive batch scheduler,FLINK-29663,13486602,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,17/Oct/22 11:41,31/Jan/23 07:38,04/Jun/24 20:41,31/Jan/23 07:37,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,,,,,"In Flink 1.15, we introduced the adaptive batch scheduler to automatically decide parallelisms of job vertices for batch jobs.  In this issue, we will  further optimize it by changing the subpartition range division algorithm: change it from dividing according to the number of subpartitions(the number of subpartitions within each subpartition range is basically the same) to dividing according to the amount of data in subpartition ranges (the amount of data within each subpartition range is basically the same).

More details see [https://docs.google.com/document/d/1Qyq3qDkBCUNupajVJpFTKp3fHQJtwIu9luM7T52k1Oo]

 This is the umbrella ticket for the improvements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24892,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-17 11:41:23.0,,,,,,,,,,"0|z19f5k:",9223372036854775807,"In 1.17, we further enhanced the adaptive batch scheduler, mainly including: 
(1) Support for evenly distributing data to downstream tasks
(2) Remove the limitation that the decided parallelism of vertices can only be a power of 2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PojoSerializerSnapshot is using incorrect ExecutionConfig when restoring serializer,FLINK-29662,13486601,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pnowojski,pnowojski,17/Oct/22 11:33,17/Oct/22 11:33,04/Jun/24 20:41,,1.17.0,,,,,,,,,,API / Type Serialization System,,,,0,,,,,"{{org.apache.flink.api.java.typeutils.runtime.PojoSerializerSnapshot#restoreSerializer}} is using freshly created {{new ExecutionConfig()}} execution config for the restored serializer, which overrides any configuration choices made by the user. Most likely this is a dormant bug, since restored serializer shouldn't be used for serializing any new data, and the {{ExecutionConfig}} is only used for subclasses serializations that haven't been cached. If this is indeed the case, I would propose to change it's value to {{null}} and safeguard accesses to that field with an {{IllegalStateException}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-17 11:33:58.0,,,,,,,,,,"0|z19f5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseCalciteSchema$getTable() cannot get statistics for partition table,FLINK-29661,13486593,13444592,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,337361684@qq.com,337361684@qq.com,17/Oct/22 11:00,23/Nov/22 01:44,04/Jun/24 20:41,23/Nov/22 01:44,1.17.0,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,,,,,"DatabaseCalciteSchema$getTable() cannot get statistics for partition table.  DatabaseCalciteShema$extractTableStats() don't consider the situation that the table is partition table, and it's stats need to be collected by catalog.getPartitionStatistics() and catalog.getPartitionColumnStatistics()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 01:43:56 UTC 2022,,,,,,,,,,"0|z19f3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 08:42;martijnvisser;Does this change when the upgrade to the newer Calcite version has been completed? ;;;","23/Nov/22 01:43;337361684@qq.com;[~martijnvisser] Hi, this is not a bug as we already introduced a new rule in table planner to get partition table stats, I will close this jira, Thanks.;;;",,,,,,,,,,,,,,,,,,,,,
Show all attempts of subtasks in WebUI,FLINK-29660,13486590,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,dangshazi,dangshazi,17/Oct/22 10:47,29/Dec/22 08:50,04/Jun/24 20:41,29/Dec/22 08:50,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,,"Web UI only show subtask metric and TM log now.



For batch jobs, It's very important to track the metric of fail attempt and jump into log stream of every attemp.

 

Feature needed: enable expanded rows of all attempts for subtasks",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30358,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 08:50:50 UTC 2022,,,,,,,,,,"0|z19f2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:14;zhuzh;To check the failed attempt, I think it's better to check the exception history, which shows the failed tasks, their locations and the failure cause.;;;","25/Oct/22 10:32;dangshazi;Exception tab is also OK. But there are some deficiencies:
 * It is not convenient for users to find the exception of specific subtask
 * It can't provide root cause sometimes

 

In our batch case, there are many exceptions if we enable failover.  It's diffcult to identify the exception rootcause for a specific subTask.

If webUI shows all attmepts fo subtasks, we can jump into log of failed attemp, and troubleshoot rootcause & slow case

 ;;;","27/Oct/22 02:14;zhuzh;In the exception history, it already shows all the root exceptions and the concurrent exceptions. You can also check any concurrent exception by filtering the task name if you think the displayed root cause is not the true root.

Showing all attempts on the vertex tab may result in performance problem because there can be too many attempts to be retrieved and shown at one time, given that tasks may failover multiple times. At the moment, the tab can be slower and harder to navigate if the parallelism is large and I prefer not to add much extra burden to it.

Maybe a better way is to show the task manager id on the exception history page, and a link to the task manager id to jump to the task manager page.
;;;","29/Dec/22 08:50;zhuzh;In FLINK-30358, the taskmanager IDs of failed tasks are now displayed the exception page. 
This IDs are also links to the corresponding taskmanager's pages.
;;;",,,,,,,,,,,,,,,,,,,
Deduplicate SavepointWriter factory method code,FLINK-29659,13486582,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Oct/22 09:54,20/Oct/22 09:53,04/Jun/24 20:41,20/Oct/22 09:53,,,,,,,1.17.0,,,,API / State Processor,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 09:53:28 UTC 2022,,,,,,,,,,"0|z19f14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 09:53;chesnay;master: 8feddfbec21b3f3a2a5ddbea06e9be54b43a9ca0;;;",,,,,,,,,,,,,,,,,,,,,,
LocalTime support when converting Table to DataStream in PyFlink,FLINK-29658,13486581,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,17/Oct/22 09:54,18/Oct/22 06:53,04/Jun/24 20:41,18/Oct/22 06:53,1.15.2,,,,,,1.15.3,,,,API / Python,,,,0,pull-request-available,,,,Support for Java LocalDate/Time/DateTime is needed when calling `to_data_stream` on tables containing DATE/TIME/TIMESTAMP fields in PyFlink.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28827,FLINK-29648,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 06:53:55 UTC 2022,,,,,,,,,,"0|z19f0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 06:53;hxb;Merged into release-1.15 via 5e4da2cc868b28bfdadd689f24bbeb80079d4d5e;;;",,,,,,,,,,,,,,,,,,,,,,
Flink hive parser considers literal floating point number differently than Hive SQL,FLINK-29657,13486566,13430553,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,Runking,Runking,17/Oct/22 08:40,16/Nov/22 12:05,04/Jun/24 20:41,16/Nov/22 12:05,,,,,,,,,,,,,,,0,,,,,"Hive SQL consider literal floating number(such as 1.1) as double, but Flink hive parser consider this as decimal, so it causes that some hive udf that accepts double arg, will not pass type check in hive parser.

Hive SQL's behavior:

hive> explain select 1.1 + false;
2022-10-17 16:37:14,286    FAILED: SemanticException [Error 10014]: Line 1:15 Wrong arguments 'false': No matching method for class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNumericPlus with ({*}double{*}, boolean)

Flink hive parser's behavior:

in NumExprProcessor#process, it process non-postfix number as decimal by default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 08:43:59 UTC 2022,,,,,,,,,,"0|z19exk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 08:52;Runking;Hi, [~luoyuxia], I am try to migrate hive sql tasks to flink. I find some problems, such as this Jira, would you like to have a look?;;;","17/Oct/22 09:00;luoyuxia;Cool!   [~Runking] Sure, I will have a look, any problems related to Hive dialect will be my first priority.  

 ;;;","17/Oct/22 09:16;luoyuxia;Just a quick question, what's the hive version are you using? I guess some lower version than 2.3.

Actaully, since Hive 2.3, literal floating number(such as 1.1) has been considered as {*}decimal type{*}. You can refer to HIVE-13945 for more detail. 

I tried with Hive 2.3, the exception message will be 
{code:java}
FAILED: SemanticException [Error 10014]: Line 1:8 Wrong arguments 'false': No matching method for class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNumericPlus with (decimal(2,1), boolean) {code};;;","17/Oct/22 09:19;luoyuxia;When support hive dialect, we follow Hive 2.3's behavior since it's wide-used and stable , so we may miss some thing in other versions.

But for your problem, it's quick to fix. We just need to adjust the  logic in `NumExprProcessor#process`;;;","17/Oct/22 13:43;Runking;Thanks for quick reply, the hive version used is 1.2, as you pointed, the behavior is not the same as hive 2.3;;;","18/Oct/22 01:55;luoyuxia;I see.  The most Hive sql statements in Hive 1.2 should also work in Hive 2.3  since the Hive sql is expected be backward compatible,  but some behavior may different between Hive 1.2, Hive 2.3.  And from my experience, most of the changes are  bugs fix or some other things that Hive commutily decide to change the behavior after a carefully discussion.

[~Runking] If you have any other problems, please let me know.  I'm really glad to help. :);;;","18/Oct/22 13:14;Runking;[~luoyuxia] No problem, thanks for your work.;;;","19/Oct/22 08:43;martijnvisser;Since Flink doesn't support Hive 1.* anymore, can we close this ticket as invalid (as the behaviour in Hive 2.* which is still supported in Flink is correct) ?;;;",,,,,,,,,,,,,,,
Modify all join-related sql tests to make having statistics as the default choice,FLINK-29656,13486562,13444592,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,17/Oct/22 08:00,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,Modify all join-related sql tests to make having statistics as the default choice. This issue is related to FLINK-29559 to make tests stable.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-17 08:00:31.0,,,,,,,,,,"0|z19ewo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split Flink CRD from flink-kubernates-operator module,FLINK-29655,13486551,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mbalassi,ZhenqiuHuang,ZhenqiuHuang,17/Oct/22 06:59,13/Dec/22 01:46,04/Jun/24 20:41,01/Nov/22 09:51,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"To launch a flink job that managed by flink operator, it is required to introduce Flink CRD in backend service as dependency to create CR. In current model, all of the flink libs  will be introduced to service. But actually what is required are model classes in the package in org.apache.flink.kubernetes.operator.crd.

It will be user friendly to split org.apache.flink.kubernetes.operator.crd to a separate module as flink-kubernetes-crd. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 09:51:40 UTC 2022,,,,,,,,,,"0|z19eu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 10:02;gyfora;I think this would make sense, instead of flink-kubernetes-crd, I would suggest flink-kubernetes-api / flink-kubernetes-operator-api;;;","17/Oct/22 14:58;ZhenqiuHuang;[~gyfora]

I feel flink-kubernetes-operator-api is more accurate. Would you please assign the jira to me?;;;","17/Oct/22 15:02;mbalassi;Makes sense, this way you could also make the dependencies of the module quite slim. Thanks.;;;","01/Nov/22 09:51;gyfora;Merged to main:
a7e6f0c8dd08cecb26822731e058c003ac0fbee5
211a326a81c77c6d048e3d2ae9bb556f33ddd5aa
b4bef2b93c22cb603b832b31ede9c38d0b604970;;;",,,,,,,,,,,,,,,,,,,
Vulnerable libraries - Flink 1.15.2,FLINK-29654,13486545,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,nagasudhakar,nagasudhakar,17/Oct/22 05:29,27/Oct/22 16:13,04/Jun/24 20:41,17/Oct/22 13:21,1.15.2,,,,,,,,,,Build System,,,,0,,,,,"Hi, our organisation ran a security scan on Flink-1.15.2 release and found the following vulnerable open source libraries being used -
JDOM1.1
kryo2.24.0
libnetty-3.9-java3.9.0.Final
Netty Project3.10.6.Final
Play2.6.11
Apache Tika1.28.1
Apache Avro1.7.7
Apache Kafka2.8.1
The recommended versions for these libraries are -
JDOM2.0.2
kryo-5.5.0
libnetty-3.9-java3.9.9.Final
Netty Project 5.0.0.Final
Play2.8.16
Apache Tika2.4.1
Apache Avro1.8.2
Apache Kafka2.8.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 16:13:17 UTC 2022,,,,,,,,,,"0|z19esw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 13:21;martijnvisser;[~nagasudhakar] Thanks for this but tickets like these aren't really helpful; the fact that something is marked as vulnerable doesn't mean that Flink itself is also vulnerable (because Flink might not use that specific part that has a vulnerability). It also doesn't help in terms of updating, because we need to link each updated dependency to an individual Jira ticket. Last but not least, Flink master is the latest version which already has updated dependencies for some of those listed here. It's not always possible to backport these since they could include a breaking API change which can only be fixed in the next Flink minor version;;;","18/Oct/22 16:53;nagasudhakar;[~martijnvisser] Thanks for the quick answer, appreciate it.
Due to these identified vulneralabilites, we are little skeptical about promoting this image to live.
1)Could you please confirm  promoting this image will not make flink vulnerable to above vulnerabilities? Please suggest here.
2) I see master having 1.17.0-snapshot version now, do we have any date for the final version of 1.17? and let us know what vulnerabilities covered as part of this version?
;;;","19/Oct/22 09:07;martijnvisser;[~nagasudhakar] There will still be reported vulnerabilities in Flink by scanners, but that doesn't mean that Flink is vulnerable to those like I've explained. Master is indeed set to 1.17, which will take several more months before being released. The next version will be 1.16, which I expect in a couple of weeks maximum. There are already release candidates for those available. ;;;","27/Oct/22 16:13;nagasudhakar;Thanks [~martijnvisser];;;",,,,,,,,,,,,,,,,,,,
close DynamicResult but it was already closed,FLINK-29653,13486542,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,shijingqi,shijingqi,shijingqi,17/Oct/22 05:09,15/Aug/23 10:35,04/Jun/24 20:41,,1.15.0,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,"We have built a test environment with apache kyuubi and flink, and experienced a problem with LocalExecutor class.

 
{code:java}
2022-09-08 22:53:37,729 WARN  org.apache.kyuubi.engine.flink.operation.ExecuteStatement    [] - Failed to clean result set 4515c4aa72d73cf368aba5ddabb675ce in session 681753c2-d945-4200-861c-6ad739e2a92c
org.apache.flink.table.client.gateway.SqlExecutionException: Could not find a result with result identifier '4515c4aa72d73cf368aba5ddabb675ce'.
    at org.apache.flink.table.client.gateway.local.LocalExecutor.cancelQuery(LocalExecutor.java:306) ~[flink-sql-client-1.15.1-SNAPSHOT.jar:1.15.1-SNAPSHOT]
    at org.apache.kyuubi.engine.flink.operation.ExecuteStatement.cleanupQueryResult(ExecuteStatement.scala:180) [kyuubi-flink-sql-engine_2.12-1.5.1-incubating-SNAPSHOT.jar:1.5.1-incubating-SNAPSHOT]
    at org.apache.kyuubi.engine.flink.operation.ExecuteStatement.runQueryOperation(ExecuteStatement.scala:167) [kyuubi-flink-sql-engine_2.12-1.5.1-incubating-SNAPSHOT.jar:1.5.1-incubating-SNAPSHOT]
    at org.apache.kyuubi.engine.flink.operation.ExecuteStatement.org$apache$kyuubi$engine$flink$operation$ExecuteStatement$$executeStatement(ExecuteStatement.scala:111) [kyuubi-flink-sql-engine_2.12-1.5.1-incubating-SNAPSHOT.jar:1.5.1-incubating-SNAPSHOT]
    at org.apache.kyuubi.engine.flink.operation.ExecuteStatement$$anon$1.run(ExecuteStatement.scala:81) [kyuubi-flink-sql-engine_2.12-1.5.1-incubating-SNAPSHOT.jar:1.5.1-incubating-SNAPSHOT]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_202]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_202]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202] {code}
 

 

ResultStore stores resultId and result, skips sessionId. But if one result reached closeSession(), all results in ResultStore will be closed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:12 UTC 2023,,,,,,,,,,"0|z19es8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 05:12;shijingqi;Anyone can assign this to me?;;;","18/Oct/22 01:59;luoyuxia;cc [~fsk119] ;;;","19/Oct/22 05:57;fsk119;Thanks for your report. I have assigned it to you.;;;","01/Nov/22 09:45;shijingqi;Hi, [~fsk119], I have opened a pr [https://github.com/apache/flink/pull/21212] .  It's ready for review.

Thanks!;;;","07/Dec/22 02:51;shijingqi;Hi, [~fsk119], [~luoyuxia],  sorry to bother you, but i wonder is there any progress on this issue?

Thank you very much.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
get duplicate result from sql-client in BATCH mode,FLINK-29652,13486541,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,shijingqi,shijingqi,shijingqi,17/Oct/22 05:08,15/Aug/23 10:35,04/Jun/24 20:41,,1.13.0,1.14.0,1.15.0,1.16.0,,,,,,,Table SQL / Client,,,,0,pull-request-available,stale-assigned,,,"In BATCH mode, we experienced problems with flink-sql-client when retrieving result record. We may get duplicate row records occasionally even if querying from a hive/hudi table which contains only one record.

 

For example, SELECT COUNT(1) AS val FROM x.test_hive_table, we may get:
{code:java}
+------+
| val  |
+------+
| 1    |
| …    |
| 1    |
+------+ {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:12 UTC 2023,,,,,,,,,,"0|z19es0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 05:12;shijingqi;Anyone can assign this to me? ;;;","17/Oct/22 08:32;luoyuxia;[~shijingqi] Feel free to open a pr. we will review for you. But Could you please explain a bit why this hapeen since it's really weird to me .

 

 ;;;","17/Oct/22 13:06;shijingqi;[~luoyuxia] The key point is MaterializedCollectResultBase.materializedTable. In BATCH mode, ResultRetrievalThread will process record and put row record into materializedTable, and MaterializedCollectResultBase.snapshot method will transfer all rows from materializedTable to snapshot array list.

 

Write and read of materializedTable array list is not record by record in BATCH mode, so result from materializedTable/snapshot array list is not exactly-once and may be duplicate or missing. 

 

For example, in our case, if ResultRetrievalThread thread has processed all record and isRunning status is not assigned false, MaterializedCollectResultBase.snapshot method still can transfer query result from materializedTable to snapshot, so we get duplicate rows.;;;","18/Oct/22 01:19;luoyuxia;Thanks for the explanation.

[~fsk119] Could you please have a look and assign this ticket to [~shijingqi] ;;;","01/Nov/22 09:45;shijingqi;Hi, [~fsk119], I have opened a pr [https://github.com/apache/flink/pull/21210] .  It's ready for review.

Thanks!;;;","07/Dec/22 02:51;shijingqi;Hi, [~fsk119], [~luoyuxia],  sorry to bother you, but i wonder is there any progress on this issue?

Thank you very much.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,
Code gen will  fail for like operator when the literal specified in user's sql hasn't be escaped ,FLINK-29651,13486520,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,17/Oct/22 01:15,15/Aug/23 10:35,04/Jun/24 20:41,,1.15.0,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,stale-assigned,,,"Can be reproduced with the following code in Flink 1.15:

 
{code:java}
// testTable contains a column `field1`
tableEnvironment
        .executeSql(
                ""select *, '1' as run from testTable WHERE field1 LIKE 'b\""cd\""e%'"")
        .print(); {code}
The exception the code generated fail to compile for it will contain the following code line

 

 
{code:java}
private final org.apache.flink.table.data.binary.BinaryStringData str$6 = 
org.apache.flink.table.data.binary.BinaryStringData.fromString(""b""cd""e"");  //  mismatched input 'cd' expecting ')'{code}
Seem it's produced by this [pr]([https://github.com/apache/flink/pull/19001]) which changes the logic for generate literal.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:12 UTC 2023,,,,,,,,,,"0|z19enc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 01:36;luoyuxia;I would like to fix it.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
Printing the contents of error lines when parsing yaml file may leak sensitive configuration values,FLINK-29650,13486483,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,lyssg,lyssg,16/Oct/22 09:33,14/Nov/22 08:18,04/Jun/24 20:41,14/Nov/22 08:17,1.16.0,,,,,,,,,,API / Core,,,,0,pull-request-available,,,,"This following is error configuration item. Password is '123456' and is displayed.
{code:java}
password:123456
#or
password 123456{code}
Could we just print file name and row number when parsing fails.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24679,,,,,,,,,,,,,,,"13/Nov/22 06:52;lyssg;image-2022-11-13-14-52-31-770.png;https://issues.apache.org/jira/secure/attachment/13052142/image-2022-11-13-14-52-31-770.png","13/Nov/22 06:53;lyssg;image-2022-11-13-14-53-01-121.png;https://issues.apache.org/jira/secure/attachment/13052143/image-2022-11-13-14-53-01-121.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 13 07:11:09 UTC 2022,,,,,,,,,,"0|z19ef4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:38;zhuzh;[~lyssg] would you share more information about what your config is like (with a mocked password) and what the error message is like?;;;","13/Nov/22 07:11;lyssg;I am sorry to miss this message.

This is the error message in flink-conf.yaml, and the password will be printed when flink load this file.

!image-2022-11-13-14-53-01-121.png!;;;",,,,,,,,,,,,,,,,,,,,,
upgrade Maven checkstyle plugin to latest version >= v8.30,FLINK-29649,13486462,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,samrat007,samrat007,15/Oct/22 19:16,19/Oct/22 08:48,04/Jun/24 20:41,19/Oct/22 08:48,,,,,,,,,,,,,,,0,,,,,"currenly while building project some of the typical warnings related to checkstyle 

 
{code:java}
[WARNING] Old version of checkstyle detected. Consider updating to >= v8.30
[WARNING] For more information see: https://maven.apache.org/plugins/maven-checkstyle-plugin/examples/upgrading-checkstyle.html
[INFO] You have 0 Checkstyle violations.{code}
 

should it be upgraded to latest stable version ?",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23542,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-15 19:16:35.0,,,,,,,,,,"0|z19eag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""LocalDateTime not supported"" error when retrieving Java TypeInformation from PyFlink",FLINK-29648,13486429,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,15/Oct/22 09:28,18/Oct/22 06:50,04/Jun/24 20:41,18/Oct/22 06:50,1.16.0,,,,,,1.16.0,,,,API / Python,,,,0,pull-request-available,,,,"The following code raises ""TypeError: The java type info: LocalDateTime is not supported in PyFlink currently."":
{code:java}
t_env.to_data_stream(t).key_by(...){code}
However, this works:
{code:java}
t_env.to_data_stream(t).map(lambda r: r).key_by(...){code}
Although we add Python coders for LocalTimeTypeInfo in 1.16, there's no corresponding typeinfo at Python side. So it works when a user immediately does processing after to_data_stream since date/time data has already been converted to Python object, but when key_by tries to retrieve typeinfo from Java TypeInformation, it fails.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29658,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 06:50:16 UTC 2022,,,,,,,,,,"0|z19e34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 06:50;hxb;Merged into master via ef0489b5131d03112d8a4ef11962a2d1ba6a9f54

Merged into release-1.16 via 59a276b38813a66f9ad80ed81b3dfcfe26decb7a;;;",,,,,,,,,,,,,,,,,,,,,,
report stackoverflow when using kryo,FLINK-29647,13486418,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Workaround,,jackin853,jackin853,15/Oct/22 05:56,03/Nov/22 02:17,04/Jun/24 20:41,27/Oct/22 07:13,1.13.2,,,,,,,,,,API / Type Serialization System,,,,0,KryoSerializer,,,,"When using kryo to report stackoverflow, the error is as follows:
{code:java}
java.lang.StackOverflowError at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:43) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44) at 
com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:44)
{code}
 I am using two-phase commit to write data to mysql, the following is part of the mysql sink code:
{code:java}
public class MySqlTwoPhaseCommitSink extends TwoPhaseCommitSinkFunction<Tuple2<String,Integer>, Connection,Void> {
    private static final Logger log = LoggerFactory.getLogger(MySqlTwoPhaseCommitSink.class);
    public MySqlTwoPhaseCommitSink(){
        super(new KryoSerializer<>(Connection.class,new ExecutionConfig()), VoidSerializer.INSTANCE);
    }
    @Override
    public void invoke(Connection connection, Tuple2<String,Integer> tp, Context context) throws Exception {
        log.info(""start invoke..."");
        //TODO
        //omit here
    }
    @Override
    public Connection beginTransaction() throws Exception {
        log.info(""start beginTransaction......."");
        String url = ""jdbc:mysql://localhost:3306/bigdata?useUnicode=true&characterEncoding=UTF-8"";
        Connection connection = DBConnectUtil.getConnection(url, ""root"", ""123456"");
        return connection;
    }
    @Override
    public void preCommit(Connection connection) throws Exception {
        log.info(""start preCommit..."");
    }
    @Override
    public void commit(Connection connection) {
        log.info(""start commit..."");
        DBConnectUtil.commit(connection);
    }
    @Override
    public void abort(Connection connection) {
        log.info(""start abort rollback..."");
        DBConnectUtil.rollback(connection);
    }
}{code}

I also found similar problem reports: https://github.com/EsotericSoftware/kryo/issues/341",flink 1.13.2 version （kryo 2.24 version）,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Nov 03 02:17:28 UTC 2022,,,,,,,,,,"0|z19e0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:36;zhuzh;[~jackin853] I think you are right that is is a known issue of kryo 2.24.0.
However, it's not easy to upgrade kryo, see https://lists.apache.org/thread/vyov944och394lql76zb3myf2g140fdw.
Therefore, I would suggest to implement serializers for your classes to not let its serializations fallback to kryo to avoid such issues.;;;","27/Oct/22 07:13;xtsong;Closing this for now as we are not upgrading kryo version right now. Feel free to re-open if there're other opinions.;;;","31/Oct/22 11:41;jackin853;Two-phase commit, use transaction method to write data to mysql, currently found no better serialization method, when the task crashes in the commit phase, the task restarts and recovers, it seems that there is no way to recover the previous transaction and re-commit

[~zhuzh] [~xtsong] Is there a good solution currently?;;;","01/Nov/22 01:04;jackin853;Our mysql database has no primary key and cannot achieve idempotent writes using upsert statements;;;","03/Nov/22 02:17;xtsong;[~jackin853],

As [~zhuzh] mentioned previously, you can implement your own serializer for your classes.

To be specific, you would need to implement the {{TypeSerializer}} interface, defining how your {{Connection}} class should be serialized/deserialized, and replacing the `KryoSerializer` with the one you implemented in the following code block. 

{code:java}
public MySqlTwoPhaseCommitSink(){
    super(new KryoSerializer<>(Connection.class,new ExecutionConfig()), VoidSerializer.INSTANCE);
}
{code}

See [this documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/] for more details.
;;;",,,,,,,,,,,,,,,,,,
SQL Gateway should return a simpler error message,FLINK-29646,13486361,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,heigebupahei,heigebupahei,14/Oct/22 14:34,11/Mar/24 12:43,04/Jun/24 20:41,,,,,,,,1.20.0,,,,Table SQL / Gateway,,,,1,pull-request-available,,,,"sql gateway should return simpler exception information
for example:
  If i execute a sql statement through sql gateway but my statement has syntax error  ：[ inset into tablea select * from tableb  ]

When I get exception information. The abnormal information returned by the server is too redundant to quickly find the Key Information. 
{code:java}
<Exception on server side:
org.apache.flink.table.gateway.api.utils.SqlGatewayException: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
    at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:77)
    at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:84)
    at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52)
    at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196)
    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83)
    at java.util.Optional.ifPresent(Optional.java:159)
    at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45)
    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80)
    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49)
    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115)
    at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94)
    at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55)
    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:210)
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69)
    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:199)
    at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:73)
    ... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation 5fb47960-4413-43c8-864b-b2703d5186b3.
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:389)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:248)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""inset"" at line 4, column 2.
Was expecting one of:
    ""INSERT"" ...
    ""UPSERT"" ...
    
    at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:82)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:102)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:90)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$0(SqlGatewayServiceImpl.java:182)
    at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:111)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:239)
    ... 7 more
Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered ""inset"" at line 4, column 2.
Was expecting one of:
    ""INSERT"" ...
    ""UPSERT"" ...
    
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.convertException(FlinkSqlParserImpl.java:483)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.normalizeException(FlinkSqlParserImpl.java:246)
    at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:140)
    at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:195)
    at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:77)
    ... 12 more
Caused by: org.apache.flink.sql.parser.impl.ParseException: Encountered ""inset"" at line 4, column 2.
Was expecting one of:
    ""INSERT"" ...
    ""UPSERT"" ...
    
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:44417)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:44228)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.RichSqlInsert(FlinkSqlParserImpl.java:7366)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStatementSet(FlinkSqlParserImpl.java:8435)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlExecute(FlinkSqlParserImpl.java:8990)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3512)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:2934)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:298)
    at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:193)
    ... 13 moreEnd of exception on server side> {code}
 
The key information is:
{code:java}
org.apache.flink.sql.parser.impl.ParseException: Encountered ""inset"" at line 4, column 2. Was expecting one of:     ""INSERT"" ...     ""UPSERT"" ... {code}
However, it is difficult for the client to see it quickly. I think sql gateway should have higher requirements for exception information.

[~Wencong Liu] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 24 02:46:22 UTC 2022,,,,,,,,,,"0|z19do0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/22 14:03;Wencong Liu;Thanks [~heigebupahei] . +1 for this change. At present, sql gateway rest endpoint returns all the call stacks of the exception to the client if there is an internal error, which needs to be trimmed to make the root cause more clear. cc [~fsk119] [~xtsong] ;;;","17/Oct/22 01:45;xtsong;I'm -1 for this change.

Reporting the root-cause exception only might be good enough in this specific case, but could make things hard to understand in some other cases. It's hard to come up with some universal standards to decide whether the exception should be pruned. IMO, the harm of missing exception information where it's needed is larger than presenting extra information where it's unnecessary.

Moreover, I believe looking into the `caused by` clauses is sort of a common practice for java users. I doubt how much extra inconvenience it causes.;;;","17/Oct/22 01:57;heigebupahei;There is no problem in reporting a complete exception. The question is whether the complete exception needs to be reported to the user accessing sql-gateway. The complete exception is kept in the gateway's log, but if the error message responded to the client still contains the complete stack information, it will make it difficult for the client to capture key information, this is my point of view;;;","17/Oct/22 02:22;xtsong;First of all, the provided PR changes the default behavior of {{ExceptionUtils}}, which affects not only sql-gateway client. But this can be fixed easily.

However, even we only prune the error message reported to the client, there're still concerns:
- There's no guarantee that the root-cause exception would be enough to understand the problem.
- In production, users may not always have access to the sql-gateway logs.;;;","17/Oct/22 02:34;heigebupahei;Maybe you can add a parameter to the client, whether you need to return the complete stack information. hiveserver2 does this. paramete:
{code:java}
hive.server2.logging.operation.verbose{code}
 ;;;","17/Oct/22 02:56;xtsong;That's probably worth-considering direction. It's quite common that softwares provide the basic information by default while allowing more verbose information to be enabled via such a knob. However, in such cases, it is usually the general conclusions that are presented by default, and the details are opt-in. In your example, the general conclusion is ""Failed to fetch results"", while ""ParseException"" is a detail of that.

TBH, I don't see a good reason for jumping directly to the bottom of the error stack while skipping all the rest. I'd rather not to change a general behavior based on the observation of just one specific example. I'm worried that we might improve the user experience a bit in some cases at the price of making it a lot worse in others.;;;","17/Oct/22 03:28;heigebupahei;I would like to explain the scenario where we use the gateway. We submit flinksql tasks through the gateway, which includes task submission and task debugging. In this scenario, we hope that the error messages that users can see are relatively simplified Yes, such as the example I gave above, there is a problem with my sql syntax, I hope the error can point out my sql syntax problem, instead of giving me a large stack of information, the user who writes the sql task will not care about the stack situation . So I think this can be used as a configurable option. Of course,
I think your concern is justified, the stack information will be returned by default. In some scenarios, returning to the root case may help users locate the problem faster.;;;","17/Oct/22 04:17;xtsong;Let me ask in this way: How do you know the last `caused by` exception is the most helpful? Why not the second last?

I assume this is because in this specific example, it happens to be a sql syntax problem that the message of the last `caused by` exception is most helpful. These are not always true. What if the root exception is thrown from a common path, where you need the second / third last `caused by` exception to understand where the error comes from?;;;","17/Oct/22 04:43;heigebupahei;Thank  [~xtsong] . It is really not the best way to use root case to simplify exception information. It may cause the problem you mentioned. This problem should be boiled down to, when we use sql-gateway for sql operation, if sql Syntax error, is it better to deal with the error as a separate strategy? Instead of all the return stack information in one go. My pr is the reference hiveserver2 mode, which may not be applicable in this scenario, but I think it is indeed possible to consider handling exceptions caused by SQL syntax errors separate. like using mysql or other databases.
 ;;;","17/Oct/22 05:18;xtsong;I agree that for this specific example, or maybe generalize to all the sql operation / syntax errors, the error information can be improved.

We may consider to append the message of the cause exception to the final exception. E.g., something like the following.
{code}
org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults, due to: Failed to execute the operation 5fb47960-4413-43c8-864b-b2703d5186b3, due to:  SQL parse failed. Encountered ...
{code}

Then we can use an config option to control whether to only display the top level exception.

WDYT?;;;","17/Oct/22 05:19;xtsong;cc [~fsk119];;;","17/Oct/22 09:04;heigebupahei;I think this is a good solution
 ;;;","18/Oct/22 06:51;fsk119;Thanks for the mention. Recently I work with [~yzl] to support Flink SQL Client to submit SQL to the SQL Gateway. We find the current exception is not enough at the Flink SQL Client side. There are two kinds of the needs at the client side:

1.  the client needs to understand the {{Exception}} type to determine whether to continue reading from the input or just notify the user the input SQL contains error. 
2. the client needs to print the exception stack or the root cause according to the user config({{sql-client.verbose}})

So I just wonder whether it's better to contains the {{ErrorDetail}} in the {{FetchResultResponse}} like Presto does[1].  The {{ErrorDetail}} may contains the {{ErrorCode}}, {{root cause}}, {{exception stack}}.

[1] https://prestodb.io/docs/current/develop/client-protocol.html#queryresults

;;;","24/Oct/22 02:46;yzl;Thanks for your suggestion. As [~fsk119] said, recently I am working on Sql Client. The Sql-Client has an option `sql-client.verbose` that can determine whether to print root cause or detail exception stack.

Here is my implementation plan: when error occurs, let Sql-gateway's REST Endpoint return an `ErrorResponse` containing both root cause and detail exception stack, then you can determine to check what you are interested in. For the Sql-client, setting `sql-client.verbose` = false can print the root cause. It is very similar to the solution in your discussion.

 

 ;;;",,,,,,,,,
BatchExecutionKeyedStateBackend is using incorrect ExecutionConfig when creating serializer,FLINK-29645,13486345,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dwysakowicz,pnowojski,pnowojski,14/Oct/22 13:22,17/Oct/22 15:27,04/Jun/24 20:41,17/Oct/22 15:27,1.12.7,1.13.6,1.14.6,1.15.2,1.16.0,1.17.0,1.15.3,1.16.0,1.17.0,,Runtime / State Backends,,,,0,pull-request-available,,,,"{{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend#getOrCreateKeyedState}} is using freshly constructed {{ExecutionConfig}}, instead of the one configured by the user from the environment.


{code:java}
    public <N, S extends State, T> S getOrCreateKeyedState(
            TypeSerializer<N> namespaceSerializer, StateDescriptor<S, T> stateDescriptor)
            throws Exception {
        checkNotNull(namespaceSerializer, ""Namespace serializer"");
        checkNotNull(
                keySerializer,
                ""State key serializer has not been configured in the config. ""
                        + ""This operation cannot use partitioned state."");

        if (!stateDescriptor.isSerializerInitialized()) {
            stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
        }
{code}

The correct one could be obtained from {{env.getExecutionConfig()}} in {{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend#createKeyedStateBackend}} ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 17 15:27:22 UTC 2022,,,,,,,,,,"0|z19dkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 07:24;dwysakowicz;This is not a significant issue as we actually do not use any of the serializers during operation. The code is there to fulfill contracts (there is a check in the stack if a serializer has been initialized). I'll fix that anyhow.;;;","17/Oct/22 15:27;dwysakowicz;Fixed in:
* master
** 0ffcfb757965ca62f7b4f1b93fd1387a45a50b2c
* 1.16
** c07d5aa98c016201eab38f883d20f2e807213113
* 1.15
** f19f032daee2fabefb4ccc6257740dd491b3a925;;;",,,,,,,,,,,,,,,,,,,,,
Reference Kubernetes operator from Flink Kubernetes deploy docs,FLINK-29644,13486341,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sriramgr,gyfora,gyfora,14/Oct/22 13:09,11/Nov/22 08:53,04/Jun/24 20:41,11/Nov/22 08:53,1.15.3,1.16.0,1.17.0,,,,,,,,Deployment / Kubernetes,Documentation,,,0,pull-request-available,starter,,,"Currently the Flink deployment/resource provider docs provide some information for the Standalone and Native Kubernetes integration without any reference to the operator. 

We should provide a bit more visibility and value to the users by directly proposing to use the operator when considering Flink on Kubernetes. 

We should make the point that for most users the easiest way to use Flink on Kubernetes is probably through the operator (where they can now benefit from both standalone and native integration under the hood). This should help us avoid cases where a new user completely misses the existence of the operator when starting out based on the Flink docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 08:53:33 UTC 2022,,,,,,,,,,"0|z19djk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 12:04;sriramgr;[~gyfora] - Can you please assign this to me?;;;","18/Oct/22 01:42;sriramgr;Please check the PR. https://github.com/apache/flink/pull/21089;;;","19/Oct/22 14:31;gyfora;master: 8f3eab244ee00f832b999b5899d206f5de08f373;;;","11/Nov/22 08:53;gyfora;merged:
master: 8f3eab244ee00f832b999b5899d206f5de08f373
release-1.16: f1d74f9db58fb6ba1175c4739c93a19cd3dba57b
release-1.15: dc6d84055cb541cc0aaf2b0abb28daf8a8e75da3;;;",,,,,,,,,,,,,,,,,,,
Possible NPE in ApplicationDispatcherBootstrap with failedJob submission and no HA,FLINK-29643,13486328,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nkruber,nkruber,14/Oct/22 12:45,30/Aug/23 12:48,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,Client / Job Submission,,,,0,,,,,"If
- {{PipelineOptionsInternal.PIPELINE_FIXED_JOB_ID}} is not set, and
- high availabibility is not activated, and
- {{DeploymentOptions.SUBMIT_FAILED_JOB_ON_APPLICATION_ERROR}} is set,
then a failure in job submission may fail with an NPE since the appropriate code in {{ApplicationDispatcherBootstrap#runApplicationEntryPoint()}} is trying to read the {{failedJobId}} from the configuration where it will not be present in these cases.

Please refer to the conditions that set the {{jobId}} in {{ApplicationDispatcherBootstrap.fixJobIdAndRunApplicationAsync()}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 12:48:29 UTC 2023,,,,,,,,,,"0|z19dgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 12:48;wangm92;[~nkruber] hi, I can't reproduce this error here. If `PIPELINE_FIXED_JOB_ID` is not configured, it will throw an ApplicationExecutionException exception in `runApplicationEntryPoint`[1]
1. [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L287]

 ;;;",,,,,,,,,,,,,,,,,,,,,,
MySqlCatalogITCase.testWithoutCatalogDB,FLINK-29642,13486309,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,14/Oct/22 10:36,14/Aug/23 12:40,04/Jun/24 20:41,14/Aug/23 12:40,1.17.0,,,,,,,,,,Connectors / JDBC,Table SQL / Ecosystem,,,0,stale-major,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24&l=15625] failed (not exclusively) because of {{MySqlCatalogITCase.testWithoutCatalogDB}} timing out.

{code}
Oct 14 02:18:03 ""main"" #1 prio=5 os_prio=0 tid=0x00007fa9bc00b800 nid=0x3528 waiting on condition [0x00007fa9c54e3000]
Oct 14 02:18:03    java.lang.Thread.State: WAITING (parking)
Oct 14 02:18:03 	at sun.misc.Unsafe.park(Native Method)
Oct 14 02:18:03 	- parking to wait for  <0x00000000aaae6cd0> (a java.util.concurrent.CompletableFuture$Signaller)
Oct 14 02:18:03 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Oct 14 02:18:03 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Oct 14 02:18:03 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Oct 14 02:18:03 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Oct 14 02:18:03 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Oct 14 02:18:03 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Oct 14 02:18:03 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Oct 14 02:18:03 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
Oct 14 02:18:03 	at org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.testWithoutCatalogDB(MySqlCatalogITCase.java:309)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32751,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:18 UTC 2023,,,,,,,,,,"0|z19dcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,
SortMergeResultPartitionReadSchedulerTest.testCreateSubpartitionReader,FLINK-29641,13486301,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,mapohl,mapohl,14/Oct/22 10:03,19/Oct/22 06:10,04/Jun/24 20:41,19/Oct/22 06:10,1.16.0,1.17.0,,,,,1.16.0,,,,Runtime / Network,Tests,,,0,pull-request-available,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8433] failed (not exclusively) due to {{SortMergeResultPartitionReadSchedulerTest.testCreateSubpartitionReader}}. The assert checking that the {{SortedMergeSubpartitionReader}} is in running state fails.

My suspicion is that the condition in [SortMergeResultPartitionReadScheduler.mayTriggerReading|https://github.com/apache/flink/blob/87d4f70e49255b551d0106117978b1aa0747358c/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SortMergeResultPartitionReadScheduler.java#L425-L428] (or something related to that condition) needs to be reconsidered since that's the only time {{isRunning}} is actually set to true.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24898,FLINK-28374,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 06:10:57 UTC 2022,,,,,,,,,,"0|z19dao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 10:09;mapohl;I'm linking some tickets that might be related to the issue. [~tanyuxin] [~kevin.cyj] [~Weijie Guo] (based on the commit history) do you have some opinion on that one?;;;","14/Oct/22 11:26;Weijie Guo;[~mapohl] Thank you for your report and investigation,I will take a look;;;","18/Oct/22 10:10;Weijie Guo;This JIRA can be closed as the pull request has been merged.;;;","19/Oct/22 06:10;hxb;Merged into master via 959fe97beeac15bbac70f4980cc6a3a110b432a2

Merged into release-1.16 via d233b39be94f330dabba593ac3e709a73eb714d2;;;",,,,,,,,,,,,,,,,,,,
Enhance the function configured by execution.shutdown-on-attached-exit by heartbeat between client and dispatcher,FLINK-29640,13486294,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Jiangang,Jiangang,Jiangang,14/Oct/22 09:16,13/Feb/23 13:15,04/Jun/24 20:41,10/Jan/23 02:20,,,,,,,1.17.0,,,,Client / Job Submission,,,,0,pull-request-available,,,,"If the param execution.shutdown-on-attached-exit is set to true, perform a best-effort cluster shutdown when the CLI is terminated abruptly.

But when the client is killed by 'kill -9', the cluster will not shutdown for that the signal can not be sent. We can enhance the behavior by building the heart beat between the client and the job. Once the job can not received any heartbeat from the client, the job cancel itself. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30958,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 13:15:28 UTC 2023,,,,,,,,,,"0|z19d94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 12:00;Jiangang;[~xtsong] Can you take the time to review the code? Thanks.;;;","29/Nov/22 01:55;xtsong;Sure, I'll take a look.;;;","10/Jan/23 02:20;xtsong;master (1.17): be240dea803fc82299c4711ebe656c1c4b159ca9;;;","13/Feb/23 13:15;mapohl;[~Jiangang] does it make sense to add release notes to this Jira issue? AFAIS, we're introducing new config parameters here which might be worth mentioning in the 1.17 release notes.;;;",,,,,,,,,,,,,,,,,,,
Add ResourceId in TransportException for debugging ,FLINK-29639,13486289,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Jiangang,Jiangang,14/Oct/22 08:50,22/Nov/22 09:15,04/Jun/24 20:41,18/Nov/22 02:44,,,,,,,1.15.4,1.16.1,1.17.0,,Runtime / Coordination,,,,0,pull-request-available,,,,"When the taskmanager is lost, only the host and port are shown in the exception. It is hard to find the exactly taskmanger by resourceId. Add ResourceId info will help a lot in debugging the job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 07:46:33 UTC 2022,,,,,,,,,,"0|z19d80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 08:51;Jiangang;Any committer can assign it to me? Thanks.;;;","17/Oct/22 03:20;xtsong;Thanks, [~Jiangang]. You are assigned;;;","31/Oct/22 03:58;xtsong;Hi [~Jiangang], is there any updates on this issue?;;;","31/Oct/22 07:57;Weijie Guo;[~Jiangang] Thank you for your proposal. I think this is very useful as I have indeed encountered the same case that it is difficult to find out which pod has a problem from the error report directly.

I have only one question about this:  Where are you going to store the resourceId of the upstream TM? Is it in the ConnectionID? I think whether it is placed here or not, we'd better test that there is no negative impact on the task deployment, because this will increase the size of the shuffle descriptor.;;;","09/Nov/22 04:06;Weijie Guo;Hi [~Jiangang], how is the progress of this work? If you don't have time, maybe I can go ahead.;;;","16/Nov/22 07:38;xtsong;There's no response from the assignee for ~1month. Reassigning this.;;;","18/Nov/22 02:44;xtsong;- master (1.17): 93c834be953f1336adb3ec5b5bf759a20e25eddf
- release-1.16: df061627591a1f62942e0700d51bb79259fb8f54
- release-1.15: 7a4fb7c86fbb28bee63bba829ebdda7b886170e3;;;","18/Nov/22 07:46;Jiangang;[~xtsong] Sorry for late reply. I am busy with something recently and thanks [~Weijie Guo] for the pr.;;;",,,,,,,,,,,,,,,
Update jackson bom because of CVE-2022-42003,FLINK-29638,13486274,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Oct/22 08:17,20/Oct/22 05:22,04/Jun/24 20:41,19/Oct/22 19:29,1.15.2,1.16.0,1.17.0,,,,1.15.3,1.16.0,1.17.0,,,,,,0,pull-request-available,,,,"There is a CVE-2022-42003 fixed in 2.13.4.1 and 2.14.0-rc1
https://nvd.nist.gov/vuln/detail/CVE-2022-42003


P.S. It seems there will not be 2.14.0 release until end of October according to https://github.com/FasterXML/jackson-databind/issues/3590#issuecomment-1270363915",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29468,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 05:10:04 UTC 2022,,,,,,,,,,"0|z19d4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 19:29;mapohl;master: bee3e9442fa2749f0b2e21d79949b5410fa422e5
1.16: e0104d58d44b1f8afd291e652ae90982b517207c
1.15: c500e97bec3bee5317d630185e19734c268cdc5d;;;","20/Oct/22 05:10;mapohl;-[~hxbks2ks] I marked it for 1.16.1. But feel free to move it into 1.16.0. This would require the hotfix commit and the FLINK-29486 backport to be included as well (see [FLINK-29638 1.16 backport PR|https://github.com/apache/flink/pull/21106] for reference)-

Ah, I just noticed that you have already update the fixVersion of FLINK-29638 and FLINK-29468. (y) Anyway, you still might want to consider the hotfix commit [1d02689|https://github.com/apache/flink/commit/1d02689655dea296245180db28508e146360ab25] as well to avoid conflicts;;;",,,,,,,,,,,,,,,,,,,,,
Improve to reuse threads in TaskManager for different tasks between jobs,FLINK-29637,13486241,13417633,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,14/Oct/22 02:40,12/Dec/22 08:55,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Runtime / Task,,,,0,,,,,"Currently `TaskManager` will create multiple threads for each task, such as execution thread, cancel thread, source thread, flush thread and etc. These threads will be destroyed when the task is finished. In olap cluster, the `TaskManager` frequently creates and destroys threads, which causes a great waste of resources. We should improve the `TaskManager` to reuse these thread after the task is finished.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 08:55:20 UTC 2022,,,,,,,,,,"0|z19cxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 08:55;dmvk;My initial impression is that this could introduce some user-classloader-related memory leaks because the task threads can reference these. 

Do you have any thoughts on whether this could be the case?;;;",,,,,,,,,,,,,,,,,,,,,,
Add micro benchmark module in flink table store,FLINK-29636,13486239,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,14/Oct/22 02:33,19/Mar/23 05:49,04/Jun/24 20:41,19/Mar/23 05:49,table-store-0.2.2,table-store-0.3.0,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"Currently there's a `flink-table-store-benchmark` to run a query in flink cluster and collect metrics to measure the performance of `flink-table-store`. There're some key operation steps such as read/write/compaction in `flink-table-store`, we should add a `flink-table-store-micro-benchmark` for them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 26 08:09:35 UTC 2023,,,,,,,,,,"0|z19cww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/23 08:09;yunta;JMH is under GPL license which is not compatible with the AL2, that's why flink-benchmark stays out of flink main repo. We can refer to https://issues.apache.org/jira/browse/FLINK-2973 for more details.
If so, can we add a module based on JMH in the main flink-table-store repo? [~zjureel], [~lzljs3620320];;;",,,,,,,,,,,,,,,,,,,,,,
Hive sink should support merge small files in batch mode,FLINK-29635,13486231,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,luoyuxia,luoyuxia,14/Oct/22 02:00,10/Feb/23 12:47,04/Jun/24 20:41,31/Jan/23 04:36,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,"When using Flink write Hive table in batch mode, there may produce small files. We should provide a mechanism to merge these small files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30951,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-14 02:00:53.0,,,,,,,,,,"0|z19cv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support periodic checkpoint triggering,FLINK-29634,13486197,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,thw,thw,13/Oct/22 19:37,21/Aug/23 14:46,04/Jun/24 20:41,02/Aug/23 13:18,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Similar to the support for periodic savepoints, the operator should support triggering periodic checkpoints to break the incremental checkpoint chain.

Support for external triggering will come with 1.17: https://issues.apache.org/jira/browse/FLINK-27101 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27101,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 13:18:31 UTC 2023,,,,,,,,,,"0|z19cnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 22:03;Jiale;[Here|https://github.com/apache/flink-kubernetes-operator/pull/249/files] is the PR by [~gyfora] I found for periodic savepoint support in flink operator.

[~thw], could you please double confirm that PR is the one you were referring to? Thanks!;;;","14/Oct/22 12:08;thw;[~Jiale] that PR it the right place to look at wrt how periodic savepoint triggering was added. Please note that a good portion of it is related to savepoint history also though. The operator does not maintain checkpoint history as checkpoints (so far) are triggered by Flink internally. Although it may be ultimately good to also keep track of checkpoints that were triggered by the operator within the CR status, perhaps it is best if we start with just the periodic triggering support? WDYT [~gyfora] ?

Please note that in order to work on this, the operator first needs to recognize Flink version 1.17 (currently it supports up to 1.16, see FlinkVersion). Then this feature needs to be built so that it is only effective for 1.17+ and operator keeps working with the other versions.

 ;;;","14/Oct/22 13:19;gyfora;I think savepoint history is mostly important so that the operator can do the cleanup, the operator should not track checkpoints that are otherwised owned and cleaned up by Flink itself.

The implementation could be similar but simpler than the savepointing because it should be enough to trigger the checkpoint, we don't need to keep track of the completion.;;;","29/Nov/22 08:36;pnowojski;I'm not sure if Flink should try to satisfy more and more non-standard checkpoint triggering behaviours for small fraction of users. One of the motivations behind FLINK-27101 was to allow users to implement their own logic outside of Flink, if they really need it.;;;","01/Dec/22 01:23;thw;[~pnowojski] please note that this ticket aims to add periodic triggering to the flink-kubernetes-operator, not to core Flink. Periodic triggering of savepoints (the existing feature) fits well into the charter of flink-kubernetes-operator. Based on FLINK-27101 it seems straightforward to extend it to let the operator also trigger full snapshots that can be used for recovery. It may even help to bridge the changed semantics of intermediate savepoints from FLINK-29856

 ;;;","01/Dec/22 11:10;pnowojski;Oh, thanks for correcting me [~thw]. Yes, I think this makes sense to me in that case (y);;;","02/Jan/23 02:00;Jiale;[~gyfora] [~thw] Thanks for the guidance!

I am trying to bump {{flink.version}} up to 1.17 (SNAPSHOT) locally to take advantage of the checkpoint rest API classes in 1.17 and start coding. However I found it a little bit tricky to deal with the dependencies, since in flink 1.17 {{io.fabric8}} classes are shaded and relocated in the {{flink-kubernetes}} in [this|https://github.com/apache/flink/commit/17d7c39bb2a9fcbaac1ead42073c099a52171d7d] commit.

This will cause some {{io.fabric8}} classes in the imports not found (which means these classes used to be only provided by {{{}flink-kubernetes{}}}), fixing their package is comparatively easy, just to change import {{io.fabric8}} classes packages to {{org.apache.flink.kubernetes.shaded.io.fabric8}}

Things get trickier when {{io.fabric8}} classes used to be provided by {{flink-kubernetes-operator}} (like [these|https://github.com/apache/flink-kubernetes-operator/blob/a1842d4c0170feb008293963ec51c0343f42771d/flink-kubernetes-standalone/pom.xml#L74-L79] ) and also available in {{flink-kubernetes}} . The new relocation change in flink 1.17 will cause {{flink-kubernetes-operator}} functions (like [this|https://github.com/apache/flink-kubernetes-operator/blob/a1842d4c0170feb008293963ec51c0343f42771d/flink-kubernetes-standalone/src/test/java/org/apache/flink/kubernetes/operator/kubeclient/parameters/ParametersTestBase.java#L92]) with arguments of {{io.fabric8}} classes now requiring {{org.apache.flink.kubernetes.shaded.io.fabric8}} classes arguments. Meanwhile it seems {{flink-kubernetes}} vs {{flink-kubernetes-operator}} are using different {{io.fabric8}} versions (5.12.3 vs 6.2.0 respectively), this means changing import class packages like previous case will cause a version downgrade for  {{io.fabric8}} classes.

And also there are certain classes which are not available in {{flink-kubernetes}} like [these|https://github.com/apache/flink-kubernetes-operator/blob/7ced741f51a99f2093ce8a45c8c92879a247f836/flink-kubernetes-standalone/src/test/java/org/apache/flink/kubernetes/operator/kubeclient/Fabric8FlinkStandaloneKubeClientTest.java#L33-L34] {{io.fabric8.kubernetes.client.server.mock.*}} classes) Calling functions from those classes will just create {{io.fabric8}} class objects which do not work with rest of code, which only accept {{org.apache.flink.kubernetes.shaded.io.fabric8}} class objects

2 work arounds I can think of
 # to have a dedicated subproject just to relocate back the {{io.fabric8}} classes in {{{}flink-kubernetes{}}}, (or other way around relocate things to {{{}org.apache.flink.kubernetes.shaded.io.fabric8{}}}) both seem a little bit ugly.
 # ignore the {{io.fabric8}} classes from {{flink-kubernetes}} and add all required {{io.fabric8}} dependencies to pom.xml, not sure if it is ok to upgrade all dependencies version to 6.2.0.

I am wondering if you folks have some good advices how to deal with this. ;;;","02/Jan/23 11:22;gyfora;Let's clear a few things up here a little [~Jiale] . The Flink version/fabric8 shading question is a prerequisite for this ticket but not directly related to the checkpointing api.

We already shade the flink-kubernetes fabric8 dependencies in flink-kubernetes-standalone, and after Flink 1.17 this should actually simplify a little as Flink shades these itself. So we can remove our custom shading logic and start using the flink.shaded.io.fabric8 classes within flink-kubernetes-standalone.

The flink-kubernetes-operator and flink-kubernetes-operator-api modules should not be affected by this as they do not directly interface with flink-kubernetes. ;;;","03/Jan/23 00:49;Jiale;I see. [~gyfora] Thanks for the input. I checked pom files again and your solution makes sense.

One more question - how can we deal with {{io.fabric8.kubernetes.client.server.mock.*}} classes? 
They are not a part of {{flink-kubernetes}}, but introduced [here|https://github.com/apache/flink-kubernetes-operator/blob/a1842d4c0170feb008293963ec51c0343f42771d/flink-kubernetes-standalone/pom.xml#L74-L78], so there are no shaded version for them.

I am running into issues [here|https://github.com/apache/flink-kubernetes-operator/blob/7ced741f51a99f2093ce8a45c8c92879a247f836/flink-kubernetes-standalone/src/test/java/org/apache/flink/kubernetes/operator/kubeclient/Fabric8FlinkStandaloneKubeClientTest.java#L58]: the code was expecting an {{org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.NamespacedKubernetesClient}} object for {{kubernetesClient}} but the mockServer.createClient() will create an {{io.fabric8.kubernetes.client.NamespacedKubernetesClient}}.;;;","03/Jan/23 07:56;gyfora;Yea, the Mock classes can be tricky [~Jiale] I don't really have a good answer yet, have to think about this.;;;","02/Aug/23 13:18;gyfora;merged to main c09671c5c51277c266b8c45d493317d3be1324c0;;;",,,,,,,,,,,,
Operator doesn't pass initialSavepointPath as fromSavepoint argument,FLINK-29633,13486188,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sap1ens,sap1ens,sap1ens,13/Oct/22 17:43,16/Oct/22 08:35,04/Jun/24 20:41,15/Oct/22 20:14,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.2.1,kubernetes-operator-1.3.0,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The Kubernetes Operator doesn't pass *initialSavepointPath* from the JobSpec as a *--fromSavepoint* argument to the JobManager. The operator does update the configuration, but in the standalone mode, Flink actually [overrides that|https://github.com/apache/flink/blob/012dc6a9b800bae0cfa5250d38de992ccbabc015/flink-container/src/main/java/org/apache/flink/container/entrypoint/StandaloneApplicationClusterEntryPoint.java#L57-L63] based on the command-line arguments. 

*CmdStandaloneJobManagerDecorator* should be updated to include *fromSavepoint.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 15 20:14:43 UTC 2022,,,,,,,,,,"0|z19clk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/22 20:14;gyfora;aa88e9a6738160b539a0974eae176db633a408f7;;;",,,,,,,,,,,,,,,,,,,,,,
Support nodeSelector in helm template for flink operator deployment,FLINK-29632,13486184,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jeesmon,jeesmon,jeesmon,13/Oct/22 17:11,13/Dec/22 01:43,04/Jun/24 20:41,14/Oct/22 13:01,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,Currently helm chart of flink-kubernetes-operator doesn't allow adding nodeSelector to operator deployment. There are cases where we want to schedule operator to specific nodes and it will be great if helm chart can support it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 13:01:52 UTC 2022,,,,,,,,,,"0|z19cko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 13:01;gyfora;merged to main 8e6ec37619992370835d5d632fa1e384c11c6143;;;",,,,,,,,,,,,,,,,,,,,,,
[CVE-2022-42003] flink-shaded-jackson,FLINK-29631,13486180,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,snuyanzin,sergiosp,sergiosp,13/Oct/22 16:52,20/Oct/22 10:03,04/Jun/24 20:41,20/Oct/22 10:03,shaded-16.0,,,,,,shaded-17.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,"flink-shaded-jackson vulnerable to 7.5 (high) [https://nvd.nist.gov/vuln/detail/CVE-2022-42003]

 

Ref:

[https://nvd.nist.gov/vuln/detail/CVE-2022-42003]

[https://repo1.maven.org/maven2/org/apache/flink/flink-shaded/16.0/flink-shaded-16.0.pom]

[https://mvnrepository.com/artifact/org.apache.flink/flink-shaded-jackson-parent/2.13.4-16.0]

[https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-jackson/2.13.4-16.0/flink-shaded-jackson-2.13.4-16.0.pom]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 10:03:57 UTC 2022,,,,,,,,,,"0|z19cjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 10:03;martijnvisser;Fixed in master: 7f6ff435bcfdf593e16a82452aae53faec8ed6c4;;;",,,,,,,,,,,,,,,,,,,,,,
Junit 5.8.1 run unit test with temporary directory will occur Failed to delete temp directory.,FLINK-29630,13486138,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Aiden Gong,Aiden Gong,13/Oct/22 12:20,19/Mar/23 05:51,04/Jun/24 20:41,19/Mar/23 05:51,table-store-0.3.0,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Junit 5.8.1 run unit test with temporary directory will occur Failed to delete temp directory.

My local :

windows 10

jdk1.8

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/22 01:12;Aiden Gong;image-2022-10-14-09-12-33-903.png;https://issues.apache.org/jira/secure/attachment/13050919/image-2022-10-14-09-12-33-903.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 01:12:38 UTC 2022,,,,,,,,,,"0|z19cag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 01:12;Aiden Gong;!image-2022-10-14-09-12-33-903.png!;;;",,,,,,,,,,,,,,,,,,,,,,
FlameGraph is empty for Legacy Source Threads,FLINK-29629,13486131,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,pvary,pvary,13/Oct/22 12:01,11/Nov/22 06:59,04/Jun/24 20:41,11/Nov/22 06:59,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,,"Thread dump gets the stack trace for the {{Custom Source}} thread, but this thread is always in {{TIMED_WAITING}}:
{code}
""Source: Custom Source -> A random source (1/2)#0"" ...
   java.lang.Thread.State: TIMED_WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.16/Native Method)
	- parking to wait for  <0x00000000ea775750> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos()
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await()
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take()
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:335)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
[..]
{code}

The actual code is run in the {{Legacy Source Thread}}:
{code}
""Legacy Source Thread - Source: Custom Source -> A random source (1/2)#0"" ...
   java.lang.Thread.State: RUNNABLE
{code}

This causes the WebUI FlameGraph to be empty of any useful data.

This is an example code to reproduce:
{code}
DataStream<RowData> inputStream = env.addSource(new RandomRecordSource(recordSize));
inputStream = inputStream.map(new CounterMapper());
FlinkSink.forRowData(inputStream).tableLoader(loader).append();
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29989,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 06:57:29 UTC 2022,,,,,,,,,,"0|z19c8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 03:12;zhuzh;Thanks for reporting this! [~pvary] 
I agree it is not ideal but I think it is not a bug. We know there are some tasks that can spawn other threads to do critical things(e.g. some sources and sinks) and they can encounter the same issue.
One solution might be to allow users to specify any thread (co-located with the given subtask) to create a flame graph. But I think we first need an end-to-end design for it.
What do you think?;;;","17/Oct/22 11:10;chesnay;We shouldn't invest additional resources into legacy APIs.;;;","17/Oct/22 18:47;pvary;[~chesnay]: Does that mean that some of the code below uses deprecated API for creating the {{Source}}?
{code}
DataStream<RowData> inputStream = env.addSource(new RandomRecordSource(recordSize));
inputStream = inputStream.map(new CounterMapper());
FlinkSink.forRowData(inputStream).tableLoader(loader).append();
{code}
Or it is just there are some ongoing effort to substitute the implementation which is ATM uses the {{Legacy}} Source and after the implementation is finished then it will be use the new Source?
Thanks,
Peter;;;","20/Oct/22 06:38;zhuzh;[~pvary] Flink is turning to [new Sources|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/Source.java]. Legacy sources (SourceFunction) will be deprecated in near future.

I agree with Chesnay to not invest more resources for legacy sources. We may have improvement to show flame graphs of working threads that are spawned by task threads. It is not just for the legacy source, but all operators which will spawn other threads to do critical things. But that should be an independent improvement and we need someone to create a FLIP(to prove the motivation, clarify the user behavior, and design the implementation steps) first. ;;;","25/Oct/22 08:16;pvary;Thanks [~zhuzh]!

Thanks for the info, I checked the linked document and agree with you and [~chesnay] that we should not sink more resources in the Legacy Sources than needed. Also +1 on adding an option to add the stack trace of the extra threads for the operator FlameGraph. In some cases they are not that important as they are not on the critical path, but they are consuming resources and may become a bottleneck, so it would be good to have an option to display them.;;;","27/Oct/22 01:58;zhuzh;Unfortunately we do not have enough resources to make this improvement recently. 
[~pvary] Not sure if you are interested and have time to do this. If yes, I can help with the design and code review. If not, we may close this ticket at the moment and pick it up in later versions.;;;","07/Nov/22 13:38;pvary;Most probably I will not have time to work on this part of the code in the near future :(. Mostly only filed the ticket to document the current situation which is not ideal. I fear that by closing the ticket the 3 of us will be the only people to remember the issue for a while, but if this is the way how we handle these situations in Flink, feel free to go ahead and close the ticket (I am just learning the processes :)).
Thanks, for all the help here [~zhuzh]!;;;","11/Nov/22 06:57;zhuzh;I opened another improvement ticket FLINK-29989 to track the future work.
Thanks for reporting the problem and all the inputs! [~pvary];;;",,,,,,,,,,,,,,,
Bump aws-java-sdk-s3 to 1.12.319,FLINK-29628,13486117,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Oct/22 11:29,07/Nov/22 20:15,04/Jun/24 20:41,07/Nov/22 20:15,,,,,,,1.17.0,,,,Connectors / FileSystem,FileSystems,,,0,pull-request-available,,,,As reported by Dependabot in https://github.com/apache/flink/pull/20285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 20:15:46 UTC 2022,,,,,,,,,,"0|z19c5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 20:15;martijnvisser;Fixed in master: 54679fd7ddba64e1edfecb5928d025e08a74def8;;;",,,,,,,,,,,,,,,,,,,,,,
Sink - Duplicate key exception during recover more than 1 committable.,FLINK-29627,13486099,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,KristoffSC,KristoffSC,KristoffSC,13/Oct/22 10:12,20/Oct/22 09:05,04/Jun/24 20:41,20/Oct/22 09:05,1.15.2,1.16.0,1.17.0,,,,1.15.3,1.16.1,1.17.0,,Connectors / Common,,,,0,,,,,"Recovery more than one Committable  causes `IllegalStateException` and prevents cluster to start.

When we recover the `CheckpointCommittableManager` we deserialize SubtaskCommittableManager instances from recovery state and we put them into `Map<Integer, SubtaskCommittableManager<CommT>>`. The key of this map is subtaskId of the recovered manager. However this will fail if we have to recover more than one committable. 

What w should do is to call `SubtaskCommittableManager::merge` if we already deserialize manager for this subtaskId.


Stack Trace:
{code:java}
28603 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Sink: Global Committer (1/1) (485dc57aca56235b9d1ab803c8c966ad_47d89856a1cf553f16e7063d953b7d42_0_1) switched from INITIALIZING to FAILED on 2ed5c848-d360-48ae-9a92-730b022c8a39 @ kubernetes.docker.internal (dataPort=-1).
java.lang.IllegalStateException: Duplicate key 0 (attempted merging values org.apache.flink.streaming.runtime.operators.sink.committables.SubtaskCommittableManager@631940ac and org.apache.flink.streaming.runtime.operators.sink.committables.SubtaskCommittableManager@7ff3bd7)
	at java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ~[?:?]
	at java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ~[?:?]
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:?]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ~[?:?]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ~[?:?]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[?:?]
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[?:?]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[?:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer$CheckpointSimpleVersionedSerializer.deserialize(CommittableCollectorSerializer.java:153) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer$CheckpointSimpleVersionedSerializer.deserialize(CommittableCollectorSerializer.java:124) ~[classes/:?]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeserializeList(SimpleVersionedSerialization.java:148) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.deserializeV2(CommittableCollectorSerializer.java:105) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.deserialize(CommittableCollectorSerializer.java:82) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.deserialize(CommittableCollectorSerializer.java:41) ~[classes/:?]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeSerialize(SimpleVersionedSerialization.java:121) ~[classes/:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterSerializer.deserializeV2(GlobalCommitterSerializer.java:128) ~[classes/:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterSerializer.deserialize(GlobalCommitterSerializer.java:99) ~[classes/:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterSerializer.deserialize(GlobalCommitterSerializer.java:42) ~[classes/:?]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeSerialize(SimpleVersionedSerialization.java:227) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.util.SimpleVersionedListState$DeserializingIterator.next(SimpleVersionedListState.java:138) ~[classes/:?]
	at java.lang.Iterable.forEach(Iterable.java:74) ~[?:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterOperator.initializeState(GlobalCommitterOperator.java:133) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:286) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:727) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:703) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:670) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[classes/:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 09:04:50 UTC 2022,,,,,,,,,,"0|z19c1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 10:18;KristoffSC;I have a fix and tests for this issue.
Will provide PR shortly;;;","14/Oct/22 09:53;KristoffSC;PR ready:
https://github.com/apache/flink/pull/21052


Pending on https://github.com/apache/flink/pull/21022.;;;","18/Oct/22 13:58;KristoffSC;New PR without SinkItTest
https://github.com/apache/flink/pull/21101;;;","19/Oct/22 13:17;KristoffSC;Backports:
1.15 - https://github.com/apache/flink/pull/21113
1.16 - https://github.com/apache/flink/pull/21115;;;","20/Oct/22 09:04;fpaul;Merged into:

master: ac044f894dc930dcad0def447823055d9386a2eb

release-1.16: 55c1095f81d6c95df753c68bc39bc249cf36a81c

release-1.15: 47bcb5a0cfa384fd8de61c6eff934d68323d566e;;;",,,,,,,,,,,,,,,,,,
Update Akka to 2.6.20,FLINK-29626,13486088,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Oct/22 09:23,02/Nov/22 11:52,04/Jun/24 20:41,02/Nov/22 11:52,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,Update Akka to the latest 2.6 version that's still under Apache license,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 11:52:36 UTC 2022,,,,,,,,,,"0|z19bzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 11:52;chesnay;master: 29069185655261d6c3b998db57aeec7ea23ab505;;;",,,,,,,,,,,,,,,,,,,,,,
Optimize changelog normalize for upsert source,FLINK-29625,13486087,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jiabao.sun,jiabao.sun,13/Oct/22 09:20,14/May/24 10:21,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Table SQL / Planner,,,,1,,,,,"Currently, Flink will add an expensive operator _changelog normalize_ to the source of the upsert changelog mode to complete the _update_before_ value. 

Even inserting directly from upsert-kafka source to upsert-kafka sink will still add this operator, and there is an extra operator to clear _upsert_before_ messages, which is obviously redundant.

In CDC scenarios, some databases do not provide update before images, such as Cassandra、MongoDB、TiDB({_}Old Value{_} is not turned on) and Postgres ({_}REPLICA IDENTITY{_} is not set to {_}FULL{_}). Using Flink SQL to process these changelog will have a lot of state overhead.

I don't know much about why this operator is needed, so I take the liberty to ask if we can get rid of changelog normalize completely or optimistic about it, adding it only if a normalized changelog is required by a downstream operator.

If this optimization is worthwhile, I'm happy to help with it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 10:18:44 UTC 2024,,,,,,,,,,"0|z19bz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 09:34;martijnvisser;The ChangelogNormalize operator is needed to deduplicate/normalize the results. Have you look at the https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config/#table-exec-source-cdc-events-duplicate option which exists specifically for this problem? ;;;","13/Oct/22 10:16;jiabao.sun;Thanks [~martijnvisser]  for the reply.

At present, this operator is not only added when deduplicating the changelog, but also for all upsert sources.

[https://github.com/apache/flink/blob/bf342d2f67a46e5266c3595734574db270f1b48c/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalTableSourceScanRule.scala#L71-L74] 
{code:java}
    if (
      isUpsertSource(resolvedSchema, table.tableSource) ||
      isSourceChangeEventsDuplicate(resolvedSchema, table.tableSource, tableConfig)
    ) {
      // generate changelog normalize node
{code}
Is this operator necessary for an exactly-once upsert stream? Like:
{code:sql}
insert into upsert_kafka_sink from select * from upsert_kafka_source;
{code};;;","13/Oct/22 10:28;martijnvisser;[~jark] I think you can answer this question better then me;;;","14/Oct/22 02:40;fsk119;[~jiabao.sun]  I think you are right we should let the downstream operator to determine whether we need the ChangeLogNormalize. ;;;","14/Oct/22 03:15;godfreyhe;+1 for the improvement;;;","14/May/24 10:18;wczhu;Hi [~jiabao.sun], I encountered the same problem, see discuss: [https://github.com/apache/flink-cdc/pull/1907#issuecomment-2105588522|http://example.com/]

I hope that the optimization of *_ChangelogNormalize_* can be disabled by option, because in the CDC multi-stream join scenario, *_UPDATE_BEFORE(-U)_* will emit *-D/+I* after passing through the _*JOIN*_ operator and be passed to the downstream. *-D* will it to be used as a dimension table. Associated to the middle state _*NULL*_;;;",,,,,,,,,,,,,,,,,
Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0,FLINK-29624,13486085,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Oct/22 09:13,01/Nov/22 10:39,04/Jun/24 20:41,01/Nov/22 10:39,,,,,,,1.17.0,,,,Connectors / Common,Connectors / FileSystem,,,0,pull-request-available,,,,Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0 to avoid being falsely flagged for CVEs CVE-2021-29425 and CVE-2020-15250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 10:39:16 UTC 2022,,,,,,,,,,"0|z19byo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 10:39;martijnvisser;Fixed in master: 68d93ab996e29694ea4d34b1a94a3e787ad2328c;;;",,,,,,,,,,,,,,,,,,,,,,
Bump Prometheus Java Client to 0.16.0,FLINK-29623,13486083,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,13/Oct/22 09:07,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Metrics,,,,0,auto-deprioritized-major,pull-request-available,,,Flink uses an old and no longer supported version of io.prometheus:simpleclient. We should upgrade to the latest version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:17 UTC 2023,,,,,,,,,,"0|z19by8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
KerberosDelegationTokenManager fails to load DelegationTokenProvider due to NoClassDefFoundError in various ITCases,FLINK-29622,13486074,13355999,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaborgsomogyi,mapohl,mapohl,13/Oct/22 08:30,05/Jan/23 10:10,04/Jun/24 20:41,05/Jan/23 10:10,1.16.0,1.17.0,,,,,1.16.0,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,,"There are multiple ITCases (e.g. {{EventTimeWindowCheckpointingITCase}}) that print an error when trying to load the {{HadoopFSDelegationTokenProvider}} which is on the classpath through {{flink-runtime}} but the corresponding hadoop dependency seems to be missing:
{code}
186348 02:25:25,492 [                main] INFO  org.apache.flink.runtime.security.token.KerberosDelegationTokenManager [] - Loading delegation token providers
 186349 02:25:25,493 [                main] ERROR org.apache.flink.runtime.security.token.KerberosDelegationTokenManager [] - Failed to initialize delegation token provider hadoopfs
 186350 java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/HdfsConfiguration
 186351         at org.apache.flink.runtime.security.token.HadoopFSDelegationTokenProvider.init(HadoopFSDelegationTokenProvider.java:68) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186352         at org.apache.flink.runtime.security.token.KerberosDelegationTokenManager.loadProviders(KerberosDelegationTokenManager.java:124) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186353         at org.apache.flink.runtime.security.token.KerberosDelegationTokenManager.<init>(KerberosDelegationTokenManager.java:109) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186354         at org.apache.flink.runtime.security.token.KerberosDelegationTokenManager.<init>(KerberosDelegationTokenManager.java:91) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186355         at org.apache.flink.runtime.security.token.KerberosDelegationTokenManagerFactory.create(KerberosDelegationTokenManagerFactory.java:47) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186356         at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:431) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186357         at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:234) ~[flink-runtime-1.16-SNAPSHOT-tests.jar:1.16-SNAPSHOT]
 186358         at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:109) ~[flink-runtime-1.16-SNAPSHOT-tests.jar:1.16-SNAPSHOT]
 186359         at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:64) ~[flink-test-utils-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
 186360         at org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.setupTestCluster(EventTimeWindowCheckpointingITCase.java:253) ~[test-classes/:?]
 186361         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
[...]
{code}

This error might be misleading/confusing to people investigating the logs. It looks like this error is actually expected since the tests not necessarily require Kerberos delegation tokens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25908,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 10:10:09 UTC 2023,,,,,,,,,,"0|z19bw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 09:01;gaborgsomogyi;As I see now the manager starts when `security.kerberos.fetch.delegation-token` enabled (default: true) and `hadoop-common` is on runtime CP. I think we can double check whether the user provided kerberos credentials and start the manager only such case. Will come back to this when FLINK-25910 merged.;;;","19/Oct/22 12:02;mbalassi;The issue is more serious than initially found. If Hadoop classes are present on the classpath, but Kerberos is not configured a RuntimeException is thrown failing the job.

cc [~hxbks2ks] .;;;","19/Oct/22 12:07;hxb;[~mbalassi] Thanks a lot for the information. After this issue and https://issues.apache.org/jira/browse/FLINK-29567 are fixed, I will start to prepare rc2.;;;","19/Oct/22 17:47;mbalassi;[98bcabb|https://github.com/apache/flink/commit/98bcabb592dbc0b62fa70f93d4f3063832f12b03] in release-1.16 and fee5b2d in master.;;;","05/Jan/23 08:06;mapohl;I noticed a similar warning when looking into FLINK-30507 for the {{HBaseDeletionTokenProvider}}:
{code}
03:55:25,784 [                main] WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Failed to initialize delegation token provider hbase
java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/HdfsConfiguration
        at org.apache.flink.runtime.security.token.hadoop.HBaseDelegationTokenProvider.getHBaseConfiguration(HBaseDelegationTokenProvider.java:69) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.security.token.hadoop.HBaseDelegationTokenProvider.init(HBaseDelegationTokenProvider.java:61) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:106) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:91) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:48) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:431) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:237) ~[flink-runtime-1.17-SNAPSHOT-tests.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:109) ~[flink-runtime-1.17-SNAPSHOT-tests.jar:1.17-SNAPSHOT]
        at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:64) ~[flink-test-utils-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.setupTestCluster(EventTimeWindowCheckpointingITCase.java:253) ~[test-classes/:?]
        at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:73) ~[test-classes/:?]
        at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:66) ~[test-classes/:?]
[...]
{code}

Is this something to look into as part of this Jira issue or shall I create a follow-up ticket?;;;","05/Jan/23 09:54;gaborgsomogyi;This is WARN message and there is no harm for the workload. In the upcoming PR for FLINK-30425 I'm not going to print out the stacktrace which is misleading. This is a planned behavior at the moment which can be cleanly solved when HBase and HDFS are externalized. All in all there is no issue at all except that the stacktrace freaks out ppls.;;;","05/Jan/23 10:10;mapohl;Thanks for looking into it. I'm gonna close the issue again as fixed then. Let's wait for FLINK-30425. (y);;;",,,,,,,,,,,,,,,,
Append-only with eventual log.consistency can not work,FLINK-29621,13486065,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Oct/22 07:55,13/Oct/22 12:02,04/Jun/24 20:41,13/Oct/22 12:02,,,,,,,table-store-0.2.2,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"{code:java}
                        ""CREATE TABLE T (i INT, j INT) WITH (""
                                + ""'log.system'='kafka', ""
                                + ""'write-mode'='append-only', ""
                                + ""'log.consistency'='eventual', ""
                                + ""'kafka.bootstrap.servers'='%s', ""
                                + ""'kafka.topic'='T')"",
{code}

Above DDL table should work.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 12:02:08 UTC 2022,,,,,,,,,,"0|z19bu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 12:02;lzljs3620320;master: 5d7af498785b5a6be2a2bad43b6195da79e6d819
release-0.2: 482eb917d0496e50ca92797bdde04316c09d9aab;;;",,,,,,,,,,,,,,,,,,,,,,
Flink deployment stuck in UPGRADING state when changing configuration,FLINK-29620,13486057,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,liadsh,liadsh,13/Oct/22 07:36,18/Oct/22 10:22,04/Jun/24 20:41,18/Oct/22 10:22,1.14.2,,,,,,1.15.0,,,,Kubernetes Operator,,,,0,,,,,"When I update the configuration of a flink deployment I observe one of two scenarios:

Success:

This happens when the job has not started - if I change the configuration quick enough:
{code:java}
2022-10-13 06:50:54,336 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][load-streaming/validator-process-124] Upgrading/Restarting running job, suspending first...
2022-10-13 06:50:54,343 o.a.f.k.o.r.d.ApplicationReconciler [INFO ][load-streaming/validator-process-124] Job is not running but HA metadata is available for last state restore, ready for upgrade
2022-10-13 06:50:54,353 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Deleting JobManager deployment while preserving HA metadata.
2022-10-13 06:50:58,415 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (5s)
2022-10-13 06:51:03,451 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (10s)
2022-10-13 06:51:06,469 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Cluster shutdown completed.
2022-10-13 06:51:06,470 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] End of reconciliation
2022-10-13 06:51:06,493 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] Starting reconciliation
2022-10-13 06:51:06,494 o.a.f.k.o.c.FlinkConfigManager [INFO ][load-streaming/validator-process-124] Generating new config
 {code}
In this scenario I see that the job manager and task manager pods are terminated and then recreated.

 

 

Failure:

This happens when I let the job start (wait more than 30-60 seconds) and change the configuration:
{code:java}
2022-10-13 06:53:06,637 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][load-streaming/validator-process-124] Upgrading/Restarting running job, suspending first...
2022-10-13 06:53:06,637 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][load-streaming/validator-process-124] Job is in running state, ready for upgrade with SAVEPOINT
2022-10-13 06:53:06,659 o.a.f.k.o.s.FlinkService       [INFO ][load-streaming/validator-process-124] Suspending job with savepoint.
2022-10-13 06:53:07,042 o.a.f.k.o.s.FlinkService       [INFO ][load-streaming/validator-process-124] Job successfully suspended with savepoint s3://cu-flink-load-checkpoints-us-east-1/validator-process-124/savepoints/savepoint-000000-947975b509b2.
2022-10-13 06:53:11,111 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (5s)
2022-10-13 06:53:16,176 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (10s)
2022-10-13 06:53:21,238 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (15s)
2022-10-13 06:53:26,293 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (20s)
2022-10-13 06:53:31,355 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (25s)
2022-10-13 06:53:36,412 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (30s)
2022-10-13 06:53:41,512 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (35s)
2022-10-13 06:53:46,568 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (40s)
2022-10-13 06:53:51,625 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (45s)
2022-10-13 06:53:56,740 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (50s)
2022-10-13 06:54:01,811 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (55s)
2022-10-13 06:54:06,866 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (60s)
2022-10-13 06:54:07,866 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Cluster shutdown completed.
2022-10-13 06:54:07,866 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] End of reconciliation
2022-10-13 06:54:07,894 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] Starting reconciliation
2022-10-13 06:54:07,894 o.a.f.k.o.o.d.ApplicationObserver [WARN ][load-streaming/validator-process-124] Running deployment generation 3 doesn't match upgrade target generation 4.
2022-10-13 06:54:07,895 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][load-streaming/validator-process-124] Detected spec change, starting reconciliation.
2022-10-13 06:54:07,941 o.a.f.k.o.s.FlinkService       [INFO ][load-streaming/validator-process-124] Deploying application cluster
2022-10-13 06:54:07,947 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Job graph in ConfigMap validator-process-124-dispatcher-leader is deleted
2022-10-13 06:54:08,029 o.a.f.c.d.a.c.ApplicationClusterDeployer [INFO ][load-streaming/validator-process-124] Submitting application in 'Application Mode'.
2022-10-13 06:54:08,031 o.a.f.r.u.c.m.ProcessMemoryUtils [INFO ][load-streaming/validator-process-124] The derived from fraction jvm overhead memory (102.400mb (107374184 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
2022-10-13 06:54:08,087 o.a.f.k.o.r.ReconciliationUtils [WARN ][load-streaming/validator-process-124] Attempt count: 0, last attempt: false
2022-10-13 06:54:08,111 i.j.o.p.e.ReconciliationDispatcher [ERROR][load-streaming/validator-process-124] Error during event processing ExecutionScope{ resource id: ResourceID{name='validator-process-124', namespace='load-streaming'}, version: 1116792084} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: org.apache.flink.client.deployment.ClusterDeploymentException: The Flink cluster validator-process-124 already exists.
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:119)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:201)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:153)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:83)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:152)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:135)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:115)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:86)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:59)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:390)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: The Flink cluster validator-process-124 already exists.
    at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:181)
    at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)
    at org.apache.flink.kubernetes.operator.service.FlinkService.submitApplicationCluster(FlinkService.java:200)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:155)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:52)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:188)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:122)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:145)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:55)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:115)
    ... 13 more {code}
In this scenario I see that the job manager pod is restarted (not recreated), task manager pods are not updated, flink config maps are not updated.

The flink deployment state changes to UPGRADING and the above exception is repeated.

error in flink deployment: org.apache.flink.client.deployment.ClusterDeploymentException: The Flink cluster validator-process-124 already exists.
Job Manager Deployment Status:  MISSING

 

Flink deployment spec:

 
{code:java}
flinkVersion: v1_14 
job:
    allowNonRestoredState: true
    args: ...
    entryClass: ...
    jarURI: ...
    parallelism: x
    savepointTriggerNonce: 0
    state: running
    upgradeMode: savepoint
jobManager:
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          configmap.reloader.stakater.com/reload: flink-config-validator-process-124,pod-template-validator-process-124
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nodeType
                  operator: In
                  values:
                  - someValue
        containers:
        - name: flink-main-container
          resources:
            limits:
              cpu: ""1""
              memory: 1.6Gi
            requests:
              cpu: ""0.2""
              memory: 1Gi
        tolerations:
        - effect: NoSchedule
          key: someValue
          value: ""true""
    replicas: 1
podTemplate:
    apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        configmap.reloader.stakater.com/reload: flink-config-validator-process-124,pod-template-validator-process-124
        prometheus.io/path: /metrics
        prometheus.io/port: ""9260""
        prometheus.io/scrape: ""true""
      labels:
        app.kubernetes.io/instance: flink-validator-process-124
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: apache-flink
        app.kubernetes.io/version: test
        helm.sh/chart: apache-flink-1.0.0
    spec:
      containers: []
      imagePullSecrets: []
  serviceAccount: validator-process-124
  taskManager:
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          configmap.reloader.stakater.com/reload: flink-config-validator-process-124,pod-template-validator-process-124
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nodeType
                  operator: In
                  values:
                  - someValue
        containers:
        - name: flink-main-container
          resources:
            limits:
              cpu: ""1""
              memory: 3.6Gi
            requests:
              cpu: ""0.2""
              memory: 3Gi
        tolerations:
        - effect: NoSchedule
          key: someValue
          value: ""true""{code}
 

Please let me know if more details are required.

 ","AWS EKS v1.21

Operator version: 1.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 10:22:11 UTC 2022,,,,,,,,,,"0|z19bsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 08:43;liadsh;might be related to FLINK-26345;;;","15/Oct/22 15:29;gyfora;I think that this might be fixed on 1.2.0 already, could you please verify?;;;","17/Oct/22 06:53;liadsh;i'll try it tomorrow, thanks :);;;","18/Oct/22 04:51;liadsh;not solved by 1.2.0, I will try to upgrade to flink 1.15 and see if this helps - https://github.com/apache/flink-kubernetes-operator/blob/c6ad96980278aa34864c33447e8c8aec3a6a5909/docs/content/docs/custom-resource/job-management.md#recovery-of-missing-job-deployments;;;","18/Oct/22 10:22;liadsh;solved in flink 1.15;;;",,,,,,,,,,,,,,,,,,
Remove redundant MeterView updater thread from KubernetesClientMetrics,FLINK-29619,13486056,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,morhidi,morhidi,,13/Oct/22 07:25,24/Nov/22 01:02,04/Jun/24 20:41,14/Oct/22 11:29,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.3.0,,,,,,,,0,pull-request-available,,,,"The `MetricRegistryImpl` already has a solution to update `MeterView` objects periodically.

https://github.com/apache/flink/blob/7a509c46e45b9a91f2b7d01f13afcdef266b1faf/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/MetricRegistryImpl.java#L404",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 11:29:19 UTC 2022,,,,,,,,,,"0|z19bs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 11:29;gyfora;merged to main f1387f723da3f274bcb3268d401d91042064255d;;;",,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOSecuredITCase.testDetachedMode timed out in Azure CI,FLINK-29618,13486054,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,mapohl,mapohl,13/Oct/22 07:14,22/May/23 07:35,04/Jun/24 20:41,22/May/23 07:35,1.17.0,,,,,,,,,,Deployment / YARN,Tests,,,0,pull-request-available,starter,test-stability,,"We experienced a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41931&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30284] that was caused (exclusively) by {{YARNSessionFIFOSecuredITCase.testDetachedMode}} running into a timeout.

The test specific logs which were extracted from the build's are attached to this Jira issue.

JUnit tries to stop the thread running the test but fails to due so because it's interrupting a sleep. The {{InterruptedException}} is not properly handled in [YarnTestBase:744|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L744] (it doesn't forward the exception). Therefore, we only see the warning being logged after 60s:
{code}
11:33:51,124 [ForkJoinPool-1-worker-25] WARN  org.apache.flink.yarn.YarnTestBase                           [] - Interruped
java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method) ~[?:1.8.0_292]
        at org.apache.flink.yarn.YarnTestBase.sleep(YarnTestBase.java:716) ~[test-classes/:?]
        at org.apache.flink.yarn.YarnTestBase.startWithArgs(YarnTestBase.java:906) ~[test-classes/:?]
        at org.apache.flink.yarn.YARNSessionFIFOITCase.runDetachedModeTest(YARNSessionFIFOITCase.java:141) ~[test-classes/:?]
        at org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.lambda$testDetachedMode$2(YARNSessionFIFOSecuredITCase.java:173) ~[test-classes/:?]
        at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:288) ~[test-classes/:?]
        at org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.testDetachedMode(YARNSessionFIFOSecuredITCase.java:160) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
[...]
{code}

The test code itself eventually continues and succeeds (despite the interruption). The job submission takes suspiciously long, though.

Removing the timeout from the test (as this is the desired approach for tests in general now) should solve this test instability.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/22 07:15;mapohl;build-20221012.7.YARNSessionFIFOSecuredITCase.testDetachedMode.log;https://issues.apache.org/jira/secure/attachment/13050873/build-20221012.7.YARNSessionFIFOSecuredITCase.testDetachedMode.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 07:35:45 UTC 2023,,,,,,,,,,"0|z19brs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 09:32;Wencong Liu;Hi [~mapohl] , do you think it's suitable to let CI judge whether it's timed out for this test? Currently it use 
{code:java}
@Timeout(value = 60) {code}
to judge the timed out status.;;;","11/May/23 07:18;mapohl;I guess, you're right. The test timed out after 60 seconds (which is where the log warning about the {{InterruptedException}} occurred). But the test itself continues because we're not forwarding the interrupt exception within the sleep call (see [YarnTestBase:744|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L744]) and succeeds eventually.

Removing the timeout would have helped here to avoid test instabilities. Even though, it would have been interesting to investigate why the job submission takes that long. Unfortunately, the build artifacts are gone already.

I updated the ticket's description.;;;","11/May/23 10:09;Wencong Liu;Thanks for the reply [~mapohl] . I'd like to remove the annotation. 😊;;;","11/May/23 11:38;mapohl;Thanks for picking it up, [~Wencong Liu]. I assigned the ticket to you.;;;","22/May/23 07:35;mapohl;master: 5b637243f27063f3a2f5fbc46a9e0d87207ff37a;;;",,,,,,,,,,,,,,,,,,
Cost too much time to start SourceCoordinator of hdfsFileSource when start JobMaster,FLINK-29617,13486050,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,dangshazi,dangshazi,13/Oct/22 07:01,15/Aug/23 10:35,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Connectors / FileSystem,Runtime / Coordination,,,0,coordination,file-system,stale-assigned,,"h1. Scenario:

Our user use flink batch to compact small files in one day. Flink version : 1.15
He split pipeline into 24 for each hour. So there are 24 source
 
I find it  costs too much time to start SourceCoordinator of hdfsFileSource when start JobMaster
 
 as follow:
 
!image-2022-10-13-19-02-18-555.png!
 
h1. Root Cause:

I got the root cause after check: 
 # AbstractFileSource will enumerateSplits when createEnumerator
 # NotSplittingRecursiveEnumerator need to get fileblockLocation of every fileblock which is a heavy IO operation

  !image-2022-10-13-19-02-29-620.png!

!image-2022-10-13-19-02-35-422.png!
 
h1. Suggestion
 # FileSource add option to disable location fetcher
 # Move location fetcher into IOExecutor",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/22 11:02;dangshazi;image-2022-10-13-19-02-18-555.png;https://issues.apache.org/jira/secure/attachment/13050897/image-2022-10-13-19-02-18-555.png","13/Oct/22 11:02;dangshazi;image-2022-10-13-19-02-29-620.png;https://issues.apache.org/jira/secure/attachment/13050898/image-2022-10-13-19-02-29-620.png","13/Oct/22 11:02;dangshazi;image-2022-10-13-19-02-35-422.png;https://issues.apache.org/jira/secure/attachment/13050899/image-2022-10-13-19-02-35-422.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:35:13 UTC 2023,,,,,,,,,,"0|z19bqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 07:55;luoyuxia;[~dangshazi] Thanks for raising it and detail explanation. I'll be much appreciated that you can take the ticket.  If you don't have time, maybe I can help take it.

I'm fine with these two suggestions. But prefer suggestion 2 since suggestion 1 will bring new option which user may hardly know it.

I have one question, have you ever tried with these suggestions? If so, what's the improvement of these two suggestions?

Btw, the images uploaded is failed. Could you please upload them again?;;;","13/Oct/22 11:03;dangshazi;No, I haven't.  Images uploaded

 ;;;","17/Oct/22 08:29;lsy;[~dangshazi] Thanks for report it, [~luoyuxia] will take it.;;;","15/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
Polish Table Store Pom file to avoid warning.,FLINK-29616,13486045,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Aiden Gong,Aiden Gong,Aiden Gong,13/Oct/22 06:49,13/Oct/22 11:06,04/Jun/24 20:41,13/Oct/22 11:06,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,!image-2022-10-13-14-49-39-582.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/22 06:49;Aiden Gong;image-2022-10-13-14-49-39-582.png;https://issues.apache.org/jira/secure/attachment/13050872/image-2022-10-13-14-49-39-582.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 11:06:27 UTC 2022,,,,,,,,,,"0|z19bps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 11:06;lzljs3620320;master: 97bb1b1aba88882eeca2db6a4c6181d86c39cc45;;;",,,,,,,,,,,,,,,,,,,,,,
MetricStore does not remove metrics of nonexistent subtasks when adaptive scheduler lowers job parallelism,FLINK-29615,13486041,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,13/Oct/22 06:45,21/Oct/22 02:20,04/Jun/24 20:41,21/Oct/22 02:20,1.15.0,1.16.0,,,,,1.16.1,1.17.0,,,Runtime / Metrics,Runtime / REST,,,0,pull-request-available,,,,"We are exploring autoscaling Flink with Reactive mode using metrics from Flink REST for guidance, and found that the metrics are not correctly updated.

 

*Problem*

MetricStore does not remove metrics of nonexistent subtasks when adaptive scheduler lowers job parallelism (aka, num of subtasks decreases) and users will see metrics of nonexistent subtasks on Web UI (e.g. the task backpressure page) or REST API response. It causes confusion and occupies extra memory.

 

*Proposed Solution*

Thanks to FLINK-29132 & FLINK-28588,  Flink will now update current execution attempts when updating metrics. Since the active subtask info is included in the current execution attempt info, we are able to retain active subtasks using the current execution attempt info.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 02:20:34 UTC 2022,,,,,,,,,,"0|z19bow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 10:18;Zhanghao Chen;[~chesnay] could you help take a look? I've prepared a quick fix for it.;;;","20/Oct/22 05:55;Zhanghao Chen;Hi, [~xtsong]. Could you help take a look at this? We encountered this issue when exploring autoscaling with Reactive mode using metrics from Flink REST for guidance, and would like to fix it.;;;","20/Oct/22 07:00;xtsong;Thanks for reporting and fixing this, [~Zhanghao Chen]. I'll take a look at the PR.;;;","21/Oct/22 02:20;xtsong;- master (1.17): f11c322467aa7e8a6d58703a72149152e8b58883
- release-1.16: 9f5ad7ea6ae04722d3934424f7f37cbfcbf2a7f5;;;",,,,,,,,,,,,,,,,,,,
Introduce Spark writer for table store,FLINK-29614,13486040,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Oct/22 06:44,30/Nov/22 09:00,04/Jun/24 20:41,30/Nov/22 08:54,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"The main difficulty is that the Spark SourceV2 interface currently does not support custom distribution, and the Table Store must have consistent distribution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30248,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 30 08:54:44 UTC 2022,,,,,,,,,,"0|z19boo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 08:54;lzljs3620320;master: ab766ec41e6c3876b38da147a92a3844e9b2a774;;;",,,,,,,,,,,,,,,,,,,,,,
Wrong message size assertion in Pulsar's batch message,FLINK-29613,13486037,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,qiaomengnan,qiaomengnan,13/Oct/22 06:29,20/Oct/22 02:37,04/Jun/24 20:41,19/Oct/22 03:45,1.15.2,1.16.0,1.17.0,,,,1.15.3,1.16.0,1.17.0,,Connectors / Pulsar,,,,0,pull-request-available,,,,"java.lang.RuntimeException: One or more fetchers have encountered exception
at nextMessageIdorg.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:109)
at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: SplitFetcher thread 1 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
... 1 more
Suppressed: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
... 7 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently.
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:61)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:43)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:94)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:160)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:52)
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
... 6 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently.
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:61)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:43)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:94)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:160)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:52)
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
... 6 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 03:45:21 UTC 2022,,,,,,,,,,"0|z19bo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 08:03;martijnvisser;I don't think this is a bug, like the error message says this is currently not supported so it would be a new feature request

Please explain how this can be reproduced. ;;;","13/Oct/22 11:56;syhily;[~martijnvisser] This should be a bug. Because we just accidentally use a wrong assertion for judging if this is a batch message. ;;;","13/Oct/22 12:18;martijnvisser;Thanks for clarifying [~syhily] - What would be a descriptive title for this ticket? ;;;","13/Oct/22 12:42;syhily;I think it should be ""Wrong message size assertion in Pulsar's batch message"";;;","19/Oct/22 01:20;tison;master via 8cb0297c2f092f42f989b4811aa438b6810aef8d
1.15 via f44ffe5a62ebb8992cfcbce0841a0062b6e3cb4d;;;","19/Oct/22 03:45;tison;1.16 via f625a3244ca2c704eda04bdf3b009ffac1735978;;;",,,,,,,,,,,,,,,,,
Extract changelog files out of DataFileMeta#extraFiles,FLINK-29612,13486027,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,13/Oct/22 04:39,21/Oct/22 08:06,04/Jun/24 20:41,21/Oct/22 08:06,table-store-0.3.0,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently changelog files are stored as extra files in {{DataFileMeta}}. However for the full compaction changelog we're about to introduce, it cannot be added as extra files because their statistics might be different from the corresponding merge tree files.

We need to extract changelog files out of DataFileMeta#extraFiles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-13 04:39:53.0,,,,,,,,,,"0|z19bls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky tests in CoBroadcastWithNonKeyedOperatorTest,FLINK-29611,13486020,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,sopan98,sopan98,13/Oct/22 04:28,31/Oct/22 11:33,04/Jun/24 20:41,31/Oct/22 11:33,,,,,,,,,,,,,,,0,pull-request-available,,,,"The test _org.apache.flink.streaming.api.operators.co.CoBroadcastWithNonKeyedOperatorTest.testMultiStateSupport_ has the following failure:

Failures:
[ERROR]   CoBroadcastWithNonKeyedOperatorTest.testMultiStateSupport:74 
Wrong Side Output: arrays first differed at element [0]; expected:<Record @ 15 : 9:key.6->6> but was:<Record @ 15 : 9:key.5->5>

I used the tool [NonDex|https://github.com/TestingResearchIllinois/NonDex] to find this flaky test. 
Command: mvn edu.illinois:nondex-maven-plugun:1.1.2:nondex -Dtest='Fully Qualified Test Name'

I analyzed the assertion failure and found that the root cause is because the test method calls ctx.getBroadcastState(STATE_DESCRIPTOR).immutableEntries() which calls the entrySet() method of the underlying HashMap. entrySet() returns the entries in a non-deterministic way, causing the test to be flaky. 

The fix would be to change _HashMap_ to _LinkedHashMap_ where the Map is getting initialized.
On further analysis, it was found that the Map is getting initialized on line 53 of org.apache.flink.runtime.state.HeapBroadcastState class.

After changing from HashMap to LinkedHashMap, the above test is passing.

Edit: Upon making this change and running the CI, it was found that the tests org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.batchKeyedBroadcastExecution and org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.batchBroadcastExecution were failing. Upon further investigation, I found that these tests were also flaky and depended on the earlier made change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 11:33:13 UTC 2022,,,,,,,,,,"0|z19bk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 08:20;martijnvisser;[~sopan98] Are you encountering this on your local machine? This test is being run during every PR, every merged commit and also during the nightly build jobs, but it has never been flaky (else it would have been registered in Jira before).;;;","18/Oct/22 21:49;sopan98;[~martijnvisser] 
This test depends on the order of iteration in `HashMap.entrySet()` and can fail for some orders. I am running this on my local machine. I have used a maven plugin NonDex, which can be used to identify such tests. One can reproduce it with the command `mvn edu.illinois:nondex-maven-plugun:1.1.2:nondex -Dtest=org.apache.flink.streaming.api.operators.co.CoBroadcastWithNonKeyedOperatorTest#testMultiStateSupport`. Even if the test was not failing during the daily jobs, it'd be good to not depend on the `HashMap.entrySet()` that gives back results in an undefined manner [JavaDoc|https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html#entrySet--] for this. Therefore, to remove non-determinism completely, we can do this change.;;;","19/Oct/22 09:04;martijnvisser;[~chesnay] WDYT?;;;","27/Oct/22 04:21;sopan98;[~martijnvisser] Here is my PR: [https://github.com/apache/flink/pull/21151]

Let me know your feedback;;;","31/Oct/22 11:33;chesnay;Sorry but we are busy enough without chasing theoretical test instabilities. Closing this issue and PR.;;;",,,,,,,,,,,,,,,,,,
Infinite timeout is used in SavepointHandlers and CheckpointTriggerHandler calls to RestfulGateway,FLINK-29610,13486003,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Jiale,Jiale,Jiale,13/Oct/22 02:10,15/Aug/23 22:34,04/Jun/24 20:41,,,,,,,,,,,,Runtime / REST,,,,0,pull-request-available,stale-assigned,,,"In {{{}SavepointHandlers{}}}, both {{[StopWithSavepointHandler|https://github.com/apache/flink/blob/cd8ea8d5b207569f68acc5a3c8db95cd2ca47ba6/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/savepoints/SavepointHandlers.java#L214]}} and {{[SavepointTriggerHandler|https://github.com/apache/flink/blob/cd8ea8d5b207569f68acc5a3c8db95cd2ca47ba6/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/savepoints/SavepointHandlers.java#L258]}} are calling {{RestfulGateway}} with {{RpcUtils.INF_TIMEOUT}}

Same thing happens in the {{[CheckpointTriggerHandler|https://github.com/apache/flink/blob/8e66be89dfcb54b7256d51e9d89222ae6701061f/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/checkpoints/CheckpointHandlers.java#L146]}}

As pointed out in [this|https://github.com/apache/flink/pull/20852#discussion_r992218970] discussion, we will need to either figure out why {{RpcUtils.INF_TIMEOUT}} is used, or remove it if there is no strong reason to use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27101,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:34:58 UTC 2023,,,,,,,,,,"0|z19bgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:25;gaoyunhaii;Thanks [~Jiale] for creating the issue. Since it seems the handler only submits a request to the timer thread of the CheckpointCoordinator, it looks to me that we might also use ASK_TIMEOUT_DURATION for these rpc calls. ;;;","20/Oct/22 23:16;Jiale;Thanks [~gaoyunhaii] for the extra context. Will look into this and potentially follow up here with a PR.;;;","27/Oct/22 07:08;gaoyunhaii;Thanks [~Jiale] for tracking the issue! I assigned the issue to you. ;;;","29/Oct/22 03:53;Jiale;[~gaoyunhaii] I am curious what is the difference between ASK_TIMEOUT_DURATION and [RestConfiguration.timeout|https://github.com/apache/flink/blob/5d66e82915eace9342c175163b17f610bfbf7fa4/flink-runtime/src/main/java/org/apache/flink/runtime/webmonitor/WebMonitorEndpoint.java#L282] and is it ok to use the latter for the fix?;;;","31/Oct/22 09:51;gaoyunhaii;Hi [~Jiale] , It looks to me RestConfiguration.timeout is used for the REST Handlers requests, while the above method  use INF_TIMEOUT while requesting to the dispatcher, which is in fact an akka actor. ;;;","04/Nov/22 06:10;Jiale;[~gaoyunhaii] thanks for the info!

I created a PR [https://github.com/apache/flink/pull/21239] as per my understanding, it would be nice if you may take a look. [~gaoyunhaii] [~chesnay]

 

Meanwhile I am tracking how this Timeout will be used all the way till [here|https://github.com/apache/flink/blob/e9f3ec93aad7cec795c765c937ee71807f5478cf/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L856-L879]. And it seems all those timeout are not being used later? 

 

FYI [~thomasWeise] this is part of unfinished work from FLINK-27101;;;","17/Nov/22 09:59;gaoyunhaii;Thanks [~Jiale] for the PR! I'll have a look;;;","15/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,
Clean up jobmanager deployment on suspend after recording savepoint info,FLINK-29609,13485911,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,12/Oct/22 14:10,16/Nov/22 09:21,04/Jun/24 20:41,16/Nov/22 09:21,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,3,pull-request-available,,,,"Currently in case of suspending with savepoint. The jobmanager pod will linger there forever after cancelling the job.

This is currently used to ensure consistency in case the operator/cancel-with-savepoint operation fails.

Once we are sure however that the savepoint has been recorded and the job is shut down, we should clean up all the resources. Optionally we can make this configurable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 09:21:06 UTC 2022,,,,,,,,,,"0|z19aw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 04:41;sriramgr;[~gyfora] - Please assign this to me.;;;","26/Oct/22 16:23;Miuler;Please, take into account the stateless batch processes, which once finished processing, should clean all the resources;;;","26/Oct/22 18:19;sriramgr;Sure [~Miuler] . I have started exploring the issue. Just incase if someone can help me with this information - Is this issue happening for both application and session mode?. ;;;","26/Oct/22 19:25;gyfora;This feature is only enabled for Application clusters. The current behaviour is fully intentional to allow job manager access after job completion / shutdown/ failure so that the operator can better track whats going on.

The improvement we are looking for here is that once the operator actually recorded the final state in the CR status we could actually shut down resources as we don't need to check anymore.;;;","28/Oct/22 09:44;sriramgr;I found this place where we are not removing the JM pod. [https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-ope[…]che/flink/kubernetes/operator/service/AbstractFlinkService.java#L350. |https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L350]

But we can't remove the JM pod as it is. Because pod upgrades and rollback also will get impacted.

Can we have conditions like the pod can be removed after no action?. Any better suggestions will be appreciated. Thanks in advance.;;;","29/Oct/22 13:33;sriramgr;Please don't reassign to others for sometime unless it is immediately required. I am spending time on this to find a better solution. ;;;","31/Oct/22 04:34;Miuler;[~sriramgr] In my opinion, this should only happen in application mode, in session mode it should continue to exist waiting for a new job.;;;","10/Nov/22 18:02;gyfora;[~sriramgr] I would like to take over this ticket unless you have made some progress.
This has become time critical for us :) ;;;","10/Nov/22 18:54;sriramgr;I made changes in the reconciliation logic and testing. But you can pick this up.;;;","16/Nov/22 09:21;gyfora;merged to main 6e597d4c0a144a01840b40e6b669e558970c55d0;;;",,,,,,,,,,,,,
使用 pyflink1.17dev   datastream 经过reduce 后 add_sink(FlinkKafkaProducer()) 有问题,FLINK-29608,13485879,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,zyfeiyu,zyfeiyu,12/Oct/22 12:15,15/Oct/22 07:37,04/Jun/24 20:41,12/Oct/22 21:16,,,,,,,,,,,API / Python,,,,0,,,,,"13> (1,missing)
6> (1,offset)
Traceback (most recent call last):
  File ""stream.py"", line 84, in <module>
    main()
  File ""stream.py"", line 79, in main
    env.execute('datastream_api_demo')
  File ""/home/ustc/anaconda3/envs/pcb_server/lib/python3.8/site-packages/pyflink/datastream/stream_execution_environment.py"", line 764, in execute
    return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
  File ""/home/ustc/anaconda3/envs/pcb_server/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1321, in __call__
    return_value = get_return_value(
  File ""/home/ustc/anaconda3/envs/pcb_server/lib/python3.8/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/home/ustc/anaconda3/envs/pcb_server/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o0.execute.
: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
        at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
        at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
        at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
        at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
        at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
        at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
        at akka.dispatch.OnComplete.internal(Future.scala:300)
        at akka.dispatch.OnComplete.internal(Future.scala:297)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
        at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
        at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
        at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
        at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
        at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
        at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
        at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
        at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
        at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
        at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
        at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
        at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
        at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
        at jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        ... 5 more
Caused by: org.apache.flink.runtime.taskmanager.AsynchronousException: Caught exception while processing timer.
        at org.apache.flink.streaming.runtime.tasks.StreamTask$StreamTaskAsyncExceptionHandler.handleAsyncException(StreamTask.java:1569)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.handleAsyncException(StreamTask.java:1544)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1684)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$deferCallbackToMailbox$22(StreamTask.java:1673)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:832)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:781)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: TimerException\{org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator}
        ... 15 more
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:96)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:77)
        at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
        at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
        at org.apache.flink.streaming.api.operators.python.process.collector.RunnerOutputCollector.collect(RunnerOutputCollector.java:52)
        at org.apache.flink.streaming.api.operators.python.process.AbstractExternalOneInputPythonFunctionOperator.emitResult(AbstractExternalOneInputPythonFunctionOperator.java:133)
        at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.invokeFinishBundle(AbstractExternalPythonFunctionOperator.java:100)
        at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByTime(AbstractPythonFunctionOperator.java:295)
        at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$open$0(AbstractPythonFunctionOperator.java:114)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1682)
        ... 14 more
*{color:red}Caused by: java.lang.ClassCastException: class org.apache.flink.api.java.tuple.Tuple2 cannot be cast to class org.apache.flink.types.Row (org.apache.flink.api.java.tuple.Tuple2 and org.apache.flink.types.Row are in unnamed module of loader 'app')*{color}
        at org.apache.flink.formats.json.JsonRowSerializationSchema.serialize(JsonRowSerializationSchema.java:73)
        at org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper.serialize(KafkaSerializationSchemaWrapper.java:71)
        at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:918)
        at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:101)
        at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:245)
        at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
        ... 27 more","python 3.8

pyflink 1.17dev",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 15 07:32:23 UTC 2022,,,,,,,,,,"0|z19ap4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 13:16;martijnvisser;[~zyfeiyu] Can you please translate your ticket to English. Else I'll have to close it due to it being invalid;;;","12/Oct/22 21:16;martijnvisser;[~zyfeiyu] If you can translate the ticket, I will re-open it. ;;;","15/Oct/22 07:32;dianfu;From the exception stack:

{code:java}
Caused by: java.lang.ClassCastException: class org.apache.flink.api.java.tuple.Tuple2 cannot be cast to class org.apache.flink.types.Row (org.apache.flink.api.java.tuple.Tuple2 and org.apache.flink.types.Row are in unnamed module of loader 'app')*
{code}

I believe that the result type of result datastream is Tuple, however the sink accepts Row type as inputs and so you need to convert the result type of the result datastream into Row type before writing it into sink. Could refer to https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/datastream/data_types/#passing-python-records-to-java-operations for more details.

If that's not the case, please provide a simple example which could reproduce this issue.;;;",,,,,,,,,,,,,,,,,,,,
Simplify controller flow by introducing FlinkControllerContext,FLINK-29607,13485877,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,12/Oct/22 12:08,05/Jan/23 19:28,04/Jun/24 20:41,05/Jan/23 19:28,,,,,,,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Currently contextual information such as observer/reconciler implementations, flink service, status recorder, generated configs are created/passed around in many pieces leading to a lot of overall code duplication in the system.

We should introduce context object that capture these bits that could be reused across the controller flow to simplify the logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 19:28:33 UTC 2023,,,,,,,,,,"0|z19aoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 12:09;gyfora;cc [~matyas] ;;;","12/Oct/22 13:49;morhidi;+1 this is a great idea;;;","05/Jan/23 19:28;gyfora;merged to main 35604761eac2c75491811e731ffba540bc7e0363;;;",,,,,,,,,,,,,,,,,,,,
Dynamic Execution Environment,FLINK-29606,13485871,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Invalid,,hamidelmaazouz,hamidelmaazouz,12/Oct/22 11:26,12/Oct/22 13:02,04/Jun/24 20:41,12/Oct/22 11:44,,,,,,,,,,,API / Core,,,,0,,,,,"The goal of this feature ticket is to discuss the possibility of supporting adding/removing sources/sinks to a running Flink job.

If it's possible, then newbie contributors like myself need to know where to start from ^^

This feature will be useful cases where we have dynamic collections of source/sink configurations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 13:02:29 UTC 2022,,,,,,,,,,"0|z19anc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 11:44;martijnvisser;[~hamidelmaazouz] Thanks for creating an issue, but these type of things should be discussed on the Dev mailing list. See https://flink.apache.org/community.html#mailing-lists for all details;;;","12/Oct/22 13:02;hamidelmaazouz;[~martijnvisser] Thanks for the info, I will move discussion of this matter to the mailing lists.;;;",,,,,,,,,,,,,,,,,,,,,
Create a bounded version of SourceTestSuiteBase#testSourceMetrics,FLINK-29605,13485864,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,echauchot,echauchot,echauchot,12/Oct/22 10:51,07/Mar/23 10:16,04/Jun/24 20:41,07/Mar/23 10:16,,,,,,,,,,,Tests,,,,0,pull-request-available,,,,"_SourceTestSuiteBase#testSourceMetrics_ is targeted to unbounded sources. Metrics tests still make sense with bounded sources.  For the bounded case, _SourceTestSuiteBase#testSourceMetrics,_ [killing the job|https://github.com/apache/flink/blob/4934bd69052f2a69e8021d337373f4480c802359/flink-test-utils-parent/flink-connector-test-utils/src/main/java/org/apache/flink/connector/testframe/testsuites/SourceTestSuiteBase.java#L469]  is not needed and in some cases (fast job) the kill is executed when the job is already in the finished state leading to a confusing exception. This is why for the bounded case the killJob should be removed. ",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29563,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 10:16:05 UTC 2023,,,,,,,,,,"0|z19als:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 10:52;echauchot;What I propose is to allow the implemeter to pass the boundedness in its ITCase to run the corresponding version of this test.;;;","12/Oct/22 10:55;echauchot;[~chesnay] can you assign me this ticket as well? thanks;;;","07/Mar/23 10:16;echauchot;This test is more targeted to streaming workloads, closing this issue.;;;",,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for CountVectorizer,FLINK-29604,13485853,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,yunfengzhou,yunfengzhou,12/Oct/22 10:02,23/Nov/22 09:29,04/Jun/24 20:41,23/Nov/22 09:29,ml-2.2.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add Estimator and Transformer for CountVectorizer.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.CountVectorizer. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 10:02:04.0,,,,,,,,,,"0|z19ajc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for StopWordsRemover,FLINK-29603,13485852,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,12/Oct/22 10:01,06/Jan/23 06:08,04/Jun/24 20:41,06/Jan/23 06:08,ml-2.2.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add Transformer for StopWordsRemover.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.StopWordsRemover. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 10:01:40.0,,,,,,,,,,"0|z19aj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for SQLTransformer,FLINK-29602,13485851,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,12/Oct/22 10:01,21/Nov/22 07:19,04/Jun/24 20:41,21/Nov/22 07:18,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add Transformer for SQLTransformer.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.SQLTransformer. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 10:01:21.0,,,,,,,,,,"0|z19aiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for UnivariateFeatureSelector,FLINK-29601,13485850,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,yunfengzhou,yunfengzhou,12/Oct/22 10:00,06/Jan/23 06:07,04/Jun/24 20:41,06/Jan/23 06:07,ml-2.2.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add Estimator and Transformer for UnivariateFeatureSelector.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.UnivariateFeatureSelector. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 10:00:43.0,,,,,,,,,,"0|z19aio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for BucketedRandomProjectionLSH,FLINK-29600,13485849,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,12/Oct/22 10:00,10/Jan/23 04:32,04/Jun/24 20:41,,ml-2.2.0,,,,,,,,,,Library / Machine Learning,,,,0,,,,,"Add Estimator and Transformer for BucketedRandomProjectionLSH.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.BucketedRandomProjectionLSH. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 10:00:11.0,,,,,,,,,,"0|z19aig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for MinHashLSH,FLINK-29599,13485848,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,yunfengzhou,yunfengzhou,12/Oct/22 09:59,10/Jan/23 04:10,04/Jun/24 20:41,10/Jan/23 04:10,ml-2.2.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,,,,,"Add Estimator and Transformer for MinHashLSH.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.MinHashLSH. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 09:59:21.0,,,,,,,,,,"0|z19ai8:",9223372036854775807,Duplicate of FLINK-30401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for Imputer,FLINK-29598,13485847,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,yunfengzhou,yunfengzhou,12/Oct/22 09:57,23/Nov/22 09:29,04/Jun/24 20:41,23/Nov/22 09:29,ml-2.2.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add Estimator and Transformer for Imputer

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.Imputer. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 09:57:52.0,,,,,,,,,,"0|z19ai0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for QuantileDiscretizer,FLINK-29597,13485845,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,12/Oct/22 09:54,10/Jan/23 05:52,04/Jun/24 20:41,,,,,,,,,,,,Library / Machine Learning,,,,0,,,,,"Add Estimator and Transformer for QuantileDiscretizer.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.QuantileDiscretizer. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 09:54:06.0,,,,,,,,,,"0|z19ahk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for RFormula,FLINK-29596,13485840,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,12/Oct/22 09:31,10/Jan/23 04:28,04/Jun/24 20:41,,ml-2.2.0,,,,,,,,,,Library / Machine Learning,,,,0,,,,,"Add Estimator and Transformer for RFormula.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.RFormula. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 09:31:48.0,,,,,,,,,,"0|z19agg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for ChiSqSelector,FLINK-29595,13485839,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Jiang Xin,yunfengzhou,yunfengzhou,12/Oct/22 09:30,10/Jan/23 05:23,04/Jun/24 20:41,10/Jan/23 05:22,ml-2.2.0,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add the Estimator and Transformer for ChiSqSelector.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.ChiSqSelector. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 05:23:50 UTC 2023,,,,,,,,,,"0|z19ag8:",9223372036854775807,This is duplicate of FLINK-29601,,,,,,,,,,,,,,,,,,,"10/Jan/23 05:23;lindong;ChiSqSelector has been be implemented as part of the UnivariateFeatureSelector.

Note that Spark has also marked ChiSqSelector as deprecated and delegate the corresponding functionality to UnivariateFeatureSelector.;;;",,,,,,,,,,,,,,,,,,,,,,
RMQSourceITCase.testMessageDelivery timed out,FLINK-29594,13485830,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,12/Oct/22 09:00,20/Aug/23 22:35,04/Jun/24 20:41,,rabbitmq-3.0.1,,,,,,,,,,Connectors/ RabbitMQ,,,,0,auto-deprioritized-major,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41843&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38211] failed (not exclusively) due to {{RMQSourceITCase.testMessageDelivery}} timing out.

I wasn't able to reproduce it locally with 200 test runs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30485,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:17 UTC 2023,,,,,,,,,,"0|z19ae8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 07:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42908&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=38551;;;","09/Nov/22 07:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42955&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38595;;;","10/Nov/22 06:48;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42999&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=38639;;;","05/Dec/22 14:32;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43726&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=40460;;;","16/Dec/22 08:19;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43954&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=41132;;;","19/Dec/22 09:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44024&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=39295;;;","19/Dec/22 09:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44034&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=38550;;;","02/Jan/23 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44358&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=40687;;;","03/Jan/23 09:10;mapohl;This issue will be resolved on {{master}} with the removal of all RabbitMQ-related code in FLINK-30485. It's still an issue in the rabbitmq code which should be resolved. I added {{rabbitmq-3.0.1}} as an affected version;;;","05/Jan/23 07:17;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44441&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=39344;;;","06/Jan/23 07:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44525&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=37809;;;","09/Jan/23 08:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44558&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=39450;;;","09/Jan/23 08:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44569&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37818;;;","09/Jan/23 08:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44572&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=39029;;;","09/Jan/23 12:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44604&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=39375;;;","10/Jan/23 10:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=40496;;;","10/Jan/23 10:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=40447;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,
Add Approximate Quantile Util,FLINK-29593,13485824,13485822,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,12/Oct/22 08:38,23/Nov/22 09:29,04/Jun/24 20:41,23/Nov/22 09:29,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add a helper to compute an approximate quantile summary of distributed or streaming datasets. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 08:38:57.0,,,,,,,,,,"0|z19acw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for RobustScaler,FLINK-29592,13485822,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,12/Oct/22 08:35,17/Nov/22 08:44,04/Jun/24 20:41,17/Nov/22 08:44,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add Transformer and Estimator for RobustScaler.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 08:35:04.0,,,,,,,,,,"0|z19acg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add built-in UDFs to convert between arrays and vectors,FLINK-29591,13485802,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,12/Oct/22 06:10,19/Apr/23 01:48,04/Jun/24 20:41,19/Apr/23 01:48,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:47:21 UTC 2023,,,,,,,,,,"0|z19a88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:47;lindong;Merged to apache/flink-ml master branch fe2966db6b5419459c98752cd1eaced9d4eb46ea;;;",,,,,,,,,,,,,,,,,,,,,,
Fix literal issue in HiveDialect,FLINK-29590,13485775,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,12/Oct/22 02:16,17/Oct/22 09:16,04/Jun/24 20:41,17/Oct/22 09:16,1.16.0,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,"in FLINK-26474, we try to fold constant, but it brings a issue that the folded constant like `Double.NAN` and no-primitive type  can't be convert into calcite literal in method  `HiveParserRexNodeConverter#convertConstant`.

For example, the following code will throw an exception ""org.apache.hadoop.hive.ql.parse.SemanticException: NaN"" in method `HiveParserRexNodeConverter#convertConstant`
{code:java}
// hive dialect
SELECT asin(2); {code}
To fix it, we need to figure out such case and then not to fold constant .

 

in FLINK-27017, we use Hive's `GenericUDFOPDivide` to do divide for better compatibility, but it bring a issue that when use a int/long literal as divisor, the result type passed and inferred type may not match.

The fix it, we need to make the result type match the inferred type.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 06:32:59 UTC 2022,,,,,,,,,,"0|z19a28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 11:27;jark;Fixed in 
 - master: dbf5dce61493cd0b7e73ae3af39146caf52c0767
 - release-1.16: 990eac8c69fc14d7dc18cc1032268619ac46edbc;;;","13/Oct/22 11:27;jark;[~luoyuxia] could you open a pull request for release-1.16 branch?;;;","14/Oct/22 01:28;luoyuxia;[~jark]  Sure, but we have to merge the pr [https://github.com/apache/flink/pull/21034]  for FLINK-29337  first since there's some conflicts between them.;;;","14/Oct/22 06:32;luoyuxia;[~jark] The pr. for release-1.16 is available in [https://github.com/apache/flink/pull/21061];;;",,,,,,,,,,,,,,,,,,,
Data Loss in Sink GlobalCommitter during Task Manager recovery,FLINK-29589,13485753,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,KristoffSC,KristoffSC,11/Oct/22 20:43,20/Oct/22 09:50,04/Jun/24 20:41,,1.14.0,,,,,,,,,,,,,,0,,,,,"Flink's Sink architecture with global committer seems to be vulnerable for data loss during Task Manager recovery. The entire checkpoint can be lost by _GlobalCommitter_ resulting with data loss.

Issue was observed in Delta Sink connector on a real 1.14.x cluster and was replicated using Flink's 1.14.6 Test Utils classes.

Scenario:
 #  Streaming source emitting constant number of events per checkpoint (20 events per commit for 5 commits in total, that gives 100 records).
 #  Sink with parallelism > 1 with committer and _GlobalCommitter_ elements.
 #  _Commiters_ processed committables for *checkpointId 2*.
 #  _GlobalCommitter_ throws exception (desired exception) during *checkpointId 2* (third commit) while processing data from *checkpoint 1* (it is expected to global committer architecture lag one commit behind in reference to rest of the pipeline).
 # Task Manager recovery, source resumes sending data.
 # Streaming source ends.
 # We are missing 20 records (one checkpoint).

What is happening is that during recovery, committers are performing ""retry"" on committables for *checkpointId 2*, however those committables, reprocessed from ""retry"" task are not emit downstream to the global committer. 

The issue can be reproduced using Junit Test build with Flink's TestSink.
The test was [implemented here|https://github.com/kristoffSC/flink/blob/Flink_1.14_DataLoss_SinkGlobalCommitter/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkITCase.java#:~:text=testGlobalCommitterMissingRecordsDuringRecovery] and it is based on other tests from `SinkITCase.java` class.
The test reproduces the issue in more than 90% of runs.

I believe that problem is somewhere around *SinkOperator::notifyCheckpointComplete* method. In there we see that Retry async task is scheduled however its result is never emitted downstream like it is done for regular flow one line above.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 09:31:53 UTC 2022,,,,,,,,,,"0|z199xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 21:04;martijnvisser;If there's indeed a reproducible case with data loss I think we should consider making this a blocker;;;","12/Oct/22 09:17;chesnay;Does this also happen in 1.15 / 1.16-SNAPSHOT and/or the v2 Sink API?;;;","12/Oct/22 09:31;KristoffSC;Hi [~chesnay]

V2 on 1.15, 1.16 and 1.17 has its own issues that we have found and actually we are working to fix them with Fabian Paul.

https://issues.apache.org/jira/browse/FLINK-29509
https://issues.apache.org/jira/browse/FLINK-29583
https://issues.apache.org/jira/browse/FLINK-29512
https://issues.apache.org/jira/browse/FLINK-29627

With those stil on the plate we cant really tell if there is a data loss on V2 since Task manager is failing to start during recovery when running Sink with global committer.


;;;",,,,,,,,,,,,,,,,,,,,
Add Flink Version and Application Version to Savepoint properties,FLINK-29588,13485731,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,claraxiong,claraxiong,11/Oct/22 17:33,13/Dec/22 14:42,04/Jun/24 20:41,13/Dec/22 14:42,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"It is common that users need to upgrade long running FlinkDeployment's. An application upgrade or a major Flink upgrade might break the applications due to schema incompatibilities, especially the latter([Link)|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/#compatibility-table]. In this case, the users need to manually restore to a savepoint that is compatible with the version the user wants to try or re-process. Currently Flink Operator returns a list of completed Savepoint's for a FlinkDeployment.  It will be helpful that the Savepoint's in SavepointHistory have the properties for Flink Version and application version so users can easily determine which savepoint to use.

 
 * {{String flinkVersion}}
 * {{String appVersion}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 08:48:51 UTC 2022,,,,,,,,,,"0|z199t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 18:16;gyfora;What do you mean by appVersion here?;;;","11/Oct/22 18:20;gyfora;Since savepoints are compatible across version upgrades I think it is not very important to record the version it was created with in practice.

Users don't usually juggle multiple Flink versions but if this is important to them they could simply change the savepoint/checkpoint directory based on their versioning scheme to record this information. Or when they make an upgrade just keep track of this manually.



[~thw]  do you see any practical value in adding this?;;;","11/Oct/22 19:00;claraxiong;appVersion is the application version of the FlinkDeployment.

If a user doesn't find a problem until after a couple of rounds of upgrades and decides to go back to re-process or troubleshoot, they have already had many savepoints taken. Timestamp is a way to look for the right one but they probably need a way to keep the chronological history of versions. ;;;","11/Oct/22 19:13;gyfora;We don't really have a concept of application version for FlinkDeployments as far as I know it.

Kubernetes resources have a generation but that is probably not very useful;;;","11/Nov/22 08:48;gyfora;should we close this ticket?;;;",,,,,,,,,,,,,,,,,,
Fail to generate code for  SearchOperator ,FLINK-29587,13485659,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,luoyuxia,luoyuxia,11/Oct/22 12:27,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,0,auto-deprioritized-major,pull-request-available,,,"Can be reproduced with the following code with Hive dialect
{code:java}
// hive dialect

tableEnv.executeSql(""create table table1 (id int, val string, val1 string, dimid int)"");
tableEnv.executeSql(""create table table3 (id int)"");

CollectionUtil.iteratorToList(
        tableEnv.executeSql(
                        ""select table1.id, table1.val, table1.val1 from table1 left semi join""
                                + "" table3 on table1.dimid = table3.id and table3.id = 100 where table1.dimid = 200"")
                .collect());{code}
The  plan is 
{code:java}
LogicalSink(table=[*anonymous_collect$1*], fields=[id, val, val1])
  LogicalProject(id=[$0], val=[$1], val1=[$2])
    LogicalFilter(condition=[=($3, 200)])
      LogicalJoin(condition=[AND(=($3, $4), =($4, 100))], joinType=[semi])
        LogicalTableScan(table=[[test-catalog, default, table1]])
        LogicalTableScan(table=[[test-catalog, default, table3]])BatchPhysicalSink(table=[*anonymous_collect$1*], fields=[id, val, val1])
  BatchPhysicalNestedLoopJoin(joinType=[LeftSemiJoin], where=[$f1], select=[id, val, val1], build=[right])
    BatchPhysicalCalc(select=[id, val, val1], where=[=(dimid, 200)])
      BatchPhysicalTableSourceScan(table=[[test-catalog, default, table1]], fields=[id, val, val1, dimid])
    BatchPhysicalExchange(distribution=[broadcast])
      BatchPhysicalCalc(select=[SEARCH(id, Sarg[]) AS $f1])
        BatchPhysicalTableSourceScan(table=[[test-catalog, default, table3]], fields=[id]) {code}
 

But it'll throw exception when generate code for it.

The exception is 

 

 
{code:java}
java.util.NoSuchElementException
    at com.google.common.collect.ImmutableRangeSet.span(ImmutableRangeSet.java:203)
    at org.apache.calcite.util.Sarg.isComplementedPoints(Sarg.java:148)
    at org.apache.flink.table.planner.codegen.calls.SearchOperatorGen$.generateSearch(SearchOperatorGen.scala:87)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:474)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:57)
    at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:143)
    at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.$anonfun$generateProcessCode$4(CalcCodeGenerator.scala:140)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:140)
    at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:164)
    at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:49)
    at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:100)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:158)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange.translateToPlanInternal(BatchExecExchange.java:136) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:17 UTC 2023,,,,,,,,,,"0|z199d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 02:29;luoyuxia;After a detail debug, I found the reason is that the reduced expression ` *200 = table1.dimid* and table1.dimid = 100`  won't be reduced to false in `RexSimplify#simplify`. Then it will constribuct SqlSearchOperator with empty range, which then brings the exception as reported in this jira.

 

To fix it, the better way is to fix the logic to make such case can be simpified, so that we can fix this Jira and the such sql `select * from t where  a = 100 and 200 = a` can benefit from it.;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,
Let write buffer spillable,FLINK-29586,13485655,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Oct/22 12:09,04/Jan/23 11:31,04/Jun/24 20:41,04/Jan/23 11:31,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,Column format and remote DFS may greatly affect the performance of compaction. We can change the writeBuffer to spillable to improve the performance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 11:31:51 UTC 2023,,,,,,,,,,"0|z199c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 11:31;lzljs3620320;master: c8f6a4e1ea4363f8a5f7500adc3183748432d441;;;",,,,,,,,,,,,,,,,,,,,,,
Migrate TableSchema to Schema for Hive connector,FLINK-29585,13485652,13478113,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,luoyuxia,luoyuxia,11/Oct/22 12:02,22/Mar/23 06:11,04/Jun/24 20:41,22/Mar/23 01:35,,,,,,,1.18.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,"`TableSchema` is deprecated, we should migrate it to Schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 01:35:36 UTC 2023,,,,,,,,,,"0|z199bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 13:13;aitozi;Hi [~luoyuxia], I would like to give a try on this, can I take this ticket ?;;;","12/Oct/22 01:20;luoyuxia;Sure.  [~jark] Could you please assign this ticket to [~aitozi] ?;;;","15/Nov/22 02:23;aitozi;fyi, I'm trying to work on this now. I will ping you for review after I finished this, thanks;;;","22/Mar/23 01:35;luoyuxia;master: 3d350485755e47d2b09ac1b2067b6119ef960b5a;;;",,,,,,,,,,,,,,,,,,,
Upgrade java 11 version on the microbenchmark worker,FLINK-29584,13485649,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,11/Oct/22 11:49,04/Nov/22 12:07,04/Jun/24 20:41,04/Nov/22 12:07,1.17.0,,,,,,1.17.0,,,,Benchmarks,,,,0,,,,,"It looks like the older JDK 11 builds have problems with JIT in the microbenchmarks, as for example visible [here|http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&env=2]. Locally I was able to reproduce this problem and the issue goes away after upgrading to a newer JDK 11 build.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 12:07:18 UTC 2022,,,,,,,,,,"0|z199aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 12:28;chesnay;Do you think this is also relevant for production?;;;","11/Oct/22 13:08;pnowojski;I think it might, but I think the issue we are seeing here is probably related to a quite ancient early access build of openjdk. I doubt it would affect many production setups.;;;","04/Nov/22 12:07;pnowojski;I've upgraded the JDK version but it didn't change anything.;;;",,,,,,,,,,,,,,,,,,,,
Ensure correct subtaskId and checkpointId is set during committer state migration,FLINK-29583,13485648,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fpaul,fpaul,11/Oct/22 11:48,11/Nov/22 08:33,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,Connectors / Common,,,,0,,,,,"We already discovered two problems during recovery of committers when a post commit topology is used

 
 * https://issues.apache.org/jira/browse/FLINK-29509
 * https://issues.apache.org/jira/browse/FLINK-29512

Both problems also apply when recovering Flink 1.14 unified sinks committer state and migrate it to the extended unified model.

As part of this ticket we should fix both issues for the migration and also increase the test coverage for the migration cases i.e. add test cases in CommitterOperatorTest and CommittableCollectorSerializerTest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-11 11:48:30.0,,,,,,,,,,"0|z199ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointWriter should be usable without any transformation,FLINK-29582,13485641,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Oct/22 11:15,20/Oct/22 17:30,04/Jun/24 20:41,20/Oct/22 17:30,,,,,,,1.17.0,,,,API / State Processor,,,,0,pull-request-available,,,,"The SavepointWriter of the state processor API currently enforces at least one transformation to be defined be the user.

This is an irritating limitation; this means you can't use the API to delete a state or use the new uid remapping function from FLINK-29457 without specifying some dummy transformation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 17:30:37 UTC 2022,,,,,,,,,,"0|z19994:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 17:30;chesnay;master: d17ceaf2f8cb0a36c2b629b9f6f87e020ce79395;;;",,,,,,,,,,,,,,,,,,,,,,
Emit warning events for session job reconciliation exception,FLINK-29581,13485634,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,11/Oct/22 10:31,08/Nov/22 09:15,04/Jun/24 20:41,08/Nov/22 09:15,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Same as FlinkDeployment, will be useful for monitoring.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/399,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 09:15:17 UTC 2022,,,,,,,,,,"0|z1997k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 09:15;gyfora;merged to main 23109e2d479f522862ce2195a0c588199fa09c42;;;",,,,,,,,,,,,,,,,,,,,,,
pulsar.consumer.autoUpdatePartitionsIntervalSeconds doesn't work and should be removed,FLINK-29580,13485632,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,syhily,syhily,syhily,11/Oct/22 10:30,07/Nov/22 07:41,04/Jun/24 20:41,18/Oct/22 05:48,1.15.2,1.16.1,1.17.0,,,,1.17.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 05:48:34 UTC 2022,,,,,,,,,,"0|z19974:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 05:26;tison;It's OK to apply on new minor releases. But will it cause breaking changes for previous versions? I tend not to pick this change since it's not a bug fix.;;;","18/Oct/22 05:27;tison;master via 2bab5adc1a1d36d35ed971a007db9e311dc0363b;;;","18/Oct/22 05:48;syhily;OK, I think you are right.;;;",,,,,,,,,,,,,,,,,,,,
Flink parquet reader cannot read fully optional elements in a repeated list,FLINK-29579,13485623,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tiansu_yu,tiansu_yu,11/Oct/22 09:33,20/Aug/23 22:35,04/Jun/24 20:41,,1.13.2,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,auto-deprioritized-major,parquet,parquetReader,SchemaValidation,"While trying to read a parquet file containing the following field as part of the schema, 
{code:java}
 optional group attribute_values (LIST) {
    repeated group list {
      optional group element {
        optional binary attribute_key_id (STRING);
        optional binary attribute_value_id (STRING);
        optional int32 pos;
      }
    }
  } {code}
 I encountered the following problem 
{code:java}
Exception in thread ""main"" java.lang.UnsupportedOperationException: List field [optional binary attribute_key_id (STRING)] in List [attribute_values] has to be required. 
	at org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.convertGroupElementToArrayTypeInfo(ParquetSchemaConverter.java:338)
	at org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.convertParquetTypeToTypeInfo(ParquetSchemaConverter.java:271)
	at org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.convertFields(ParquetSchemaConverter.java:81)
	at org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.fromParquetType(ParquetSchemaConverter.java:61)
	at org.apache.flink.formats.parquet.ParquetInputFormat.<init>(ParquetInputFormat.java:120)
	at org.apache.flink.formats.parquet.ParquetRowInputFormat.<init>(ParquetRowInputFormat.java:39) {code}
The main code that raises the problem goes as follows:
{code:java}
private static ObjectArrayTypeInfo convertGroupElementToArrayTypeInfo(
            GroupType arrayFieldType, GroupType elementType) {
        for (Type type : elementType.getFields()) {
            if (!type.isRepetition(Type.Repetition.REQUIRED)) {
                throw new UnsupportedOperationException(
                        String.format(
                                ""List field [%s] in List [%s] has to be required. "",
                                type.toString(), arrayFieldType.getName()));
            }
        }
        return ObjectArrayTypeInfo.getInfoFor(convertParquetTypeToTypeInfo(elementType));
    } {code}
I am not very familiar with internals of Parquet schema. But the problem looks like to me is that Flink is too restrictive on repetition types inside certain nested fields. Would love to hear some feedbacks on this (improvements, corrections / workarounds).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:18 UTC 2023,,,,,,,,,,"0|z19954:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 09:39;tiansu_yu;The source code that reproduces the issue:
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

String schemaString =
    ""message spark_schema {\n""
        + ""  optional binary field_a (STRING);\n""
        + ""  optional binary field_b (STRING);\n""
        + ""  optional int96 field_c;\n""
        + ""}"";
MessageType schema = MessageTypeParser.parseMessageType(schemaString);
ParquetInputFormat format = new ParquetRowInputFormat(new Path(<some-local-parquet-file.parquet>), schema);
var stream = env.createInput(format);
stream.print();

env.execute(); {code}
For privacy reasons, I cannot share the source data itself.;;;","11/Oct/22 09:53;martijnvisser;[~lzljs3620320] Any thoughts on this or anyone else with Parquet/Flink knowledge? ;;;","11/Oct/22 11:42;lzljs3620320;Old ParquetInputFormat has been dropped in 1.14, it is recommend to use new implementation.;;;","11/Oct/22 12:36;tiansu_yu;Thanks for the information [~lzljs3620320]. But currently AWS KDA only supports Flink up to 1.13.2, therefore a Flink update is not an option for us, unfortunately.;;;","11/Oct/22 12:41;martijnvisser;[~tiansu_yu] The Flink community doesn't support Flink 1.13 anymore, only the latest two releases (so Flink 1.14 and Flink 1.15 at the moment, soon Flink 1.15 and Flink 1.16);;;","11/Oct/22 13:20;tiansu_yu;Thanks. This motivates me to take a look on the latest stable version of the Parquet IO section ([https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/formats/parquet/)]

I notice that the last section for reading Avro ReflectionRecord, the example uses a class called [AvroParquetReaders|https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/index.html?org/apache/flink/formats/parquet/avro/AvroParquetReaders.html] but the class itself is annotated with Experimental. I wonder if it is expected that Parquet IO will be changed again soon some time in the future? ;;;","11/Oct/22 13:28;martijnvisser;That's not planned, but new interfaces are usually introduced as Experimental, then go to PublicEvolving before becoming Public. Experimental can be broken between patch version, PublicEvolving only between minor versions and Public only with major versions;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,
Multiple executions of `runVertexCentricIteration` cause memory errors (flink gelly),FLINK-29578,13485620,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,shunyangl,shunyangl,11/Oct/22 09:20,19/Oct/22 11:28,04/Jun/24 20:41,19/Oct/22 11:28,1.15.1,,,,,,,,,,Library / Graph Processing,,,,0,,,,,"When the runVertexCentricIteration function is executed twice it will throw an error: 
java.lang.IllegalArgumentException: Too few memory segments provided. Hash Table needs at least 33 memory segments.
 
We solved this issue by setting the following configuration:
```
VertexCentricConfiguration parameter = new VertexCentricConfiguration();
parameter.setSolutionSetUnmanagedMemory(true);
```
 
However, the execution times cannot be greater than 4 (>4).  After executing four times it will throw an error: java.lang.IllegalArgumentException: Too little memory provided to sorter to perform task. Required are at least 12 pages. Current page size is 32768 bytes.","* Flink 1.15.1
 * Java 11
 * Gradle 7.4.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/22 09:20;shunyangl;example.java;https://issues.apache.org/jira/secure/attachment/13050284/example.java",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Wed Oct 19 11:28:08 UTC 2022,,,,,,,,,,"0|z1994g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 11:28;martijnvisser;Gelly has been removed, see FLINK-29668;;;",,,,,,,,,,,,,,,,,,,,,,
Disable rocksdb wal when restore from full snapshot,FLINK-29577,13485607,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cailiuyang,cailiuyang,cailiuyang,11/Oct/22 08:26,16/Jun/23 06:11,04/Jun/24 20:41,18/May/23 07:30,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"For now, RocksDBFullRestoreOperation and RocksDBHeapTimersFullRestoreOperation does's pass RocksDB::WriteOptions to RocksDBWriteBatchWrapper when restore kv-data, which will use RocksDBWriteBatchWrapper‘s default WriteOptions(doesn't disable rocksdb wal explicitly, see code below), so during restoring from full snapshot, wal is enabled(use more disk and maybe affect rocksdb-write-performance when restoring)

 
{code:java}
// First: RocksDBHeapTimersFullRestoreOperation::restoreKVStateData() doesn't pass WriteOptions to RocksDBWriteBatchWrapper(null as default)

private void restoreKVStateData(
        ThrowingIterator<KeyGroup> keyGroups,
        Map<Integer, ColumnFamilyHandle> columnFamilies,
        Map<Integer, HeapPriorityQueueSnapshotRestoreWrapper<?>> restoredPQStates)
        throws IOException, RocksDBException, StateMigrationException {
    // for all key-groups in the current state handle...
    try (RocksDBWriteBatchWrapper writeBatchWrapper =
            new RocksDBWriteBatchWrapper(this.rocksHandle.getDb(), writeBatchSize)) {
        HeapPriorityQueueSnapshotRestoreWrapper<HeapPriorityQueueElement> restoredPQ = null;
        ColumnFamilyHandle handle = null;
   ......
}


// Second: RocksDBWriteBatchWrapper::flush function doesn't disable wal explicitly when user doesn't pass WriteOptions to RocksDBWriteBatchWrapper
public void flush() throws RocksDBException {
    if (options != null) {
        db.write(options, batch);
    } else {
        // use the default WriteOptions, if wasn't provided.
        try (WriteOptions writeOptions = new WriteOptions()) {
            db.write(writeOptions, batch);
        }
    }
    batch.clear();
}

{code}
 

 

As we known, rocksdb's wal is usesless for flink, so i think we can disable wal for RocksDBWriteBatchWrapper's default WriteOptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32326,,,,,,,,FLINK-11138,,,,,,,"20/Oct/22 08:08;cailiuyang;image-2022-10-20-16-08-15-746.png;https://issues.apache.org/jira/secure/attachment/13051233/image-2022-10-20-16-08-15-746.png","20/Oct/22 08:11;cailiuyang;image-2022-10-20-16-11-04-211.png;https://issues.apache.org/jira/secure/attachment/13051235/image-2022-10-20-16-11-04-211.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 06:34:15 UTC 2022,,,,,,,,,,"0|z1991k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 13:28;yunta;I wonder will the full snapshot restoring duration would be reduced with this patch?;;;","11/Oct/22 14:27;cailiuyang;[~yunta] Sorry, i only observe disk usage doubled during restoring, write performance doesn't test, but i think disable wal maybe improve performance more or less(if i'm wrong, please correct me);;;","12/Oct/22 07:15;yunta;[~cailiuyang] If the disk usage could be reduced indeed, I think this ticket deserved to be merged.;;;","20/Oct/22 03:47;ym;[~cailiuyang] 

Would you share some performance numbers here? I would be interested to see:
 # restore speed/time spent
 # disk usage (as you mentioned)

I can take a look at the PR if the improvement is promising.

 ;;;","20/Oct/22 08:09;cailiuyang;[~ym] 

This is my test case:

1. disable rocksdb managed memory
2. use rocksdb full snapshot strategy
3. KeyProcessor have one subtask, five state(state1, state2, state3, state4, state5), total state size is 2GB
4. the size of state1 is small, the total size of state1 is less than rocksdb default memtable size(64M), such as state1 just have one record

after restoring, rocksdb wal can not be gc(see the png1), the reason i guess is state1's memtable doesn't flushed, and only by gced until state1's memtable be flushed (Rocksdb Full snapshot only take a snapshot).

During my test, I found if state1 ~ state4 size is big but state5 is small(only have one record), then there only be one wal file(see png2)

In my test, disable wal doesn't imporve restore speed, two case(disable / enable wal during restore) is almost the same.

 

!image-2022-10-20-16-08-15-746.png!

!image-2022-10-20-16-11-04-211.png!;;;","21/Oct/22 06:34;yunta;[~cailiuyang] I'm not sure whether the fast disk performance could help mitigate the unnecessary storage impact, especially considering write-ahead-log is flushed sequentially.

If we can really avoid to generate the write-ahead-log, I think this PR deserved to be merged (it should have a unit test to prove the write options has disabled the WAL).;;;",,,,,,,,,,,,,,,,,
Serialized OperatorCoordinators are collected in a not thread-safe ArrayList even though serialization was parallelized in FLINK-26675,FLINK-29576,13485591,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,11/Oct/22 07:34,14/Oct/22 09:36,04/Jun/24 20:41,14/Oct/22 09:36,1.16.0,1.17.0,,,,,1.16.0,1.17.0,,,API / DataStream,Runtime / Coordination,,,0,pull-request-available,test-stability,,,"There's a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41843&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8523] being caused by {{SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution}} on {{master}}:
{code}
Oct 11 01:45:36 [ERROR] Errors: 
Oct 11 01:45:36 [ERROR]   SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution:89 » Runtime Fail...
Oct 11 01:45:36 [INFO] 
Oct 11 01:45:36 [ERROR] Tests run: 1931, Failures: 0, Errors: 1, Skipped: 4
{code}

The actual cause might be a missing {{OperatorCoordinatorHolder}} in {{DefaultOperatorCoordinatorHandler}} (see attached Maven logs that were extracted from the linked build):
{code}
01:44:28,248 [flink-akka.actor.default-dispatcher-5] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3] (1/4)#0 (9babb402557eb959216c28116aabddbe_1dd2eb40b0971d6d849b9e4a69494c88_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkException: No coordinator registered for operator bc764cd8ddf7a0cff126f51c16239658
        at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:117)
        at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031)
        at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588)
        at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26675,,,,,,,,,,,,"11/Oct/22 07:45;mapohl;SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution.log;https://issues.apache.org/jira/secure/attachment/13050282/SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 09:36:45 UTC 2022,,,,,,,,,,"0|z198y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 08:15;mapohl;The issue is reproducible locally by running the test repeatedly until failure.;;;","11/Oct/22 08:20;mapohl;Maybe unrelated, but one test run failed with a different stacktrace while initializing the {{JobMaster}}:
{code}
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:488) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:223) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:901) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:891) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:848) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:830) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:203) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:156) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:361) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:206) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:369) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:346) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[classes/:?]
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[classes/:?]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_345]
	... 3 more
{code};;;","11/Oct/22 08:24;mapohl;I'm increasing the issue's priority to CRITICAL because it seems to be coordination-related and might affect other components. I added {{1.16.0}} because I haven't seen any evidence, yet, that this is a recently added problem.;;;","11/Oct/22 08:55;mapohl;Adding additional log output reveals that the Operator ID in question, indeed, is not registered with the {{DefaultOperatorCoordinatorHandler}}:
{code}
38013 [jobmanager-io-thread-1] DEBUG org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler [] - OperatorIds for ExecutionJobVertex[initialized=true] MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]: [bc764cd8ddf7a0cff126f51c16239658, 605b35e407e90cda15ad084365733fdd]
38013 [jobmanager-io-thread-1] DEBUG org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler [] - OperatorIds for ExecutionJobVertex[initialized=true] Sink: Data stream collect sink [Source: source-1, Source: source-2, Source: source-3]: [ad41044d493a326176171462a740339f]
38013 [jobmanager-io-thread-1] DEBUG org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler [] - Registered Operators: [bc764cd8ddf7a0cff126f51c16239658, 605b35e407e90cda15ad084365733fdd, ad41044d493a326176171462a740339f]
{code}
...but the run fails with not finding the {{OperatorHandler}} for a completely different ID {{feca28aff5a3958840bee985ee7de4d3}}:
{code}
38096 [flink-akka.actor.default-dispatcher-10] WARN  org.apache.flink.runtime.taskmanager.Task [] - MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3] (2/4)#0 (dfa76d436eae78327927f1d9bf6db514_1dd2eb40b0971d6d849b9e4a69494c88_1_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkException: No coordinator registered for operator feca28aff5a3958840bee985ee7de4d3
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:134)
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031)
	at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588)
[...]
{code};;;","11/Oct/22 10:28;mapohl;I finally found the cause of this issue: FLINK-26675 introduces parallel serialization for the operators. These serialized operators are added to the JobVertex's operatorCoordinators which is a basic {{ArrayList}} and, therefore, not thread-safe. Concurrently adding the serialized OperatorCoordinators might result in dropping some of them occassionally.

As a consequence, I'm marking this one as a blocker for 1.16.;;;","11/Oct/22 10:32;mapohl;FYI: [~guoyangze];;;","14/Oct/22 09:36;mapohl;master: dc83e69dbabdc53e89bdc787a93f1fe904211cee
1.16: b05fac1560add86dcb41b0b5a8297326dbc99a19;;;",,,,,,,,,,,,,,,,
Can't Add Temporary Function USING JAR,FLINK-29575,13485517,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,angelok,angelok,11/Oct/22 01:44,11/Oct/22 12:36,04/Jun/24 20:41,11/Oct/22 12:36,1.15.1,,,,,,,,,,Documentation,,,,0,,,,,"In the create documentation for Flink SQL it says:


{code:java}
CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION 
  [IF NOT EXISTS] [catalog_name.][db_name.]function_name 
  AS identifier [LANGUAGE JAVA|SCALA|PYTHON] 
  [USING JAR '<path_to_filename>.jar' [, JAR '<path_to_filename>.jar']* ] {code}
However, this:
{code:java}
CREATE TEMPORARY SYSTEM FUNCTION MyFunc AS
    'com.ballista.my.classpath.MyFunc' LANGUAGE SCALA 
USING JAR 'path-to-my.jar'{code}
Throws this exception:
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Encountered ""USING"" at line 3, column 3.
Was expecting one of:
    <EOF> 
    "";"" ... {code}

This works:
{code:java}
ADD JAR 'path-to-my.jar'; 
CREATE TEMPORARY SYSTEM FUNCTION MyFunc AS        com.ballista.my.classpath.MyFunc' LANGUAGE SCALA;{code}

Is this a bug in the code, or is the documentation perhaps not up to date?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 12:36:49 UTC 2022,,,,,,,,,,"0|z198i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 02:26;hxb;This feature is introduced in release-1.16, the version of flink you are using is 1.15.1?;;;","11/Oct/22 08:22;martijnvisser;I'm guessing the documentation that you're reading is for master, not release-1.15. ;;;","11/Oct/22 12:34;angelok;oh my goodness... where's the facepalm emoji?

Yes. Sorry everyone.;;;","11/Oct/22 12:36;martijnvisser;No worries :);;;",,,,,,,,,,,,,,,,,,,
Upgrade software.amazon.glue:schema-registry-common and software.amazon.glue:schema-registry-serde dependency from 1.1.8 to 1.1.14,FLINK-29574,13485507,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,10/Oct/22 22:50,12/Oct/22 08:03,04/Jun/24 20:41,12/Oct/22 08:03,,,,,,,1.17.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,We should update the software.amazon.glue:schema-registry-common and software.amazon.glue:schema-registry-serde dependencies from 1.1.8 to 1.1.14 to be up to date with the latest version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 07:15:42 UTC 2022,,,,,,,,,,"0|z198g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 07:15;dannycranmer;Merged commit [{{9407245}}|https://github.com/apache/flink/commit/940724507699dc815cb8235740a64453ba9b8b4a] into master;;;","12/Oct/22 07:15;dannycranmer;Thanks for the contribution [~liangtl] ! ;;;",,,,,,,,,,,,,,,,,,,,,
Flink does not work for windows if installing on other drive except for C drive,FLINK-29573,13485476,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,kriswang,kriswang,10/Oct/22 17:04,17/Nov/22 09:52,04/Jun/24 20:41,17/Nov/22 09:52,1.14.4,,,,,,,,,,Runtime / Configuration,,,,0,,,,,"{{ flink_service.bat }}{-}{{-}}> \{{flink_inner.bat }}{-}{{-}}> \{{start-cluster.sh }}{{-}}{-}>\{{{}config.sh}}
The {{config.sh}} which is under the _*D:\xxxxx\flink-1.14.4\bin*_ (Flink’s directory)
has _**_ {{runBashJavaUtilsCmd()}} --> the current derived value of {{class_path}} after mangling the Path is
*D;C:\xxxxx\flink-1.14.4\bin\bash-java-utils.jar;D:\xxxxx\flink-1.14.4\lib\flink-dist_2.12-1.14.4.jar*

Here, {{runBashJavaUtilsCmd()}} internally sets {{class_path}} by calling a function called {{manglePathList}} 
{{manglePathList}} is causing this behavior where
_:\xxxxx\flink-1.13.5/bin/bash-java-utils.jar_ is getting converted to _C:\xxxxx\flink-1.14.4\bin\bash-java-utils.jar_ 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 09:52:17 UTC 2022,,,,,,,,,,"0|z19894:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 07:30;kevin.cyj;[~kriswang] Thanks for reporting this issue, do you know if this issue also exists in newer flink version?;;;","17/Nov/22 09:52;xtsong;We are not supporting running Flink on Windows any more.

See the community discussion:
https://lists.apache.org/thread/lgc7lggn5ll2kf6s4spdd98trjhmlbks;;;",,,,,,,,,,,,,,,,,,,,,
Flink Task Manager skip loopback interface for resource manager registration,FLINK-29572,13485474,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,gzkevinl,gzkevinl,10/Oct/22 16:56,17/Nov/22 10:04,04/Jun/24 20:41,17/Nov/22 10:04,1.15.3,,,,,,,,,,API / Core,,,,0,,,,,"Currently Flink Task Manager use different local interface to bind to connect to Resource Manager. First one is Loopback interface. Normally if Job Manager is running on remote host/container, using loopback interface to connect will fail and it will pick up correct IP address.

However, if Task Manager is running with some proxy, loopback interface can connect to remote host as well. This will result 127.0.0.1 reported to Resource Manager during registration, even Job Manager/Resource Manager runs on remote host, and problem will happen. For us, only one Task Manager can register in this case.

I suggest adding configuration to skip Loopback interface check if we know Job/Resource Manager is running on remote host/container.

 ","Flink 1.15.2

Kubernetes with Istio Proxy",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27341,,,FLINK-24474,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 10:03:55 UTC 2022,,,,,,,,,,"0|z1988o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 16:57;gzkevinl;{quote}Task Manager Log:
2022-10-08 17:22:32,983 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.
2022-10-08 17:22:32,984 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - TaskManager will try to connect for PT10S before falling back to heuristics
2022-10-08 17:22:33,356 DEBUG org.apache.flink.runtime.net.ConnectionUtils                 [] - Retrieved new target address flink-jobmanager/172.20.133.241:6123 for akka URL 
[akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_*]
.
2022-10-08 17:22:33,357 DEBUG org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to [flink-jobmanager/172.20.133.241:6123] from local address [localhost/127.0.0.1] with timeout [100]
2022-10-08 17:22:33,361 DEBUG org.apache.flink.runtime.net.ConnectionUtils                 [] - Using InetAddress.getLoopbackAddress() immediately for connecting address
2022-10-08 17:22:33,361 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address 'localhost' (127.0.0.1) for communication.
2022-10-08 17:22:33,416 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 127.0.0.1:6122, bind address 0.0.0.0:6122.{quote};;;","11/Oct/22 02:39;xtsong;I think this was introduced by FLINK-24474.

In FLINK-24474, we change the default bind address to the loopback address. However, when using loopback, the address reported to JM/RM should be different from the bind address. Thus, `ConnectionUtils#findAddressUsingStrategy` should return `InetAddress.getLocalHost()` in this case.

[~chesnay], could you confirm this?;;;","12/Oct/22 02:32;xtsong;After looking into this a bit more, I think this might not be a problem.

You can configure the address (which is reported to RM) and bind-address separately, via:
- taskmanager.host
- taskmanager.bind-host

[~gzkevinl], would this solve your problem?;;;","14/Oct/22 15:39;gzkevinl;No, it wouldn't. This problem happens for K8s deployment. For K8s, all task managers share the same configuration, which was converted from config-map. I think we just need a configuration flag to skip loopback check since we know Job Manager is not running on localhost.

As indicated from doc: 

{code:java}
The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file.
{code}
;;;","17/Oct/22 02:06;xtsong;[~gzkevinl],

I'm still not entirely sure whether this is a valid requirement. Could you explain a bit more about your use case:

1. Why do you need the TMs to run behind a proxy?
2. How does skipping the loopback address help in this case? Do you mean JM / RM can connect to the TM via its hostname / ip-address? If yes, then it come back to question 1).
3. Can this issue be resolved by setting the proxy to reject the loopback interface?;;;","17/Oct/22 04:17;gzkevinl;1. It is called service mesh, basically all ingress/egress traffic are captured by proxy and proxies are connected as service mesh so that apps are transparent for service discovery and many more. https://istio.io/latest/docs/ops/deployment/architecture/

2. With service mesh proxy deployed, TM can connect JM using loopback address. If this works, TM will report its address as 127.0.0.1:6223. JM can RPC this address as well. But as soon as you have multiple TMs, all of them will report their address as 127.0.0.1:6223. Obviously only one will succeed. This result JM can only connect with one TM, which is the one got success.

3. Capturing loopback traffic and forward to remote is how proxy working. Disable this will make proxy useless. Pls check the link in No.1.;;;","17/Oct/22 05:50;xtsong;[~gzkevinl],

I probably haven't understand this service mesh completely, so correct me if I'm wrong.

Say you have a task manager with pod ip address 1.2.3.4, and a proxy located in the same pod as a sidecar. Can the proxy capture all the traffics sent from the interface 1.2.3.4 while block traffics from 127.0.0.1? IIUC, if the proxy can only capture the 127.0.0.1 traffics, then the task manager skipping the loopback interface will also skip the proxy anyway.

Moreover, the taskmanager does not have to bind to a fixed port. If you leave `taskmanager.rpc.port` to un-configured, it will use a random port. 

Currently, Flink does support proxied networks, but would require the external address (the one JM can use to access TM/proxy) to be explicitly configured for each TM. IIUC, you are suggesting a proxy that the external address is the same as internal address (the one that TM itself sees) so that it needs not to be explicitly configured. I think I'm not convinced that this is a general / common requirement.;;;","27/Oct/22 00:19;gzkevinl;The sidecar proxy allows application binding to 127.0.0.1 to connect remote IP address (where Job Manager runs), which it shouldn't under normal situation. This will make Task Manager report its IP as 127.0.0.1 to Job Manager, instead of its real IP, such as 1.2.3.4. It has nothing with port.

Under this situation, all TMs will report their IP as 127.0.0.1, this confuse the Job Manager and eventually no TM can communicate with JM.;;;","27/Oct/22 01:59;xtsong;bq. If this works, TM will report its address as 127.0.0.1:6223. JM can RPC this address as well. But as soon as you have multiple TMs, all of them will report their address as 127.0.0.1:6223. Obviously only one will succeed.

You said that it would work if there's only one TM using a 127.0.0.1 address. Then I don't understand why it couldn't work with multiple TMs using the same address but different ports. A {{TaskManagerLocation}} is uniquely by its address *and* port. Having multiple TMs using the same IP address with different ports should not confuse JM.;;;","27/Oct/22 19:32;gzkevinl;It will work if we configure different ports for each different task manager. But that will be cumbersome. If you have 10 task manager, you need to create 10 different deployments for each of them. Also autoscale could be issue too. Rather than you have one deployment with 10 replicas and they can scale up and down.

I downgrade my Flink to 1.14.6 and it works fine. Looks like it was introduced by  FLINK-24474.;;;","28/Oct/22 02:15;xtsong;You don't have to configure different ports for each task manager. You just need to remove `taskmanager.rpc.port` from your configuration, and Flink by default should use random ports.

IMHO, allowing components from different hosts to bi-directionally communicate with each other via the loopback address does not sounds like a common use case. It seems to be a violation of the loopback address semantics. This is probably more a problem of the proxy software, rather than Flink.

I'd love to help you find a workaround to get things work if possible. However, as one of the maintainers of the Flink project, I'd be negative to introduce such a knob for such an unusual use case. ;;;","28/Oct/22 22:07;gzkevinl;Hi, Xintong, thanks for your help first. However, this is not some vague proxy software, it is part of Service Mesh implementation and now become very popular now, especially in Kubernetes world. https://medium.com/microservices-in-practice/service-mesh-for-microservices-2953109a3c9a

Keep in mind that this FLINK-24474 is not available before 1.15. Original purpose is to make Flink cluster more secure if both JM/TMs run on the same node/computer, which is not really a case for production deployment. Also the way it probes the location of Job Manager is wrong if such proxy exists. That's why I recommended to add an option to disable/skip the loopback check since we know JM is not running on the same node as TM. So in my opinion, it is a bug.;;;","31/Oct/22 01:56;xtsong;I'm still not convinced that this is a bug of Flink. Maybe we can live with the disagreement for now and focus solving your problem first.

bq. You don't have to configure different ports for each task manager. You just need to remove `taskmanager.rpc.port` from your configuration, and Flink by default should use random ports.
Have you tried the random port approach?

One more question: when not binding to the loopback address, does all the traffics still go through the proxy?;;;","04/Nov/22 09:51;huwh;Hi, @songxintong, I think this was introduced by FLINK-24474. It uses the loopback address as the default address. In these configs, it only supports flink clusters running on a single host, and taskmanager must use the loopback interface to connect with the jobmanager, since the jobmanager only binds the loopback interface. But if we don't set the bind-address to localhost, taskmanager should not use the loopback interface to find its external address. otherwise, this will cause other TaskManagers to not connect with it. 

This cause [FLINK-27341|https://issues.apache.org/jira/browse/FLINK-27341] too.

IMO, we can determine whether to use the loopback interface by whether the taskmanager.bind-host is loopback address.;;;","07/Nov/22 02:08;xtsong;Thanks for the pointer, [~huwh].

I think FLINK-27341 is indeed a valid issue. A TM being able to connect to JM via the loopback interface does not mean other TMs can also connect to it using the loopback interface.

Let's wait and see how FLINK-27341 is resolved. My gut feeling is this ticket may no longer be an issue after resolving FLINK-27341.;;;","17/Nov/22 06:35;AlexXXX;We faced the same problem at k8s. To be honest, we are confused that why using the loopback as the default strategy? Especially, at productive k8s environment, loopback is useless at remote shuffle. After changing the default strategy as LOCAL_HOST we have fixed this bug.;;;","17/Nov/22 07:40;huwh;[~AlexXXX] IMO, it's a bug to use loopback. we will remove it in https://issues.apache.org/jira/browse/FLINK-27341;;;","17/Nov/22 10:03;gaoyunhaii;Hi all, very thanks for the discussion! I'll first close this issue for its duplicated. Let's head to https://issues.apache.org/jira/browse/FLINK-27341 to track the progress. ;;;",,,,,
"Flink configuration only supports Map<String, String>",FLINK-29571,13485453,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,mxm,mxm,mxm,10/Oct/22 14:44,15/Aug/23 22:34,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Configuration,,,,0,pull-request-available,stale-assigned,,,"The Flink configuration parsing logic only supports maps with string key and value types. It should support {{Map<String, T>}}. T should allow the same value types that are already supported.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:34:59 UTC 2023,,,,,,,,,,"0|z19840:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 14:58;chesnay;That'd also imply that T can also be a Map/List. I'm not sure if the parser can handle that.;;;","10/Oct/22 15:52;gyfora;cc [~twalthr] ;;;","15/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Bump org.jsoup:jsoup to v1.15.3,FLINK-29570,13485441,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,10/Oct/22 13:54,12/Oct/22 14:12,04/Jun/24 20:41,12/Oct/22 14:12,,,,,,,1.17.0,,,,Documentation,,,,0,,,,,Bump JSoup to avoid getting flagged for CVE-2022-36033 (which doesn't affect Flink directly),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 14:12:29 UTC 2022,,,,,,,,,,"0|z1981c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 14:12;chesnay;master: 2a00737a1a68e7d4b16525042a78ee75ae234ecc;;;",,,,,,,,,,,,,,,,,,,,,,
Replace usages of deprecated expand shortcode,FLINK-29569,13485435,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Oct/22 13:23,11/Oct/22 14:25,04/Jun/24 20:41,11/Oct/22 14:25,,,,,,,1.17.0,,,,Documentation,,,,0,pull-request-available,,,,"The expand shortcode is deprecated; use {{<details>}} instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 14:25:37 UTC 2022,,,,,,,,,,"0|z19800:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 14:25;chesnay;master: 4934bd69052f2a69e8021d337373f4480c802359;;;",,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary whitespace in request/response blocks,FLINK-29568,13485430,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Oct/22 12:35,11/Oct/22 11:10,04/Jun/24 20:41,11/Oct/22 11:10,,,,,,,1.17.0,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 11:10:50 UTC 2022,,,,,,,,,,"0|z197yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 11:10;chesnay;master: 6934032acd09cfaae9945ca9c36ea1077c9e1eeb;;;",,,,,,,,,,,,,,,,,,,,,,
Revert sink output metric names from numRecordsSend back to numRecordsOut,FLINK-29567,13485429,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,renqs,renqs,10/Oct/22 12:26,03/Nov/22 02:24,04/Jun/24 20:41,20/Oct/22 01:12,1.15.3,1.16.0,,,,,1.15.3,1.16.0,,,Connectors / Common,,,,0,pull-request-available,,,,"As discussed in [the mailing list|https://lists.apache.org/thread/vxhty3q97s7pw2zn0jhkyd6sxwwodzbv], all{color:#333333} sink metrics with name “numXXXOut” defined in FLIP-33 are replace by “numXXXSend” in FLINK-26126 and FLINK-26492. {color}

{color:#333333}Considering metric names are public APIs, this is a breaking change to end users and not backward compatible. We need to revert these metric names back. {color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 09:13:06 UTC 2022,,,,,,,,,,"0|z197yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 01:12;renqs;master: d0e855090683920d57922acbddb64c9a99dceccd

release-1.16: 6e743963cc689931acdab68f3559884d5d48fcc4

release-1.15: 221e9f7c5e59265b7448eba70aea393b60a1fe42;;;","01/Nov/22 02:57;hxbks2ks; Hi [~renqs], the fixed PR hasn't been backported to release-1.15?
;;;","02/Nov/22 09:13;renqs;Ah thanks for the reminder [~hxb] ! I created the backport PR just now: https://github.com/apache/flink/pull/21220;;;",,,,,,,,,,,,,,,,,,,,
Reschedule the cleanup logic if cancel job failed,FLINK-29566,13485420,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,10/Oct/22 11:30,30/Mar/23 06:37,04/Jun/24 20:41,30/Mar/23 06:37,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,starter,,,"Currently, when we remove the FlinkSessionJob object,

we always remove the object even if the Flink job is not being canceled successfully.

 

This is *not semantic consistent* if the FlinkSessionJob has been removed but the Flink job is still running.

 

One of the scenarios is that if we deploy a FlinkDeployment with HA mode.

When we remove the FlinkSessionJob and change the FlinkDeployment at the same time,

or if the TMs are restarting because of some bugs such as OOM.

Both of these will cause the cancelation of the Flink job to fail because the TMs are not available.

 

We should *reschedule* the cleanup logic if the FlinkDeployment is present.

And we can add a new ReconciliationState DELETING to indicate the FlinkSessionJob's status.

 

The logic will be


{code:java}
if the FlinkDeployment is not present
    delete the FlinkSessionJob object
else
    if the JM is not available
        reschedule
    else
        if cancel job successfully
            delete the FlinkSessionJob object
        else
            reschedule{code}
When we cancel the Flink job, we need to verify all the jobs with the same name have been deleted in case of the job id is changed after JM restarted.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 06:37:14 UTC 2023,,,,,,,,,,"0|z197wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 13:30;gyfora;I think this improvement makes sense :) ;;;","16/Oct/22 14:02;haoxin;Will submit a PR for that.;;;","13/Feb/23 03:14;spencerx;Hi [~haoxin], did you start working on this issue? If not, I can help take a look. ;;;","14/Feb/23 09:45;haoxin;I did the required change, but need some time to add the unit tests.

Will finish the unit tests if I can get some free time this week. :);;;","30/Mar/23 06:37;gyfora;merged to main e2a6775ab992234dde2591234cc59d1706823c07;;;",,,,,,,,,,,,,,,,,,
"In Flink per job mode, the logs printed by taskManager on the web UI will not be highlighted, because the log contents are annotated due to special symbols, which will affect the use experience. For more information, see Fig",FLINK-29565,13485411,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,1336399775,1336399775,1336399775,10/Oct/22 10:46,15/Aug/23 22:34,04/Jun/24 20:41,,1.14.3,,,,,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,stale-assigned,,," 

!image-2022-10-10-19-03-27-670.png|width=580,height=317!

!image-2022-10-10-18-43-53-713.png|width=726,height=47!

This kind of '/*' content will appear in the print log of the logEnvironmentInfo method in the EnvironmentInformation class. The following logs will be commented out without highlighting

*verification*

!image-2022-10-10-18-45-17-228.png|width=880,height=161!

After manually printing '*/' in the business code, the log is normal

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/22 10:40;1336399775;image-2022-10-10-18-40-27-721.png;https://issues.apache.org/jira/secure/attachment/13050238/image-2022-10-10-18-40-27-721.png","10/Oct/22 10:43;1336399775;image-2022-10-10-18-43-31-897.png;https://issues.apache.org/jira/secure/attachment/13050237/image-2022-10-10-18-43-31-897.png","10/Oct/22 10:43;1336399775;image-2022-10-10-18-43-53-713.png;https://issues.apache.org/jira/secure/attachment/13050236/image-2022-10-10-18-43-53-713.png","10/Oct/22 10:45;1336399775;image-2022-10-10-18-45-17-228.png;https://issues.apache.org/jira/secure/attachment/13050235/image-2022-10-10-18-45-17-228.png","10/Oct/22 11:02;1336399775;image-2022-10-10-19-02-29-796.png;https://issues.apache.org/jira/secure/attachment/13050239/image-2022-10-10-19-02-29-796.png","10/Oct/22 11:03;1336399775;image-2022-10-10-19-03-27-670.png;https://issues.apache.org/jira/secure/attachment/13050240/image-2022-10-10-19-03-27-670.png",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:34:59 UTC 2023,,,,,,,,,,"0|z197uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:16;kevin.cyj;[~1336399775] Thanks for reporting this. Do you have a solution for this?;;;","27/Oct/22 07:13;gaoyunhaii;Also cc [~junhan] ;;;","28/Oct/22 02:50;1336399775;yes, I have;;;","28/Oct/22 02:57;gaoyunhaii;Hi [~1336399775] then would you like to open a PR to fix this issue? ;;;","28/Oct/22 03:48;1336399775;yes ;;;","28/Oct/22 05:32;gaoyunhaii;Thanks [~1336399775] for helping fix the issue! I have assigned the issue to you. ;;;","22/Dec/22 08:52;xtsong;[~1336399775], how's the progress of this ticket?;;;","15/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,
Migrate pull request to flink-connector-mongodb repository,FLINK-29564,13485409,13071642,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,10/Oct/22 10:39,08/Feb/23 16:01,04/Jun/24 20:41,08/Feb/23 16:01,,,,,,,mongodb-1.0.0,,,,Connectors / MongoDB,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 16:01:57 UTC 2023,,,,,,,,,,"0|z197u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 16:01;chesnay;main: 151336a37010da481622678a8de504d33c76445f;;;",,,,,,,,,,,,,,,,,,,,,,
SourceTestSuiteBase#testSourceMetrics enters an infinite waiting loop in case the number of records counter is wrong,FLINK-29563,13485408,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,echauchot,echauchot,echauchot,10/Oct/22 10:39,16/Aug/23 11:45,04/Jun/24 20:41,16/Aug/23 11:45,,,,,,,,,,,Tests,,,,0,pull-request-available,stale-assigned,,,"The call to _CommonTestUtils#waitUntilCondition_ (1) makes the test wait for the condition _Precision.equals(allRecordSize, sumNumRecordsIn)_. In case the reported number of records is incorrect, the waiting loop never ends.

[1] https://github.com/apache/flink/blob/a6092b1176d15a7af32a7eb19f59cdfeab172034/flink-test-utils-parent/flink-connector-test-utils/src/main/java/org/apache/flink/connector/testframe/testsuites/SourceTestSuiteBase.java#L451 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29605,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:35:00 UTC 2023,,,,,,,,,,"0|z197u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 08:39;echauchot;Another related thing: _testSourceMetrics_ is targeted to unbounded sources (which was  not specified in the test javadoc). I think metrics tests still make sense with bounded sources. So I propose to make a version of this test also for bounded sources. 

As a more general comment, maybe we should introduce junit tags to  specify tests targeted to unbounded sources and dynamically ignore them for bounded sources. [~chesnay] WDYT ? Should I create a new ticket for that ?;;;","11/Oct/22 14:21;chesnay;??As a more general comment, maybe we should introduce junit tags to specify tests targeted to unbounded sources and dynamically ignore them for bounded sources. Chesnay Schepler WDYT ? Should I create a new ticket for that???

ehhh I'm not a fan of that. So far we only used categories/tags for exceptional circumstances, like known bugs or incompatibilities (e.g., java 11). To me it should be obvious whether a test is run or not.

Why is it relevant whether the source is bounded or not for this test?;;;","11/Oct/22 14:43;echauchot;Ok let's forget about automatically disabling the not-suited tests (for ex the ones targeted to unbounded sources when the tested source is bounded). A possible solution could be to override them and disable them in the ITCase (that extends the test suite).

For the particular case of the metrics tests, as I wrote this test should indeed be run in both bounded and unbounded cases. It is just that for the bounded case, [killing the job|https://github.com/apache/flink/blob/4934bd69052f2a69e8021d337373f4480c802359/flink-test-utils-parent/flink-connector-test-utils/src/main/java/org/apache/flink/connector/testframe/testsuites/SourceTestSuiteBase.java#L469]  is not needed and in some cases (fast job) the kill is executed when the job is already in the finished state leading to a confusing exception. This is why for the bounded case the killJob should be removed.  What I propose is to allow the implemeter to pass the boundedness in its ITCase to run the corresponding version of this test.

 

WDYT ?

 

 ;;;","12/Oct/22 10:55;echauchot;[~chesnay], the bounded case of SourceTestSuiteBase#testSourceMetrics is a different topic, so I created [FLINK-29605|https://issues.apache.org/jira/browse/FLINK-29605] to track it.;;;","15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,
JM/SQl gateway OpenAPI specs should have different titles,FLINK-29562,13485407,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Oct/22 10:24,20/Oct/22 02:33,04/Jun/24 20:41,10/Oct/22 13:36,,,,,,,1.16.0,1.17.0,,,Documentation,Runtime / REST,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 13:36:05 UTC 2022,,,,,,,,,,"0|z197ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 13:36;chesnay;master: 1c2fd3584b899412f6a94303ecdd29513664e9dc
1.16: ae5c7046f11504d5399f11e5be1762cf378e3c30;;;",,,,,,,,,,,,,,,,,,,,,,
Log the job id when clean up session job failed,FLINK-29561,13485403,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,10/Oct/22 09:57,10/Oct/22 16:42,04/Jun/24 20:41,10/Oct/22 16:42,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"If so, we can delete it by Flink rest API manually, and no need to query it by name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/398,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 16:42:03 UTC 2022,,,,,,,,,,"0|z197sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 16:42;mbalassi;[{{8e57e5b}}|https://github.com/apache/flink-kubernetes-operator/commit/8e57e5b75af385c22dccaef85014175af35b47e8] in main;;;",,,,,,,,,,,,,,,,,,,,,,
Create a flink-connector-mongodb repository,FLINK-29560,13485401,13071642,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,10/Oct/22 09:53,10/Oct/22 13:56,04/Jun/24 20:41,10/Oct/22 11:17,,,,,,,mongodb-1.0.0,,,,Connectors / MongoDB,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 11:17:03 UTC 2022,,,,,,,,,,"0|z197sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 11:17;martijnvisser;Repository created and initialized at https://github.com/apache/flink-connector-mongodb;;;",,,,,,,,,,,,,,,,,,,,,,
Disable Join Reorder and Hash join for these tables whose statistics are unavailable in batch mode,FLINK-29559,13485387,13444592,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,10/Oct/22 07:51,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,"For these tables without table statistics, we need to disable join reorder and hash join(broadcast hash join and shuffle hash join) for these tables in batch mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-10 07:51:50.0,,,,,,,,,,"0|z197pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use select count(*) from xxx; and get SQL syntax",FLINK-29558,13485384,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,StarBoy1005,StarBoy1005,10/Oct/22 07:37,30/Jan/23 08:12,04/Jun/24 20:41,30/Jan/23 08:12,1.15.3,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Hi, I use flink sql to make kafka records to mysql.
so I create these 2 tables in flink sql,here is the mysql ,and I created the table in mysql before I did the insert action in flink sql.

  CREATE TABLE mysql_MyUserTable (
  id STRING,
  name STRING,
  age STRING,
  status STRING,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://10.19.29.170:3306/fromflink152',
   'table-name' = 'users',
   'username' = 'root',
   'password' = '******'
);

In mysql, I created database ""fromflink152"" then created the table like this way

 CREATE TABLE `users` (
  `id` varchar(64) NOT NULL DEFAULT '',
  `name` varchar(255) DEFAULT NULL,
  `age` varchar(255) DEFAULT NULL,
  `status` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
)  

After executed insert sql,I found 'select * from mysql_MyUserTable' can get correct result,but ’select count(\*) from mysql_MyUserTable‘  or ’select count(id) from mysql_MyUserTable‘ ,the collect job in flink app keep restarting again and again.The exception is:
 !image-2022-10-10-15-31-34-341.png! 

So I wonder which config that I missed about the table in flink or mysql side :(

","flink 1.15.2
CentOS Linux release 7.9.2009 (Core)
5.7.32-log MySQL Community Server (GPL)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/22 07:32;StarBoy1005;image-2022-10-10-15-31-34-341.png;https://issues.apache.org/jira/secure/attachment/13050230/image-2022-10-10-15-31-34-341.png","19/Oct/22 09:55;jiabao.sun;image-2022-10-19-17-55-14-700.png;https://issues.apache.org/jira/secure/attachment/13051173/image-2022-10-19-17-55-14-700.png","03/Nov/22 10:17;StarBoy1005;image-2022-11-03-18-16-12-127.png;https://issues.apache.org/jira/secure/attachment/13051744/image-2022-11-03-18-16-12-127.png","14/Oct/22 03:25;StarBoy1005;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13050925/screenshot-1.png","20/Oct/22 02:16;StarBoy1005;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13051215/screenshot-2.png",,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 08:12:48 UTC 2023,,,,,,,,,,"0|z197oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 13:06;martijnvisser;[~StarBoy1005] You should change your password as this was now shared publicly. 

The error that's being reported is coming from MySQL: have you verified that this syntax is compatible with your MySQL server? ;;;","14/Oct/22 02:57;StarBoy1005;[~martijnvisser] Thanks for the security notice.What's more, if I used flink sql to connect hudi or hive table(there do exist the table and some record in hudi or hive sides),execute sql like 'select  count(\*) from table1;' could get the correct result, but when do the save query  to mysql or Tidb(both connect by jdbc), the exception always show up,and I connected mysql by mysql client to check the table with same sql execution, 'select  count(\*) from table1;'  result is fine and correct.
If you never meet this situation,I guess the problem have some relation with mysql-connector-java**.jar in flink/lib 
;;;","14/Oct/22 03:25;StarBoy1005;It seems not work when I exchange mysql-connector-java-**.jar    , neither mysql-connector-java-5.1.49.jar    nor   mysql-connector-java-8.0.29.jar .

 !screenshot-1.png! ;;;","18/Oct/22 07:34;martijnvisser;Hive (and I assume Hudi, that I don't know for sure) use the Hive dialect for SQL. This can be different from the other JDBC dialects. ;;;","19/Oct/22 10:01;jiabao.sun;Hi [~martijnvisser].

I did some tests to reproduce the problem. Maybe there is some problem when extracting the columns of the COUNT function.

I have tried SUM and MAX function and the column can be extracted correctly.

!image-2022-10-19-17-55-14-700.png!;;;","19/Oct/22 14:08;martijnvisser;[~RocMarshal] Any thoughts on this?;;;","20/Oct/22 07:44;StarBoy1005;[~jiabao.sun] Hi,if column like 'id',and 'id' is the primary key ,then use select count(id) from xxx; can cause the problem.I guess some logic is not suit with JDBC situation.Maybe in StreamPhysicalRel  or something else;;;","03/Nov/22 09:40;StarBoy1005;I tried flink 1.16.0 with the same env,now in sql-client can swiftly get the exception：""[ERROR] Could not execute SQL statement. Reason:
java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FROM `users1013a`' at line 1""  ,then the collect job just failed and never retry 

 !image-2022-11-03-18-16-12-127.png! ;;;","14/Jan/23 05:48;tanjialiang;[~StarBoy1005] It had been fixed in FLINK-27268, but i found a problem, when using flink sql like this ""SELECT COUNT( * ) FROM table"" or ""SELECT COUNT(1) FROM table"", projection always return 'ROW<> NOT NULL', so it cause mysql connector transform to ""SELECT FROM table"". After FLINK-27268, it transform to ""SELECT '' FROM table"". But is it projection's bug? When projection can not match something, it will be return 'ROW<> NOT NULL'. I think maybe not only mysql connector has this bug. cc [~martijnvisser] [~jark] ;;;","17/Jan/23 09:37;martijnvisser;[~lincoln.86xy] Is this a ticket that fits for you? If so, could you have a look?;;;","17/Jan/23 13:19;lincoln.86xy;[~martijnvisser] I looked at the problem and it is indeed a planner bug, I'll fix it.;;;","18/Jan/23 01:17;lincoln.86xy;[~godfreyhe] Could you also take a look at this when you have time? There're several existing cases already tested the case  which select nothing from source, e.g., ""SELECT COUNT(1) FROM T"" in 
 `org.apache.flink.table.planner.plan.stream.sql.TableSourceTest#testProjectWithoutInputRef`
and the expected plan is:
{code}
  <TestCase name=""testProjectWithoutInputRef"">
    <Resource name=""sql"">
      <![CDATA[SELECT COUNT(1) FROM T]]>
    </Resource>
    <Resource name=""ast"">
      <![CDATA[
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
+- LogicalProject($f0=[0])
   +- LogicalTableScan(table=[[default_catalog, default_database, T]])
]]>
    </Resource>
    <Resource name=""optimized exec plan"">
      <![CDATA[
GroupAggregate(select=[COUNT(*) AS EXPR$0])
+- Exchange(distribution=[single])
   +- TableSourceScan(table=[[default_catalog, default_database, T, project=[], metadata=[]]], fields=[])
]]>
    </Resource>
  </TestCase>
{code}

so is there any contract that connectors should follow in such cases？;;;","29/Jan/23 05:52;lincoln.86xy;After discussed with [~godfreyhe], the problem is confirmed. I've updated the pr to move on.;;;","30/Jan/23 08:12;lincoln.86xy;fixed in master: 6d17ba8855683fb6c1f8291ff4bad1c9abc01cb9;;;",,,,,,,,,
The SinkOperator's OutputFormat function is not recognized,FLINK-29557,13485363,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,10/Oct/22 04:21,20/Nov/22 10:02,04/Jun/24 20:41,20/Nov/22 10:02,,,,,,,1.17.0,,,,API / Core,Table SQL / API,,,0,pull-request-available,,,,"In the {{SimpleOperatorFactory#of}}, only {{StreamSink}} is handled to register as {{SimpleOutputFormatOperatorFactory}}. So it will lost the output format information in  {{SinkOperator}}. Then some hook functions like {{FinalizeOnMaster}} will have no chance to be executed.
Due to the {{SinkOperator}} is in the table module, it can't be reached directly in the {{flink-streaming-java}}. So maybe we need introduce an extra common class eg: {{SinkFunctionOperator}} to describe the {{Sink}} operator and handle it individually.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 20 10:02:19 UTC 2022,,,,,,,,,,"0|z197k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 02:20;aitozi;Anyone can help confirm this issue ? ;;;","14/Oct/22 09:31;gaoyunhaii;Hi [~aitozi] may I have a double confirmation on the scenario of this issue? Namely why we could not directly use OutputFormatSinkFunction so that it could be registered as SimpleOutputFormatOperatorFactory? ;;;","15/Oct/22 11:16;aitozi;Thanks [~gaoyunhaii] for your reply. The {{OutputFormatProvider}} in the table module will generate the {{SinkOperator}}. And the {{SinkOperator}} with {{OutputFormatSinkFunction}} will not be treated as an operator with {{OutputFormat}} in the {{SimpleOperatorFactory}}. In the end, it will miss the chance to execute the hook method eg: {{InitializeOnMaster}};;;","21/Oct/22 04:17;aitozi;ping [~gaoyunhaii] ;;;","25/Oct/22 07:10;gaoyunhaii;Thanks [~aitozi] for reporting the issue! Now I have understood why there is issue here, I'll have a double confirmation of the proper way to fix it here, and do you already have some thoughts on it?;;;","25/Oct/22 08:39;aitozi;I make a solution to add a common interface {{SinkFunctionOperator}} in the {{flink-streaming-java}} module and let the {{SinkOperator}} and {{StreamSink}} extends from it. So that we can check for the {{SinkFunctionOperator#sinkFunction}} in the {{SimpleOperatorFactory}} directly. what do you think [~gaoyunhaii] ?;;;","27/Oct/22 05:47;gaoyunhaii;Hi [~aitozi] LGTM, very thanks for tracking the issue. Would you like to open a PR to fix the issue? Both side is ok to me. ;;;","27/Oct/22 09:38;aitozi;Yes, I will open a PR to fix this. Please help assign the ticket;;;","20/Nov/22 10:02;gaoyunhaii;Merged on master via a56af3b5702f805b477f8cf11a27167e626ad9fe.;;;",,,,,,,,,,,,,,
Flink Sql: implicit type conversion between BIGINT and VARCHAR(2147483647) is not supported on join's condition now,FLINK-29556,13485328,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luochg,luochg,09/Oct/22 16:35,10/Oct/22 10:45,04/Jun/24 20:41,,1.13.6,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Zeppelin Flink Interpreter

 

Create some temporary views first：

{code:java}
%flink
val table = btenv.sqlQuery(""""""
select a.agentcode,b.upagent
from b_186_LAAGENT a,b_186_LATREE b 
where a.agentcode = b.agentcode and b.agentgrade = 'P2'
"""""");
btenv.dropTemporaryView(""b_186_P2"")
btenv.createTemporaryView(""b_186_P2"", table)

 

table = btenv.sqlQuery(""""""SELECT _t.* from b_186_P2 _t"""""");
btenv.dropTemporaryView(""b_186_P2_1"")
btenv.createTemporaryView(""b_186_P2_1"", table)

 

table = btenv.sqlQuery(""""""SELECT _t.*,(select sum(attrate) from b_186_LAATTENDANCE where agentcode = _t.agentcode and indexcalno = '202209') as ccccc from b_186_P2_1 _t """""");
btenv.dropTemporaryView(""b_186_P2_2"")
btenv.createTemporaryView(""b_186_P2_2"", table)

{code}

error occurs while query the temporary view b_186_P2_2

{code:java}

%flink.bsql
SELECT * from b_186_P2_2

 

Fail to run sql command: SELECT * from b_186_P2_2 org.apache.flink.table.api.TableException: implicit type conversion between BIGINT and VARCHAR(2147483647) is not supported on join's condition now at org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule$$anonfun$onMatch$1.apply(JoinConditionTypeCoerceRule.scala:76) at org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule$$anonfun$onMatch$1.apply(JoinConditionTypeCoerceRule.scala:65) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.onMatch(JoinConditionTypeCoerceRule.scala:65) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) at scala.collection.immutable.List.foreach(List.scala:392) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:791) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1225) at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:577) at org.apache.zeppelin.flink.Flink113Shims.collectToList(Flink113Shims.java:227) at org.apache.zeppelin.flink.FlinkZeppelinContext.showData(FlinkZeppelinContext.scala:110) at org.apache.zeppelin.interpreter.ZeppelinContext.showData(ZeppelinContext.java:67) at org.apache.zeppelin.flink.FlinkBatchSqlInterpreter.callInnerSelect(FlinkBatchSqlInterpreter.java:60) at org.apache.zeppelin.flink.FlinkSqlInterpreter.callSelect(FlinkSqlInterpreter.java:494) at org.apache.zeppelin.flink.FlinkSqlInterpreter.callCommand(FlinkSqlInterpreter.java:257) at org.apache.zeppelin.flink.FlinkSqlInterpreter.runSqlList(FlinkSqlInterpreter.java:151) at org.apache.zeppelin.flink.FlinkSqlInterpreter.internalInterpret(FlinkSqlInterpreter.java:109) at org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:55) at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110) at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:860) at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752) at org.apache.zeppelin.scheduler.Job.run(Job.java:172) at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132) at org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:750)
 
{code}
========================================================================

*table b_186_P2:*

!image-2022-10-10-00-47-38-627.png!

*table b_186_P2_1:*

!image-2022-10-10-00-47-58-672.png!

 

*Error Info:*

*!image-2022-10-10-00-40-03-422.png!*

*Zeppelin Interpreter configuration:*
 !screenshot-1.png! 

 ","Flink 1.13.6, Apache Zeppelin, Mysql",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/22 16:40;luochg;image-2022-10-10-00-40-03-422.png;https://issues.apache.org/jira/secure/attachment/13050198/image-2022-10-10-00-40-03-422.png","09/Oct/22 16:41;luochg;image-2022-10-10-00-41-18-313.png;https://issues.apache.org/jira/secure/attachment/13050200/image-2022-10-10-00-41-18-313.png","09/Oct/22 16:42;luochg;image-2022-10-10-00-42-29-986.png;https://issues.apache.org/jira/secure/attachment/13050199/image-2022-10-10-00-42-29-986.png","09/Oct/22 16:47;luochg;image-2022-10-10-00-47-38-627.png;https://issues.apache.org/jira/secure/attachment/13050202/image-2022-10-10-00-47-38-627.png","09/Oct/22 16:47;luochg;image-2022-10-10-00-47-58-672.png;https://issues.apache.org/jira/secure/attachment/13050201/image-2022-10-10-00-47-58-672.png","10/Oct/22 00:49;luochg;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13050206/screenshot-1.png",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 08:21:58 UTC 2022,,,,,,,,,,"0|z197cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 08:21;martijnvisser;Please verify this with Flink 1.15 as Flink 1.13 is no longer supported in the community. There have been many changes to the CAST functionality in more recent versions;;;",,,,,,,,,,,,,,,,,,,,,,
GlobalStreamingCommitterHandler not call notifyCheckpointCompleted after endOfInput,FLINK-29555,13485318,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liliwei,liliwei,09/Oct/22 11:25,09/Oct/22 11:27,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,,"env:

Flink 1.14.3

 
{code:java}
EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings.newInstance();
settingsBuilder.inStreamingMode();
StreamExecutionEnvironment env = StreamExecutionEnvironment
.getExecutionEnvironment(MiniClusterResource.DISABLE_CLASSLOADER_CHECK_CONFIG);
env.enableCheckpointing(400);
env.setMaxParallelism(2);
env.setParallelism(2);
tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());

{code}
 

code:
{code:java}
// Register the rows into a temporary table.
 getTableEnv().createTemporaryView(""sourceTable"",
    getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),
        Expressions.row(1, ""hello""),
        Expressions.row(2, ""world""),
        Expressions.row(3, (String) null),
        Expressions.row(null, ""bar"")
    )
);

// Redirect the records from source table to destination table.
sql(""INSERT INTO %s SELECT id,data from sourceTable"", TABLE_NAME); 


{code}
Actual:

The data is not inserted into the table.

 

Here I use a custom global committer implement from `GlobalCommitter`. When I debug this method, I notice that the close method is called immediately after the endinput method is called. The committable is not sent to my global committer. 

 

Currently, because this statement executes quickly and the task ends after execution, it does not generate checkpoints even though I have checkpoints turned on.

 

The notifyCheckpointCompleted method in the GlobalStreamingCommitterHandler class is not called after endOfInput. This doesn't seem to be the intended effect.

It seemed to be a bug so I raised this issuse.

[https://github.com/apache/flink/blob/98997ea37ba08eae0f9aa6dd34823238097d8e0d/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/GlobalStreamingCommitterHandler.java#L80-L100]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-09 11:25:34.0,,,,,,,,,,"0|z197ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add partial-update.ignore-delete option to avoid exception after join,FLINK-29554,13485304,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Oct/22 08:49,13/Oct/22 12:01,04/Jun/24 20:41,13/Oct/22 12:01,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"When the partial update input is a normal database cdc input, it can work normally as long as there is no delete data.
However, if a join is performed previously, the join node in flink job will generate delete messages, which will cause the partial update insertion to throw an exception.
We can add an option to decide whether to ignore the delete message in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 12:01:28 UTC 2022,,,,,,,,,,"0|z1977c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 12:01;lzljs3620320;master: 8e63f1f67fcfe83773a7264068687c7340bc01cb;;;",,,,,,,,,,,,,,,,,,,,,,
Support UNIX_TIMESTAMP in Table API ,FLINK-29553,13485302,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hehuiyuan,hehuiyuan,09/Oct/22 08:17,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:18 UTC 2023,,,,,,,,,,"0|z1976w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
"Fix documentation usage examples for DAYOFYEAR, DAYOFMONTH, and DAYOFWEEK functions",FLINK-29552,13485286,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,cun8cun8,cun8cun8,09/Oct/22 03:08,20/Oct/22 02:32,04/Jun/24 20:41,09/Oct/22 08:37,1.15.0,,,,,,1.16.0,1.17.0,,,Documentation,,,,0,pull-request-available,,,,!image-2022-10-09-11-21-56-033.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/22 03:21;cun8cun8;image-2022-10-09-11-21-56-033.png;https://issues.apache.org/jira/secure/attachment/13050172/image-2022-10-09-11-21-56-033.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 09 08:37:22 UTC 2022,,,,,,,,,,"0|z1973c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/22 03:28;cun8cun8;Hi [~dianfu]  I'm interested in this ticket, could you please assign it to me?;;;","09/Oct/22 08:37;dianfu;Merged to
- master via c0df986333ab2f2a04ec623914b1d89fc1e1471e
- release-1.16 via 6d3b120d00f6a44a6b279a69ecb7357d4844b558;;;",,,,,,,,,,,,,,,,,,,,,
Improving adaptive hash join by using sort merge join strategy per partition instead of all partitions ,FLINK-29551,13485284,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,09/Oct/22 02:27,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,1.20.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"Currently, regarding adaptive hash join, when falling back to sort-merge join, it sorts all partitions globally, however, due to the keys of each partition are disjoint, we can sort it per partition, which can improve the performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-09 02:27:37.0,,,,,,,,,,"0|z1972w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"example ""basic-checkpoint-ha.yaml"" not working",FLINK-29550,13485273,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,roa,roa,08/Oct/22 17:10,14/Oct/22 13:32,04/Jun/24 20:41,14/Oct/22 13:32,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Hi,

I'm a flink beginner. and I'm considering using the kubernetes operator.

Before using it, we are testing these features and examples.

But, when I tried to apply basic-checkpoint-ha.yaml, I faced the below error.
{code:java}
2022-10-08 17:04:08,261 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
java.util.concurrent.CompletionException: java.lang.IllegalStateException: The base directory of the JobResultStore isn't accessible. No dirty JobResults can be restored.
    at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) [?:?]
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) [?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
    at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.lang.IllegalStateException: The base directory of the JobResultStore isn't accessible. No dirty JobResults can be restored.
    at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-dist-1.15.2.jar:1.15.2]
    at org.apache.flink.runtime.highavailability.FileSystemJobResultStore.getDirtyResultsInternal(FileSystemJobResultStore.java:181) ~[flink-dist-1.15.2.jar:1.15.2]
    at org.apache.flink.runtime.highavailability.AbstractThreadsafeJobResultStore.withReadLock(AbstractThreadsafeJobResultStore.java:118) ~[flink-dist-1.15.2.jar:1.15.2]
    at org.apache.flink.runtime.highavailability.AbstractThreadsafeJobResultStore.getDirtyResults(AbstractThreadsafeJobResultStore.java:100) ~[flink-dist-1.15.2.jar:1.15.2]
    at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResults(SessionDispatcherLeaderProcess.java:190) ~[flink-dist-1.15.2.jar:1.15.2]
    at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.supplyUnsynchronizedIfRunning(AbstractDispatcherLeaderProcess.java:198) ~[flink-dist-1.15.2.jar:1.15.2]
    at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResultsIfRunning(SessionDispatcherLeaderProcess.java:184) ~[flink-dist-1.15.2.jar:1.15.2]
    ... 4 more
2022-10-08 17:04:08,268 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
2022-10-08 17:04:08,270 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally.. {code}


Could you let me know why that error occurs?","* Kubernetes: EKS 1.22
 * Node: bottlerocket linux
 * Manifest: https://github.com/apache/flink-kubernetes-operator/blob/release-1.1/examples/basic-checkpoint-ha.yaml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 13:32:04 UTC 2022,,,,,,,,,,"0|z1970g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 17:17;roa;and below is logged configurations (sorted)
{code:java}
blob.server.port, 6124
execution.checkpointing.externalized-checkpoint-retention, RETAIN_ON_CANCELLATION
execution.shutdown-on-application-finish, false
execution.submit-failed-job-on-application-error, true
execution.target, kubernetes-application
high-availability, org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
high-availability.cluster-id, basic-checkpoint-ha-example
high-availability.jobmanager.port, 6123
high-availability.storageDir, file:///flink-data/ha
internal.cluster.execution-mode, NORMAL
job-result-store.delete-on-commit, false
job-result-store.storage-path, file:///flink-data/ha/job-result-store/basic-checkpoint-ha-example/1dbb3b0a-5051-4bf7-9ffa-6f4f73f42800
jobmanager.memory.process.size, 2048m
jobmanager.rpc.port, 6123
kubernetes.cluster-id, basic-checkpoint-ha-example
kubernetes.container.image, flink:1.15
kubernetes.internal.jobmanager.entrypoint.class, org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint
kubernetes.jobmanager.annotations, flinkdeployment.flink.apache.org/generation:7
kubernetes.jobmanager.cpu, 1.0
kubernetes.jobmanager.replicas, 1
kubernetes.namespace, devops
kubernetes.operator.metrics.reporter.slf4j.factory.class, org.apache.flink.metrics.slf4j.Slf4jReporterFactory
kubernetes.operator.metrics.reporter.slf4j.interval, 5 MINUTE
kubernetes.operator.observer.progress-check.interval, 5 s
kubernetes.operator.reconcile.interval, 15 s
kubernetes.pod-template-file, /tmp/flink_op_generated_podTemplate_1562664763208101297.yaml
kubernetes.rest-service.exposed.type, ClusterIP
kubernetes.service-account, flink
kubernetes.taskmanager.cpu, 1.0
parallelism.default, 2
pipeline.jars, local:///opt/flink/examples/streaming/StateMachineExample.jar
queryable-state.proxy.ports, 6125
state.checkpoints.dir, file:///flink-data/checkpoints
state.savepoints.dir, file:///flink-data/savepoints
taskmanager.memory.process.size, 2048m
taskmanager.numberOfTaskSlots, 2
taskmanager.rpc.port, 6122
web.cancel.enable, false{code};;;","08/Oct/22 17:42;roa;when I change volume to ebs and added below, It looks work.
{code:java}
      securityContext:
        fsGroup: 9999 {code}
maybe our node's security config is one of reason. but I guess fsGroup config is needed for other user also. could you check it?;;;","14/Oct/22 13:32;gyfora;The examples are mainly designed for minikube, other kubernetes envs may require additional settings.;;;",,,,,,,,,,,,,,,,,,,,
Add Aws Glue Catalog support in Flink,FLINK-29549,13485272,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,08/Oct/22 16:55,26/Oct/23 15:25,04/Jun/24 20:41,,,,,,,,,,,,Connectors / AWS,Connectors / Common,,,0,pull-request-available,,,,"Currently , Flink sql hive connector support hive metastore as hardcoded metastore-uri. 
It would be good if flink provide feature to have configurable metastore (eg. AWS glue).

This would help many Users of flink who uses AWS Glue([https://docs.aws.amazon.com/glue/latest/dg/start-data-catalog.html]) as their common(unified) catalog and process data. 

cc [~prabhujoseph] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 26 15:25:16 UTC 2023,,,,,,,,,,"0|z19708:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 07:25;dannycranmer;[~samrat007] are you suggesting to add AWS Glue via the Hive API? This has been done for Spark already here: https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore

We have developed a Flink shim, such that you can use the same client with Flink (as a SQL catalog). However, we have not upstreamed it. If this is what you are proposing we can talk about combining efforts to upstream this? If we want to add this as a native Flink feature it will need a FLIP.

Thoughts? ;;;","13/Oct/22 08:10;samrat007;*There are two Use Cases:*

 
 # Using Glue as a catalog for Flink HiveSource. Currently the HiveShim directly create HiveMetastoreClient (HMS).
   Instead, it will check the configured factory class (HMS or Glue) introduced by        https://issues.apache.org/jira/browse/HIVE-12679 and based on that create the IMetaStoreClient.


 # Using Glue as an external catalog for Flink. This might require a new *GlueCatalog* similar to HiveCatalog/JdbcCatalog ([https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/catalogs/]).
   which will directly use Glue APIs without any Hive Dependencies.;;;","02/Dec/22 16:55;samrat007;Associated [FLIP-277|https://cwiki.apache.org/confluence/display/FLINK/FLIP-277%3A+Native+GlueCatalog+Support+in+Flink] ;;;","27/Sep/23 05:40;samrat007;Please review 
[https://github.com/apache/flink-connector-aws/pull/47];;;","26/Oct/23 12:22;samrat007;please help reviewing the PR [https://github.com/apache/flink-connector-aws/pull/47] for glue Catalog. 
PR is open for very long time time. 

 ;;;","26/Oct/23 14:51;martijnvisser;[~dannycranmer] Do you have eyes on this?;;;","26/Oct/23 15:25;dannycranmer;[~martijnvisser] yes, but have not had capacity to finish the code review recently. I will get back to it but appreciate support if anyone has spare cycles;;;",,,,,,,,,,,,,,,,
Remove deprecated MiniClusterResource,FLINK-29548,13485266,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,RocMarshal,RocMarshal,08/Oct/22 12:17,12/Oct/22 16:16,04/Jun/24 20:41,12/Oct/22 16:16,,,,,,,1.17.0,,,,Tests,,,,0,pull-request-available,,,,"Remove 

 

flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/MiniClusterResource.java

flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/MiniClusterResourceConfiguration.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 16:16:13 UTC 2022,,,,,,,,,,"0|z196yw:",9223372036854775807,"The deprecated MiniClusterResource in flink-test-utils has been removed.
The MiniClusterWithClientResource is a drop-in replacement.",,,,,,,,,,,,,,,,,,,"12/Oct/22 16:16;chesnay;master: 7a135c299f13c62832b93d7ceadb91663c7d1b53;;;",,,,,,,,,,,,,,,,,,,,,,
Select a[1] which is  array type for parquet complex type throw ClassCastException,FLINK-29547,13485265,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,08/Oct/22 11:43,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,1.20.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"Regarding the following SQL test in HiveTableSourceITCase, it will throw ClassCastException.
{code:java}
batchTableEnv.executeSql(
        ""create table parquet_complex_type_test(""
                + ""a array<int>, m map<int,string>, s struct<f1:int,f2:bigint>) stored as parquet"");
String[] modules = batchTableEnv.listModules();
// load hive module so that we can use array,map, named_struct function
// for convenient writing complex data
batchTableEnv.loadModule(""hive"", new HiveModule());
batchTableEnv.useModules(""hive"", CoreModuleFactory.IDENTIFIER);

batchTableEnv
        .executeSql(
                ""insert into parquet_complex_type_test""
                        + "" select array(1, 2), map(1, 'val1', 2, 'val2'),""
                        + "" named_struct('f1', 1,  'f2', 2)"")
        .await();

Table src = batchTableEnv.sqlQuery(""select a[1] from parquet_complex_type_test"");
List<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());{code}
The exception stack: 

Caused by: java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Ljava.lang.Integer;
    at BatchExecCalc$37.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:313)
    at org.apache.flink.streaming.api.operators.source.NoOpTimestampsAndWatermarks$TimestampsOnlyOutput.collect(NoOpTimestampsAndWatermarks.java:98)
    at org.apache.flink.streaming.api.operators.source.NoOpTimestampsAndWatermarks$TimestampsOnlyOutput.collect(NoOpTimestampsAndWatermarks.java:92)
    at org.apache.flink.connector.file.src.impl.FileSourceRecordEmitter.emitRecord(FileSourceRecordEmitter.java:45)
    at org.apache.flink.connector.file.src.impl.FileSourceRecordEmitter.emitRecord(FileSourceRecordEmitter.java:35)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:144)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:401)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)

 

After debugging the code, I found the root cause is that source operator reads array data from parquet in the vectorized way, and it returns ColumnarArrayData, then in the calc operator we convert it to GenericArrayData, the object array is Object[] type instead of Integer[], so if we call the ArrayObjectArrayConverter#toExternal method converts it to Integer[], it still returns Object[] type, and then if convert the array to Integer[] type forcedly, we will get the exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-08 11:43:05.0,,,,,,,,,,"0|z196yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UDF：Failed to compile split code, falling back to original code",FLINK-29546,13485262,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,WangHui,WangHui,08/Oct/22 11:14,17/Aug/23 10:20,04/Jun/24 20:41,,1.15.0,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"2022-10-08 19:05:23 [GroupWindowAggregate[11] (1/1)#0] WARN  org.apache.flink.table.runtime.generated.GeneratedClass -Failed to compile split code, falling back to original code
org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
    at org.apache.flink.table.runtime.operators.window.AggregateWindowOperator.compileGeneratedCode(AggregateWindowOperator.java:148)
    at org.apache.flink.table.runtime.operators.window.WindowOperator.open(WindowOperator.java:274)
    at org.apache.flink.table.runtime.operators.window.AggregateWindowOperator.open(AggregateWindowOperator.java:139)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
    ... 15 common frames omitted
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
    ... 18 common frames omitted
Caused by: org.codehaus.commons.compiler.CompileException: Line 17, Column 28: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873)
    at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6499)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6494)
    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855)
    at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494)
    at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
    ... 24 common frames omitted","my pom:

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-api-java-uber</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-runtime</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner-loader</artifactId>
</dependency>

jdk 1.8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/23 10:18;luca.yang;image-2023-08-17-18-18-50-937.png;https://issues.apache.org/jira/secure/attachment/13062242/image-2023-08-17-18-18-50-937.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 10:19:47 UTC 2023,,,,,,,,,,"0|z196y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 08:48;martijnvisser;[~WangHui] Can you share your reproducible code that results in this error? With only your POM file and this stacktrace it's not possible to determine what's causing this issue, since it's most likely coming from your application code. ;;;","17/Aug/23 10:18;luca.yang;i hava same error in flink1.15.0

 

 
{code:java}
//代码占位符
CREATE TABLE t2
(
    `log` STRING,
    `uid` AS get_json_object(`log`,'$.data.uid'),
    `phoneCode` AS get_json_object(`log`,'$.data.phoneCode'),
    `isProduct` AS get_json_object(`log`, '$.data.isProduct'),
    `ct` AS  get_json_object(`log`,'$.data.dialogDO.ctime',0),
    `ct1` AS TO_TIMESTAMP_LTZ(
        get_json_object(`log`, '$.data.dialogDO.ctime',0)
        ,0
        ),
    proc_time AS PROCTIME()
    , WATERMARK FOR `ct1` AS `ct1` - INTERVAL '15' SECOND
) WITH (
      'connector' = 'kafka',
      'properties.bootstrap.servers' = 'localhost:9092',
      'scan.startup.mode' = 'earliest-offset',
      'topic' = 'councilFeedbackRealtime',
      'properties.group.id' = 'aaa',
      'format' = 'raw'
      ); {code}
 

 

 

 
{code:java}
//代码占位符
package com.aa.tlink.udf.common
import com.alibaba.fastjson2.{JSONPath, JSONReader}
import org.apache.flink.table.functions.ScalarFunction
import scala.util.Try
import org.apache.flink.configuration.{Configuration, RestOptions}
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.table.api.Expressions.row
import org.apache.flink.table.api._
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment
object GetJsonObject {
def main(args: Array[String]): Unit =
{ val conf = new Configuration conf.setInteger(RestOptions.PORT, 38080) val env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf) val tEnv = StreamTableEnvironment.create(env) val table = tEnv.fromValues( DataTypes.ROW( DataTypes.FIELD(""id"", DataTypes.DECIMAL(10, 2)) , DataTypes.FIELD(""name"", DataTypes.STRING()) ), row(2L, """"""\{""k1"":""v111"",""k2"":100}
"""""")
)
tEnv.createTemporaryView(""tb1"", table)
tEnv.createFunction(""get_json_object"", classOf[GetJsonObject])
val tb = tEnv.executeSql(
""""""




select id,get_json_object(name,'$.k3',999) as v from tb1


"""""".stripMargin)




println(tb.getTableSchema)
tb.print()
}
}
/**

适合获取一个json值
支持 多余转义字符 \ 的json
支持完全jsonpath eg: $.f1.f2
*/
class GetJsonObject extends ScalarFunction {

/**

@param json 兼容 空字符串
@param path
@return
*/
def eval(json: String, path: String): String = {
// paths eg: $.f1.f2
// path分步解析,实现支持 反斜杠的json 同时兼容 $.f1.f2 或者 f1
val path_arr = path.replace(""$."", """") // f1.f2 或者f1
.split('.') // [f1,f2] 或者 [f1]
.map(s => s""$$.${s}"")

var jsobj: String = json
path_arr.foreach {
path_part =>
{ jsobj = Try(JSONPath.of(path_part).extract(JSONReader.of(jsobj)).toString).getOrElse("""") }
}
jsobj
}
def eval(json: String, path: String,defaultValue:String): String = {
// paths eg: $.f1.f2
// path分步解析,实现支持 反斜杠的json 同时兼容 $.f1.f2 或者 f1
val path_arr = path.replace(""$."", """") // f1.f2 或者f1
.split('.') // [f1,f2] 或者 [f1]
.map(s => s""$$.${s}"")
var jsobj: String = json
path_arr.foreach {
path_part =>
{ jsobj = Try(JSONPath.of(path_part).extract(JSONReader.of(jsobj)).toString).getOrElse(defaultValue) }
}
jsobj
}
def eval(json: String, path: String, defaultValue: Long): Long = {
// paths eg: $.f1.f2
// path分步解析,实现支持 反斜杠的json 同时兼容 $.f1.f2 或者 f1
val path_arr = path.replace(""$."", """") // f1.f2 或者f1
.split('.') // [f1,f2] 或者 [f1]
.map(s => s""$$.${s}"")
var jsobj: String = json
path_arr.foreach {
path_part =>
{ jsobj = Try(JSONPath.of(path_part).extract(JSONReader.of(jsobj)).toString).getOrElse(defaultValue.toString) }
}
jsobj.toLong
}
} {code}
 

 ;;;","17/Aug/23 10:18;luca.yang;!image-2023-08-17-18-18-50-937.png!;;;","17/Aug/23 10:19;luca.yang;error log:
{code:java}
//代码占位符
2023-08-17 15:46:02java.lang.RuntimeException: Could not instantiate generated class 'WatermarkGenerator$0'	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)	at org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.createWatermarkGenerator(GeneratedWatermarkGeneratorSupplier.java:62)	at org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.createMainOutput(ProgressiveTimestampsAndWatermarks.java:104)	at org.apache.flink.streaming.api.operators.SourceOperator.initializeMainOutput(SourceOperator.java:426)	at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:402)	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:387)	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)	... 16 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)	... 18 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)	... 21 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 30, Column 72: Cannot determine simple type name ""org""	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7121)	at org.codehaus.janino.UnitCompiler.access$17000(UnitCompiler.java:215)	at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6529)	at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6490)	at org.codehaus.janino.Java$NewClassInstance.accept(Java.java:5190)	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)	at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9237)	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)	at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)	at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)	at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)	... 27 more

 {code}
 ;;;",,,,,,,,,,,,,,,,,,,
kafka consuming stop when trigger first checkpoint,FLINK-29545,13485241,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,08/Oct/22 08:04,20/Aug/23 10:35,04/Jun/24 20:41,,1.13.3,,,,,,,,,,Runtime / Checkpointing,Runtime / Network,,,0,auto-deprioritized-critical,pull-request-available,,,"the task dag is like attached file. the task is started to consume from earliest offset, it will stop when the first checkpoint triggers.

 

is it normal?, for sink is busy 0 and the second operator has 100 backpressure

 

and check the checkpoint summary, we can find some of the sub task is n/a.

I tried to debug this issue and found in the 

triggerCheckpointAsync , the 

triggerCheckpointAsyncInMailbox took  a lot time to call

 

 

looks like this has something to do with 

logCheckpointProcessingDelay, Has any fix on this issue?

 

 

can anybody help me on this issue?

 

 

 

 

thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/22 08:16;zhoujira86;backpressure 100 busy 0.png;https://issues.apache.org/jira/secure/attachment/13050158/backpressure+100+busy+0.png","08/Oct/22 08:17;zhoujira86;task acknowledge na.png;https://issues.apache.org/jira/secure/attachment/13050159/task+acknowledge+na.png","08/Oct/22 08:12;zhoujira86;task dag.png;https://issues.apache.org/jira/secure/attachment/13050156/task+dag.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:05 UTC 2023,,,,,,,,,,"0|z196tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 08:21;zhoujira86;i can help collect if further evidence needed;;;","09/Oct/22 07:52;masteryhx;Hi, Could you provide more information about it ?
1. You mean that only the checkpoint is executed, the consumer will stop ?

2. You found the processor is blocked at logCheckpointProcessingDelay ? How do you get it ? By flame graph or thread stack? Could you also share them ? It's a bit strange that logCheckpointProcessingDelay is just a simple method of log.;;;","09/Oct/22 09:11;zhoujira86;1， yes, I have debug this task for many times, every time consumer stop is when checkpoint is triggered.  this is a EXACTLY ONCE case. I have debug for AT_LEAST_ONCE, the problem didn't appear.

 

2, I don't think processor is blocked at logCheckpointProcessingDelay, I mention it because some subtask can success and display checkpoint duration, others only shows n/a. (check the attache picture). And I found the success subtask can call the function    

SubtaskCheckpointCoordinatorImpl# checkpointState at the source task in the dag.

 

but the 'n/a' subtask only call 

StreamTask#  triggerCheckpointAsync

not sure why the 'checkpointState' did not run by mailbox executor.

 

And I have 500 taskmanager, it's hard to judge I should dump which one's thread stack

[~masteryhx] ;;;","09/Oct/22 11:11;zhoujira86; I tried to get some info when block happen

it's 

 
""Source: Custom Source (10/40)#0"" Id=67 BLOCKED on java.lang.Object@6f54b364 owned by ""Legacy Source Thread - Source: Custom Source (10/40)#0"" Id=81
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
    - blocked on java.lang.Object@6f54b364
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:344)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:684)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:639)
    at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$388/2041732781.run(Unknown Source);;;","10/Oct/22 08:18;martijnvisser;Can you please verify that this problem still exists in Flink 1.15? Flink 1.13 is no longer supported by the community;;;","11/Oct/22 02:17;zhoujira86;[~martijnvisser] this can also happen in 1.15. 

 

""Source: Custom Source (10/40)#0"" Id=67 BLOCKED on java.lang.Object@6f54b364 owned by ""Legacy Source Thread - Source: Custom Source (10/40)#0"" Id=81
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
    - blocked on java.lang.Object@6f54b364
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at

 

the Legacy Source Thread - Source: Custom Source (10/40)#0 is reqeustMemorySegment, and the localBufferPool recycle method need to be called by Source: Custom Source (10/40)#0 thread, this can be deadlock when checkpoint happen;;;","12/Oct/22 10:20;zhoujira86;[~martijnvisser]  Can you please suggest which part to look into, if found downward task busy 0 and upward task 100% backpressure

 ;;;","14/Oct/22 09:54;pnowojski;It looks like the problem is with the upstream sources. What might be happening is that downstream sub tasks are waiting for checkpoint barriers from those remaining sources, that for some reason do not arrive, and this is blocking the alignment process on the downstream subtasks, blocking the progress of the whole job. I don't know, maybe this state would be reported as backpressured.

Nevertheless I would dig deeper why, in this screen shot (below), those 4 subtasks haven't finished the checkpoint. It looks like they might have deadlocked. You could for example show thread dump from a task manager that is running one of those source subtasks (and tell us what is the name/subtask id of the problematic subtask). Given that you have a custom source, you can also double check if it is implemented correctly. Especially when it comes to acquisition of the checkpoint lock and/or {{CheckpointedFunction#snapshotState}} / {{CheckpointedFunction#initializeState}} methods. I would expect some problem with your implementation of that source. Can you maybe share the source code of that source?

!task acknowledge na.png|height=400!;;;","17/Oct/22 06:58;zhoujira86;[~pnowojski] This turns out to be a network issue, I added the idle state detection, and found the idle reader event. I added a PR, Could you please help review it;;;","17/Oct/22 12:26;zhoujira86;Sorry， I was to rude to get the network issue conclusion.  Maybe it's not the root cause.

 

I was using the KafkaSource code from 1.13

 

kafkaSource = KafkaSource
.<RawNode>builder()
.setBootstrapServers(props.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG))
.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId)
.setProperty(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchSize)
.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, pollRecords)
.setTopicPattern(pattern)
.setGroupId(groupId)
.setStartingOffsets(OffsetsInitializer.earliest())
.setDeserializer(new KafkaDeserializationRecordSchema())
.build();

 

 

logMessageDs = senv.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), ""kafkaSource"")
.rebalance()....

 

 

BTW, would you please suggest other possibility?;;;","19/Oct/22 14:08;pnowojski;Huh. On the screenshots that you attached [~zhoujira86], the source shows as ""Custom Source"", which suggests it wasn't Kafka? Are those screenshots from a different job?

If the problem also happens with KafkaSource, I wouldn't expect a bug there (but who knows). Anyway I would still point you towards the same thing that I wrote before:
{quote}
Nevertheless I would dig deeper why, in this screen shot (below), those 4 subtasks haven't finished the checkpoint. It looks like they might have deadlocked. You could for example show thread dump from a task manager that is running one of those source subtasks (and tell us what is the name/subtask id of the problematic subtask).
{quote};;;","24/Oct/22 14:24;zhoujira86;[~pnowojski] Hi Master, I could clearly find the netty client side has quite a few successful netty listener call back(10 sec periodic heartbeat), but the netty server side could trigger the idle timeout for 30 second. so I think the heart beat for the data communication is also necessary... I have created a PR, please kindly review...;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,
Update Flink doc,FLINK-29544,13485228,13437879,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,ConradJam,ConradJam,ConradJam,08/Oct/22 04:50,15/Nov/22 08:57,04/Jun/24 20:41,15/Nov/22 08:57,1.17.0,,,,,,,,,,Documentation,,,,0,,,,,"update flink doc add configuration field describe

[https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jars-jarid-run]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-08 04:50:34.0,,,,,,,,,,"0|z196qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jar Run Rest Handler Support Flink Configuration,FLINK-29543,13485227,13437879,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ConradJam,ConradJam,ConradJam,08/Oct/22 04:45,15/Nov/22 09:09,04/Jun/24 20:41,15/Nov/22 08:57,1.17.0,,,,,,1.17.0,,,,Runtime / REST,,,,0,pull-request-available,,,,Flink JM Rest Api Support Flink Configuration field,,,,,,,,,,,,,,,,,,,,,,,FLINK-28651,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 08:57:02 UTC 2022,,,,,,,,,,"0|z196q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 08:57;chesnay;master: 962a3474e55f23bca0877957130cac83b38941dd;;;",,,,,,,,,,,,,,,,,,,,,,
Unload.md wrongly writes UNLOAD operation as LOAD operation,FLINK-29542,13485223,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,jiaoqb,jiaoqb,08/Oct/22 03:22,19/May/23 06:52,04/Jun/24 20:41,19/May/23 06:52,1.17.0,,,,,,1.16.3,1.17.1,1.18.0,,Documentation,,,,0,pull-request-available,,,,"UNLOAD statements can be executed with the {{executeSql()}} method of the {{{}TableEnvironment{}}}. The {{executeSql()}} method returns ‘OK’ for a successful {color:#FF0000}LOAD{color} operation; otherwise it will throw an exception.

 

which should be {color:#FF0000}UNLOAD {color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 06:52:16 UTC 2023,,,,,,,,,,"0|z196pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 06:52;wanglijie;Fixed via
master: 522fac5c1d0429621be5cac2d83b1e9ff8da106e
release-1.17: 2d4df6f6406103a6b74cb6b3996bedaac29b8710
release-1.16: a6f1f97f6193c402d656ac98dae6e14978b4a482

BTW, for the minor fixs/improvements of docs, you can directly open a hotfix PR without creating a JIRA [~jiaoqb];;;",,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner,FLINK-29541,13485220,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,wanglijie,wanglijie,08/Oct/22 02:50,22/Nov/23 06:10,04/Jun/24 20:41,22/Nov/23 06:10,1.18.0,,,,,,1.19.0,,,,Table SQL / Planner,Tests,,,0,,,,,,,,,,,,,,,,,,,FLINK-27940,,,FLINK-25325,,,,,,,,,FLINK-32967,,,,,,,,,,FLINK-31674,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 06:14:08 UTC 2023,,,,,,,,,,"0|z196oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 13:21;rskraba;Hello!  There's a LOT of unit tests in flink-table-planner – are you currently working on this?

I was thinking about breaking this up by taking one of the lower level test cases like (like {{BatchAbstractTestBase}} and doing the entire hierarchy of tests across all modules that depend on it one way or another.  Just doing that PR would touch many dozens of files (most of them inside flink-table-planner, but also across flink-format), but they would all be related.  What do you think?

I'd be willing to help out in this module if I can!;;;","27/Oct/22 02:09;wanglijie;No, I'm not working on this, because I'm not familiar with flink-table-planner. I create this issue since the migration of this module does block another work I'm doing (FLINK-27940).  Feel free to take it if you want :) [~rskraba] ;;;","27/Oct/22 12:59;rskraba;I can start attacking these tests!  I think my proposed strategy of doing a ""hierarchy"" of related tests that share a common abstract base class but have implementations across modules is probably a good way to attack this, but I'll start with a single PR to demonstrate.

Can this JIRA be assigned to me?;;;","28/Oct/22 05:36;wanglijie;Sure, just assigned to you :). It would be better to create a separate issue to track these abstract base classes, just like what you did in FLINK-28542;;;","12/Mar/23 07:11;jark;Hi [~rskraba], what's the progress now? I agree with you this might be a huge work and we can separate the issue into sub-tasks. ;;;","14/Mar/23 15:54;rskraba;Hey thanks for checking in!  I had quite a bit of progress on this, but let it slide during the last few months.  I'd love to get back into it.

Would it be possible to retitle this Jira ""[JUnit5 Migration] Module: flink-table-planner (BatchAbstractTestBase hierarchy)"" ?  At the minimum this would let other devs know that the other hierarchies of test cases starting from flink-table-planner bases are available for migration!;;;","15/Mar/23 01:32;jark;Hi [~rskraba], I would suggest keeping this issue as is and creating issues for individual tests migration. This helps us to track the overall migration progress. We can't create sub-tasks under a sub-task, so may need to use issue links to linking the sub issues. ;;;","30/Mar/23 14:28;rskraba;I created the linked Jira to address only the {{BatchAbstractTestBase}} (but I'm willing to take on another tree of tests once the PR for that one is done!);;;","08/Apr/23 12:48;jark;Considering migrating for flink-table-planner is a huge effort, I converted this issue from sub-task into an umbrella issue. [~rskraba] feel free to create other sub-issues under it when you finish the {{BatchAbstractTestBase}}. ;;;","27/Aug/23 16:11;jiabao.sun;Hey [~jark],

I found that this issue has not been updated for a long time. 
Could you help assign this ticket to me?;;;","28/Aug/23 06:14;leonard;Hi, [~rskraba] I assigned this ticket to [~jiabao.sun] as this ticket hasn't been updated for a long time, you can also help to review PR if you have time slot.;;;",,,,,,,,,,,,
SubQueryAntiJoinTest started to fail after Calcite 1.27,FLINK-29540,13485211,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,07/Oct/22 22:54,03/Nov/23 04:04,04/Jun/24 20:41,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"Probably the reason is https://issues.apache.org/jira/browse/CALCITE-4560

 

some tests are failing with
{noformat}
java.lang.NullPointerException
	at org.apache.calcite.sql2rel.RelDecorrelator.createValueGenerator(RelDecorrelator.java:858)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateInputWithValueGenerator(RelDecorrelator.java:1070)
	at org.apache.calcite.sql2rel.RelDecorrelator.maybeAddValueGenerator(RelDecorrelator.java:987)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1199)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1165)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:771)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:760)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1236)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:771)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:760)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1186)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1165)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:771)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:760)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:771)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:760)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:531)
	at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:729)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelate(RelDecorrelator.java:315)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateQuery(RelDecorrelator.java:209)
	at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateQuery(RelDecorrelator.java:174)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.optimize(FlinkDecorrelateProgram.scala:41)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:93)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:322)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlanNotExpected(TableTestBase.scala:617)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlanNotExpected(TableTestBase.scala:607)
	at org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryAntiJoinTest.testNotInWithCorrelatedOnWhere_Case6(SubQueryAntiJoinTest.scala:354)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)



{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27998,FLINK-20873,FLINK-33446,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 13:27:39 UTC 2023,,,,,,,,,,"0|z196mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 14:30;twalthr;[~Sergey Nuyanzin] I moved this to 1.29 update. Feel free to close it if it has been solved already on the way?;;;","24/Jan/23 13:27;Sergey Nuyanzin;I workarounded it in Flink code
however it seems it should be fixed on Calcite level and then changes could be removed (will put a comment about that)
there is a similar issue for that https://issues.apache.org/jira/browse/CALCITE-5390
traces are a bit different however i think this is because in Calcite issue the trace is from main branch;;;",,,,,,,,,,,,,,,,,,,,,
dnsPolicy in FlinkPod is not overridable ,FLINK-29539,13485210,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,carloscastro,carloscastro,carloscastro,07/Oct/22 22:44,28/Oct/22 09:59,04/Jun/24 20:41,28/Oct/22 09:58,,,,,,,1.15.3,1.16.1,1.17.0,,Deployment / Kubernetes,,,,0,pull-request-available,,,,"With this PR [https://github.com/apache/flink/pull/18119 |https://github.com/apache/flink/pull/18119]it stopped being possible to override the dnsPolicy in the FlinkPod spec.

To fix it, it should check first if the dnsPolicy is not null before applying the default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 09:59:12 UTC 2022,,,,,,,,,,"0|z196mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 09:58;wangyang0918;Fixed via:

master: bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7

release-1.16: 6325adf40a1baa8c3ac82aa06c57425c3c6005c4

release-1.15: d9413d6bc6548ddd4c2e4d6e05db0903da064476;;;","28/Oct/22 09:59;wangyang0918;Thanks [~carloscastro] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,
Setup CI based on the new Elasticsearch CI setup,FLINK-29538,13485165,13279026,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,07/Oct/22 14:33,28/Apr/23 14:04,04/Jun/24 20:41,28/Apr/23 14:04,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-07 14:33:20.0,,,,,,,,,,"0|z196cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename flink-connector-redis repository to flink-connector-redis-streams,FLINK-29537,13485160,13279026,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,07/Oct/22 14:13,10/Oct/22 11:43,04/Jun/24 20:41,10/Oct/22 11:43,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-23752,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 11:43:36 UTC 2022,,,,,,,,,,"0|z196bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 11:43;martijnvisser;Fixed via Infra ticket;;;",,,,,,,,,,,,,,,,,,,,,,
Add WATCH_NAMESPACES env var to kubernetes operator,FLINK-29536,13485159,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tagarr,tagarr,tagarr,07/Oct/22 14:13,08/Dec/22 23:15,04/Jun/24 20:41,08/Dec/22 23:15,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Provide the ability to set the namespaces watched by the operator using an env var. Whilst the additional config can still be used, the presence of the env var will take priority.

 

Reasons for issue
 # Operator will take effect of the setting immediately as pod will roll (rather than waiting for the config to be refreshed)
 # If the operator is to be olm bundled we will be able to set the target namespace using the following 

{{                    env:}}

  {{                      - name: WATCHED_NAMESPACE}}

      {{                        valueFrom:}}

          {{                          fieldRef:}}

             {{                            fieldPath: metadata.annotations['olm.targetNamespaces']}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 23:15:06 UTC 2022,,,,,,,,,,"0|z196bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 18:16;tagarr;[~gyfora] can you assign this to me thanks;;;","31/Oct/22 16:35;tagarr;Created PR for the required changes;;;","02/Nov/22 05:14;jbusche;[~tagarr] [~tedhtchang] 

Helm:  I believe it works as expected
Using the code in the PR, I built a new operator image and then installed it in the default namespace and then tried the basic.yaml
{quote}
h4. _helm install flink-kubernetes-operator helm/flink-kubernetes-operator --set image.repository=$(oc registry info --internal)/$NAMESPACE/debian-release --set image.tag=tony-29536_
h4. {{_kubectl create -f [https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-1.2/examples/basic.yaml]_}}

oc get pods

NAME                                         READY   STATUS    RESTARTS   AGE

basic-example-56876dc586-qtbj4               1/1     Running   0          69s

basic-example-taskmanager-1-1                1/1     Running   0          56s

flink-kubernetes-operator-7fc7c6fd77-77hp4   2/2     Running   0          6m48s
{quote}
Looking inside the operator pod, the WATCH_NAMESPACE remains the same as before:
{quote}oc rsh flink-kubernetes-operator-7fc7c6fd77-77hp4 env |grep WATCH

*WATCH*_NAMESPACES=
{quote};;;","02/Nov/22 05:24;jbusche;When I build an OLM bundle of the PR and deploy it on OpenShift, the behavior changes as expected.

I installed in both the default namespace and testing121 namespaces:
{quote}oc rsh -n testing121  flink-kubernetes-operator-7c9b585b76-sxwfw env |grep WATCH

*WATCH*_NAMESPACES=testing121

[root@api.jim-fips.cp.fyre.ibm.com ~]# oc rsh -n default  flink-kubernetes-operator-d955cc679-9sx85 env |grep WATCH

*WATCH*_NAMESPACES=default
{quote}
When I installed in all namespaces, then the WATCH_NAMESPACES= remained empty (watching all the namespaces, as expected.

If you want to try to my OLM bundle, you could do the following:
{quote} # Install OLM if you don't already have it:
{quote}curl -sL [https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.22.0/install.sh] | bash -s v0.22.0{quote}
 # Deploy my catsrc:cat <<EOF | kubectl apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: flink-29536-operator-catalog
  namespace: openshift-marketplace
spec:
  sourceType: grpc
  image: docker.io/jimbdocker/flink-29536-catalog:1.2.1
EOF

 # Create a namespace (or use the default) and then deploy a subscription like this:oc create ns testing121

oc project testing121

 

cat << EOF | kubectl apply -f -
apiVersion: operators.coreos.com/v1alpha2
kind: OperatorGroup
metadata:
  name: default-og
  namespace: testing121
spec:
  targetNamespaces:
  - testing121
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: flink-operator
  namespace: testing121
spec:
  channel: alpha
  name: flink-kubernetes-operator
  source: flink-29536-operator-catalog
  sourceNamespace: openshift-marketplace
  installPlanApproval: Automatic
  startingCSV: flink-kubernetes-operator.v1.2.1
EOF

 

oc rsh flink-kubernetes-operator-7c9b585b76-sxwfw env |grep WATCH

*WATCH*_NAMESPACES=testing121
{quote}
 

 ;;;","08/Dec/22 23:15;morhidi;fixed via:
65b30d07da56259165936b5e0519f92640174cd6
686990a15db88789e538609e01a4dde73e966c12;;;",,,,,,,,,,,,,,,,,,
Flink Operator Certificate renew issue,FLINK-29535,13485154,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,applike-ss,applike-ss,07/Oct/22 13:31,07/Oct/22 14:30,04/Jun/24 20:41,07/Oct/22 14:30,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"It seems that there is an issue with the Kubernetes Operator (at least in version 1.1.0) when it comes to certificates for the webhook.

We've seen this error message pop up in the logs:
| |
|An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.|
| 
and

javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate at sun.security.ssl.Alert.createSSLException(Unknown Source) ~[?:?] at sun.security.ssl.Alert.createSSLException(Unknown Source) ~[?:?] at sun.security.ssl.TransportContext.fatal(Unknown Source) ~[?:?] at sun.security.ssl.Alert$AlertConsumer.consume(Unknown Source) ~[?:?] at sun.security.ssl.TransportContext.dispatch(Unknown Source) ~[?:?] at sun.security.ssl.SSLTransport.decode(Unknown Source) ~[?:?] at sun.security.ssl.SSLEngineImpl.decode(Unknown Source) ~[?:?] at sun.security.ssl.SSLEngineImpl.readRecord(Unknown Source) ~[?:?] at sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source) ~[?:?] at sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source) ~[?:?] at javax.net.ssl.SSLEngine.unwrap(Unknown Source) ~[?:?] at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:296) ~[flink-kubernetes-operator-1.1.0-shaded.jar:1.1.0] at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1342) ~[flink-kubernetes-operator-1.1.0-shaded.jar:1.1.0] at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1235) ~[flink-kubernetes-operator-1.1.0-shaded.jar:1.1.0] at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1284) ~[flink-kubernetes-operator-1.1.0-shaded.jar:1.1.0] at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) ~[flink-kubernetes-operator-1.1.0-shaded.jar:1.1.0] at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ~[flink-kubernetes-operator-1.1.0-shaded.jar:1.1.0]|

It happens when our fluxcd is trying to update the FlinkDeployment resource.

This seems to trigger a webhook to an endpoint (in the operator) which is serving a (then) invalid certificate.

We've noticed this after 18 days of it running, so maybe something shortlived was not renewed correctly?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 14:30:06 UTC 2022,,,,,,,,,,"0|z196a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 14:26;gyfora;I think this might be already fixed in 1.2.0: https://issues.apache.org/jira/browse/FLINK-28272

 ;;;","07/Oct/22 14:28;applike-ss;Great News! I will check it out soon. Thanks! :);;;","07/Oct/22 14:30;gyfora;please reopen it in case the other fix is not working;;;",,,,,,,,,,,,,,,,,,,,
@TypeInfo on field requires field type to be valid Pojo ,FLINK-29534,13485117,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,exidex,exidex,07/Oct/22 10:19,07/Oct/22 11:38,04/Jun/24 20:41,07/Oct/22 11:38,1.14.0,1.15.0,,,,,,,,,API / Type Serialization System,,,,0,,,,,"The ability to place @TypeInfo on field was added in [https://github.com/apache/flink/pull/8344] . But it seams like the fact that it requires field to be a valid POJO was overlooked. In my case I was trying to add custom serializer for Jackson's ObjectNode (wrapped in List but not sure if this is relevant https://issues.apache.org/jira/browse/FLINK-26470) which is not a valid POJO, and this requirement seams to defeat the whole purpose of such feature. It also doesn't look like like there's a way to register {{org.apache.flink.api.common.typeutils.TypeSerializer}} globally on 3rd-party types

code snippet from TypeExtractor:
{code:java}
Type fieldType = field.getGenericType();
if (!isValidPojoField(field, clazz, typeHierarchy) && clazz != Row.class) {
    LOG.info(
            ""Class ""
                    + clazz
                    + "" cannot be used as a POJO type because not all fields are valid POJO fields, ""
                    + ""and must be processed as GenericType. {}"",
            GENERIC_TYPE_DOC_HINT);
    return null;
}
try {
    final TypeInformation<?> typeInfo;
    List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);
    TypeInfoFactory factory = getTypeInfoFactory(field);
    if (factory != null) {{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 11:38:26 UTC 2022,,,,,,,,,,"0|z19620:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 10:43;chesnay;It's not the field most be a POJO; the annotated class must actually go through Flinks serialization stack.

If the ObjectNode is within a List then the whole thing goes through Kryo; the List is serialized with Kryo and Flink neither does nor can further analyze the contents.
If you'd annotate the list with {{@TypeInfo}} and set it up to go through Flinks ListSerializer than TypeInfo will also work on the contained ObjectNodes.;;;","07/Oct/22 10:45;chesnay;??It also doesn't look like like there's a way to register org.apache.flink.api.common.typeutils.TypeSerializer globally on 3rd-party types??

Yes, this was removed at some point.
;;;","07/Oct/22 11:38;exidex;You are right the problem I had was in different place. Here it worked properly, but there was still a log in logs which misled me.;;;",,,,,,,,,,,,,,,,,,,,
Add proper table style to Flink website,FLINK-29533,13484938,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,mapohl,mapohl,07/Oct/22 06:33,25/May/23 12:46,04/Jun/24 20:41,25/May/23 12:46,1.16.0,1.17.0,,,,,,,,,Project Website,,,,0,starter,,,,"Tables can be created using simple markdown syntax. But the corresponding rendered table lacks proper styling:  !Screenshot from 2022-10-07 08-23-01.png!

Several blog post work around that by adding a custom style:
 * [Apache Flink Kubernetes Operator 1.0.0 Release Announcement|https://flink.apache.org/news/2022/06/05/release-kubernetes-operator-1.0.0.html]
 * [Improving speed and stability of checkpointing with generic log-based incremental checkpoints|https://flink.apache.org/2022/05/30/changelog-state-backend.html]

What about coming up with a common style that doesn't require people to come up with their own custom style per post.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/22 06:30;mapohl;Screenshot from 2022-10-07 08-23-01.png;https://issues.apache.org/jira/secure/attachment/13050129/Screenshot+from+2022-10-07+08-23-01.png","12/May/23 18:14;phs-sakshi;image-2023-05-12-14-14-58-020.png;https://issues.apache.org/jira/secure/attachment/13058048/image-2023-05-12-14-14-58-020.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 12:45:53 UTC 2023,,,,,,,,,,"0|z194y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 14:36;martijnvisser;With FLINK-22922 completed I've integrated a new table shortcode that allows styling. It uses https://pablotron.org/articles/table-shortcode-examples/ as you can see for example in https://github.com/apache/flink-web/blob/asf-site/docs/content/community.md?plain=1;;;","10/May/23 20:14;phs-sakshi;Can I work on this issue?;;;","11/May/23 07:40;martijnvisser;[~phs-sakshi] Definitely, please use the example that I've mentioned in my previous comment. I'll assign the ticket to you;;;","11/May/23 17:19;phs-sakshi;Thank you so much. Will definitely take a look at the example and start working on the issue.;;;","12/May/23 18:16;phs-sakshi;Can you please guide me on where the issue is? The styling looks as expected to me. Is there something I am missing?
!image-2023-05-12-14-14-58-020.png|width=736,height=262!;;;","25/May/23 12:45;martijnvisser;This actually might have already been resolved after the migration to Hugo. Sorry! Closing this ticket;;;",,,,,,,,,,,,,,,,,
Update Pulsar dependency to 2.10.1,FLINK-29532,13484876,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,06/Oct/22 21:23,20/Oct/22 02:33,04/Jun/24 20:41,10/Oct/22 11:28,,,,,,,1.16.0,1.17.0,,,Connectors / Pulsar,,,,0,pull-request-available,,,,Update the Pulsar dependency to 2.10.1 to benefit of the fixes highlights at https://github.com/apache/pulsar/releases/tag/v2.10.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 08:03:43 UTC 2022,,,,,,,,,,"0|z194kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 06:05;syhily;It's OK to upgrade the Pulsar to 2.10.1. But the 2.11.0 will be released soon, I prefer to bump the Pulsar dependency to 2.11.0 directly.;;;","10/Oct/22 08:03;martijnvisser;Fixed in:

master: f5f2e16fb055a4aa7ea788c8273c0543bc7e309d
release-1.16: 04c1b5a23f320904fa541204ebb4f9d2d231ce0f;;;",,,,,,,,,,,,,,,,,,,,,
Bump protoc and protobuf-java dependencies to 3.21.7,FLINK-29531,13484857,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,06/Oct/22 20:06,07/Oct/22 11:22,04/Jun/24 20:41,07/Oct/22 11:22,,,,,,,1.17.0,,,,API / Python,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,,,Bump protoc and protobuf-java dependencies to at least 3.21.7 to avoid false positive scans on Protobuf vulnerabilities,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 11:22:31 UTC 2022,,,,,,,,,,"0|z194gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 11:22;martijnvisser;Fixed in master: 9a5cb26099575494387205269da38e587e31fc28;;;",,,,,,,,,,,,,,,,,,,,,,
CommittableMessageTypeInfo#toString may force client to have S3 credentials,FLINK-29530,13484804,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,chesnay,chesnay,06/Oct/22 14:59,11/Mar/24 12:43,04/Jun/24 20:41,,1.15.0,,,,,,1.20.0,,,,Connectors / FileSystem,,,,0,,,,,"The {{toString}} implementation calls the committable serializer factory, which in the case of the {{FileSink}} loads a filesystem and thus may require credentials.
As a result something as basic as logging the type info can force the client to require S3 credentials.

It shouldn't have such side-effects......

Stacktrace that a user provided for another issue:
{code}
Exception in thread ""main"" org.apache.flink.util.FlinkRuntimeException: Could not create committable serializer.
	at org.apache.flink.connector.file.sink.FileSink.getCommittableSerializer(FileSink.java:180)
	at org.apache.flink.streaming.api.connector.sink2.CommittableMessageTypeInfo.toString(CommittableMessageTypeInfo.java:120)
	at java.base/java.lang.String.valueOf(String.java:2951)
	at java.base/java.lang.StringBuilder.append(StringBuilder.java:172)
	at org.apache.flink.api.dag.Transformation.toString(Transformation.java:556)
	at java.base/java.lang.String.valueOf(String.java:2951)
	at java.base/java.lang.StringBuilder.append(StringBuilder.java:172)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:506)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.access$200(StreamGraphGenerator.java:131)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator$ContextImpl.transform(StreamGraphGenerator.java:933)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator$SinkExpander.expand(SinkTransformationTranslator.java:157)
	at org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator$SinkExpander.access$000(SinkTransformationTranslator.java:99)
	at org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.translateInternal(SinkTransformationTranslator.java:89)
	at org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.translateForStreaming(SinkTransformationTranslator.java:77)
	at org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.translateForStreaming(SinkTransformationTranslator.java:61)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:825)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:555)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.generate(StreamGraphGenerator.java:316)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2135)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2121)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1967)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 08:26:07 UTC 2023,,,,,,,,,,"0|z1944o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 15:05;chesnay;{{equals()}} and {{hashcode()}} also load the serializer.;;;","06/Oct/22 15:06;chesnay;Maybe the issue isn't that they load the serializer (which one should always be able to do client-side), but that {{FileSink#getCommittableSerializer}} derives its serializers from a {{BulkBucketWriter}} instance that it has to create first and which needs a filesystem path.;;;","29/Aug/23 08:26;renqs;[~chesnay] Any update on this one? Thanks;;;",,,,,,,,,,,,,,,,,,,,
Update flink version in flink-python-example of flink k8s operator,FLINK-29529,13484803,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sriramgr,bgeng777,bgeng777,06/Oct/22 14:57,24/Oct/22 18:32,04/Jun/24 20:41,24/Oct/22 18:32,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Currently, we hardcoded the version of both flink image and pyflink pip package as 1.15.0 in the example's Dockerfile. It is not the best practice as the flink has new 1.15.x releases.
We had better do following improvements:
{{FROM flink:1.15.0 -> FROM flink:1.15}}
{{RUN pip3 install apache-flink==1.15.0 -> RUN pip3 install ""apache-flink>=1.15.0,<1.16.0""}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 24 18:32:01 UTC 2022,,,,,,,,,,"0|z1944g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 14:54;sriramgr;[~bgeng777]/[~gyfora] - Please assign this to me.;;;","22/Oct/22 11:19;sriramgr;[~bgeng777]/[~gyfora] - https://github.com/apache/flink-kubernetes-operator/pull/408;;;","24/Oct/22 18:32;gyfora;merged to main a978375daeefbf23b548aab940a7ad3c366aa661;;;",,,,,,,,,,,,,,,,,,,,
Check if core-default-shaded is still required,FLINK-29528,13484794,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/Oct/22 14:35,15/Nov/22 13:31,04/Jun/24 20:41,15/Nov/22 13:31,,,,,,,1.17.0,,,,FileSystems,,,,0,pull-request-available,,,,"fs-hadoop-shaded contains a core-default-shaded.xml that was originally meant to point Hadoop to relocated classes (back when we were relocating Hadoop as well), and to that end we even copied classes from Hadoop.

Now that the filesystems no longer relocate Hadoop I'm wondering if we even need this anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 13:31:27 UTC 2022,,,,,,,,,,"0|z1942g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 13:31;chesnay;master: f8b30119e36fa3b24c2b8a0f92de1468237c1e58;;;",,,,,,,,,,,,,,,,,,,,,,
Make unknownFieldsIndices work for single ParquetReader,FLINK-29527,13484729,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,suns,suns,suns,06/Oct/22 09:39,04/Aug/23 05:51,04/Jun/24 20:41,04/Aug/23 05:51,1.16.0,,,,,,1.19.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"Currently, from the improvement FLINK-23715, Flink use a collection named `unknownFieldsIndices` to track the nonexistent fields, and it is kept inside the `ParquetVectorizedInputFormat`, and applied to all parquet files under given path.

However, some fields may only be nonexistent in some of the historical parquet files, while exist in latest ones. And based on `unknownFieldsIndices`, flink will always skip these fields, even thought they are existing in the later parquets.

As a result, the value of these fields will become empty when they are nonexistent in some historical parquet files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 05:51:52 UTC 2023,,,,,,,,,,"0|z193o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 09:41;suns; [~lirui] Please assign this issue to me, thanks, I will fix it.;;;","14/Oct/22 03:54;lirui;[~suns] Assigned. Thanks for taking the issue;;;","04/Aug/23 05:51;tison;master via 50622df1f01cd9cade78bfe2add5a7faff678e3e

Shall we pick to other version? It seems somehow a feature catch up or fix.;;;",,,,,,,,,,,,,,,,,,,,
Java doc mistake in SequenceNumberRange#contains(),FLINK-29526,13484685,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,06/Oct/22 02:50,19/Oct/22 11:48,04/Jun/24 20:41,19/Oct/22 11:48,,,,,,,1.17.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"!image-2022-10-06-10-50-16-927.png|width=554,height=106!

Hi [~masteryhx] , It seems a typo, I have submit a pr for it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/22 02:50;Feifan Wang;image-2022-10-06-10-50-16-927.png;https://issues.apache.org/jira/secure/attachment/13050116/image-2022-10-06-10-50-16-927.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 11:48:06 UTC 2022,,,,,,,,,,"0|z193e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 03:06;Yanfei Lei;Thanks for reporting this, you're right. From the implementation in {{{}GenericSequenceNumberRange#contains{}}}, the range should be left-bounded.;;;","19/Oct/22 11:48;ym;merged commit c84c9d7 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,
Support INSTR、LEFT、RIGHT built-in function in Table API,FLINK-29525,13484684,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 02:06,09/Oct/22 02:12,04/Jun/24 20:41,09/Oct/22 02:12,1.16.0,,,,,,1.16.0,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 02:06:09.0,,,,,,,,,,"0|z193e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support DECODE、ENCODE built-in function in Table API,FLINK-29524,13484683,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 02:05,09/Oct/22 02:10,04/Jun/24 20:41,09/Oct/22 02:10,1.16.0,,,,,,1.16.0,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 02:05:14.0,,,,,,,,,,"0|z193ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support STR_TO_MAP、SUBSTR built-in function in Table API,FLINK-29523,13484682,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 02:03,09/Oct/22 02:11,04/Jun/24 20:41,09/Oct/22 02:11,1.16.0,,,,,,1.16.0,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 02:03:20.0,,,,,,,,,,"0|z193dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SPLIT_INDEX built-in function in Table API,FLINK-29522,13484681,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 02:02,09/Oct/22 02:11,04/Jun/24 20:41,09/Oct/22 02:11,1.16.0,,,,,,1.16.0,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 02:02:22.0,,,,,,,,,,"0|z193dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support REVERSE built-in function in Table API,FLINK-29521,13484680,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 02:01,09/Oct/22 02:12,04/Jun/24 20:41,09/Oct/22 02:12,1.16.0,,,,,,1.16.0,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 02:01:33.0,,,,,,,,,,"0|z193d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PARSE_URL built-in function in Table API,FLINK-29520,13484679,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 02:00,09/Oct/22 02:13,04/Jun/24 20:41,09/Oct/22 02:13,1.16.0,,,,,,1.16.0,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 02:00:07.0,,,,,,,,,,"0|z193cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support DAYOFYEAR、DAYOFMONTH built-in function in Table API,FLINK-29519,13484678,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 01:56,06/Oct/22 02:56,04/Jun/24 20:41,,1.16.0,,,,,,,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 01:56:59.0,,,,,,,,,,"0|z193co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support YEAR、QUARTER、MONTH、WEEK、HOUR、MINUTE、SECOND built-in function in Table API,FLINK-29518,13484677,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 01:56,06/Oct/22 02:56,04/Jun/24 20:41,,1.16.0,,,,,,,,,,API / Python,Table SQL / API,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 01:56:19.0,,,,,,,,,,"0|z193cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation about DATE_FORMAT to state that it's supported in Table API,FLINK-29517,13484675,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 01:33,08/Oct/22 09:36,04/Jun/24 20:41,08/Oct/22 09:36,1.16.0,,,,,,1.17.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 09:36:20 UTC 2022,,,,,,,,,,"0|z193c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 09:36;dianfu;Merged to master via d1cb177c91b41a5387814ad60d1799c08caf3ad9;;;",,,,,,,,,,,,,,,,,,,,,,
Support TIMESTAMPADD built-in function in Table API,FLINK-29516,13484674,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,cun8cun8,cun8cun8,cun8cun8,06/Oct/22 01:31,15/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,API / Python,Table SQL / API,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:35:00 UTC 2023,,,,,,,,,,"0|z193bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,
Document KafkaSource behavior with inconsistent state,FLINK-29515,13484669,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,06/Oct/22 00:25,13/Oct/22 06:19,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Connectors / Kafka,Documentation,,,0,,,,,KafkaSource can be in an inconsistent state if the offsets change and if the some topics are deleted. This typically happens in Kafka cluster migration and there are a few manual steps that should be documented,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 00:25:44.0,,,,,,,,,,"0|z193ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Minikdc to v3.2.4,FLINK-29514,13484663,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,05/Oct/22 22:15,07/Oct/22 11:23,04/Jun/24 20:41,07/Oct/22 11:23,,,,,,,1.17.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 11:23:23 UTC 2022,,,,,,,,,,"0|z1939c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 11:23;martijnvisser;Fixed in master: b38abd4cf874ebce3ef9468b217aecbb6e82f424;;;",,,,,,,,,,,,,,,,,,,,,,
