Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Completes),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Fix incorrect path in tpcds-tool README,FLINK-31513,13529110,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,GoTT,GoTT,GoTT,19/Mar/23 08:08,22/Mar/23 10:24,04/Jun/24 20:41,22/Mar/23 10:23,1.18.0,,,,,,,,,,,1.18.0,,,,,,,,0,pull-request-available,,,,"when run the flink-tpcds-test, find a path in readme need to update",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/23 08:07;GoTT;截屏2023-03-19 15.58.45.png;https://issues.apache.org/jira/secure/attachment/13056484/%E6%88%AA%E5%B1%8F2023-03-19+15.58.45.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 10:23:53 UTC 2023,,,,,,,,,,"0|z1gopc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 08:11;GoTT;https://github.com/apache/flink/pull/22212;;;","22/Mar/23 10:23;Weijie Guo;master(1.18) via 18869f6dfd04abd017cf110932c48a33e91c6bb5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SqlAlterView conversion logic to SqlAlterViewConverter,FLINK-31512,13529106,13528594,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jark,jark,jark,19/Mar/23 06:59,19/Mar/23 07:00,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Introduce {{SqlAlterViewConverter}} and move the conversion logic of {{SqlAlterView}} -> {{AlterViewOperation}} to it.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-19 06:59:40.0,,,,,,,,,,"0|z1goog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate documentation sql_functions_zh.yml  to the latest version,FLINK-31511,13529074,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,ruanhang1993,ruanhang1993,ruanhang1993,18/Mar/23 12:37,26/Jan/24 09:55,04/Jun/24 20:41,26/Jan/24 09:55,1.18.1,,,,,,,,,,,1.19.0,,,,Documentation,Table SQL / API,,,0,pull-request-available,stale-assigned,,,"Some content of these functions in sql_functions_zh.yml is outdated.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 09:55:37 UTC 2024,,,,,,,,,,"0|z1gohc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/23 12:37;ruanhang1993;Hi, all, 

I would like to help update this document as some content is out dated.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","26/Jan/24 09:55;leonard;Resolved via master: 36feab0fb6db9ba2e284352a19db8925cb6e8874;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use getMemorySize instead of getMemory,FLINK-31510,13529036,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,18/Mar/23 01:14,11/Apr/23 06:02,04/Jun/24 20:41,11/Apr/23 06:02,1.18.0,,,,,,,,,,,1.18.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"In YARN-4844, use getMemorySize instead of getMemory, because using int to represent memory may exceed the bounds in some cases and produce negative numbers.
This change was merged in HADOOP-2.8.0, we should use getMemorySize instead of getMemory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 11 06:02:12 UTC 2023,,,,,,,,,,"0|z1go8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 03:39;taoran;Hi. [~slfan1989] because the memory unit you explained is MB. so we can use int, why must long ?;;;","20/Mar/23 03:43;slfan1989;[~lemonjing] we use int32 for memory now, if a cluster has 10k nodes, each node has 210G memory, we will get a negative total cluster memory.

 ;;;","20/Mar/23 10:01;taoran;[~slfan1989] thanks for explanations. got it.;;;","11/Apr/23 06:02;Weijie Guo;master(1.18) via 0c4db2dd577fdfca1a12c115948bf67931d0bcde.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST Service missing sessionAffinity causes job run failure with HA cluster,FLINK-31509,13529028,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,eleroy,eleroy,17/Mar/23 22:35,31/Aug/23 14:17,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"When using a Session Cluster with multiple Job Managers, the -rest service load balances the API requests to all job managers, not just the master.

When submitting a FlinkSessionJob, I often see errors like: `jar <jar_id>.jar was not found`, because the submission is done in 2 steps: 
 * upload the jar with `v1/jars/upload` which returns the `jar_id`
 * run the job with `v1/jars/<jar_id>/run`

Unfortunately, with the Service load balacing between nodes, it is often the case that the jar is uploaded on a JM, and the run request happens on another, where the jar doesn't exist.

A simple fix is to append the `sessionAffinity: ClientIP` on the -rest service, where the API calls from a given originating IP will always be routed to the same node.

This issue is especially problematic with Beam, where the Beam job submission does not retry to run the job with the jar_id, and will fail, causing it to re-upload a new jar and retrying, until it is lucky enough to get the 2 calls in a row routed to the same node.

 ","Flink 1.15 on Flink Operator 1.4.0 on Kubernetes 1.25.4, (optionally with Beam 2.46.0)

but the issue was observed on Flink 1.14, 1.15 and 1.16 and on Flink Operator 1.2, 1.3, 1.3.1, 1.4.0

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 14:17:37 UTC 2023,,,,,,,,,,"0|z1go74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 12:38;bgeng777;I believe that it should be a bug to route API requests to all JM instead of the master. 
Besides, it should be ok to use K8s HA with one JM is launched as when this JM crashes, a new one will created from the HA data stored in config map. Multiple JM may reduce the recovery time but not so necessary.;;;","20/Mar/23 15:50;eleroy;[~bgeng777] 
I agree ideally the Service should only targets the master, however that is not trivial in kubernetes. The operator would need to scan for the master periodically, and update a label on the JM pods for the Service to target only the one labeled master.

The sessionAffinity solution is simple to implement (literally, it's a 1-liner in the Service definition) and works fine for the purpose.;;;","31/Aug/23 06:38;gyfora;I think this is a problem in the Flink Kubernetes integration right (not really in the operator)?;;;","31/Aug/23 13:57;eleroy;[~gyfora] It is a k8s integration issue, but it is effectively an operator issue since it is the operator that deploys these resources from the cluster definition manifest.;;;","31/Aug/23 14:17;gyfora;I will update the component [~eleroy] . Operator issues are the ones caused by the operator logic and can be fixed in the operator repo. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-304: Pluggable failure handling for Apache Flink,FLINK-31508,13529017,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pgaref,pgaref,pgaref,17/Mar/23 19:25,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,,,,,,1.20.0,,,,Runtime / Coordination,Runtime / REST,,,0,,,,,"This is an umbrella ticket for [FLIP-304|https://cwiki.apache.org/confluence/display/FLINK/FLIP-304%3A+Pluggable+Failure+Enrichers]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-17 19:25:49.0,,,,,,,,,,"0|z1go4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move execution logic of ShowOperation out from TableEnvironmentImpl,FLINK-31507,13528993,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,17/Mar/23 15:56,21/Mar/23 12:35,04/Jun/24 20:41,21/Mar/23 12:35,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,This should implement {{ExecutableOperation}} for all the {{ShowOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 12:35:00 UTC 2023,,,,,,,,,,"0|z1gnzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 09:45;aitozi;Hello, [~jark] May I help to do this refactor ?;;;","19/Mar/23 10:16;jark;[~aitozi] I just finished the migration. ;;;","21/Mar/23 12:35;jark;Fixed in master: 96b17a63c51d39a83f24945f4ae15a1280f4f5a8 and 452986dd05b193d492e0fc7b817f61b9695871d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move execution logic of AlterOperation out from TableEnvironmentImpl,FLINK-31506,13528992,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,jark,jark,17/Mar/23 15:53,19/Mar/23 23:13,04/Jun/24 20:41,19/Mar/23 23:13,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,This should implement {{ExecutableOperation}} for all the {{AlterOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 23:13:28 UTC 2023,,,,,,,,,,"0|z1gnz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 23:13;Sergey Nuyanzin;Merged to master at 1d97e443032335b0d8b93cc6850d0b4d1308a568;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move execution logic of DropOperation out from TableEnvironmentImpl,FLINK-31505,13528991,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,jark,jark,17/Mar/23 15:53,19/Mar/23 07:33,04/Jun/24 20:41,19/Mar/23 07:33,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,This should implement {{ExecutableOperation}} for all the {{DropOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 07:33:38 UTC 2023,,,,,,,,,,"0|z1gnyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 07:33;jark;Fixed in master: 06be368a3f1e821bdb1063b828009748f2f4239c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move execution logic of CreateOperation out from TableEnvironmentImpl,FLINK-31504,13528990,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,17/Mar/23 15:52,19/Mar/23 19:16,04/Jun/24 20:41,19/Mar/23 19:16,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,This should implement {{ExecutableOperation}} for all the {{CreateOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 19:16:10 UTC 2023,,,,,,,,,,"0|z1gnyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 19:16;Sergey Nuyanzin;Merged in master: 3e6a831eb30883790aaf37c2f49dfcb5e1b5e8ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""org.apache.beam.sdk.options.PipelineOptionsRegistrar: Provider org.apache.beam.sdk.options.DefaultPipelineOptionsRegistrar not a subtype"" is thrown when executing Python UDFs in SQL Client ",FLINK-31503,13528980,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,17/Mar/23 14:10,19/Mar/23 06:49,04/Jun/24 20:41,19/Mar/23 06:49,1.15.0,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,API / Python,,,,0,pull-request-available,,,,"The following exception will be thrown when executing SQL statements containing Python UDFs in SQL Client:
{code}
Caused by: java.util.ServiceConfigurationError: org.apache.beam.sdk.options.PipelineOptionsRegistrar: Provider org.apache.beam.sdk.options.DefaultPipelineOptionsRegistrar not a subtype
        at java.util.ServiceLoader.fail(ServiceLoader.java:239)
        at java.util.ServiceLoader.access$300(ServiceLoader.java:185)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:415)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet$Builder.addAll(ImmutableSet.java:507)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSortedSet$Builder.addAll(ImmutableSortedSet.java:528)
        at org.apache.beam.sdk.util.common.ReflectHelpers.loadServicesOrdered(ReflectHelpers.java:199)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.initializeRegistry(PipelineOptionsFactory.java:2089)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.<init>(PipelineOptionsFactory.java:2083)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.<init>(PipelineOptionsFactory.java:2047)
        at org.apache.beam.sdk.options.PipelineOptionsFactory.resetCache(PipelineOptionsFactory.java:581)
        at org.apache.beam.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:547)
        at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:241)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 06:49:00 UTC 2023,,,,,,,,,,"0|z1gnwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 15:37;dianfu;If your Flink version doesn't contain this fix, you could simply work around this issue by adding the following configuration:

SET 'classloader.parent-first-patterns.additional' = 'org.apache.beam.';;;;","19/Mar/23 06:49;dianfu;Fixed in:
- master via de258f3ce01e720d23bec67c20892133f89293d3
- release-1.17 via 3163b8f9caa53e9487ce062eba2c3d399dfe08a2
- release-1.16 via d438b3bdc48a0456088594700d438725d0fb1480;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the number of concurrent scale operations to reduce cluster churn,FLINK-31502,13528955,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,17/Mar/23 12:14,14/Mar/24 14:33,04/Jun/24 20:41,,,,,,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,,"Until we move to using the upcoming Rescale API which recycles pods, we need to be mindful with how many deployments we scale at the same time because each of them is going to give up all its pods and require the new number of required pods. 

This can cause churn in the cluster and temporary lead to ""unallocatable"" pods which triggers the k8s cluster autoscaler to add more cluster nodes. That is often not desirable because the actual required resources after the scaling have been settled, are lower.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 14:29:44 UTC 2023,,,,,,,,,,"0|z1gnqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 11:55;mxm;This turned out to not be the main error source.;;;","07/Dec/23 14:29;mxm;Reopening because this is an actual issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SqlCreateView conversion logic to SqlCreateViewConverter,FLINK-31501,13528952,13528594,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jark,jark,jark,17/Mar/23 11:45,12/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,,"Introduce {{SqlCreateViewConverter}} and move the conversion logic of SqlCreateView -> CreateViewOperation to it.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 10:35:07 UTC 2023,,,,,,,,,,"0|z1gnq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SqlAlterTableSchema conversion logic to AlterTableSchemaConverter,FLINK-31500,13528951,13528594,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,taoran,jark,jark,17/Mar/23 11:44,12/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,,Introduce {{AlterTableSchemaConverter}} and move the conversion logic of SqlAlterTableSchema -> AlterTableChangeOperation to it. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 10:35:07 UTC 2023,,,,,,,,,,"0|z1gnq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 04:09;taoran;[~jark] hi, jark. i'm glad to fix this. it will help to learn this new conversion way. Can u assign this ticket to me? i will try to support it.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SqlCreateTable conversion logic to SqlCreateTableConverter,FLINK-31499,13528950,13528594,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xzw0223,jark,jark,17/Mar/23 11:43,12/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,,Introduce {{SqlCreateTableConverter}} and move the conversion logic of SqlCreateTable -> CreateTableOperation to it. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 10:35:07 UTC 2023,,,,,,,,,,"0|z1gnps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 13:02;xzw0223;[~jark] Can you assign it to me.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeclartiveSlotManager always request redundant task manager when resource is not enough,FLINK-31498,13528940,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,17/Mar/23 10:09,22/May/23 05:56,04/Jun/24 20:41,22/May/23 05:56,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"Currently redundant task manager check in DeclarativeSlotManager only compare free slots with required redundant slots. 

when there are no enough resources in YARN/Kubernetes, this mechanism will always try to request new task manager. 

there are two way to address this.
1. maintain the state of redundant workers to avoid request twice
2. only try to request redundant workers when there is no pending worker

The first way will make the logic of redundant worker too complicated, I would like to choose the second way

Looking forward to any suggestion.

 !image-2023-03-17-18-05-43-088.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/23 10:05;huwh;image-2023-03-17-18-05-43-088.png;https://issues.apache.org/jira/secure/attachment/13056450/image-2023-03-17-18-05-43-088.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 05:56:21 UTC 2023,,,,,,,,,,"0|z1gnnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 03:24;Weijie Guo;Thanks [~huwh] for reporting this, you are assigned.;;;","11/May/23 07:57;huwh;[~xtsong] Do you think we need to fix the problem?

Given that DeclarativeSlotManager will be removed in a subsequent release, it's fine for me not to fix it;;;","22/May/23 05:56;Weijie Guo;master(1.18) via abe0fbf433f66bb94f5b42ab46a00348ea951ade.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop the deprecated CatalogViewImpl ,FLINK-31497,13528919,13359276,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,17/Mar/23 08:14,28/Mar/24 23:11,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"After https://issues.apache.org/jira/browse/FLINK-29585

CatalogViewImpl not used in Flink project now, we may can drop it now cc [~snuyanzin]

But, we may have to check whether it is used in other connector's system",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 24 07:34:32 UTC 2023,,,,,,,,,,"0|z1gniw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 07:34;Sergey Nuyanzin;I think same could be done for {{CatalogTableImpl}} once this issue is solved https://issues.apache.org/jira/browse/FLINK-31604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-293: Introduce Flink Jdbc Driver For Sql Gateway,FLINK-31496,13528918,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,zjureel,zjureel,zjureel,17/Mar/23 08:10,20/Jun/23 13:36,04/Jun/24 20:41,20/Jun/23 08:23,1.18.0,,,,,,,,,,,1.18.0,,,,Table SQL / Gateway,Table SQL / JDBC,,,0,,,,,Issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-293%3A+Introduce+Flink+Jdbc+Driver+For+Sql+Gateway,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 08:23:59 UTC 2023,,,,,,,,,,"0|z1gnio:",9223372036854775807,"Apache Flink now supports JDBC driver to access to sql-gateway, you can use the driver in any cases that support standard JDBC extension to connect to Flink cluster.",,,,,,,,,,,,,,,,,,,"20/Jun/23 08:23;libenchao;All the subtasks have been finished, I'm closing this issue as implemented now. [~zjureel] Thanks for your great work!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve metrics tab on flink ui,FLINK-31495,13528911,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,haoyan.zhang,haoyan.zhang,17/Mar/23 07:19,04/May/23 05:27,04/Jun/24 20:41,04/May/23 02:12,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,,"When metric is too long, the select component can not display the whole metric name

!image-2023-03-17-15-13-06-815.png|width=289,height=154!

We should use a modal or make the options horizontal scrollable",,,,,,,,,,,,,,,,,,,,,FLINK-20473,,,,,,,,,,,,,,,"17/Mar/23 07:13;haoyan.zhang;image-2023-03-17-15-13-06-815.png;https://issues.apache.org/jira/secure/attachment/13056438/image-2023-03-17-15-13-06-815.png","14/Apr/23 11:17;haoyan.zhang;image-2023-04-14-19-17-07-773.png;https://issues.apache.org/jira/secure/attachment/13057280/image-2023-04-14-19-17-07-773.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 05:27:14 UTC 2023,,,,,,,,,,"0|z1gnh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 07:22;haoyan.zhang;requesting for discussion and assign to me;;;","21/Mar/23 02:14;zlzhang0122;IMHO this is not a problem, if you mouseover the selection, you'll see the whole metric name;;;","21/Mar/23 05:25;haoyan.zhang;I see,  but it is still very hard to look through all the metrics, user need mouseover the options one by one to find the one they want;;;","30/Mar/23 08:43;Weijie Guo;Thanks [~haoyan.zhang] for reporting this, could you post a picture of the modified version for reference?;;;","13/Apr/23 08:36;Weijie Guo;Temporarily close this ticket because there is no feedback for a long time, feel free to reopen it.;;;","14/Apr/23 11:23;haoyan.zhang;sorry for my late reply

since there is no pop-ups in flink ui, so I think best solution is just widen the options, but it still need a max width to make sure it will not exceed the border of tab, which means there is still possibility that the option can not be completely displayed here

!image-2023-04-14-19-17-07-773.png!;;;","28/Apr/23 19:25;netvl;This is a big usability issue, and apparently there are multiple tickets about exactly the same thing:
https://issues.apache.org/jira/browse/FLINK-16570
https://issues.apache.org/jira/browse/FLINK-20257
https://issues.apache.org/jira/browse/FLINK-20473
All of them, unfortunately, are ""deprioritized"" :(;;;","04/May/23 02:12;haoyan.zhang;so sad,  closed since it is duplicated;;;","04/May/23 05:20;Weijie Guo;Thanks [~haoyan.zhang] and [~netvl] for the reply. I'm a bit curious if we can add a horizontal scrollbar to enhance the user experience? Alternatively, other approach such as FLINK-20473.

I'm very sorry that these tickets have been downgraded due to not being responded in a timely manner. However, this is indeed a valuable issue, let's start tracking it in FLINK-20473, as a pull request has already been created there. Welcome everyone to help review or propose better solutions there.;;;","04/May/23 05:27;Weijie Guo;> IMHO this is not a problem, if you mouseover the selection, you'll see the whole metric name

[~zlzhang0122] Is this exactly what you want to solve in FLINK-20473? Or is it that this issue has already been fixed?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SqlNodeConverter for SqlToOperationConverter,FLINK-31494,13528910,13528594,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,xzw0223,xzw0223,17/Mar/23 07:17,17/Mar/23 15:42,04/Jun/24 20:41,17/Mar/23 15:42,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Introduce {{SqlNodeConverter}} for {{SqlToOperationConverter}}, following Timo's idea in FLINK-31368
class like:

{code:java}
public interface SqlNodeConverter<S extends SqlNode> {

    Operation convertSqlNode(S node, ConvertContext context);

}


/** Registry of SqlNode converters. */
public class SqlNodeConverters {

    private static final Map<Class<?>, SqlNodeConverter<?>> CONVERTERS = new HashMap<>();

    static {
        // register all the converters here
        register(new SqlCreateCatalogConverter());
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 15:42:10 UTC 2023,,,,,,,,,,"0|z1gngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 07:26;xzw0223;[~jark]  [~luoyuxia]  Is this the expected effect? I don’t know if there is any problem with my understanding. Please point it out.;;;","17/Mar/23 07:42;jark;Hi [~xzw0223], I can help to build a basic framework for the pluggable node converters. You can migrate all the existing SqlNode conversions when the basic framework is finished. ;;;","17/Mar/23 08:00;xzw0223;Ok, thank you [~jark] .;;;","17/Mar/23 10:13;xzw0223;Hi [~jark] , Will SqlNodeConverters register all SQLnodes? My initial idea is the same, but I feel that although this reduces the amount of SqlToOperationConverter code, it will swell in SqlNodeConverters.;;;","17/Mar/23 12:14;jark;[~xzw0223], the key idea is not putting all the conversion logic in a single class, otherwise it's huge and hard to maintain. ;;;","17/Mar/23 15:42;jark;Fixed in master: ba03d4db7ca01c7a2436253b9f9429a2a3fb3219 and fa0dd3559e9697e21795a2634d8d99a0b7efdcf3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"helm upgrade does not work, because repo path does not follow helm standards",FLINK-31493,13528869,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,eleroy,eleroy,16/Mar/23 21:21,31/Aug/23 13:59,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,1,helm,,,,"the helm repo for flink-operator is a folder that includes the version, which is not following the helm chart repo standards.

In a standard helm repo, the repo URL is the name of the product (without version) and then the folder includes the different versions of the chart.

This is an issue because the repo itself needs to be installed every time the version is upgraded, as opposed to adding the repo once and then upgrading the version.

When attempting to add the latest repo, helm will complain that the repo already exists. It is necessary to first remove the repo, and then add the updated one.

When trying to upgrade the chart, it doesn't work, because helm expects the chart of the previous version to be in the same repo, but it cannot be found in the newly added repo.

So the chart needs to be uninstalled, then the new one installed.

The solution is to use a common path for all versions of the chart, and maintain a manifest with the various versions (instead of different folders with different manifests)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:09 UTC 2023,,,,,,,,,,"0|z1gn7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWS Firehose Connector misclassifies IAM permission exceptions as retryable,FLINK-31492,13528830,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samuelsiebenmann,samuelsiebenmann,samuelsiebenmann,16/Mar/23 16:40,28/Mar/23 17:11,04/Jun/24 20:41,28/Mar/23 17:11,aws-connector-4.1.0,,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / AWS,Connectors / Firehose,,,0,pull-request-available,,,,"The AWS Firehose connector uses an exception classification mechanism to decide if errors writing requests to AWS Firehose are fatal (i.e. non-retryable) or not (i.e. retryable).
{code:java}
private boolean isRetryable(Throwable err) {
    if (!FIREHOSE_FATAL_EXCEPTION_CLASSIFIER.isFatal(err, getFatalExceptionCons())) {
        return false;
    }
    if (failOnError) {
        getFatalExceptionCons()
                .accept(new KinesisFirehoseException.KinesisFirehoseFailFastException(err));
        return false;
    }

    return true;
} {code}
([github|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws-kinesis-firehose/src/main/java/org/apache/flink/connector/firehose/sink/KinesisFirehoseSinkWriter.java#L252])

This exception classification mechanism compares an exception's actual type with known, fatal exception types (by using Flink's [FatalExceptionClassifier.withExceptionClassifier|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/throwable/FatalExceptionClassifier.java#L60]).  An exception is considered fatal if it is assignable to a given known fatal exception ([code|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/ExceptionUtils.java#L479]).

The AWS Firehose SDK throws fatal IAM permission exceptions as [FirehoseException|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/firehose/model/FirehoseException.html]s, e.g.
{code:java}
software.amazon.awssdk.services.firehose.model.FirehoseException: User: arn:aws:sts::000000000000:assumed-role/example-role/kiam-kiam is not authorized to perform: firehose:PutRecordBatch on resource: arn:aws:firehose:us-east-1:000000000000:deliverystream/example-stream because no identity-based policy allows the firehose:PutRecordBatch action{code}
At the same time, certain subtypes of FirehoseException are retryable and non-fatal (e.g.[https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/firehose/model/LimitExceededException.html]).

The AWS Firehose connector currently wrongly classifies the fatal IAM permission exception as non-fatal. However, the current exception classification mechanism does not easily handle a case where a super-type should be considered fatal, but its child type shouldn't.

To address this issue, AWS services and the AWS SDK use error codes (see e.g. [Firehose's error codes|https://docs.aws.amazon.com/firehose/latest/APIReference/CommonErrors.html] or [S3's error codes|https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList], see API docs [here|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/awscore/exception/AwsErrorDetails.html#errorCode()] and [here|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/awscore/exception/AwsServiceException.html#awsErrorDetails()]) to uniquely identify error conditions and to be used to handle errors by type.

The AWS Firehose connector (and other AWS connectors) currently log to debug when retrying fully failed records ([code|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws-kinesis-firehose/src/main/java/org/apache/flink/connector/firehose/sink/KinesisFirehoseSinkWriter.java#L213]). This makes it difficult for users to root cause the above issue without enabling debug logs.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 17:11:32 UTC 2023,,,,,,,,,,"0|z1gmz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 16:48;dannycranmer;Thanks for reporting this [~samuelsiebenmann] . I have assigned the Jira to you.;;;","28/Mar/23 17:11;dannycranmer;Merged commit [{{d166ee2}}|https://github.com/apache/flink-connector-aws/commit/d166ee24bdd2b238f1d909912ec1d038732ec1c4] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink table planner NestedLoopJoinTest.testLeftOuterJoinWithFilter failed in branch release1.16,FLINK-31491,13528827,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,taoran,taoran,16/Mar/23 16:23,16/Mar/23 16:58,04/Jun/24 20:41,16/Mar/23 16:57,1.16.1,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"branch release1.16 NestedLoopJoinTest.testLeftOuterJoinWithFilter failed.

Mar 16 12:30:00 [ERROR] Failures: 
Mar 16 12:30:00 [ERROR]   NestedLoopJoinTest.testLeftOuterJoinWithFilter1:37 optimized exec plan expected:<...[InnerJoin], where=[[true], select=[a, e, f], build=[left])
Mar 16 12:30:00    :- Exchange(distribution=[broadcast])
Mar 16 12:30:00    :  +- Calc(select=[a], where=[(a = 10)])
Mar 16 12:30:00    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 16 12:30:00    +- Calc(select=[e, f], where=[(d = 10])])
Mar 16 12:30:00       +- LegacyT...> but was:<...[InnerJoin], where=[[(a = d)], select=[a, d, e, f], build=[left])
Mar 16 12:30:00    :- Exchange(distribution=[broadcast])
Mar 16 12:30:00    :  +- Calc(select=[a], where=[SEARCH(a, Sarg[10])])
Mar 16 12:30:00    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 16 12:30:00    +- Calc(select=[d, e, f], where=[SEARCH(d, Sarg[10]])])
Mar 16 12:30:00       +- LegacyT...>

at org.apache.flink.table.planner.utils.DiffRepository.assertEquals(DiffRepository.java:438)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.assertEqualsOrExpand(TableTestBase.scala:1075)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:1008)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:849)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:636)
    at org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.testLeftOuterJoinWithFilter1(NestedLoopJoinTest.scala:37)

 

azure ci: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47244&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]

And it can be reproduced in local. I think the caused commit may be: f0361c720cb18c4ae7dc669c6a5da5b09bc8f563

 ",,,,,,,,,,,,,,,,,,,,,FLINK-31477,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-16 16:23:25.0,,,,,,,,,,"0|z1gmyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten times out,FLINK-31490,13528823,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,16/Mar/23 16:02,19/Aug/23 10:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47221&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9448

{code}
Mar 16 02:00:54 ""main"" #1 prio=5 os_prio=0 tid=0x00007f488800b800 nid=0x5a15 waiting on condition [0x00007f488fe14000]
Mar 16 02:00:54    java.lang.Thread.State: WAITING (parking)
Mar 16 02:00:54 	at sun.misc.Unsafe.park(Native Method)
Mar 16 02:00:54 	- parking to wait for  <0x00000000e4065228> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
Mar 16 02:00:54 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Mar 16 02:00:54 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
Mar 16 02:00:54 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
Mar 16 02:00:54 	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.lambda$waitForNewLeader$0(TestingRetrievalBase.java:50)
Mar 16 02:00:54 	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase$$Lambda$1377/797057570.get(Unknown Source)
Mar 16 02:00:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
Mar 16 02:00:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 16 02:00:54 	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.waitForNewLeader(TestingRetrievalBase.java:48)
Mar 16 02:00:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten(ZooKeeperLeaderElectionTest.java:479)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 16:40;mapohl;FLINK-31490.mvn.log;https://issues.apache.org/jira/secure/attachment/13056424/FLINK-31490.mvn.log","16/Mar/23 16:40;mapohl;FLINK-31490.zookeeper-client.log;https://issues.apache.org/jira/secure/attachment/13056425/FLINK-31490.zookeeper-client.log","16/Mar/23 16:40;mapohl;FLINK-31490.zookeeper-server.log;https://issues.apache.org/jira/secure/attachment/13056426/FLINK-31490.zookeeper-server.log",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:10 UTC 2023,,,,,,,,,,"0|z1gmxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 16:39;mapohl;The ZK server logs reveal that there is a constant ping request received every 20s. The corresponding test itself doesn't test 1.17-specific code: 
* {{ZooKeeperLeaderElectionDriver}} is still marked as deprecated since 1.15 and is not used in production right now (no relevant [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box] changes ended up in release-1.17 in this regards).
* {{TestingLeaderElectionEventHandler}} was touched by FLINK-27848 which isn't 1.17-specific, either
* {{LeaderRetrievalDriver}} code that is also used in this test wasn't touched in 1.17 at all

Therefore, I'm downgrading the priority to critical.;;;","16/Mar/23 16:40;mapohl;I extracted the test-related logs and attached it to the issue for further investigation.;;;","17/Mar/23 17:35;chesnay;But FLINK-27848 was fixed in 1.17.0; if that change did cause this issue then it could be 1.17 specific.;;;","20/Mar/23 06:52;mapohl;Thanks for going over it, [~chesnay]. My rational was that FLINK-27848 isn't only fixed in 1.17.0. It was also fixed in 1.15.1, for instance, and then ""forward-ported"" to the release-1.17 branch as well. Therefore, the fix isn't 1.17.0-specific and doesn't justify a blocker priority here.

I'm going to look into it today.;;;","20/Mar/23 13:42;chesnay;I think we only use such an argument if the bug was present in a previous .0 version.;;;","20/Mar/23 14:37;mapohl;ok, makes sense. FLINK-27848 is kind of strange, anyway, because it was fixed in older versions first and then brought forward to newer versions as well. But I should have pointed out as well that FLINK-27848 is kind of special: It's fixing ""dead code"" that we might want to revive as part of [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box] which justifies this test instability not being considered a blocker, still.;;;","20/Mar/23 17:21;mapohl;hm, the server logs show that the session were stopped:
{code}
01:43:05,477 [      SessionTracker] INFO  org.apache.zookeeper.server.SessionTrackerImpl               [] - SessionTrackerImpl exited loop!
01:43:05,478 [      SessionTracker] INFO  org.apache.zookeeper.server.SessionTrackerImpl               [] - SessionTrackerImpl exited loop!
01:43:05,479 [      SessionTracker] INFO  org.apache.zookeeper.server.SessionTrackerImpl               [] - SessionTrackerImpl exited loop!
{code}
There is no error reported which indicates that the corresponding session tracker was shut down. The last log in the test code happened at {{01:43:05,163}} coming from the LeaderRetrievalService indicating that the leader information change was recognized.

The 3 sessions come from the LeaderElectionDriver, the custom curator client and the LeaderRetrievalDriver, I'd assume.;;;","20/Mar/23 17:26;mapohl;There is something that looks like a hickup of 5s in the Maven logs around the time the test is executed (but that could be also due to tests running in parallel without any of them finishing):
{code}
Mar 16 01:43:03 [INFO] Running org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest
Mar 16 01:43:03 [INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.491 s - in org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest
Mar 16 01:43:03 [INFO] Running org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest
Mar 16 01:43:08 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.174 s - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest
Mar 16 01:43:08 [INFO] Running org.apache.flink.runtime.leaderelection.DefaultLeaderElectionServiceTest
{code}
But this shouldn't cause any problems because it's still within the timeout range of 60s.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperationManagerTest.testCloseOperation failed because it couldn't find a submitted operation,FLINK-31489,13528822,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,mapohl,mapohl,16/Mar/23 15:56,17/Mar/23 11:36,04/Jun/24 20:41,17/Mar/23 11:36,1.17.0,,,,,,,,,,,1.17.1,1.18.0,,,Table SQL / Gateway,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47218&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=13386

{code}
Mar 16 02:49:52 [ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.433 s <<< FAILURE! - in org.apache.flink.table.gateway.service.operation.OperationManagerTest
Mar 16 02:49:52 [ERROR] org.apache.flink.table.gateway.service.operation.OperationManagerTest.testCloseOperation  Time elapsed: 0.042 s  <<< ERROR!
Mar 16 02:49:52 org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 1734d6cf-cf52-40c5-804f-809e48a9818a.
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:487)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:518)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:482)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.awaitOperationTermination(OperationManager.java:149)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManagerTest.testCloseOperation(OperationManagerTest.java:199)
Mar 16 02:49:52 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 10:49:52 UTC 2023,,,,,,,,,,"0|z1gmxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 15:58;mapohl;[~shengkai] this issue failed on {{master}}. Please deprioritize this issue to Critical if it's not a 1.17 issue or a test code instability.;;;","17/Mar/23 04:34;lincoln.86xy;[~mapohl] seems it's an instable test(confirmed with [~shengkai] offline, he's on vocation), the test code can be improved, I've submited a fix and deprioritize this issue to Critical.;;;","17/Mar/23 10:49;leonard;Fixed in :

master: ff08c7b1d7fb9334d624aa74bc5788d4d7ac1edc

release-1.17: f0a7dcc4a1c4b3c4b2e4e7029e8f60fb7e7720d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dependency convergence failed due to dependency not being available,FLINK-31488,13528816,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,16/Mar/23 15:33,21/Aug/23 08:52,04/Jun/24 20:41,21/Aug/23 08:52,1.17.0,,,,,,,,,,,,,,,Test Infrastructure,,,,0,auto-deprioritized-critical,test-stability,,,"{code}
[ERROR] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
Could not build dependency tree Could not collect dependencies: org.apache.flink:flink-end-to-end-tests-common-kafka:jar:1.17-SNAPSHOT
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47163&view=logs&j=cc2c0b9a-de49-5517-79e5-b1df1cadc1e5&t=e46d8929-052e-573c-00f4-50453f4d6369&l=23815

Looks like a temporary infrastructure issue. I leave it at critical.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 08:52:04 UTC 2023,,,,,,,,,,"0|z1gmw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 08:52;mapohl;It looks like it was an infratructure issue. I'm gonna close this one as ""Cannot Reproduce"" because it hasn't re-appeared;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add targetColumns to DynamicTableSink#Context,FLINK-31487,13528793,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Mar/23 12:35,26/Jul/23 01:23,04/Jun/24 20:41,20/Mar/23 09:55,,,,,,,,,,,,1.18.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"FLIP-300: Add targetColumns to DynamicTableSink#Context to solve the null overwrite problem of partial-insert

https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=240885081",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 01:23:48 UTC 2023,,,,,,,,,,"0|z1gmqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 09:55;lincoln.86xy;Fixed in master: 0180284381bf7999781d542219d2a097f3cbc098;;;","04/Apr/23 17:34;martijnvisser;[~lincoln.86xy] Is this breaking existing connectors? For example, Elasticsearch can't compile against 1.18-SNAPSHOT https://github.com/apache/flink-connector-elasticsearch/actions/runs/4610232254/jobs/8148440894#step:13:154

;;;","05/Apr/23 07:38;martijnvisser;Same for Opensearch https://github.com/apache/flink-connector-opensearch/actions/runs/4615625503/jobs/8159702829#step:13:142;;;","05/Apr/23 14:15;lincoln.86xy;[~martijnvisser] As expected by flip-300, the `SinkRuntimeProviderContext` provided to the connector has a new constructor that supports the `targetColumns` parameter, so there are no compatibility issues in the production code path. However, the compilation error here appears to be that the connector built the mockContext for testing purposes, which does require adaptation to the new interface, I can fix these errors, but the connector main branch I checked out only sees a dependency on flink-1.17, so how do we switch to flink-1.18?;;;","25/Jul/23 11:27;knaufk;[~lincoln.86xy] Is this feature already documented for users/developers? Should it be documented beyond JavaDocs?;;;","26/Jul/23 01:23;lincoln.86xy;[~knaufk] Yes, only the javadoc is currently available. It may be beneficial for connector developers to add appropriate notes to the current documentation. I've created a subtask and updated the doc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using KeySelector in IterationBody causes ClassCastException,FLINK-31486,13528790,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,16/Mar/23 12:10,21/Mar/23 03:06,04/Jun/24 20:41,21/Mar/23 03:06,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"We have the following code which uses CoGroup along with KeySelector in an IterationBody. When we submit to Flink Session cluster, the exception raises.
{code:java}
public static void main(String[] args) throws Exception {
    Configuration config = new Configuration();
    config.set(HeartbeatManagerOptions.HEARTBEAT_TIMEOUT, 5000000L);
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);
    env.setStateBackend(new EmbeddedRocksDBStateBackend());
    env.getConfig().enableObjectReuse();
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.setParallelism(1);
    env.getCheckpointConfig().disableCheckpointing();

    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

    int num = 400;
    int types = num / 10;

    Random rand = new Random(0);
    long[] randoms = new long[types];
    for (int i = 0; i < types; i++) {
        randoms[i] = rand.nextInt(types);
    }

    SourceFunction<Row> rowGenerator =
            new SourceFunction<Row>() {
                @Override
                public final void run(SourceContext<Row> ctx) throws Exception {
                    int cnt = 0;
                    while (cnt < num) {
                        ctx.collect(
                                Row.of(
                                        randoms[cnt % (types)],
                                        randoms[cnt % (types)],
                                        new DenseVector(10)));
                        cnt++;
                    }
                }

                @Override
                public void cancel() {}
            };

    Table trainDataTable =
            tEnv.fromDataStream(
                    env.addSource(rowGenerator, ""sourceOp-"" + 1)
                            .returns(
                                    Types.ROW(
                                            Types.LONG,
                                            Types.LONG,
                                            DenseVectorTypeInfo.INSTANCE)));

    testCoGroupWithIteration(tEnv, trainDataTable);
}

public static void testCoGroupWithIteration(StreamTableEnvironment tEnv, Table trainDataTable)
        throws Exception {
    DataStream<Row> data1 = tEnv.toDataStream(trainDataTable);
    DataStream<Row> data2 = tEnv.toDataStream(trainDataTable);
    DataStreamList coResult =
            Iterations.iterateBoundedStreamsUntilTermination(
                    DataStreamList.of(data1),
                    ReplayableDataStreamList.notReplay(data2),
                    IterationConfig.newBuilder().build(),
                    new TrainIterationBody());

    List<Integer> counts = IteratorUtils.toList(coResult.get(0).executeAndCollect());
    System.out.println(counts.size());
}

private static class TrainIterationBody implements IterationBody {

    @Override
    public IterationBodyResult process(
            DataStreamList variableStreams, DataStreamList dataStreams) {

        DataStreamList feedbackVariableStream =
                IterationBody.forEachRound(
                        dataStreams,
                        input -> {
                            DataStream<Row> dataStream1 = variableStreams.get(0);
                            DataStream<Row> dataStream2 = dataStreams.get(0);

                            DataStream<Row> coResult =
                                    dataStream1
                                            .coGroup(dataStream2)
                                            .where(
                                                    (KeySelector<Row, Long>)
                                                            t2 -> t2.getFieldAs(0))
                                            .equalTo(
                                                    (KeySelector<Row, Long>)
                                                            t2 -> t2.getFieldAs(1))
                                            .window(EndOfStreamWindows.get())
                                            .apply(
                                                    new RichCoGroupFunction<Row, Row, Row>() {
                                                        @Override
                                                        public void coGroup(
                                                                Iterable<Row> iterable,
                                                                Iterable<Row> iterable1,
                                                                Collector<Row> collector) {
                                                            for (Row row : iterable1) {
                                                                collector.collect(row);
                                                            }
                                                        }
                                                    });
                            return DataStreamList.of(coResult);
                        });

        DataStream<Integer> terminationCriteria =
                feedbackVariableStream
                        .get(0)
                        .flatMap(new TerminateOnMaxIter(2))
                        .returns(Types.INT);

        return new IterationBodyResult(
                feedbackVariableStream, feedbackVariableStream, terminationCriteria);
    }
} {code}
The exception is as below. Note that the exception can not be reproduced in the unittest with MiniCluster since all classes are in the Java classpath.
{code:java}
Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Could not instantiate state partitioner. at org.apache.flink.streaming.api.graph.StreamConfig.getStatePartitioner(StreamConfig.java:662) at org.apache.flink.iteration.operator.OperatorUtils.createWrappedOperatorConfig(OperatorUtils.java:96) at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:168) at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:146) at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:68) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) at java.lang.Thread.run(Thread.java:748) 

Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionKeySelector.keySelector1 of type org.apache.flink.api.java.functions.KeySelector in instance of org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionKeySelector at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2302) at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1432) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2409) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2403) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:501) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:459) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543) at org.apache.flink.streaming.api.graph.StreamConfig.getStatePartitioner(StreamConfig.java:659) ... 17 more  {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 03:06:12 UTC 2023,,,,,,,,,,"0|z1gmq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 01:40;zhangzp;Is this similar to this one [1]?

 

 [1] https://issues.apache.org/jira/browse/FLINK-31255;;;","17/Mar/23 04:57;Jiang Xin;[~zhangzp] I'm afraid not. This issue is most likely causing by incorrect class loader, but FLINK-31255 seems not.;;;","21/Mar/23 03:06;lindong;Merged to apache/flink-ml master branch fa5f47ea2a09360143aab5f39b85b373675636ad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connecting to Kafka and Avro Schema Registry fails with ClassNotFoundException,FLINK-31485,13528760,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,16/Mar/23 09:04,17/Mar/23 07:56,04/Jun/24 20:41,17/Mar/23 07:56,1.17.0,,,,,,,,,,,1.17.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"When running the SQL Client and using flink-sql-connector-kafka, flink-sql-avro and flink-sql-avro-confluent-registry and trying to query Schema Registry, the job will fail with

{code:bash}
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: com.google.common.base.Ticker
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 07:56:24 UTC 2023,,,,,,,,,,"0|z1gmjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 07:56;martijnvisser;Fixed in

master: a7605b0f5b0a7b47c0d698912e105ffafd76e9aa

release-1.17: 64f17ee01ae2d526176f4adba8a5041adec50322;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-web compile failed due to local node tar corrupted ,FLINK-31484,13528748,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,16/Mar/23 07:50,16/Mar/23 15:26,04/Jun/24 20:41,16/Mar/23 15:26,1.17.0,,,,,,,,,,,,,,,Build System / CI,Runtime / Web Frontend,,,0,,,,,"{noformat}
[ERROR] The archive file /__w/2/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-linux-x64.tar.gz is corrupted and will be deleted. Please try the build again. [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.11.0:install-node-and-npm (install node and npm) on project flink-runtime-web: Could not extract the Node archive: Could not extract archive: '/__w/2/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-linux-x64.tar.gz': EOFException -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.{noformat}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47149&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5",,,,,,,,,,,,,,,,,,,,,,FLINK-30719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-16 07:50:48.0,,,,,,,,,,"0|z1gmgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Split Deletion Support in Flink Kafka Connector,FLINK-31483,13528740,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ruibin,ruibin,16/Mar/23 06:21,14/Nov/23 03:30,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / Kafka,Connectors / Parent,,,0,,,,,"Currently, the Flink Kafka Connector does not support split deletion and is left as a [TODO|[https://github.com/apache/flink-connector-kafka/blob/9f72be91f8abdfc9b5e8fa46d15dee3f83e71332/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/enumerator/KafkaSourceEnumerator.java#L305]]. I want to add this feature by doing these steps:

1. Add SplitsDeletion event to flink-connector-base, which currently only has SplitsAddition.
2. Add a `deleteSplits` method in SplitEnumeratorContext, so it can send a SplitsDeletion event to the source operator. To maintain compatibility, a default empty implementation for this method will be added.
3. Make SourceOperator handle the SplitsDeletion event, notifiying the SourceReader to delete splits.
4. Create a deleteSplits method in SourceReader to remove splits, including remove them from Split state and stopping SourceReader from reading the deleted splits.

As an alternative, without modifying the flink-connector-base, KafkaSplitsEnumerator could send a custom SourceEvent to SourceOperator for splits deletion and deal with it in the kafka-connector-specific code. But I think it's better to have SplitsDeletion in flink-connector-base, so other connectors can use it too.

Let me know if you have any thoughts or ideas. Thanks!

Related Issues: FLINK-30490",,,,,,,,,,,,,,,,,,,,,,FLINK-30490,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 14 03:30:08 UTC 2023,,,,,,,,,,"0|z1gmf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 07:04;ruanhang1993;Hi, Ruibin,

Actually a deletion event(SplitsRemoval) is planned to be added for the FLIP-208(https://cwiki.apache.org/confluence/display/FLINK/FLIP-208%3A+Add+RecordEvaluator+to+dynamically+stop+source+based+on+de-serialized+records). Here is the PR([https://github.com/apache/flink/pull/21589).]

But this PR don't aim at adding the split deletion for the enumerator. The deletion event only is passed in the reader itself.

Maybe some part in this issue will be contained after this FLIP finished.

Best, Hang;;;","16/Mar/23 07:38;ruibin;[~ruanhang1993] Thanks for the clarification. I can work on the enumerator part if you don't have a plan for it at the moment. Do you have any idea when this PR will be merged? Also, I left a comment on the PR. It would be really helpful if you could provide some more details about the existing problems.
 
 ;;;","21/Mar/23 07:04;ruanhang1993;Hi, [~ruibin] , 

I found the mails about this part. Hope it is helpful.

 

 [1] [https://lists.apache.org/thread/7r4h7v5k281w9cnbfw9lb8tp56r30lwt]

 [2] [https://lists.apache.org/thread/3wxfr39t2rz1wvxw2vsz5hsrp9t8qrwx];;;","22/Mar/23 02:36;ruibin;[~ruanhang1993] Thanks! I will look into it.;;;","14/Nov/23 03:30;hilmialf;Hi [~ruibin] [~ruanhang1993] Any update on this topic? We are really looking forward to it..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support count jobmanager-failed failover times,FLINK-31482,13528737,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Fei Feng,Fei Feng,16/Mar/23 05:37,30/Mar/23 08:50,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Runtime / Coordination,Runtime / Metrics,,,0,,,,,"we have a  metric `numRestarts` which indicate how many times a job failover ， but we don't have a metric indicate the job recover from ha ( high availability).

there are two problems:

1. when a  jobmanager process crashed , we have no way of knowing that jobmanager is crash and job was recovered from metric system 

2. when a new jobmanager become leader, the  `numRestarts`  will started from zero, 
Sometimes misleading our users。most user think that whether failover because of a JM failure or because of a job failure, these failover is same , the effect, at least, is the same.
 
I suggest we can 
1. add new metric that indicate how many time the job was recovered from ha
2. metric `numRestarts` also count the times recover from ha  
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 08:50:04 UTC 2023,,,,,,,,,,"0|z1gmeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 08:19;martijnvisser;Wouldn't you normally detect these type of things in your metric system like Prometheus or Grafana? ;;;","20/Mar/23 03:49;Fei Feng;[~martijnvisser]  Of course we will detect job's running metric. I mean we can not detect the job's failover times by ha now.

if job's `uptime` and `numRestarts` metric go down to zero and start counting again，we may think this job's jobmanager was failover by ha. Sometimes this change can also be caused by the user restarting the job

so I think we need need a more direct and accurate indicator to respond the job's jobmanager was failover by ha.;;;","30/Mar/23 08:50;martijnvisser;[~Fei Feng] I would think that you detect that a job is restarting by looking at your metrics and then you do an investigation for the cause; that would probably also require you to look into logging etc. If it's a customers restart, you would have a different lifecycle state then it would be for a failing job imho, which you can already determine from the metrics.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support enhanced show databases syntax,FLINK-31481,13528733,13526484,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jeyhunkarimov,taoran,taoran,16/Mar/23 04:28,15/Jan/24 11:53,04/Jun/24 20:41,15/Jan/24 11:52,,,,,,,,,,,,1.19.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"As FLIP discussed. To avoid bloat, this ticket supports ShowDatabases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 11:52:51 UTC 2024,,,,,,,,,,"0|z1gmdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/23 16:56;jeyhunkarimov;Hi [~taoran] could you please assign this task to me or give me an access to self-assign the task? Thanks!;;;","15/Jan/24 11:52;dwysakowicz;Implemented in 6c050e92040802f6866eb54120f5070c34af7a4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Typo in YarnClusterDescriptor,FLINK-31480,13528728,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,16/Mar/23 04:14,16/Mar/23 11:11,04/Jun/24 20:41,16/Mar/23 09:51,1.18.0,,,,,,,,,,,1.18.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"There is a typo in the comment for YarnClusterDescriptor, this jira will fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 09:51:43 UTC 2023,,,,,,,,,,"0|z1gmcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 09:51;Weijie Guo;master(1.18) via db3a0a59519b279d016489e1bf29927a4f62a6ed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close blocking iterators in tests,FLINK-31479,13528720,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dianer17,dianer17,16/Mar/23 03:10,23/Mar/23 05:32,04/Jun/24 20:41,19/Mar/23 05:31,,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,Several blocking iterators are not closed in `ContinuousFileStoreITCase`,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:31:12 UTC 2023,,,,,,,,,,"0|z1gmao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:31;lzljs3620320;Use github issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TypeError: a bytes-like object is required, not 'JavaList' is thrown when ds.execute_and_collect() is called on a KeyedStream",FLINK-31478,13528716,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,16/Mar/23 02:44,17/Mar/23 15:16,04/Jun/24 20:41,17/Mar/23 15:16,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,API / Python,,,,0,pull-request-available,,,,"{code}
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
import argparse
import logging
import sys

from pyflink.common import WatermarkStrategy, Encoder, Types
from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode
from pyflink.datastream.connectors.file_system import (FileSource, StreamFormat, FileSink,
                                                       OutputFileConfig, RollingPolicy)


word_count_data = [""To be, or not to be,--that is the question:--"",
                   ""Whether 'tis nobler in the mind to suffer"",
                   ""The slings and arrows of outrageous fortune"",
                   ""Or to take arms against a sea of troubles,"",
                   ""And by opposing end them?--To die,--to sleep,--"",
                   ""No more; and by a sleep to say we end"",
                   ""The heartache, and the thousand natural shocks"",
                   ""That flesh is heir to,--'tis a consummation"",
                   ""Devoutly to be wish'd. To die,--to sleep;--"",
                   ""To sleep! perchance to dream:--ay, there's the rub;"",
                   ""For in that sleep of death what dreams may come,"",
                   ""When we have shuffled off this mortal coil,"",
                   ""Must give us pause: there's the respect"",
                   ""That makes calamity of so long life;"",
                   ""For who would bear the whips and scorns of time,"",
                   ""The oppressor's wrong, the proud man's contumely,"",
                   ""The pangs of despis'd love, the law's delay,"",
                   ""The insolence of office, and the spurns"",
                   ""That patient merit of the unworthy takes,"",
                   ""When he himself might his quietus make"",
                   ""With a bare bodkin? who would these fardels bear,"",
                   ""To grunt and sweat under a weary life,"",
                   ""But that the dread of something after death,--"",
                   ""The undiscover'd country, from whose bourn"",
                   ""No traveller returns,--puzzles the will,"",
                   ""And makes us rather bear those ills we have"",
                   ""Than fly to others that we know not of?"",
                   ""Thus conscience does make cowards of us all;"",
                   ""And thus the native hue of resolution"",
                   ""Is sicklied o'er with the pale cast of thought;"",
                   ""And enterprises of great pith and moment,"",
                   ""With this regard, their currents turn awry,"",
                   ""And lose the name of action.--Soft you now!"",
                   ""The fair Ophelia!--Nymph, in thy orisons"",
                   ""Be all my sins remember'd.""]


def word_count(input_path, output_path):
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_runtime_mode(RuntimeExecutionMode.BATCH)
    # write all the data to one file
    env.set_parallelism(1)

    # define the source
    if input_path is not None:
        ds = env.from_source(
            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),
                                                       input_path)
                             .process_static_file_set().build(),
            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),
            source_name=""file_source""
        )
    else:
        print(""Executing word_count example with default input data set."")
        print(""Use --input to specify file input."")
        ds = env.from_collection(word_count_data)

    def split(line):
        yield from line.split()

    # compute word count
    ds = ds.flat_map(split) \
           .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \
           .key_by(lambda i: i[0])
           # .reduce(lambda i, j: (i[0], i[1] + j[1]))

    # define the sink
    if output_path is not None:
        ds.sink_to(
            sink=FileSink.for_row_format(
                base_path=output_path,
                encoder=Encoder.simple_string_encoder())
            .with_output_file_config(
                OutputFileConfig.builder()
                .with_part_prefix(""prefix"")
                .with_part_suffix("".ext"")
                .build())
            .with_rolling_policy(RollingPolicy.default_rolling_policy())
            .build()
        )
    else:
        print(""Printing result to stdout. Use --output to specify output path."")
        a = list(ds.execute_and_collect())


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',
        dest='input',
        required=False,
        help='Input file to process.')
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)

    word_count(known_args.input, known_args.output)
{code}

For the above job, the following exception will be thrown:
{code}
Traceback (most recent call last):
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/udf/test_udf_perf.py"", line 131, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/udf/test_udf_perf.py"", line 110, in word_count
    a = list(ds.execute_and_collect())
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 2920, in __next__
    return self.next()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 2931, in next
    return convert_to_python_obj(self._j_closeable_iterator.next(), self._type_info)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/utils.py"", line 72, in convert_to_python_obj
    fields.append(pickled_bytes_to_python_converter(data, field_type))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/utils.py"", line 91, in pickled_bytes_to_python_converter
    data = pickle.loads(data)
TypeError: a bytes-like object is required, not 'JavaList'
{code}

See more details on https://apache-flink.slack.com/archives/C03G7LJTS2G/p1678894062180649",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 15:16:48 UTC 2023,,,,,,,,,,"0|z1gm9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 15:16;dianfu;Fixed in:
- master via 5e059efee864e17939a33f29272a848d00598531
- release-1.17 via ec5a09b3ce56426d1bdc8eeac4bf52cac9be015b
- release-1.16 via cadf4b35fb6f20c8cba310fa54626d0b9bae1361;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NestedLoopJoinTest.testLeftOuterJoinWithFilter failed on azure ,FLINK-31477,13528710,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,leonard,leonard,16/Mar/23 01:40,20/Mar/23 07:23,04/Jun/24 20:41,17/Mar/23 02:14,1.16.2,,,,,,,,,,,1.16.2,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"{noformat}
 Failures: 
Mar 15 15:52:32 [ERROR]   NestedLoopJoinTest.testLeftOuterJoinWithFilter1:37 optimized exec plan expected:<...[InnerJoin], where=[[true], select=[a, e, f], build=[left])
Mar 15 15:52:32    :- Exchange(distribution=[broadcast])
Mar 15 15:52:32    :  +- Calc(select=[a], where=[(a = 10)])
Mar 15 15:52:32    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 15 15:52:32    +- Calc(select=[e, f], where=[(d = 10])])
Mar 15 15:52:32       +- LegacyT...> but was:<...[InnerJoin], where=[[(a = d)], select=[a, d, e, f], build=[left])
Mar 15 15:52:32    :- Exchange(distribution=[broadcast])
Mar 15 15:52:32    :  +- Calc(select=[a], where=[SEARCH(a, Sarg[10])])
Mar 15 15:52:32    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 15 15:52:32    +- Calc(select=[d, e, f], where=[SEARCH(d, Sarg[10]])])
Mar 15 15:52:32       +- LegacyT...>{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47202&view=logs&j=086353db-23b2-5446-2315-18e660618ef2&t=6cd785f3-2a2e-58a8-8e69-b4a03be28843",,,,,,,,,,,,,,,,,,,,,,FLINK-31491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 07:23:45 UTC 2023,,,,,,,,,,"0|z1gm8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 15:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47191&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","16/Mar/23 15:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47194&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13073;;;","16/Mar/23 15:52;mapohl;Same build, multiple failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13432
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13071
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=13166
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=13901
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=13432;;;","17/Mar/23 02:14;leonard;Fixed in release-1.16 : 1bd25a48fa444390971149515810d057324b642b;;;","20/Mar/23 07:23;mapohl;The following build didn't contain the aforementioned fix, yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47263&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler should take lower bound parallelism settings into account,FLINK-31476,13528664,13527017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dmvk,dmvk,15/Mar/23 16:30,19/Jan/24 09:42,04/Jun/24 20:41,05/Jul/23 15:58,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32623,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 15:58:28 UTC 2023,,,,,,,,,,"0|z1gly8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 15:42;gyfora;[~chesnay] please let me know if you have a POC/PR to review :) ;;;","05/Jul/23 15:58;chesnay;master: 38f4d133a784a58318f6b6cb9617646ab7a70fa1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow project to be user-defined in release scripts,FLINK-31475,13528653,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Mar/23 15:05,15/Mar/23 15:55,04/Jun/24 20:41,15/Mar/23 15:55,,,,,,,,,,,,,,,,Connectors / Common,Release System,,,0,pull-request-available,,,,"The connector release scripts derive the project name from the repository.
For some esoteric cases (like the flink-connector-shared-utils repo) it would be beneficial to be able to override this on the command-line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 15:55:37 UTC 2023,,,,,,,,,,"0|z1glvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 15:55;chesnay;release_utils: 9995aec5d314351c190fbed9e9a1b439fdba217f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add failure information for out-of-order checkpoints,FLINK-31474,13528650,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Ming Li,Ming Li,15/Mar/23 14:44,16/Aug/23 11:42,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,1,,,,,"At present, when the checkpoint is out of order, only out-of-order logs will be printed on the {{Task}} side, while on the {{JM}} side, the checkpoint can only fail through timeout, and the real reason cannot be confirmed.

Therefore, I think we should add failure information on the JM side for the out-of-order checkpoint.
{code:java}
if (lastCheckpointId >= metadata.getCheckpointId()) {
    LOG.info(
            ""Out of order checkpoint barrier (aborted previously?): {} >= {}"",
            lastCheckpointId,
            metadata.getCheckpointId());
    channelStateWriter.abort(metadata.getCheckpointId(), new CancellationException(), true);
    checkAndClearAbortedStatus(metadata.getCheckpointId());
    return;
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 11:42:06 UTC 2023,,,,,,,,,,"0|z1glv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 11:42;masteryhx;Hi, [~Ming Li] 
I think it makes sense. 
Would you like to provide a pr to fix this ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new show operations docs,FLINK-31473,13528642,13526484,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,taoran,taoran,15/Mar/23 14:07,16/Mar/23 08:34,04/Jun/24 20:41,16/Mar/23 04:26,,,,,,,,,,,,,,,,Documentation,,,,0,,,,,Add enhanced show sql syntax docs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 08:33:33 UTC 2023,,,,,,,,,,"0|z1gltc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 08:33;taoran;docs will be added in each single ticket. so close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncSinkWriterThrottlingTest failed with Illegal mailbox thread,FLINK-31472,13528633,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chalixar,taoran,taoran,15/Mar/23 12:45,16/Apr/24 08:06,04/Jun/24 20:41,16/Apr/24 08:06,1.16.1,1.17.0,1.18.0,1.19.0,1.20.0,,,,,,,1.18.2,1.19.1,1.20.0,,Connectors / Common,,,,0,pull-request-available,test-stability,,,"when run mvn clean test, this case failed occasionally.
{noformat}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.955 s <<< FAILURE! - in org.apache.flink.connector.base.sink.writer.AsyncSinkWriterThrottlingTest
[ERROR] org.apache.flink.connector.base.sink.writer.AsyncSinkWriterThrottlingTest.testSinkThroughputShouldThrottleToHalfBatchSize  Time elapsed: 0.492 s  <<< ERROR!
java.lang.IllegalStateException: Illegal thread detected. This method must be called from inside the mailbox thread!
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkIsMailboxThread(TaskMailboxImpl.java:262)
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:137)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.yield(MailboxExecutorImpl.java:84)
        at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.flush(AsyncSinkWriter.java:367)
        at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.lambda$registerCallback$3(AsyncSinkWriter.java:315)
        at org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService$CallbackTask.onProcessingTime(TestProcessingTimeService.java:199)
        at org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.setCurrentTime(TestProcessingTimeService.java:76)
        at org.apache.flink.connector.base.sink.writer.AsyncSinkWriterThrottlingTest.testSinkThroughputShouldThrottleToHalfBatchSize(AsyncSinkWriterThrottlingTest.java:64)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
        at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
        at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
        at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
        at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
        at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
        at java.util.Iterator.forEachRemaining(Iterator.java:116)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
        at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 08:06:15 UTC 2024,,,,,,,,,,"0|z1glrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 13:54;martijnvisser;[~lemonjing] My initial thinking would be that this is a local issue, given that the CI also runs these tests yet never has reported a failure on this one. ;;;","15/Mar/23 15:14;chalixar;I agree with [~martijnvisser] suggestion!

I haven't encountered a similar issue. I will double check though and update the ticket if I find an issue.;;;","16/Mar/23 02:08;taoran;Yes. I've found this failure to occur occasionally, not consistently. Might be a problem with local computer..;;;","27/Nov/23 09:00;mapohl;I'm reopening this issue because it now also appeared in CI (master, i.e. 1.19):
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54895&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10265;;;","16/Dec/23 13:40;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55581&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10381;;;","16/Dec/23 13:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55582&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7247;;;","16/Dec/23 13:47;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55583&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10634;;;","18/Dec/23 09:53;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55601&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10380;;;","18/Dec/23 09:55;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55602&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10634;;;","18/Dec/23 10:15;chalixar;hi [~Sergey Nuyanzin] could you assign me the ticket, I will take a look ASAP.;;;","18/Dec/23 10:31;Sergey Nuyanzin;[~chalixar], thanks for volunteering, done ;;;","18/Dec/23 15:51;chalixar;[~Sergey Nuyanzin] I have pushed a PR with the fix, would be great if you reviewed it.
I am going to rebase the fix on 1.17, 1.18 once this is merged.;;;","20/Dec/23 11:19;Sergey Nuyanzin;Thanks
Merged as [6f6ab1ea3527d96d57739eb4202bccd3c73c6458|https://github.com/apache/flink/commit/6f6ab1ea3527d96d57739eb4202bccd3c73c6458];;;","07/Jan/24 23:15;Sergey Nuyanzin;Have to reopen since it is reproduced after the fix
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55769&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10380;;;","07/Jan/24 23:15;Sergey Nuyanzin;[~chalixar] could you please have a look?;;;","07/Jan/24 23:18;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55770&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10575;;;","07/Jan/24 23:19;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55809&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10636;;;","07/Jan/24 23:24;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55827&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10398;;;","07/Jan/24 23:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55835&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10398;;;","07/Jan/24 23:33;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55882&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10398;;;","07/Jan/24 23:38;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55883&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7247;;;","07/Jan/24 23:51;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55930&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10572;;;","08/Jan/24 00:05;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55949&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7183;;;","08/Jan/24 00:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55951&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10377;;;","08/Jan/24 00:15;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55956&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10568;;;","08/Jan/24 00:20;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55964&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10377;;;","08/Jan/24 00:21;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55965&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7182;;;","08/Jan/24 00:26;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55982&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10572;;;","08/Jan/24 00:56;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56014&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10570;;;","08/Jan/24 01:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10392;;;","08/Jan/24 10:58;chalixar;That's weird, [~Sergey Nuyanzin] I will have a look today;;;","09/Jan/24 06:48;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56124&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10413;;;","09/Jan/24 06:48;Sergey Nuyanzin;Thanks for looking into thi [~chalixar]l;;;","09/Jan/24 06:49;Sergey Nuyanzin;Converting to critical since it started appearing very often;;;","12/Jan/24 10:50;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56286&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10574;;;","16/Jan/24 14:11;mapohl;master (1.19): [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56340&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10581];;;","16/Jan/24 14:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56359&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10582;;;","16/Jan/24 14:30;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56417&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10576;;;","16/Jan/24 14:32;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10574;;;","16/Jan/24 14:49;mapohl;any updates on that one [~chalixar] ?;;;","16/Jan/24 15:04;chalixar;[~mapohl] unfortunately not so far,  I have taken a quick look  but will try to spend more time on it this week. 
I will update the ticket by the end of the week If I hadn't reached a fix so we can disable the test temporarily given the current frequency. 
;;;","16/Jan/24 15:06;mapohl;Thank you for the quick update. :) Much appreciated.;;;","22/Jan/24 15:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56671&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10575;;;","22/Jan/24 15:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56659&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10576;;;","22/Jan/24 15:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56603&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10575;;;","23/Jan/24 11:23;chalixar;[~mapohl]

Could we merge this [PR|https://github.com/apache/flink/pull/24175] while the investigation is going? I don't want to block the pipeline further.;;;","24/Jan/24 07:33;mapohl;Sure, if we are certain that this is a test issue and not an issue that was introduced with 1.19?!;;;","25/Jan/24 10:51;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56861&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10575;;;","26/Jan/24 15:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56942&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10596;;;","26/Jan/24 15:41;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56944&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10575;;;","29/Jan/24 14:31;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57008&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10576;;;","29/Jan/24 14:33;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57023&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10575;;;","30/Jan/24 07:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57079&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10596;;;","30/Jan/24 09:09;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57080&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd&l=10576;;;","30/Jan/24 09:17;chalixar;> Sure, if we are certain that this is a test issue and not an issue that was introduced with 1.19?!

the stacktrace shows that the issue is that the timer is triggered by the test itself, so It is unlikely it is an issue from the sinkwriter. 
I will make sure to double check the impact as well. ;;;","20/Feb/24 07:43;mapohl;https://github.com/apache/flink/actions/runs/7967481900/job/21750506043#step:10:10473;;;","21/Feb/24 08:18;mapohl;1.20: https://github.com/apache/flink/actions/runs/7982933635/job/21797491998#step:10:10476;;;","26/Feb/24 07:19;mapohl;https://github.com/apache/flink/actions/runs/8017168989/job/21901631147#step:10:10472;;;","26/Feb/24 07:35;mapohl;https://github.com/apache/flink/actions/runs/8034840376/job/21946982660#step:10:10472;;;","26/Feb/24 09:29;mapohl;[~chalixar] have you had the chance to look into it? I'm hesitant to disable the test but rather would have it fixed.;;;","27/Feb/24 07:39;mapohl;https://github.com/apache/flink/actions/runs/8058552366/job/22011816468#step:10:10569;;;","28/Feb/24 07:07;mapohl;https://github.com/apache/flink/actions/runs/8074215128/job/22059455554#step:10:10706;;;","29/Feb/24 07:16;mapohl;* https://github.com/apache/flink/actions/runs/8079627963/job/22074788571#step:10:10480
* https://github.com/apache/flink/actions/runs/8089966279/job/22106940120#step:10:10483;;;","01/Mar/24 10:23;mapohl;https://github.com/apache/flink/actions/runs/8105495552/job/22154143977#step:10:10481;;;","04/Mar/24 07:47;mapohl;https://github.com/apache/flink/actions/runs/8127069864/job/22212010030#step:10:10567;;;","05/Mar/24 07:46;mapohl;* https://github.com/apache/flink/actions/runs/8149964640/job/22275696652#step:10:10617
* https://github.com/apache/flink/actions/runs/8149964640/job/22275708777#step:10:10591;;;","08/Mar/24 10:47;rskraba;* [https://github.com/apache/flink/actions/runs/8170012874/job/22335616234#step:10:10480]
 * [https://github.com/apache/flink/actions/runs/8172972703/job/22344777586#step:10:10475];;;","11/Mar/24 08:24;chalixar;[~mapohl] Sorry I was out of office, Since I am back now I am happy to take a look and not disable the test. I will post updates on the ticket.;;;","11/Mar/24 13:17;lincoln.86xy;[~chalixar] I'm going to change this issue to version 1.20.0 since it is not a blocker for 1.19.0, If the release 1.19 branch will be fixed later, please add the 1.19.1 version when it's done.;;;","11/Mar/24 13:25;chalixar;[~lincoln.86xy] yes, I will publish fixes for 1.19 and 1.18 after we merge it for master (1.20);;;","11/Mar/24 14:44;mapohl;https://github.com/apache/flink/actions/runs/8211401080/job/22460442826#step:10:10480;;;","12/Mar/24 08:37;chalixar;[~lincoln.86xy] [~mapohl] 
Hello, could you please review the [PR|https://github.com/apache/flink/pull/24481].
Let me add some context

h2.Why is the test failing?

So the flakiness arises from setting processing time within the test to trigger the timer flush of the writer, This caused the concurrent thread access of the mailbox which caused the failure, The problem was within the test not the AsyncWriter. 

h2.Why is it intermittent?

This is because we are also writing batches of records so there was a race condition between both batch size trigger and timer trigger, in other words we used to add a new batch and a set the time to trigger the flush, had the batch trigger flushed the buffer the timer callback would be discarded safely.
h2.Why do I believe this refactor should fix the test?
Because I have removed the time setting from the test it self, The size of batches sent should be enough to trigger the flush which is needed for the test.

h2.What could go wrong?

There should be no newly introduced issues here since the batch size is unchanged we expect enough flushes triggered by batch size only to stabilize the rate limiting value as expected.

h2.How did I verify the fix?

I have run a sampler till failure for a some time and haven't reported any. I am aware local setup is different than CI but the test should be less sensitive to delays now so I expect we are green to go.;;;","21/Mar/24 10:50;mapohl;* https://github.com/apache/flink/actions/runs/8297049625/job/22707710859#step:10:10483
* https://github.com/apache/flink/actions/runs/8304571223/job/22730583765#step:10:10570
* https://github.com/apache/flink/actions/runs/8312246681/job/22747082696#step:10:10710
* https://github.com/apache/flink/actions/runs/8320242265/job/22764905648#step:10:10478;;;","27/Mar/24 10:05;rskraba;1.19 https://github.com/apache/flink/actions/runs/8445595235/job/23133325729#step:10:10473;;;","29/Mar/24 08:42;rskraba;1.20, jdk8: [https://github.com/apache/flink/actions/runs/8476237146/job/23225687148#step:10:10480];;;","08/Apr/24 14:52;rskraba;jdk8 https://github.com/apache/flink/actions/runs/8593664911/job/23545710832#step:10:10821;;;","15/Apr/24 12:31;dannycranmer;Merged commit [{{f886687}}|https://github.com/apache/flink/commit/f886687453065046665fa59e9444340cb5af92a3] into apache:master ;;;","15/Apr/24 12:34;dannycranmer;[~chalixar] please open PR for 1.19/1.18 branch too;;;","15/Apr/24 12:56;chalixar;Thanks [~dannycranmer] 
I created followups for 1.18, 1.19
[https://github.com/apache/flink/pull/24668]

[https://github.com/apache/flink/pull/24669]

 ;;;","15/Apr/24 13:43;rskraba;1.20 Hadoop 3.1.3: Test (module: connect) https://github.com/apache/flink/actions/runs/8670257008/job/23778244834#step:10:10910 (before the fix)
1.18 Java 17: Test (module: connect) https://github.com/apache/flink/actions/runs/8682562103/job/23807599803#step:10:10638 *(thanks for the backport!)*
;;;","16/Apr/24 07:58;chalixar;Is there a reason why the issue is not Resolved?;;;","16/Apr/24 08:06;dannycranmer;I was waiting for the backports to be merged. Now merged:
 * Merged commit [{{70822da}}|https://github.com/apache/flink/commit/70822da6f3b964ba4fdb8e63a47f97c7c65e1b7f] into apache:release-1.18
 * Merged commit [{{34c6777}}|https://github.com/apache/flink/commit/34c67778ba4e8b295d71563777ddc9f4ae414f9b] into apache:release-1.19 ;;;"
Allow changing per-vertex desired parallelism through WEB UI,FLINK-31471,13528625,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,15/Mar/23 12:04,14/Jun/23 08:46,04/Jun/24 20:41,14/Apr/23 09:29,,,,,,,,,,,,1.18.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-31469,,,,,,,,,,,,,,,,FLINK-31823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 09:14:39 UTC 2023,,,,,,,,,,"0|z1glpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 09:14;dmvk;master: 40d2cfbc11f4c3598dcba5cbc7367237a9ddbf2f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce integration tests for Externalized Declarative Resource Management,FLINK-31470,13528624,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,15/Mar/23 12:04,14/Apr/23 09:43,04/Jun/24 20:41,05/Apr/23 14:38,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31803,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 05 14:38:56 UTC 2023,,,,,,,,,,"0|z1glpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/23 14:38;dmvk;master: d4e3b6646f389eeb395a4dbb951d13bab02cb8db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow setting JobResourceRequirements through REST API,FLINK-31469,13528622,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,15/Mar/23 12:02,26/Sep/23 12:01,04/Jun/24 20:41,04/Apr/23 08:41,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,Runtime / REST,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-31468,FLINK-31471,,,,,,,,,,,FLINK-31935,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 08:41:30 UTC 2023,,,,,,,,,,"0|z1glow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 08:41;dmvk;master: a539eb2e84cccb21f046902026d4178d864fba7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow setting JobResourceRequirements through DispatcherGateway,FLINK-31468,13528621,13527017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dmvk,dmvk,15/Mar/23 12:02,31/Mar/23 12:54,04/Jun/24 20:41,31/Mar/23 12:54,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-31469,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 12:54:08 UTC 2023,,,,,,,,,,"0|z1gloo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 09:20;huwh;Hi, [~dmvk] I would like take this ticket if possible;;;","21/Mar/23 11:38;chesnay;[~huwh] We have already written the code for this ticket. If you want I can ping you for a review though.;;;","21/Mar/23 11:40;huwh;[~chesnay] Thanks, I will follow it.;;;","31/Mar/23 12:54;chesnay;master: 3072e176ad4a894ef56c51003dd9baef976e600a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support time travel for Spark 3.3,FLINK-31467,13528619,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yzl,yzl,15/Mar/23 11:56,19/Mar/23 05:31,04/Jun/24 20:41,19/Mar/23 05:31,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,Support Spark 3.3 `VERSION AS OF` and `TIMESTAMP AS OF` Syntax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:31:38 UTC 2023,,,,,,,,,,"0|z1glo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:31;lzljs3620320;Use github issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Backport ""FilterJoinRule misses opportunity to push filter to semijoin input"" to FlinkFilterJoinRule",FLINK-31466,13528615,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,15/Mar/23 11:16,22/Mar/23 23:20,04/Jun/24 20:41,22/Mar/23 23:20,,,,,,,,,,,,1.18.0,,,,,,,,0,pull-request-available,,,,"In https://issues.apache.org/jira/browse/CALCITE-4499 there has been done an optimization.
Since Flink has it's own copy of slightly changed {{FilterJoiRule}} this optimization does not come with 1.28 update.

The idea is to apply this change to {{FlinkFilterJoinRule}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 23:20:25 UTC 2023,,,,,,,,,,"0|z1glnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 23:20;Sergey Nuyanzin;Merged to master at [e91eb5ec2fea9a2f28f3f55a06bc140e4ce4b5f5|https://github.com/apache/flink/commit/e91eb5ec2fea9a2f28f3f55a06bc140e4ce4b5f5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink] Fix shortcode errors in docs,FLINK-31465,13528603,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,15/Mar/23 10:21,17/Mar/23 02:08,04/Jun/24 20:41,17/Mar/23 02:08,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"When running docs with hugo, I get the following exception:
{code:java}
hugo v0.111.3+extended darwin/amd64 BuildDate=unknown
Error: Error building site: ""/xxx/flink-table-store/docs/content/docs/how-to/writing-tables.md:303:1"": failed to extract shortcode: shortcode ""tabs"" must be closed or self-closed{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 02:08:40 UTC 2023,,,,,,,,,,"0|z1glko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 02:08;lzljs3620320;master: dd05a70d0b66fd3bbf1afe0cd1e8362405f024c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SqlNode conversion logic out from SqlToOperationConverter,FLINK-31464,13528594,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xzw0223,luoyuxia,luoyuxia,15/Mar/23 09:24,22/Mar/23 15:04,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Similar to FLINK-31368, the  `SqlToOperationConverter` is a bit bloated. We can refactor it to avoid the code length for this class grow quickly.

We can follow the idea proposed in FLINK-31368.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 15:04:12 UTC 2023,,,,,,,,,,"0|z1glio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 04:19;jark;I would suggest using Timo's approach proposed in FLINK-31368, because most SqlNodes are not maintained in the Flink repo, and we can't make them extend some converter interface. ;;;","16/Mar/23 07:19;xzw0223;Is anyone doing it, if no one is doing it, I would like to complete this task.;;;","16/Mar/23 07:35;luoyuxia;Thanks [~jark]  for your sugegestion.;;;","16/Mar/23 07:35;luoyuxia;[~xzw0223] Cool! Thanks for volunteering. Assigned to you~;;;","17/Mar/23 06:42;jark;Hi [~xzw0223], this might also be a major work that can be divided into sub-tasks like FLINK-31368.;;;","17/Mar/23 06:48;xzw0223;[~jark]  ok, then i'll go back and review it;;;","20/Mar/23 08:44;twalthr;[~jark] thanks for kicking this off. It will help cleaning the messy code base. Why does the SqlNode conversion logic actually need a context? I was always wondering why it needs access to e.g. CatalogManager. Ideally, those converters should simply convert Calcite parser classes to Flink POJO classes. There should be no resolution logic involved.;;;","20/Mar/23 09:23;taoran;[~twalthr] i think currently we may need CatalogManager in context to convert unresolved identifiers to resolved identifiers and pass to flink POJO classes, another usage is to get ResovledTable for converting some SqlAlterTables (may need table informations). ;;;","20/Mar/23 09:33;twalthr;However, the question arises why we need resolved identifiers at this location or why alter table needs resolved tables at this location. In the end, the POJO structure should represent an AST without any bigger validation. Validation can be done by the operation execution logic.;;;","20/Mar/23 11:16;jark;I totally agree with you [~twalthr]. The Operation should represent an equivalent of SqlNode. But this is a huge technical debt. I think it's better to refactor it after we finish this cleanup.;;;","20/Mar/23 13:00;taoran;got it. thanks for explanations.;;;","22/Mar/23 09:01;twalthr;bq. But this is a huge technical debt. I think it's better to refactor it after we finish this cleanup.

Absolutely, but maybe we can already write some JavaDoc about the interfaces so that the technical debt will not become larger. And methods in Context of SqlNodeConverter are deprecated or contain a warning that e.g. catalog manager should not be accessed.

;;;","22/Mar/23 15:04;jark;[~twalthr] Sure, I created an issue for the JavaDoc. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When I use apache flink1.12.2 version, the following akka error often occurs.",FLINK-31463,13528586,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,liu zhuang,liu zhuang,15/Mar/23 08:52,28/Nov/23 06:40,04/Jun/24 20:41,15/Mar/23 09:01,1.15.4,,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,,"When I use apache flink1.12.2 version, the following akka error often occurs.

java.util.concurrent.TimeoutException: Remote system has been silent for too
long. (more than 48.0 hours)
at
akka.remote.ReliableDeliverySupervisor$$anonfun$idle$1.applyOrElse(Endpoint.scala:375)
at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
at akka.remote.ReliableDeliverySupervisor.aroundReceive(Endpoint.scala:203)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
at akka.actor.ActorCell.invoke(ActorCell.scala:495)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
at akka.dispatch.Mailbox.run(Mailbox.scala:224)
at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


------------------------------------------------------------------------------------------------
I checked that 48 hours ago, there was indeed a process hang inside flink, and the flink job was restarted.How to deal with this? Is this a bug in akka or flink? Thank you !



 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 06:40:26 UTC 2023,,,,,,,,,,"0|z1glgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 09:01;martijnvisser;[~liu zhuang] This question is better suited for Slack or the Flink User mailing list, see https://flink.apache.org/community/ for when to use Jira vs mailing list;;;","28/Nov/23 06:40;littleeleventhwolf;[~martijnvisser] How to solve this question?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink] Supports full calculation from specified snapshots in streaming mode,FLINK-31462,13528583,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Ming Li,Ming Li,15/Mar/23 08:32,29/Mar/23 07:07,04/Jun/24 20:41,19/Mar/23 05:32,,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"Currently, the table store provides a startup mode for incremental consumption from a specified snapshot in streaming mode. We can provide a startup mode for incremental consumption after full calculation from a specified snapshot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:32:06 UTC 2023,,,,,,,,,,"0|z1glg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 11:20;lzljs3620320;Thanks [~Ming Li] for your reporting, can you explain what case for?;;;","15/Mar/23 12:37;Ming Li;Hi, [~lzljs3620320] ,

For example, I consume two tables A and B. The data in table A is wrong after a certain snapshot, so I need to re-import this part of the data. The data in table B is correct, so there is no need to re-import.

When I want to backtrack, I hope that table A and table B can go back to the same time period, instead of only backtracking table A, which will cause the job to not see the data changes of table B in the corresponding time period, only to see to the latest data.

One situation is that if the watermark is also derived from the data, there will be no data in table B to participate in the calculation within the corresponding time period. There may be problems in queries based on these snapshots, because the data in table B does not actually participate in the calculation.;;;","17/Mar/23 08:15;lzljs3620320;Watermark will always move forward. What I don't understand is whether these changes are necessary.;;;","19/Mar/23 05:32;lzljs3620320;Use github pr/issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports schema historical version expiring,FLINK-31461,13528544,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,15/Mar/23 02:39,20/Mar/23 00:55,04/Jun/24 20:41,19/Mar/23 05:32,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,"Schema evolution will generate multiple versions of schema. When the specified version of the schema is no longer referenced by snapshot, it should be deleted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:32:27 UTC 2023,,,,,,,,,,"0|z1gl7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 03:48;lzljs3620320;I think this is good!;;;","19/Mar/23 05:32;lzljs3620320;Use github pr/issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix hive catalog and connector jar name in the create release script for table store,FLINK-31460,13528542,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,15/Mar/23 01:58,15/Mar/23 11:59,04/Jun/24 20:41,15/Mar/23 11:59,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 11:59:59 UTC 2023,,,,,,,,,,"0|z1gl74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 11:59;lzljs3620320;master: d51ca84c8d711f74bbdd38e4acab627de78c2681;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add UPDATE COLUMN POSITION for flink table store,FLINK-31459,13528540,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,15/Mar/23 01:44,17/Mar/23 02:27,04/Jun/24 20:41,17/Mar/23 02:27,table-store-0.3.1,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 02:27:15 UTC 2023,,,,,,,,,,"0|z1gl6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 02:27;lzljs3620320;master: 14d87e4a85e716771c59a5e2fbb840fd1852a0f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support waiting for required resources in DefaultScheduler during job restart,FLINK-31458,13528491,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,a.pilipenko,a.pilipenko,14/Mar/23 19:04,14/Mar/23 19:13,04/Jun/24 20:41,14/Mar/23 19:13,1.15.3,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"Currently Flink support [waiting for required resources to become available|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-adaptive-scheduler-resource-stabilization-timeout] during job restart only while using adaptive scheduler.
On the other hand, if cluster is using default scheduler and there is not enough slots available - restart attempts will fail with `NoResourceAvailableException` until resource requirements are satisfied.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-14 19:04:07.0,,,,,,,,,,"0|z1gkvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support waiting for required resources in DefaultScheduler during job restart,FLINK-31457,13528490,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,a.pilipenko,a.pilipenko,14/Mar/23 19:03,16/Mar/23 03:44,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"Currently Flink support [waiting for required resources to become available|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-adaptive-scheduler-resource-stabilization-timeout] during job restart only while using adaptive scheduler.
On the other hand, if cluster is using default scheduler and there is not enough slots available - restart attempts will fail with `NoResourceAvailableException` until resource requirements are satisfied.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 03:44:47 UTC 2023,,,,,,,,,,"0|z1gkvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 04:38;JunRuiLi;[~a.pilipenko] I'm not sure what is the scenario where `NoResourceAvailableException` will be reported after job restart? Can you describe it in detail?

IIUC, if it is a session cluster, the slot may be occupied by other jobs after slot idle timeout. Maybe you can increase the slot.idle.timeout.

In addition, the adaptive scheduler has a mechanism to wait for resources because it can dynamically adjust the parallelism, and run jobs with a small parallelism when resources are insufficient, while the default scheduler does not have such a capability, so when resources are insufficient, it will report `NoResourceAvailableException`. If you want to run jobs even when resources are insufficient, you can use the adaptive scheduler in stream job.;;;","15/Mar/23 13:31;a.pilipenko;[~JunRuiLi], thank you for your response.

Issue is triggered by loss of TaskManager during job execution. Before new instance will become available - every attempt to restart the job will result in NoResourceAvailableException

Setup is: standalone cluster running single job with number of slots matching job requirements.

Example scenario, leading to this issue:

When job restarts there is a chance that subtask won't be able to cancel gracefully within task.cancellation.timeout (e.g. due to issues like FLINK-30304). This results in TaskManager being shutdown.

Before new instance of TaskManager will become available, every attempt to schedule job will immediately fail with NoResourceAvailableException. If configured restart delay is less than task cancellation timeout - first restart attempt will be performed immediately after cancellation is finished, i.e. right after TaskManager has been stopped.

 

Standalone resource manager support waiting for required resources, but only during startup. [[1]|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#resourcemanager-standalone-start-up-time] [[2]|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/StandaloneResourceManager.java#L110-L121];;;","16/Mar/23 03:44;huwh;[~a.pilipenko] IIUC, this issue is caused by ""slot.request.timeout"" not taking effect in standalone cluster. 
The relevant context is that standalone clusters cannot request task managers dynamically, so in most case, the wait of slot.request.timeout is meaningless.

For example: we have a standalone cluster with 10 slots on 2 task manager. When we submit a job with parallelism is 15, this job will never started in this cluster. In this scenario, fast failover with NoResourceAvailableException is a better way to inform user.

In your case, when some task managers crashed, the Flink cluster has no way of knowing if other task managers will be started. So the job will immediately fail with NoResourceAvailableException. 
You can reserve more task managers to prevent accidental crash, or just increase the restart number or restart delay to  give the new task manager a chance to register before job failed. 



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove useless panics from Golang SDK,FLINK-31456,13528469,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,avelex,avelex,14/Mar/23 16:14,14/Mar/23 16:18,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Stateful Functions,,,,0,,,,,"h3. *The main goal of PR is to remove unnecessary panic where it should not be.*

For example: 
{{msg, err := message.ToMessage()}}
{{if err != nil {}}
{{    panic(err)}}
{{}}}

So If such code is placed in a function which does not mean panic is present, unpleasant things may happen in the future. This is why panics have been removed from the _statefunContext_ and _storage_ module.

Also, the prefix Must has been added to functions that must be completed and cannot be avoided in the future

 

pull request: [https://github.com/apache/flink-statefun/pull/324]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-14 16:14:52.0,,,,,,,,,,"0|z1gkqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a simple test project for running CI workflow in shared repo,FLINK-31455,13528457,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Mar/23 14:50,15/Mar/23 12:36,04/Jun/24 20:41,15/Mar/23 12:36,,,,,,,,,,,,,,,,Build System / CI,Connectors / Common,,,0,pull-request-available,,,,"connector-shared-utils has no CI for the CI workflow, which has repeatedly shown to be a problem.
Setup some simple workflows that at least run CI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 12:36:00 UTC 2023,,,,,,,,,,"0|z1gko8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 12:36;chesnay;ci_utils: d672ed1aded3dc26785b40e0baf6de94e78d1894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shared CI workflow always caches snapshot binaries,FLINK-31454,13528455,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Mar/23 14:48,15/Mar/23 12:34,04/Jun/24 20:41,15/Mar/23 12:34,,,,,,,,,,,,,,,,Build System / CI,Connectors / Common,,,0,pull-request-available,,,,The if conditions need to work on strings because they use environment variables.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 12:34:43 UTC 2023,,,,,,,,,,"0|z1gkns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 12:34;chesnay;ci_utils: de0b6a36e625e53dbae66a0364b8d4a51db96f42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the test utils to call methods explicitly,FLINK-31453,13528430,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,14/Mar/23 12:32,15/Mar/23 02:19,04/Jun/24 20:41,15/Mar/23 02:19,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Refactor the `TestUtils.saveAndReload` method to call the static `load` method of specific Stage explicitly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 02:19:35 UTC 2023,,,,,,,,,,"0|z1gki8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 02:19;lindong;Merged to apache/flink-ml master branch c33bdc1e27b4d8185eb45d4aba52c5bccf024e1a.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show databases should better return sorted results,FLINK-31452,13528428,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taoran,taoran,taoran,14/Mar/23 12:28,23/Mar/23 08:33,04/Jun/24 20:41,23/Mar/23 08:33,,,,,,,,,,,,1.18.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"flink show databases not support sorted function. I think we can add support for these operations. So when returned results are large, user can locate result easily. we can see mature engines such as mysql did support it.

Flink SQL> show databases;
+------------------+
|   database name|

+------------------+
|default_database|
|            test|
|             sys|
|           ca_db|

+------------------+

e.g. mysql

+--------------------+
|Database          |

+--------------------+
|information_schema|
|mysql              |
|performance_schema|
|sys                |
|test              |
|test_db            |

e.g.  pg:

    Name    |  Owner   | Encoding |   Collate   
------------+----------+----------+-------------
 pgbench    | postgres | UTF8     | en_US.UTF-8 
 postgres   | postgres | UTF8     | en_US.UTF-8 
 slonmaster | postgres | UTF8     | en_US.UTF-8 
 slonslave  | postgres | UTF8     | en_US.UTF-8 
 template0  | postgres | UTF8     | en_US.UTF-8 
 template1  | postgres | UTF8     | en_US.UTF-8 
 test       | postgres | UTF8     | en_US.UTF-8 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 08:33:16 UTC 2023,,,,,,,,,,"0|z1gkhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 13:49;taoran;[~jark] many mature engines such as mysql/pg support sorted databases. what do you think? ;;;","23/Mar/23 08:33;jark;Fixed in master: 027db9002407c4cfe06cab7f18b9cdc0c6932a5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table Store Ecosystem: Introduce Presto Reader for table store,FLINK-31451,13528393,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,s7monk,s7monk,s7monk,14/Mar/23 09:56,20/Mar/23 10:04,04/Jun/24 20:41,19/Mar/23 05:32,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,, Introduce Presto Reader for table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:32:39 UTC 2023,,,,,,,,,,"0|z1gka0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:32;lzljs3620320;Use github pr/issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ExecutableOperation for operations to execute,FLINK-31450,13528392,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,14/Mar/23 09:52,17/Mar/23 07:06,04/Jun/24 20:41,17/Mar/23 07:05,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Colocating the execution logic within the Operation, just like how RunnableCommand and V2CommandExec do in Spark. We can introduce a class like:

{code:java}
public interface ExecutableOperation {

    TableResultInternal execute(Context ctx);

    interface Context {
        CatalogManager getCatalogManager();

        FunctionCatalog getFunctionCatalog();

        ResourceManager getResourceManager();

        Configuration getConfiguration();
    }
}
{code}

Many base interfaces can extend it (AlterOperation, CreateOperation, DropOperation, etc.). This approach improves code readability (not spread code across different classes) and make supporting a new statement by just adding an Operation class instead of 3 classes (Operation class, Executor class, and the mapping class).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 07:05:29 UTC 2023,,,,,,,,,,"0|z1gk9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 07:05;jark;Fixed in master: d95f69bb459b5efb44c7cd468485d85cd08aef50...768c09c5183c47692bd4af6968fd0381b7a0a006;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove DeclarativeSlotManager related logic,FLINK-31449,13528363,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huweihua,huwh,huwh,14/Mar/23 08:03,18/Jan/24 04:35,04/Jun/24 20:41,18/Jan/24 04:35,,,,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"The DeclarativeSlotManager and related configs will be completely removed in the next release after the default SlotManager change to FineGrainedSlotManager.

 

We should do the job in 1.19 version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 04:35:02 UTC 2024,,,,,,,,,,"0|z1gk3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 06:26;lincoln.86xy;[~huwh] Any updates to this since it marked as a blocker? If we want to address it in 1.19, it would be good to start a flip discussion.;;;","16/Nov/23 05:28;huweihua;[~lincoln.86xy] Thanks for the reminding. 

The remove of DeclarativeSlotManager was already discussed in  [FLIP-298 . |https://cwiki.apache.org/confluence/display/FLINK/FLIP-298%3A+Unifying+the+Implementation+of+SlotManager]So, I think we can do this thing in 1.19 without start a new flip discussion.

 ;;;","16/Nov/23 15:38;lincoln.86xy;[~huweihua]thank you for your reply! Can we just change the priority from blocker to major due to the 'Tickets Priorities' of flink?( https://cwiki.apache.org/confluence/display/FLINK/Flink%20Jira%20Process#FlinkJiraProcess-TicketsPriorities );;;","18/Jan/24 04:35;huweihua;resolved in master: 551b776a82193eeb6bb4a9b8a6925a386ea502e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FineGrainedSlotManager as the default SlotManager,FLINK-31448,13528362,13528350,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,14/Mar/23 08:01,08/May/23 09:24,04/Jun/24 20:41,08/May/23 09:24,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 08 09:24:13 UTC 2023,,,,,,,,,,"0|z1gk34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 09:24;xtsong;master (1.18): 3df65910025ccba93d75b3a885ef5d0b67becd17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aligning unit tests of FineGrainedSlotManager with DeclarativeSlotManager,FLINK-31447,13528361,13528350,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,14/Mar/23 08:01,07/Apr/23 03:58,04/Jun/24 20:41,07/Apr/23 03:57,1.18.0,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,There's the DeclarativeSlotManagerTest that covers some specific issues that should be ported to the fine grained slot manager.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 07 03:57:01 UTC 2023,,,,,,,,,,"0|z1gk2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 07:57;huwh;The list of unit tests for DeclarativeSlotManager and it's mirror in FineGrainedSlotManager

https://docs.google.com/spreadsheets/d/1km5gwECvdC0pu6HwvjBfBdHB4RIXYn5ls7AggHkH3Hk/edit?usp=sharing;;;","07/Apr/23 03:57;Weijie Guo;master(1.18) via 69131d29497fadf5ca55532ff0f7ed55c5e72c73.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkITCase$IntegrationTests.testMetrics failed because topic XXX already exists,FLINK-31446,13528360,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,martijnvisser,martijnvisser,14/Mar/23 08:00,18/Sep/23 06:58,04/Jun/24 20:41,18/Sep/23 06:58,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,,"{code:java}

Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Mar 14 02:07:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Mar 14 02:07:46 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 14 02:07:46 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 14 02:07:46 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 14 02:07:46 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 14 02:07:46 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 14 02:07:46 Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TopicExistsException: Topic 'kafka-single-topic-1095096269466403022' already exists.
Mar 14 02:07:46 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Mar 14 02:07:46 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Mar 14 02:07:46 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Mar 14 02:07:46 	at org.apache.flink.connector.kafka.sink.testutils.KafkaSinkExternalContext.createTopic(KafkaSinkExternalContext.java:101)
Mar 14 02:07:46 	... 110 more
Mar 14 02:07:46 Caused by: org.apache.kafka.common.errors.TopicExistsException: Topic 'kafka-single-topic-1095096269466403022' already exists.
{code}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47127&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36477",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 08:30:00 UTC 2023,,,,,,,,,,"0|z1gk2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 08:30;mapohl;Here is one where it fails due to a timeout:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47750&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=36635
{code}
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
	... 4 more
Caused by: org.apache.kafka.common.errors.TimeoutException: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000ms while awaiting InitProducerId
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000ms while awaiting InitProducerId
{code}
I'm adding it here because we're seeing the createTopic and timeout issues constantly in different tests. We have to stabilize the Kafka tests in this regards, I guess. I moved this issue under the umbrella ticket FLINK-31145;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split resource allocate/release related logic from FineGrainedSlotManager to TaskManagerTracker,FLINK-31445,13528359,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,huwh,huwh,huwh,14/Mar/23 07:57,12/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,stale-assigned,,,"Currently the FineGrainedSlotManager is response to slots allocations and resources request/release. This makes the logical of FineGrainedSlotManager complicated, So we will move task manager related work from FineGrainedSlotManager to TaskManagerTracker, which already tracks task managers but not including request/release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 10:35:08 UTC 2023,,,,,,,,,,"0|z1gk2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager reclaims slots when job is finished,FLINK-31444,13528358,13528350,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,14/Mar/23 07:56,24/Mar/23 08:30,04/Jun/24 20:41,24/Mar/23 08:30,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"implementation of [FLINK-21751|https://issues.apache.org/jira/browse/FLINK-21751] in FineGrainedSlotManager",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 24 08:30:12 UTC 2023,,,,,,,,,,"0|z1gk28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 08:30;xtsong;master (1.18): 15fe1653acec45d7c7bac17071e9773a4aa690a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager maintain some redundant task managers,FLINK-31443,13528357,13528350,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,14/Mar/23 07:54,06/Mar/24 15:35,04/Jun/24 20:41,05/May/23 08:48,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"implementation of [FLINK-18625|https://issues.apache.org/jira/browse/FLINK-18625] in FineGrainedSlotManager.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 08:48:46 UTC 2023,,,,,,,,,,"0|z1gk20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 08:48;xtsong;master (1.18): 00b1d4cf88022d06ec99fe3f229e1a0100fe14ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala suffix checker fails for release-1.15,FLINK-31442,13528356,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,14/Mar/23 07:53,14/Mar/23 10:20,04/Jun/24 20:41,14/Mar/23 10:20,1.15.5,,,,,,,,,,,,,,,,,,,0,,,,,"{code:bash}
08:53:07,511 ERROR org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker     [] - Violations found:
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-formats/flink-sequence-file/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hadoop-compatibility/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-1.4/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-2.2/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hive/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-table/flink-sql-client/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-tests/pom.xml'.
	Scala-free module 'flink-hcatalog' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-1.2.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-2.2.0' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-2.3.6' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.3.6/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-3.1.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-3.1.2/pom.xml'.
==============================================================================

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47102&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=25873",,,,,,,,,,,,,,,,,,,,,,,FLINK-31227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 07:56:07 UTC 2023,,,,,,,,,,"0|z1gk1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 07:56;martijnvisser;Reverted ""[FLINK-31227][docs] Remove Scala suffix for ORC and Parquet format download links on the the FileSystem documentation. This closes #22039""

78bae43288ad64511c298a067d66fa37667771d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager support select slot evenly,FLINK-31441,13528354,13528350,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,14/Mar/23 07:51,24/Mar/23 08:25,04/Jun/24 20:41,24/Mar/23 08:25,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"with [FLINK-12122|https://issues.apache.org/jira/browse/FLINK-12122], we support spread out tasks evenly to available task managers. 

FineGrainedSlotManager should support this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 24 08:25:43 UTC 2023,,,,,,,,,,"0|z1gk1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 08:25;xtsong;master (1.18): 3d5a60402aea7e13a6434777af13d1423fcf4b9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split hive catalog to each module of each version,FLINK-31440,13528351,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,zhuangchong,zhuangchong,14/Mar/23 07:15,15/Mar/23 01:24,04/Jun/24 20:41,15/Mar/23 01:24,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 01:21:57 UTC 2023,,,,,,,,,,"0|z1gk0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 07:18;zhuangchong;Consistent with the hive connector module, each module for each version.


I can do this.;;;","15/Mar/23 01:21;zhuangchong;In [#569|https://github.com/apache/flink-table-store/pull/569] , we found hive catalog with hive-2.3.9 can be used for every hive metastore versions, so we just keep one version for hive catalog.

 
I will close it.
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-298: Unifying the Implementation of SlotManager,FLINK-31439,13528350,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,14/Mar/23 07:12,15/May/23 11:04,04/Jun/24 20:41,15/May/23 11:04,1.18.0,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,,,,,"This is an umbrella ticket for [FLIP-298|https://cwiki.apache.org/confluence/display/FLINK/FLIP-298%3A+Unifying+the+Implementation+of+SlotManager].


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-14 07:12:05.0,,,,,,,,,,"0|z1gk0g:",9223372036854775807,Fine-grained resource management are now enabled by default. You can use it by specify the resource requirement. More details can be found at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/finegrained_resource/#usage.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improved exception assertion for ContinuousFileStoreITCase,FLINK-31438,13528344,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,14/Mar/23 06:25,15/Mar/23 01:12,04/Jun/24 20:41,15/Mar/23 01:12,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 01:12:18 UTC 2023,,,,,,,,,,"0|z1gjz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 01:12;lzljs3620320;master: 02807897b252085adb9ce2cc6d937dca628d64a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong key 'lookup.cache.caching-missing-key' in connector documentation,FLINK-31437,13528341,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,gaara,gaara,14/Mar/23 05:46,15/Mar/23 09:44,04/Jun/24 20:41,15/Mar/23 09:41,,,,,,,,,,,,1.16.2,1.17.1,,,Connectors / HBase,Connectors / JDBC,Documentation,,0,pull-request-available,,,,"'lookup.cache.caching-missing-key' change should be configured as 'lookup.partial-cache.caching-missing-key'.
An error occurred when I configured a dimension table.
The configuration given by the official website is not available.
!image-2023-03-14-05-45-06-230.png!
!image-2023-03-14-05-45-44-616.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 05:45;gaara;image-2023-03-14-05-45-06-230.png;https://issues.apache.org/jira/secure/attachment/13056295/image-2023-03-14-05-45-06-230.png","14/Mar/23 05:45;gaara;image-2023-03-14-05-45-44-616.png;https://issues.apache.org/jira/secure/attachment/13056294/image-2023-03-14-05-45-44-616.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 09:41:35 UTC 2023,,,,,,,,,,"0|z1gjyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 05:49;JunRuiLi;[~gaara] Thanks for creating this issue, and *[xuzhiwen1255|https://github.com/xuzhiwen1255]* has proposed a hot-fix pr to fix this bug: https://github.com/apache/flink/pull/22167;;;","14/Mar/23 05:53;gaara;ok;;;","14/Mar/23 05:59;gaara;flink-connector-jdbc has the same issue.;;;","14/Mar/23 06:03;JunRuiLi;[~gaara] -flink-connector-jdbc connector has been removed in release 1.17:- -https://issues.apache.org/jira/browse/FLINK-30465-

Sorry, I made a mistake, flink-connector-jdbc was moved to a separate repository, so you can fix it in flink version 1.16.2, and also fix in https://github.com/apache/flink-connector-jdbc. cc [~renqs] ;;;","14/Mar/23 07:50;martijnvisser;I've lowered the priority to Major, per https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process;;;","14/Mar/23 08:08;gaara;I found that this issue exists in versions 1.16 and later.;;;","14/Mar/23 08:31;JunRuiLi;[~gaara] Yes, you're right. I made a mistake and I've updated the comments. You can fix flink-connector-jdbc docs in flink version 1.16.2, and also fix in [https://github.com/apache/flink-connector-jdbc]. :D;;;","14/Mar/23 08:40;gaara;I just want to ask a question, I don't know how to solve it.:(

I see that the PR changes above are modifying the documentation. I don't know whether this situation requires changing the documentation or the code, but I believe it should involve modifying the code. I hope you can take a look and determine the best course of action.;;;","14/Mar/23 08:50;JunRuiLi;[~zhuzh] Could I take this tickets? Thx!;;;","15/Mar/23 09:41;leonard;Fixed by: 
 * Flink main:  0848815d41ac5bbafa230e378af049a20306ae76
 * Flink-connector-jdbc main: d15c59bb54a61e2dc12856d72eabbacc9c639488
 * Flink release-1.17: 43fafd43dff30a33f15d478c8f20ef4d1a6bcff1
 * Flink release-1.16: 5c723dd7a3d9f2be450bbc2f284acdeb990b9b8c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove schemaId from constructor of FileStoreCommitImpl and ManifestFile in Table Store,FLINK-31436,13528340,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 05:43,14/Mar/23 09:16,04/Jun/24 20:41,14/Mar/23 09:16,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"As schema may change during a CTAS streaming job, the schema ID of snapshots and manifest files may also change. We should remove \{{schemaId}} from their constructor and calculate the real \{{schemaId}} on the fly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 09:16:16 UTC 2023,,,,,,,,,,"0|z1gjy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 09:16;TsReaper;master: a651431f6c9e3e742e2b505920c43e2c18d76502;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce event parser for MySql Debezium JSON format in Table Store,FLINK-31435,13528333,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 03:04,03/Apr/23 08:06,04/Jun/24 20:41,29/Mar/23 03:13,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,MySQL is widely used among Flink CDC connector users. We should first support consuming changes from MySQL.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-14 03:04:33.0,,,,,,,,,,"0|z1gjwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce CDC sink for Table Store,FLINK-31434,13528332,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 03:03,22/Mar/23 07:49,04/Jun/24 20:41,22/Mar/23 07:49,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"To directly consume changes from Flink CDC connectors, we need a special CDC sink for Flink Table Store.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 07:49:33 UTC 2023,,,,,,,,,,"0|z1gjwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 07:49;TsReaper;master: ecce2cc876c1bd9095abef4fdf24400bf179fefa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make SchemaChange serializable,FLINK-31433,13528331,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 03:01,22/Mar/23 07:49,04/Jun/24 20:41,22/Mar/23 07:49,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"To avoid concurrent changes to table schema, CDC sinks for Flink Table Store should send all \{{SchemaChange}} to a special process function. This process function only has 1 parallelism and it is dedicated for schema changes.

 

To pass \{{SchemaChange}} through network, \{{SchemaChange}} must be serializable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 07:49:47 UTC 2023,,,,,,,,,,"0|z1gjw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 07:49;TsReaper;master: 3179e184da24a09fa67e3bab6bbbcd9a338634e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a special StoreWriteOperator to deal with schema changes,FLINK-31432,13528328,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 02:56,21/Mar/23 02:50,04/Jun/24 20:41,21/Mar/23 02:50,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Currently \{{StoreWriteOperator}} is not able to deal with schema changes. We need to introduce a special \{{StoreWriteOperator}} to deal with schema changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 02:50:45 UTC 2023,,,,,,,,,,"0|z1gjvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 02:50;TsReaper;master: 556443f098252a98610a03450ed8ad2a661f27dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support copying a FileStoreTable with latest schema,FLINK-31431,13528327,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 02:54,15/Mar/23 02:18,04/Jun/24 20:41,15/Mar/23 02:18,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"To capture schema changes, CDC sinks of Flink Table Store should be able to use the latest schema at any time. This requires us to copy a \{{FileStoreTable}} with latest schema so that we can create \{{TableWrite}} with latest schema.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 02:18:04 UTC 2023,,,,,,,,,,"0|z1gjvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 02:18;TsReaper;master: c155b9d7c0a6ef40327116cac88659c6be975919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support migrating states between different instances of TableWriteImpl and AbstractFileStoreWrite,FLINK-31430,13528326,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 02:51,17/Mar/23 03:31,04/Jun/24 20:41,17/Mar/23 03:31,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently {{Table}} and {{TableWrite}} in Flink Table Store have a fixed schema. However to consume schema changes, Flink Table Store CDC sinks should have the ability to change its schema during a streaming job.

This require us to pause and store the states of a {{TableWrite}}, then create a {{TableWrite}} with newer schema and recover the states in the new {{TableWrite}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 03:31:55 UTC 2023,,,,,,,,,,"0|z1gjv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 03:31;lzljs3620320;master: 1af16a5a1cb27b6b72bbcef9e5862fda84d8c996;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support CTAS(create table as) streaming job with schema changes in table store,FLINK-31429,13528325,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Mar/23 02:47,29/Mar/23 03:13,04/Jun/24 20:41,29/Mar/23 03:13,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Currently CDC connectors for Flink has the ability to stream out records changes and schema changes of a database table. Flink Table Store should have the ability to directly consume these changes, including schema changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:32:58 UTC 2023,,,,,,,,,,"0|z1gjuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:32;lzljs3620320;Please use github pr/issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add user callbacks to  PulsarSource and PulsarSink,FLINK-31428,13528303,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dialalpha,dialalpha,13/Mar/23 21:14,14/Mar/23 22:23,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,"We'd like to add support for user callbacks in {{PulsarSource}} and {{{}PulsarSink{}}}. This enables specific use cases such as event tracing which requires access to low level message properties such as message IDs after an event is produced, topic partitions, etc...

The functionality required is similar to {{ConsumerInterceptor}} and {{ProducerInterceptor}} in the Pulsar Client. However, there is a case to be made for adding new APIs that would help avoid the extra cost of ser/deser when getting the message body through the {{Message}} interface in the interceptors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 22:23:30 UTC 2023,,,,,,,,,,"0|z1gjq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 01:55;syhily;I see, the ser/deser issue matters. You want to intercept only on the real instance instead of the Pulsar `Message` instance, right?;;;","14/Mar/23 13:46;dialalpha;We want to have access to both the value and the raw `Message`. Access to the `Message` gives us things like topic partition, message publish time, etc... Also any potential trace spans in the `Message` properties bag from upstream.;;;","14/Mar/23 17:33;syhily;I don't think it's ok to expose the deserialized/serialized values. You can get and modify it only in PulsarXXXXXSchema. The callbacks can use the {{ConsumerInterceptor}} and {{ProducerInterceptor}}, add a trace id in interceptor and access it in Pulsar schema for serialized values. The context of the trace can be stored in {{ThreadLocal}}, this is common used in the trace related monitor systems.

We can add this support and that's enough, I think.;;;","14/Mar/23 17:45;syhily;It's better to extend the {{ConsumerInterceptor}} and {{ProducerInterceptor}} interfaces with extra {{open}} method and {{Serializable}} interface. We will manually open the interceptor in each parallelism. And no need to add a factory.;;;","14/Mar/23 20:15;dialalpha;Ok I see. Yes, the reason we added the factory is in case the callback isn't serializable. But we would also want the callback/interceptor to implement `AutoCloseable` so we can safely dispose of unmanaged resources during app shutdown.

I assume going with extending the `*Interceptor` types would be a bit more work and require some change to the Pulsar client libraries as well?;;;","14/Mar/23 22:23;syhily;Add a `AutoCloseable` do make sense to me. BTW, I don't think it requires more work and extra changes to the Pulsar client libraries when using the `*Interceptor` types.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Catalog support with Schema translation,FLINK-31427,13528282,13428958,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,syhily,syhily,syhily,13/Mar/23 17:56,31/Aug/23 02:58,04/Jun/24 20:41,31/Aug/23 02:58,pulsar-4.0.0,,,,,,,,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,stale-assigned,,,"This task will make the Pulsar serve as the Flink catalog. It will expose the Pulsar's namespace as the Flink's database, the topic as the Flink's table. You can easily create a table and database on Pulsar. The table can be consumed by other clients with a valid schema check.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 02:58:27 UTC 2023,,,,,,,,,,"0|z1gjlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","31/Aug/23 02:58;tison;It's rare that users would use Pulsar as Catalog source. Postpone for later.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade the deprecated UniqueConstraint to the new one ,FLINK-31426,13528266,13478113,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,13/Mar/23 15:56,09/Apr/23 03:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / Hive,Table SQL / Planner,,,0,pull-request-available,,,,https://github.com/apache/flink/pull/21522#discussion_r1133642525,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-13 15:56:02.0,,,,,,,,,,"0|z1gjhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support submitting a job with streamgraph ,FLINK-31425,13528261,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tophei,tophei,13/Mar/23 15:39,20/Mar/23 07:30,04/Jun/24 20:41,,,,,,,,,,,,,,,,,API / DataStream,,,,0,,,,,"Currently, we have rest api to submit a job via jobgraph, which is aligned to the way of flink cli running the entry class locally and submit the compiled binary to remote cluster for execution.

This is convenient in its own right. However it also seems to bring in some confusion and 'blackbox' feeling in that the payload of rest api is a binary object and thus not self-descriptive and it's relative a low-level presentation of the job executions whose interface is more likely to change as version evolves. 

Do you think it make more sense to build an api that accepts streamgraph as input which may be presented with a json(just like visualizer did for an execution plan visualization) plus additional runtime related configs and resources? This may make the rest interface more descriptive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 07:30:03 UTC 2023,,,,,,,,,,"0|z1gjgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 03:25;JunRuiLi;[~tophei] Thanks for creating this issue. IIUC, according to the description, I think this issue can be divided into two parts:
1. Submit and run jobs by submitting a streamGraph, which is very meaningful. Submitting and running jobs through streamGraph can expand the dynamic adjustment capability of flink runtime. This is a relatively large and very meaningful change, and we are currently doing related research.
2. Make StreamGraph public so that users can get streamGraph. I'm not sure if this part has enough meaning to expose an internal interface to users, which may bring additional burden.;;;","14/Mar/23 10:28;chesnay;??However it also seems to bring in some confusion and 'blackbox' feeling in that the payload of rest api is a binary object??

It's not intended to be used by anything but the client.

??Do you think it make more sense to build an api that accepts streamgraph as input which may be presented with a json??

That JSON would still have to contain many binary blobs for the user-code.


While there has been a desire for a more language-agnostic way to submit jobs I don't see this as being the way to do it. Expanding the API more without a particularly strong benefit doesn't make much sense to me.;;;","20/Mar/23 07:30;tophei;Thanks [~JunRuiLi]  and [~chesnay]  for the insight.

For item 2

'Make StreamGraph public so that users can get streamGraph. I'm not sure if this part has enough meaning to expose an internal interface to users, which may bring additional burden'

This is proposed to serve as alternative interface to JobGraph based api. Agree it's sort of internal interface, but it's a relative higher level abstraction compared with JobGraph and should be more developer friendly.

[~chesnay] some existing integration like Beam Flink Runner has to leverage on JobGraph interface for job submission, [https://github.com/apache/beam/blob/master/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java#L136,] where it appears like a blackbox and not feasible for some runtime check or debug log maybe. That makes me think whether a StreamGraph based interface can make more sense.

By the way, do we have some FLIP or mail thread that tracks discussion of 'language-agnostic way to submit jobs'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointer when using StatementSet for multiple sinks,FLINK-31424,13528254,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,qingyue,jirawech.s,jirawech.s,13/Mar/23 14:56,23/Mar/23 04:05,04/Jun/24 20:41,23/Mar/23 04:05,1.16.1,,,,,,,,,,,1.17.1,1.18.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,"I got following error when i tried to execute multiple sinks using StatementSet. I am not sure what it is and where to find possible solution.

Error
{code:java}
// code placeholder
Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.flink.table.planner.plan.metadata.FlinkRelMdWindowProperties.getJoinWindowProperties(FlinkRelMdWindowProperties.scala:373)
    at org.apache.flink.table.planner.plan.metadata.FlinkRelMdWindowProperties.getWindowProperties(FlinkRelMdWindowProperties.scala:349)
    at GeneratedMetadataHandler_WindowProperties.getWindowProperties_$(Unknown Source)
    at GeneratedMetadataHandler_WindowProperties.getWindowProperties(Unknown Source)
    at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getRelWindowProperties(FlinkRelMetadataQuery.java:261)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.createIntermediateRelTable(StreamCommonSubGraphBasedOptimizer.scala:287)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeBlock(StreamCommonSubGraphBasedOptimizer.scala:138)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.$anonfun$optimizeBlock$1(StreamCommonSubGraphBasedOptimizer.scala:111)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.$anonfun$optimizeBlock$1$adapted(StreamCommonSubGraphBasedOptimizer.scala:109)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47){code}
You can test the code, please see the attachment",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/23 14:56;jirawech.s;HelloFlinkWindowJoin.java;https://issues.apache.org/jira/secure/attachment/13056282/HelloFlinkWindowJoin.java",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Mar 23 01:57:16 UTC 2023,,,,,,,,,,"0|z1gjf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 08:05;zhengyiweng;Hi,[~jirawech.s] .It seems be a bug caused by that when executing multi sinks, it will optimize the sinkBlocks with collecting the statistics.But when getting the window properties of join, join relnode's child are exchanges, their window properties are null. ;;;","14/Mar/23 09:17;jirawech.s;[~zhengyiweng] I see. Thank you so much. Could you mind walk me through how to start if i want to fix this kind of issue? I am not familiar with Java so much. I do not know where to start. For example, I can clone the repo and start to run unit test for Flink Table API? If yes, which unit test i need to check.;;;","14/Mar/23 10:22;martijnvisser;[~godfreyhe] [~lincoln.86xy] WDYT?;;;","14/Mar/23 11:41;zhengyiweng;[~jirawech.s] Of course.But I am also new in Flink, the following suggestion may be completely right.You can clone the repo and repeat the error in the repo. Then you can try to find where the error is and to fix it. This kind of issue are mostly about Planner, so you can check the unit test of the Planner. But It seem that there isn't unit test about this issue. If you don't mind, could I try to fix it if it is a real bug? And you can have a review. Hoping that we can make progress together.;;;","14/Mar/23 15:07;jirawech.s;[~zhengyiweng] Sure thing. Maybe you can take a look first and then i can help review it.;;;","15/Mar/23 12:37;qingyue;Hi, [~jirawech.s], thanks for reporting this issue.

When multiple sinks are involved, the optimizer will try to break the DAG into rel blocks. According to the provided SQL, the optimized intermediate relNode(root)  is the `StreamPhysicalWindowJoin`. It is wrapped as the new table scan source by `StreamCommonSubGraphBasedOptimizer#createIntermediateRelTable`.

The NPE is thrown because the `FlinkRelMdWindowProperties` does not support `StreamPhysicalGlobalWindowAggregate`. (Here, the input of `StreamPhysicalExchange` is `StreamPhysicalGlobalWindowAggregate`).

 

I would like to fix it. cc [~lincoln.86xy];;;","15/Mar/23 12:58;lincoln.86xy;[~qingyue] thanks for investigating this! assigned to you;;;","15/Mar/23 13:07;lincoln.86xy;[~zhengyiweng] Thank you for troubleshooting this! As [~qingyue] appears to have provided a clearer reason for the problem and is willing to fix it, we've assigned it to her to fix it, also welcome everyone to follow up and review the pull request together.



;;;","21/Mar/23 01:32;qingyue;I opened a PR, and it would be great if you could help to review it. cc [~lincoln.86xy] [~zhengyiweng] ;;;","22/Mar/23 08:32;lincoln.86xy;fixed in master: 824259ecf53fd82061d177b508594e6510e69060
release-1.17: 71f23066562018910e5599719d2a1bc0eb17f430;;;","23/Mar/23 01:57;jirawech.s;[~qingyue] Thank you so much (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update external connector workflow to use actions that use Node16 instead of Node12 under the hood,FLINK-31423,13528250,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Mar/23 14:36,15/Mar/23 15:06,04/Jun/24 20:41,15/Mar/23 15:06,,,,,,,,,,,,,,,,Build System,Build System / CI,,,0,pull-request-available,,,,"Currently, running workflows for externalized connectors results in warnings such as:

{code:bash}
Node.js 12 actions are deprecated. Please update the following actions to use Node.js 16: actions/checkout@v2, actions/setup-java@v2, stCarolas/setup-maven@v4.2. For more information see: https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/.
{code}

We should update the used Github actions to their latest versions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 15:06:47 UTC 2023,,,,,,,,,,"0|z1gje8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 15:06;martijnvisser;Fixed in ci_utils: c835c9301ea13b5990f8f3c9a66c78e4c7de8093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Servable for Logistic Regression Model,FLINK-31422,13528236,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,13/Mar/23 12:49,20/Mar/23 10:41,04/Jun/24 20:41,20/Mar/23 10:39,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add Servable for LogisticRegressionModel.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 10:40:13 UTC 2023,,,,,,,,,,"0|z1gjb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 10:40;lindong;Merged apache/flink-ml master adc1898920f2bf715efaa9fc7e343af8a61a49b6 and 6a92e363f6452531761414994e0f9a5501415764;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataGeneratorSourceITCase.testGatedRateLimiter failed on CI,FLINK-31421,13528220,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,13/Mar/23 10:46,17/Apr/24 14:49,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Connectors / Common,,,,0,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47044&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=23109]
{code:java}
[ERROR] org.apache.flink.connector.datagen.source.DataGeneratorSourceITCase.testGatedRateLimiter  Time elapsed: 1.557 s  <<< FAILURE!
Mar 11 04:43:12 java.lang.AssertionError: 
Mar 11 04:43:12 
Mar 11 04:43:12 Expected size: 8 but was: 7 in:
Mar 11 04:43:12 [1L, 1L, 1L, 1L, 1L, 1L, 1L]
Mar 11 04:43:12 	at org.apache.flink.connector.datagen.source.DataGeneratorSourceITCase.testGatedRateLimiter(DataGeneratorSourceITCase.java:200) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 14:49:44 UTC 2024,,,,,,,,,,"0|z1gj7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 10:48;renqs;[~chesnay] may you take a look at this? Thanks;;;","17/Apr/24 14:49;rskraba;1.19 Hadoop 3.1.3: Test (module: misc) https://github.com/apache/flink/actions/runs/8715237951/job/23907182643#step:10:23996;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThreadInfoRequestCoordinatorTest.testShutDown failed on CI,FLINK-31420,13528219,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,renqs,renqs,13/Mar/23 10:29,15/Mar/23 14:46,04/Jun/24 20:41,15/Mar/23 14:46,1.16.2,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,Runtime / REST,,,,0,pull-request-available,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47076&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8394]
{code:java}
Mar 13 03:58:48 [ERROR] Failures: 
Mar 13 03:58:48 [ERROR]   ThreadInfoRequestCoordinatorTest.testShutDown:207 
Mar 13 03:58:48 Expecting
Mar 13 03:58:48   <CompletableFuture[Failed with the following stack trace:
Mar 13 03:58:48 java.lang.RuntimeException: Discarded
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator$PendingStatsRequest.discard(TaskStatsRequestCoordinator.java:266)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.handleFailedResponse(TaskStatsRequestCoordinator.java:114)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.lambda$requestThreadInfo$1(ThreadInfoRequestCoordinator.java:152)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Mar 13 03:58:48 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Mar 13 03:58:48 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Mar 13 03:58:48 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
Mar 13 03:58:48 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
Mar 13 03:58:48 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Mar 13 03:58:48 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Mar 13 03:58:48 	at java.lang.Thread.run(Thread.java:748)
Mar 13 03:58:48 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request timeout.
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.lambda$createMockTaskManagerGateway$2(ThreadInfoRequestCoordinatorTest.java:259)
Mar 13 03:58:48 	... 6 more
Mar 13 03:58:48 Caused by: java.util.concurrent.TimeoutException: Request timeout.
Mar 13 03:58:48 	... 7 more
Mar 13 03:58:48 ]>
Mar 13 03:58:48 not to be done.
Mar 13 03:58:48 Be aware that the state of the future in this message might not reflect the one at the time when the assertion was performed as it is evaluated later on {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 14:45;Weijie Guo;image-2023-03-15-22-45-54-585.png;https://issues.apache.org/jira/secure/attachment/13056376/image-2023-03-15-22-45-54-585.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 09:56:04 UTC 2023,,,,,,,,,,"0|z1gj7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 10:29;renqs;[~fanrui] Could you take a look on this one? Thanks;;;","13/Mar/23 14:17;Weijie Guo;After checking the code, I found that this is a test code issue. IIUC, This should be a problem that has existed for a long time, I don't know why it has not been revealed until now, but the probability of triggering this exception is very low. I will fix it asap.

The key point is {{ThreadInfoRequestCoordinatorTest#testShutDown}} using two gateways contains a timeout gateway but expected all request futures not done before {{shutdown}}. If the timeout reached, this future will be failed, result in request futures to be done. This makes the test strongly dependent on the execution order of assertion and timeout, thus causing this test unstable.;;;","15/Mar/23 09:56;Weijie Guo;master(1.18) via 7fccd5992f6222df62ed850542ef50b0714cd647.
release-1.17 via 6eb213d9093d90a7bd91801abfb822edf51f90bb.
release-1.16 via 3ffd113365e69185b871b2b683067b1541a17e5c.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-JDBC should allow . in field names,FLINK-31419,13528218,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lumo,lumo,13/Mar/23 10:27,13/Mar/23 10:30,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"Currently, the method org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl#parseNamedStatement parses . (period) in file name as an illegal character, so the statement
```

INSERT INTO `tbl`(`action.id`, `action.name`, `email`, `ts`, `field1`, `field_2`, `__field_3__`) VALUES (:id, :name, :email, :ts, :field1, :field_2, :__field_3__)

``` does not get parsed into

```

INSERT INTO `tbl`(`action.id`, `action.name`, `email`, `ts`, `field1`, `field_2`, `__field_3__`) VALUES (:id, :action.name, :email, :ts, :field1, :field_2, :__field_3__)

```

Instead, action.id and action.name will be parsed to the same name - action, this causes incompatible schema with the database",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 10:30:47 UTC 2023,,,,,,,,,,"0|z1gj74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 10:30;lumo;Fix is to add the period to be recognized as a legal character in the function org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl#parseNamedStatement

  while (j < length && (Character.isJavaIdentifierPart(sql.charAt(j))  || '.' == sql.charAt(j))) {;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeResultPartitionReadSchedulerTest.testRequestBufferTimeout failed on CI,FLINK-31418,13528215,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanyuxin,renqs,renqs,13/Mar/23 10:17,16/May/23 11:46,04/Jun/24 20:41,16/May/23 11:46,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.0,1.18.0,,Runtime / Task,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47077&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8756]

Error message:
{code:java}
Mar 13 05:22:10 [ERROR] Failures: 
Mar 13 05:22:10 [ERROR]   SortMergeResultPartitionReadSchedulerTest.testRequestBufferTimeout:278 
Mar 13 05:22:10 Expecting value to be true but was false{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 11:46:03 UTC 2023,,,,,,,,,,"0|z1gj6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 10:17;renqs;[~tanyuxin] Could you take a look on this one? Thanks;;;","13/Mar/23 10:40;tanyuxin;I think https://issues.apache.org/jira/browse/FLINK-31396 may resolve it. But it's an occasional issue, and can not be produced in the local env to check whether it can be resolved.

But when we discuss it offline, [~kevin.cyj] have some different inputs about it. Could you please take a look?;;;","13/Mar/23 10:50;kevin.cyj;I will also try to reproduce it. If it can not reproduce, let's downgrade this issue. I think it not need to be a blocker, because it may exits since 1.16 and there is no relevant change recently.;;;","14/Mar/23 02:27;renqs;Thanks for the feedback [~kevin.cyj]. I'll downgrade this to critical as replied.;;;","14/Mar/23 02:38;kevin.cyj;It is a test issue, we will fix it soon.;;;","20/Mar/23 07:21;mapohl;Looks like this issue also exists in 1.16?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47263&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7119

I'm reopening the issue and updating the affected versions accordingly.;;;","20/Mar/23 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47328&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7090;;;","16/May/23 07:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49021&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8387;;;","16/May/23 07:13;Weijie Guo;I will try to backport this to 1.16 and see if the problem can be solved.;;;","16/May/23 07:20;Weijie Guo;It seems that the commits information has been missed

master(1.18) via f949fe4eb1b83a324b9139d41a6db5976a8db539.
release-1.17 via 4e528036e70a9909b4cf1525be2e1e56da2d22f8.
release-1.16 via ffa58e1de3de0d084d820594304b4dbdbf705c9b.;;;","16/May/23 07:31;tanyuxin;[~Sergey Nuyanzin]   [~Weijie Guo], The patch is not in release-1.16. I created a PR to backport it.https://github.com/apache/flink/pull/22589;;;","16/May/23 11:46;Weijie Guo;Feel free to reopen this If it can still reproduce.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop version unknown when TrinoPageSourceBase.getNextPage,FLINK-31417,13528211,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nonggia,nonggia,13/Mar/23 10:07,19/Mar/23 05:35,04/Jun/24 20:41,19/Mar/23 05:35,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,,,,,"Exception thrown when quering flink-table-store by trino
{code:java}
2023-03-13T11:46:36.694+0800    ERROR   SplitRunner-11-113      io.trino.execution.executor.TaskExecutor        Error processing Split 20230313_034504_00000_jdcet.1.0.0-11 {} (start = 3.599627617710298E10, wall = 89264 ms, cpu = 0 ms, wait = 1 ms, calls = 1)java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.createReader(KeyValueFileStoreRead.java:204)        at org.apache.flink.table.store.table.source.KeyValueTableRead.createReader(KeyValueTableRead.java:44)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.createPageSource(TrinoPageSourceProvider.java:76)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.lambda$createPageSource$0(TrinoPageSourceProvider.java:52)        at org.apache.flink.table.store.trino.ClassLoaderUtils.runWithContextClassLoader(ClassLoaderUtils.java:30)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.createPageSource(TrinoPageSourceProvider.java:51)        at io.trino.split.PageSourceManager.createPageSource(PageSourceManager.java:68)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:308)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)  2023-03-13T11:46:36.775+0800    ERROR   remote-task-callback-2  io.trino.execution.scheduler.PipelinedStageExecution    Pipelined stage execution for stage 20230313_034504_00000_jdcet.1 failedjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more  2023-03-13T11:46:36.777+0800    ERROR   stage-scheduler io.trino.execution.scheduler.SqlQueryScheduler  Failure in distributed stage for query 20230313_034504_00000_jdcetjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more  2023-03-13T11:46:36.784+0800    ERROR   stage-scheduler io.trino.execution.StageStateMachine    Stage 20230313_034504_00000_jdcet.1 failedjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more {code}
Seems the common-version-info.properties file in flink-shaded-hadoop-2-uber-2.8.3-10.0.jar is not found by the classloader. The stacks tell that the call is from TrinoPageSourceBase.getNextPage, where the classloader of the current thread is AppClassloader, rather than PluginClassloader.

Can we fix it by using runWithContextClassLoader to run TrinoPageSourceBase.getNextPage with TrinoPageSourceBase.class.getClassloader?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:34:55 UTC 2023,,,,,,,,,,"0|z1gj5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:34;lzljs3620320;Thanks [~nonggia] , this has been fixed in 0.3, will cp to 0.4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow skipping of dependency convergence check,FLINK-31416,13528203,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Mar/23 09:14,13/Mar/23 16:22,04/Jun/24 20:41,13/Mar/23 16:21,,,,,,,,,,,,,,,,Build System,,,,0,pull-request-available,,,,"In order to resolve FLINK-30794 we need a mechanism that allows us to skip the dependency convergence check, which is necessary when testing against multiple Flink versions (like now is being done in nightly builds)",,,,,,,,,,,,,FLINK-30794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 16:21:30 UTC 2023,,,,,,,,,,"0|z1gj3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 16:21;martijnvisser;Fixed in flink-connector-shared-utils@ci_utils: 1c08ad3b60f3ae8305750082c5e9714583e54d3d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support target alias and using ddls to create source,FLINK-31415,13528189,13527066,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,13/Mar/23 07:58,14/Mar/23 01:31,04/Jun/24 20:41,14/Mar/23 01:31,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 01:31:06 UTC 2023,,,,,,,,,,"0|z1gj0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 01:31;lzljs3620320;master: 3fe06f7384bb8ce5fab5997b5d0e81761874f092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exceptions in the alignment timer are ignored,FLINK-31414,13528186,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,13/Mar/23 07:23,15/Mar/23 10:54,04/Jun/24 20:41,15/Mar/23 10:18,1.13.6,1.14.6,1.15.3,1.16.1,,,,,,,,1.16.2,1.17.1,1.18.0,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"Alignment timer task in alternating aligned checkpoint run as a future task in mailbox thread, causing the exceptions ([SingleCheckpointBarrierHandler#registerAlignmentTimer()|https://github.com/apache/flink/blob/65ab8e820a3714d2134dfb4c9772a10c998bd45a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/checkpointing/SingleCheckpointBarrierHandler.java#L327]) to be ignored. These exceptions should have failed the task, but now this will cause the same checkpoint to fire twice initInputsCheckpoints in my test.

 
{code:java}
 switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: unable to send request to worker
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:247)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.addInputData(ChannelStateWriterImpl.java:161)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.prepareSnapshot(StreamTaskNetworkInput.java:103)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.prepareSnapshot(StreamOneInputProcessor.java:83)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.prepareSnapshot(StreamMultipleInputProcessor.java:122)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInputSnapshot(StreamTask.java:518)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.prepareInflightDataSnapshot(SubtaskCheckpointCoordinatorImpl.java:655)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.initInputsCheckpoint(SubtaskCheckpointCoordinatorImpl.java:515)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.initInputsCheckpoint(SingleCheckpointBarrierHandler.java:516)
        at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:46)
        at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.barrierReceived(AbstractAlternatingAlignedBarrierHandlerState.java:54)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
        at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.io.IOException: java.lang.IllegalStateException: writer not found for request start 17
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:235)
                at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:564)
                at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:551)
                at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:255)
                at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
                at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
                at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:943)
                at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
                at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
                at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
                ... 3 more
        Caused by: java.lang.IllegalStateException: writer not found for request start 17
                at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:75)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
                ... 1 more
Caused by: java.lang.IllegalStateException: not running
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:244)
        ... 27 more
        [CIRCULAR REFERENCE:java.lang.IllegalStateException: writer not found for request start 17] {code}
 

 

see : [BarrierAlignmentUtil#createRegisterTimerCallback()|https://github.com/apache/flink/blob/65ab8e820a3714d2134dfb4c9772a10c998bd45a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/checkpointing/BarrierAlignmentUtil.java#L50]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 10:18:17 UTC 2023,,,,,,,,,,"0|z1gj00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 07:47;martijnvisser;[~pnowojski] WDYT?;;;","13/Mar/23 18:05;pnowojski;[~Feifan Wang], can you elaborate a bit more what's the problem? I see a couple of discrepancies in your description and the stack trace that you posted:
*  the stack trace doesn't match to the master code, so I'm not sure what Flink version you are using?
* doesn't the error message ""switched from RUNNING to FAILED"" refer to actually subtask/task switching to FAILED state, contradicting your statement that the exception is being ignored?
* in the PR, I don't see a test coverage - a working unit test/ITCase that used to be failing without your fix would be nice to have. Both for explaining what is the issue and for actually providing regression test coverage;;;","14/Mar/23 03:28;Feifan Wang;Thanks for reply [~pnowojski] , sorry for the lack of clarity in the previous description, let me answer your question first :
{quote}the stack trace doesn't match to the master code, so I'm not sure what Flink version you are using?
{quote}
based on *release-1.16.1* , cherry-picked some bug fix.
{quote}doesn't the error message ""switched from RUNNING to FAILED"" refer to actually subtask/task switching to FAILED state, contradicting your statement that the exception is being ignored?
{quote}
Yes, it is a subtask switching to FAILED state. I mean the exception thrown in the alignment timer task is being ignored, causing the subtask thread to continue executing to trigger the exception I posted above.

Here is the more complete log ( I change some log level from debug to info ) :

 
{code:java}
2023-03-10 12:09:42,416 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1): Received barrier from channel InputChannelInfo{gateIdx=1, inputChannelIdx=586} @ 17.
2023-03-10 12:09:42,673 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 starting checkpoint 17 (CheckpointOptions{checkpointType=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}, targetLocation=(default), alignmentType=UNALIGNED, alignedCheckpointTimeout=9223372036854775807})
2023-03-10 12:09:42,673 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 put ChannelStateWriteResult : 17
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1): Triggering checkpoint 17 on the barrier announcement at 1678421367671.
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.tasks.StreamTask           - triggerCheckpointOnBarrier Starting checkpoint 17 CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD} on task MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.tasks.StreamTask           - Starting checkpoint 17 CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD} on task MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 requested write result, checkpoint 17
2023-03-10 12:09:42,676 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.state.changelog.ChangelogKeyedStateBackend   - snapshot of MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 for checkpoint 17, change range: 39..46, materialization ID 4
2023-03-10 12:09:42,677 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.tasks.RegularOperatorChain  - Could not complete snapshot 17 for operator MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1. Failure reason: Checkpoint was declined.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 17 for operator MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1. Failure reason: Checkpoint was declined.
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:345)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:228)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:213)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:730)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:363)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$13(StreamTask.java:1291)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1279)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1236)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:489)
    at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:50)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$registerAlignmentTimer$3(SingleCheckpointBarrierHandler.java:321)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: The upload for 44 has already failed previously
    at org.apache.flink.changelog.fs.FsStateChangelogWriter.ensureCanPersist(FsStateChangelogWriter.java:429)
    at org.apache.flink.changelog.fs.FsStateChangelogWriter.persistInternal(FsStateChangelogWriter.java:213)
    at org.apache.flink.changelog.fs.FsStateChangelogWriter.persist(FsStateChangelogWriter.java:208)
    at org.apache.flink.state.changelog.ChangelogKeyedStateBackend.snapshot(ChangelogKeyedStateBackend.java:402)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:246)
    ... 31 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /flink-yg-test01/feifan-changelog-test/dstl/460359d4d9311744142797ba23e69d16/dstl is exceeded: limit=1048576 items=1048576
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1056)
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1101)
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:967)
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addFile(FSDirectory.java:531)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2997)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2856)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2691)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:815)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:450)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:713)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1002)
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:923)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1726)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2786)    at org.apache.hadoop.ipc.Client.call(Client.java:1578)
    at org.apache.hadoop.ipc.Client.call(Client.java:1505)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
    at com.sun.proxy.$Proxy22.create(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:332)
    at sun.reflect.GeneratedMethodAccessor181.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
    at com.sun.proxy.$Proxy23.create(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor181.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hdfs.RpcResponseHandler.invoke(RpcResponseHandler.java:55)
    at com.sun.proxy.$Proxy23.create(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1982)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1937)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1871)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:452)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:448)
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:391)
    at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:179)
    at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.create(ChRootedFileSystem.java:189)
    at org.apache.hadoop.fs.viewfs.ViewFileSystem.create(ViewFileSystem.java:323)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1027)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1008)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:154)
    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:37)
    at org.apache.flink.changelog.fs.DuplicatingStateChangeFsUploader.prepareStream(DuplicatingStateChangeFsUploader.java:96)
    at org.apache.flink.changelog.fs.AbstractStateChangeFsUploader.uploadInternal(AbstractStateChangeFsUploader.java:76)
    at org.apache.flink.changelog.fs.AbstractStateChangeFsUploader.upload(AbstractStateChangeFsUploader.java:69)
    at org.apache.flink.changelog.fs.BatchingStateChangeUploadScheduler$1.tryExecute(BatchingStateChangeUploadScheduler.java:310)
    at org.apache.flink.changelog.fs.BatchingStateChangeUploadScheduler$1.tryExecute(BatchingStateChangeUploadScheduler.java:307)
    at org.apache.flink.changelog.fs.RetryingExecutor$RetriableActionAttempt.run(RetryingExecutor.java:225)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
2023-03-10 12:09:42,677 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 aborting, checkpoint 17, cleanup:true
2023-03-10 12:09:42,723 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1): Received barrier from channel InputChannelInfo{gateIdx=0, inputChannelIdx=324} @ 17.
2023-03-10 12:09:42,723 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 starting checkpoint 17 (CheckpointOptions{checkpointType=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}, targetLocation=(default), alignmentType=UNALIGNED, alignedCheckpointTimeout=9223372036854775807})
2023-03-10 12:09:42,724 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 put ChannelStateWriteResult : 17
2023-03-10 12:09:42,724 INFO  [Channel state writer MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 discarding 645 drained requests
2023-03-10 12:09:42,724 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 discarding 1023 drained requests
2023-03-10 12:09:42,725 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.state.common.PeriodicMaterializationManager  - Shutting down PeriodicMaterializationManager.
2023-03-10 12:09:42,725 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 aborting, checkpoint 17, cleanup:false
2023-03-10 12:09:42,726 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 discarding 1 drained requests
2023-03-10 12:09:43,099 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend  - Closed RocksDB State Backend. Cleaning up RocksDB working directory /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671247042382_4677267/tm_container_e74_1671247042382_4677267_01_000039/tmp/job_460359d4d9311744142797ba23e69d16_op_KeyedCoProcessOperator_4f7e0f4c19a43f929bda6907ee1f3150__4517_4800__uuid_785a6359-d191-4691-9272-d0590d2d696b.
2023-03-10 12:09:43,125 WARN  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.taskmanager.Task                     - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: unable to send request to worker
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:247)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.addInputData(ChannelStateWriterImpl.java:161)
    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.prepareSnapshot(StreamTaskNetworkInput.java:103)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.prepareSnapshot(StreamOneInputProcessor.java:83)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.prepareSnapshot(StreamMultipleInputProcessor.java:122)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInputSnapshot(StreamTask.java:518)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.prepareInflightDataSnapshot(SubtaskCheckpointCoordinatorImpl.java:655)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.initInputsCheckpoint(SubtaskCheckpointCoordinatorImpl.java:515)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.initInputsCheckpoint(SingleCheckpointBarrierHandler.java:516)
    at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:46)
    at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.barrierReceived(AbstractAlternatingAlignedBarrierHandlerState.java:54)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
    Suppressed: java.io.IOException: java.lang.IllegalStateException: writer not found for request start 17
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:235)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:564)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:551)
        at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:255)
        at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
        at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:943)
        at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
        ... 3 more
    Caused by: java.lang.IllegalStateException: writer not found for request start 17
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:75)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
        ... 1 more
Caused by: java.lang.IllegalStateException: not running
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:244)
    ... 27 more
    [CIRCULAR REFERENCE:java.lang.IllegalStateException: writer not found for request start 17]
2023-03-10 12:09:43,125 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.taskmanager.Task                     - Freeing task resources for MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1). {code}
 

 

Later I will try to write a test to explain the problem.;;;","14/Mar/23 08:49;pnowojski;Ah thanks for the explanation, I understand the problem now. I see that bug has been in the code base since at least https://issues.apache.org/jira/browse/FLINK-19682. ;;;","15/Mar/23 10:18;pnowojski;merged commit cbfeef6 to master
0dfb5abf038 to release-1.17
f2f9104dc46 to release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove provided dependency of flink-table-planner from Hive connector,FLINK-31413,13528181,13433321,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/Mar/23 06:52,26/Jun/23 03:11,04/Jun/24 20:41,26/Jun/23 03:04,,,,,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,,Can move on after finish FLINK-31409. We still need some follow-up tasks before removing provided table-planner dependency in hive connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 03:04:39 UTC 2023,,,,,,,,,,"0|z1giyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 03:04;luoyuxia;master:

9ae5359b94dbb321296dbb3925f0d6d7a77650a2

8eb590cf79bf954a811edf8ed8cf6fe8a59969af

39d15999754b73158ac590a3a013a9eda45095db

a5d52550c512a28c60f02b16e5b98778ac002f86

71c4358965c200561a43a2aa3db712d5f6e9e3f2

43f5d038fb21669ee2c203e7dbf4df1512bfb1e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog rollback schema if alter table failed in table store,FLINK-31412,13528179,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,13/Mar/23 06:40,13/Mar/23 12:02,04/Jun/24 20:41,13/Mar/23 12:02,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Rollback schema in dfs if HiveCatalog alter table failed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 12:02:55 UTC 2023,,,,,,,,,,"0|z1giyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 12:02;lzljs3620320;master: 9ea7a02db5aacfaa53e733c3ec7538f7062d58be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lookup.cache.caching-missing-key变更的配置应为lookup.partial-cache.caching-missing-key,FLINK-31411,13528175,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Invalid,,gaara,gaara,13/Mar/23 06:07,13/Mar/23 06:23,04/Jun/24 20:41,13/Mar/23 06:23,,,,,,,,,,,,,,,,,,,,0,,,,,!image-2023-03-13-06-06-24-432.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/23 06:06;gaara;image-2023-03-13-06-06-24-432.png;https://issues.apache.org/jira/secure/attachment/13056271/image-2023-03-13-06-06-24-432.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 06:23:07 UTC 2023,,,,,,,,,,"0|z1gixk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 06:23;jark;Please use English and add descriptions for what is wrong and what you want to change. We can reopen this issue, once it's ready. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ListStateWithCache Should support incremental snapshot,FLINK-31410,13528168,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhangzp,zhangzp,13/Mar/23 04:17,13/Mar/23 04:17,04/Jun/24 20:41,,ml-2.2.0,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,,,,,"In Flink ML, we use ListStateWithCache [2] to enable caching data in memory and filesystem. However, it does not support incremental snapshot now — It writes all the data to checkpoint stream whenever calling snapshot [1], which could be inefficient.

 

 [1][https://github.com/apache/flink-ml/blob/master/flink-ml-iteration/src/main/java/org/apache/flink/iteration/datacache/nonkeyed/DataCacheSnapshot.java#L116]

 [2][https://github.com/apache/flink-ml/blob/master/flink-ml-iteration/src/main/java/org/apache/flink/iteration/datacache/nonkeyed/ListStateWithCache.java] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-13 04:17:17.0,,,,,,,,,,"0|z1giw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect should use public interfaces in Hive connector,FLINK-31409,13528153,13433321,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/Mar/23 02:17,24/Jan/24 03:59,04/Jun/24 20:41,24/Jan/24 03:59,,,,,,,,,,,,1.18.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,"Currently, for the Hive dialect part in Hive connector, it depends much internal interfaces in flink-table-planner or other module. We should avoid it and use public interfaces proposed in  [FLIP-216|[https://cwiki.apache.org/confluence/display/FLINK/FLIP-216%3A++Introduce+pluggable+dialect+and+plan+for+migrating+Hive+dialect]]",,,,,,,,,,,,,,,,,,,,FLINK-30667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 03:59:46 UTC 2024,,,,,,,,,,"0|z1giso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 02:07;luoyuxia;close via:

642484484b9b1bef6a4ccec17e651f1a79ba6d86

d3450fb6c850ea029814fc0fcac98ab9d1d2b498

f476675d4545ef2fa547ecabeebaf991ed1d91dd;;;","24/Jan/24 03:28;libenchao;[~luoyuxia] Could you fill the fixVersion of this issue?;;;","24/Jan/24 03:59;luoyuxia;[~libenchao] Thanks for reminder.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add EXACTLY_ONCE support to upsert-kafka,FLINK-31408,13528137,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Gerrrr,Gerrrr,Gerrrr,12/Mar/23 20:35,12/Jul/23 12:52,04/Jun/24 20:41,12/Jul/23 12:52,,,,,,,,,,,,kafka-3.1.0,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"{{upsert-kafka}} connector should support optional {{EXACTLY_ONCE}} delivery semantics.

[upsert-kafka docs|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/upsert-kafka/#consistency-guarantees] suggest that the connector handles duplicate records from {{{}AT_LEAST_ONCE{}}}. However, at least 2 reasons exist to configure the connector with {{{}EXACTLY_ONCE{}}}.

First, there might be other non-Flink topic consumers that would rather not have duplicated records.

Second, multiple {{upsert-kafka}} producers might cause keys to roll back to previous values. Consider a scenario with 2 producing jobs A and B, writing to the same topic with {{AT_LEAST_ONCE}} and a consuming job reading from the topic. Both producers write unique, monotonically increasing sequences to the same key. Job A writes {{x=a1,a2,a3,a4,a5…}} Job B writes {{{}x=b1,b2,b3,b4,b5,...{}}}. With this setup, we can have the following sequence:
 # Job A produces x=a5.
 # Job B produces x=b5.
 # Job A produces the duplicate write x= 5.

The consuming job would observe {{x}} going to {{{}a5{}}}, then to {{{}b5{}}}, then back {{{}a5{}}}. {{EXACTLY_ONCE}} would prevent this behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 12:52:16 UTC 2023,,,,,,,,,,"0|z1gip4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 12:52;tzulitai;Merged to flink-connector-kafka via:

main - 9109722c0cf27299b60232c64ae29e00c62934a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Fabric8 version to 6.5.0,FLINK-31407,13528134,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,gyfora,gyfora,12/Mar/23 19:01,01/Jun/23 12:00,04/Jun/24 20:41,01/Jun/23 12:00,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Fabric8 6.5.0 has been released recently with a number of major improvements:
[https://github.com/fabric8io/kubernetes-client/releases/tag/v6.5.0]

This is a very important version for the operator as it also fixes some outstanding issues with timing out informers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 08:27:09 UTC 2023,,,,,,,,,,"0|z1giog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 08:27;mbalassi;91cf233 in main, pending further changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not delete jobgraph on scale only last-state upgrades,FLINK-31406,13528133,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,nicholasjiang,gyfora,gyfora,12/Mar/23 19:00,04/Dec/23 09:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,stale-assigned,,,"Currently the operator always deletes the jobgraph from HA metadata so that it's regenerated for last-state upgrades. 

This is unnecessary for scale only operations. Keeping the jobgraph can greately speed up startup time for some jobs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 29 22:35:09 UTC 2023,,,,,,,,,,"0|z1gio8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/23 19:30;gyfora;This should also apply to regular restarts / redeployments done using the last-state mechanism such as deployment recovery, unhealthy job restart etc.;;;","29/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor tests to git rid of timeout of CompletableFuture assertions.,FLINK-31405,13528090,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Mar/23 19:11,14/Aug/23 11:00,04/Jun/24 20:41,20/Mar/23 10:23,1.18.0,,,,,,,,,,,1.18.0,,,,Tests,,,,0,pull-request-available,,,,"In general, we want to avoid relying on local timeouts in the Flink test suite to get additional context (thread dump) in case something gets stuck(see [Code Style and Quality Guide|https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#avoid-timeouts-in-junit-tests]).
Some of timeout in tests are introduced by assertions for {{CompleteFuture}}. After FLINK-31385, we can refactor these tests to git rid of timeout for {{CompletableFuture}} assertions.",,,,,,,,,,,,,,,,,,,,FLINK-26839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 12:58:30 UTC 2023,,,,,,,,,,"0|z1gieo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 10:23;Weijie Guo;master(1.18) via d88f39c4486786d1b146ca71390400d9be3375a4.;;;","23/Mar/23 11:53;chesnay;[~Weijie Guo] Please remember to set the fix version when closing the ticket.;;;","23/Mar/23 12:58;Weijie Guo;I forgot it accidentally, thanks [~chesnay] for reminding me and fixing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support extracting all mapping of Calc node's input references,FLINK-31404,13528069,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,11/Mar/23 12:01,12/Mar/23 13:24,04/Jun/24 20:41,12/Mar/23 13:24,1.17.0,1.17.1,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Currently, `StreamNonDeterministicUpdatePlanVisitor` only supports extracting a limited kind of RexNode to its input node for the Calc.
 * RexInputRef
 * RexCall with the operator kind as `AS` and `CAST`

It can be optimized to support all kinds of RexNode by FlinkRexUtil#findAllInputRefs.

cc [~lincoln.86xy] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 12 13:24:18 UTC 2023,,,,,,,,,,"0|z1gia0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/23 13:24;lincoln.86xy;fixed in master: a43a63c165ccb37cf0feb5c948943abc73c6f423;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientITCase print unexpected border when printing explain results,FLINK-31403,13528058,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,11/Mar/23 05:05,13/Mar/23 07:55,04/Jun/24 20:41,13/Mar/23 07:55,1.17.0,1.18.0,,,,,,,,,,1.17.0,,,,Table SQL / Client,,,,0,,,,,"The CliClientITCase fails in the https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47010&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=16287.

After comparing the actual output and expected output, the differences are the length of the border when printing explain results.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 07:55:50 UTC 2023,,,,,,,,,,"0|z1gi7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 01:49;fsk119;I think it has been fixed by the [commit|https://github.com/apache/flink/pull/21322/commits/8c6a9ab1952c7d397fd39d952448206123081ac7] in the master. I will cherry pick it to the release-1.17.;;;","13/Mar/23 07:55;leonard;Fixed in release-1.17:  1838619bdc3585768ded050e17b8abcfdbf5811c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove ConverterUtils Deprecated Method,FLINK-31402,13528056,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,11/Mar/23 03:35,11/Mar/23 07:58,04/Jun/24 20:41,11/Mar/23 07:58,1.17.0,,,,,,,,,,,1.18.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"Many methods in ConverterUtils have been Deprecated, we can replace the Deprecated methods and remove this class(ConverterUtils).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 07:57:28 UTC 2023,,,,,,,,,,"0|z1gi74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/23 07:57;wanglijie;Done via

master: a7b0c2707518a8b7f589e50fa4eafad791d7179f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testTransformationSetParallelism fails on 10 core machines,FLINK-31401,13527999,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mbalassi,mbalassi,mbalassi,10/Mar/23 15:30,12/Mar/23 18:52,04/Jun/24 20:41,12/Mar/23 18:52,1.17.0,,,,,,,,,,,1.18.0,,,,API / DataStream,Tests,,,0,pull-request-available,,,,"StreamingJobGraphGenerator#testTransformationSetParallelism fails if it is run in an environment where the default parallelism is 10:

{noformat}
org.opentest4j.AssertionFailedError: 
expected: 3
 but was: 2
Expected :3
Actual   :2
{noformat}

The fix is trivial, we need to make an implicit assumption in the test about paralellisms explicit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 12 18:52:49 UTC 2023,,,,,,,,,,"0|z1ghug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 18:23;JunRuiLi;[~mbalassi] Thanks so much for your volunteering and contribution!:D As you said, this is a bug: testTransformationSetParallelism implies that the map and source should have different parallelism so that they will not be chained together, but when the default parallelism is 10, it will break the rule. ;;;","12/Mar/23 18:52;mbalassi;Sure, thanks for the quick confirmation.;;;","12/Mar/23 18:52;mbalassi;64d5f54 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add autoscaler integration for Iceberg source,FLINK-31400,13527977,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masc,mxm,mxm,10/Mar/23 13:31,16/Apr/24 08:04,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,,"A very critical part in the scaling algorithm is setting the source processing rate correctly such that the Flink pipeline can keep up with the ingestion rate. The autoscaler does that by looking at the {{pendingRecords}} Flink source metric. Even if that metric is not available, the source can still be sized according to the busyTimeMsPerSecond metric, but there will be no backlog information available. For Kafka, the autoscaler also determines the number of partitions to avoid scaling higher than the maximum number of partitions.

In order to support a wider range of use cases, we should investigate an integration with the Iceberg source. As far as I know, it does not expose the pedingRecords metric, nor does the autoscaler know about other constraints, e.g. the maximum number of open files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-10 13:31:37.0,,,,,,,,,,"0|z1ghpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler is able to handle changes in job resource requirements,FLINK-31399,13527974,13527017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,10/Mar/23 13:10,28/Mar/23 11:39,04/Jun/24 20:41,28/Mar/23 11:39,,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"In FLINK-31317, we've introduced data structures for setting job resource requirements. In this ticket, we'll enable the AdaptiveScheduler to track the resource requirements of the current job and handle external changes (they'll be pushed by the JobMaster eventually).",,,,,,,,,,,,FLINK-31317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 11:39:48 UTC 2023,,,,,,,,,,"0|z1ghow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 11:39;chesnay;master:
8986bf0f599d6547f6988cf3259c1b403cd4e317..0dd9bcb0a9c8d905e3907dbd17947926f2432a31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't wrap with TemporaryClassLoaderContext in OperationExecutor,FLINK-31398,13527967,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,luoyuxia,luoyuxia,10/Mar/23 12:09,21/Apr/23 17:18,04/Jun/24 20:41,21/Apr/23 17:18,,,,,,,,,,,,1.17.1,1.18.0,,,Connectors / Hive,Table SQL / Client,,,0,pull-request-available,,,,"Currently, method OperationExecutor#executeStatement in sql client will wrap currently with `

sessionContext.getSessionState().resourceManager.getUserClassLoader()`. Actually, it's not necessary. What' worse, 

it'll will cause the exception 'Trying to access closed classloader. Please check if you store xxx'  after quiting sql client. 

The reason is in `ShutdownHookManager`, it will register a hook after jvm shutdown. In `ShutdownHookManager`, it will

create `Configuration`. It will then access `Thread.currentThread().getContextClassLoader()` which is FlinkUserClassLoader, the FlinkUserClassLoader has been closed before. So, it'll then cause `'Trying to access closed classloader` exception.

 

 ",,,,,,,,,,,,,,,,,,,,FLINK-31629,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 08:22:50 UTC 2023,,,,,,,,,,"0|z1ghnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 12:18;luoyuxia;Just as the code said, we can remove it:
{code:java}
public ResultFetcher executeStatement(OperationHandle handle, String statement) {
    // Instantiate the TableEnvironment lazily
    // TODO: remove the usage of the context classloader until {@link
    // HiveParserUtils}#getFunctionInfo use ResourceManager explicitly.
    try (TemporaryClassLoaderContext ignored =
            TemporaryClassLoaderContext.of(
                    sessionContext.getSessionState().resourceManager.getUserClassLoader())) {
        TableEnvironmentInternal tableEnv = getTableEnvironment();
        List<Operation> parsedOperations = tableEnv.getParser().parse(statement);
        if (parsedOperations.size() > 1) {
            throw new UnsupportedOperationException(
                    ""Unsupported SQL statement! Execute statement only accepts a single SQL statement or ""
                            + ""multiple 'INSERT INTO' statements wrapped in a 'STATEMENT SET' block."");
        }
        Operation op = parsedOperations.get(0);
        return sessionContext.isStatementSetState()
                ? executeOperationInStatementSetState(tableEnv, handle, op)
                : executeOperation(tableEnv, handle, op);
    }
} {code};;;","17/Mar/23 07:14;luoyuxia;Anyone who is instrested in this one can take it.;;;","29/Mar/23 07:52;Weijie Guo;[~luoyuxia] I'd like to fix this to address the problem of FLINK-31629.;;;","21/Apr/23 03:35;Weijie Guo;master(1.18) via 921267bcd06c586d4fad28bbf37b4532593c9e3c.
release-1.17 via d40d4dd26ac544307583477b6930e7af50330935.;;;","21/Apr/23 08:12;luoyuxia;[~Weijie Guo] Could you please back port to 1.17?;;;","21/Apr/23 08:22;Weijie Guo;Thanks [~luoyuxia] for the remind,  I have re-opened this ticket and created a [backport pull request|https://github.com/apache/flink/pull/22444] to release-1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce write-once hash lookup store,FLINK-31397,13527954,13527914,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,10/Mar/23 09:44,14/Mar/23 03:17,04/Jun/24 20:41,14/Mar/23 03:17,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Introduce interface for lookup changelog producer:
{code:java}
/**
 * A key-value store for lookup, key-value store should be single binary file written once and ready
 * to be used. This factory provide two interfaces:
 *
 * <ul>
 *   <li>Writer: written once to prepare binary file.
 *   <li>Reader: lookup value by key bytes.
 * </ul>
 */
public interface LookupStoreFactory {

    LookupStoreWriter createWriter(File file) throws IOException;

    LookupStoreReader createReader(File file) throws IOException;
}
 {code}
We can convert remote columnar data to local lookup store, and ready to be used to lookup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 03:17:04 UTC 2023,,,,,,,,,,"0|z1ghkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 03:17;lzljs3620320;master: 0f5744a92bc52f52bc0f6644dc063dcb60fe326c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occasional inaccurate timeout time calculation with System.nanotime in batch read buffer pool,FLINK-31396,13527942,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,10/Mar/23 08:29,10/Mar/23 14:40,04/Jun/24 20:41,10/Mar/23 14:40,1.18.0,,,,,,,,,,,1.18.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"When running TPC-DS tests, I encountered the read buffer request timeout because of configuring too less read buffers. But I found the timeout time may be less than 5m occasionally, 5m is the expected time. 
I read the docs of System.nanotime, the docs say that  t1 < t0 should not be used, because of the possibility of numerical overflow. I tested the System.currentTimeMillis and it can work as expected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 14:40:52 UTC 2023,,,,,,,,,,"0|z1ghhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 14:40;Weijie Guo;master(1.18) via 49929f844975418850ec0334adf4d06c7c895d08.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractPartitionDiscoverer.discoverPartitions calls remove on immutable collection,FLINK-31395,13527934,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,10/Mar/23 07:23,16/Oct/23 07:31,04/Jun/24 20:41,16/Oct/23 07:31,1.16.1,1.17.1,,,,,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47009&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8459

{{FlinkKafkaConsumerBaseTest.testClosePartitionDiscovererWithCancellation}} failed because of that.

{code}
[...]
Mar 10 01:48:27 Caused by: java.lang.RuntimeException: java.lang.UnsupportedOperationException
Mar 10 01:48:27 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.runWithPartitionDiscovery(FlinkKafkaConsumerBase.java:846)
Mar 10 01:48:27 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:828)
Mar 10 01:48:27 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.lambda$testNormalConsumerLifecycle$9(FlinkKafkaConsumerBaseTest.java:695)
Mar 10 01:48:27 	at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:49)
Mar 10 01:48:27 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
Mar 10 01:48:27 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
Mar 10 01:48:27 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 10 01:48:27 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 10 01:48:27 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 10 01:48:27 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 10 01:48:27 Caused by: java.lang.UnsupportedOperationException
Mar 10 01:48:27 	at java.util.Collections$1.remove(Collections.java:4686)
Mar 10 01:48:27 	at org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.discoverPartitions(AbstractPartitionDiscoverer.java:165)
Mar 10 01:48:27 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.lambda$createAndStartDiscoveryLoop$2(FlinkKafkaConsumerBase.java:880)
Mar 10 01:48:27 	at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 22:35:12 UTC 2023,,,,,,,,,,"0|z1ghg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 10:02;xzw0223;It should be that kafkaConsumer.run() was completed before testKafkaConsumer.close() was executed, which caused this problem, but this problem should be occasional.

 

I think the asynchronous thread should sleep for a while before executing run(), in case the execution of run is completed before close.;;;","10/Mar/23 13:16;mapohl;# Is it a test-code-only issue or does it reveal a problem in production?
# Is this something that was newly introduced due to some backports and, therefore, might affect the 1.17 release?;;;","11/Mar/23 14:02;xzw0223;I guess it's the test code, but I'll keep watching;;;","13/Mar/23 01:30;xzw0223;[~mapohl] I think this is a test bug.;;;","02/Aug/23 07:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51893&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=7407;;;","02/Aug/23 07:47;mapohl;[~xzw0223] did you continue looking into the issue?;;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","24/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix spark jar name in the create release script for table store,FLINK-31394,13527931,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,10/Mar/23 06:54,10/Mar/23 08:14,04/Jun/24 20:41,10/Mar/23 08:14,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 08:14:38 UTC 2023,,,,,,,,,,"0|z1ghfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 08:14;lzljs3620320;master: 54bfd291563e907e26ab8aa2c1584b88688703ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsFileDataManager use an incorrect default timeout,FLINK-31393,13527922,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Mar/23 06:08,12/Mar/23 15:02,04/Jun/24 20:41,12/Mar/23 15:02,1.16.1,1.17.0,,,,,,,,,,1.17.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"For batch shuffle(i.e. hybrid shuffle & sort-merge shuffle), If there is a fierce contention of the batch shuffle read memory, it will throw a {{TimeoutException}} to fail downstream task to release memory. But for hybrid shuffle, It uses an incorrect default timeout(5ms), this will make the job very easy to fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 13:47:52 UTC 2023,,,,,,,,,,"0|z1ghdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/23 13:47;Weijie Guo;master(1.18) via b63c65a10bc76b4dadd6ae17305ee1941b773601.
release-1.17 via 02814d686592950b1b47b152cceca308d805af80.
release-1.16 via a97a50703ed62634baf4e017e9f32bdec163c0fa.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor classes code of full-compaction,FLINK-31392,13527915,13527914,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,10/Mar/23 04:30,11/Mar/23 08:57,04/Jun/24 20:41,11/Mar/23 08:57,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Refactor classes code of full-compaction, this is to prepare some shared codes for lookup changelog producer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 08:57:58 UTC 2023,,,,,,,,,,"0|z1ghbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/23 08:57;lzljs3620320;master: c3e24fa4b0fd052a227296b03f9dfb0521ad833e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce lookup changelog producer,FLINK-31391,13527914,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,10/Mar/23 04:28,19/Mar/23 05:35,04/Jun/24 20:41,19/Mar/23 05:35,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Currently, only full-compaction can produce changelog, some merge-engine must have changelog producing, for example, partial-update and aggregation. But full-compaction is very heavy, write amplification is big huge...

We should introduce a new changelog producer, supports lower latency to produce changelog.

 

POC: https://github.com/apache/flink-table-store/pull/590",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-10 04:28:48.0,,,,,,,,,,"0|z1ghbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the FlinkChangelogModeInferenceProgram by avoiding unnecessary traversals.,FLINK-31390,13527909,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,aitozi,aitozi,10/Mar/23 03:06,14/Sep/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-minor,pull-request-available,,,"We can avoid the unnecessary traversals of the RelNode tree, since we are only interested in the first satisfied plan.

 

FlinkChangelogModeInferenceProgram
{code:java}
    val updateKindTraitVisitor = new SatisfyUpdateKindTraitVisitor(context)
    val finalRoot = requiredUpdateKindTraits.flatMap {
      requiredUpdateKindTrait =>
        updateKindTraitVisitor.visit(rootWithModifyKindSet, requiredUpdateKindTrait)
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 14 22:35:20 UTC 2023,,,,,,,,,,"0|z1ghag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix spark jar name in docs for table store,FLINK-31389,13527901,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,10/Mar/23 01:28,10/Mar/23 04:38,04/Jun/24 20:41,10/Mar/23 04:38,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 04:38:08 UTC 2023,,,,,,,,,,"0|z1gh8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 01:29;zhuangchong;https://github.com/apache/flink-table-store/pull/588;;;","10/Mar/23 04:38;lzljs3620320;master: 0ffa6654b2d64fc65c430e453e656fa68ce74632;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"restart from savepoint fails with ""userVisibleTail should not be larger than offset. This is a bug.""",FLINK-31388,13527889,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,danderson,danderson,09/Mar/23 23:04,09/Mar/23 23:06,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Table SQL / Client,,,,0,,,,,"I took a savepoint, then used 
{code:java}
SET 'execution.savepoint.path' = ...
{code}
to set the savepoint path, and then re-executed the query that had been running before the stop-with-savepoint.

It was not an INSERT INTO job, but rather a ""collect"" job running a SELECT query.

It then failed with
{code:java}
userVisibleTail should not be larger than offset. This is a bug.
{code}

Perhaps there is an unstated requirement that using the sql-client to restart from a savepoint only works with INSERT INTO jobs?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 23:06:49 UTC 2023,,,,,,,,,,"0|z1gh60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 23:06;danderson;[~fsk119] maybe you have an idea about this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired failed with an assertion,FLINK-31387,13527850,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,mapohl,mapohl,09/Mar/23 16:04,13/Sep/23 08:42,04/Jun/24 20:41,10/Mar/23 09:24,1.18.0,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46994&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9253

{code}
Mar 09 14:04:42 [ERROR] org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired  Time elapsed: 0.018 s  <<< FAILURE!
Mar 09 14:04:42 java.lang.AssertionError: 
Mar 09 14:04:42 
Mar 09 14:04:42 Expecting AtomicInteger(0) to have value:
Mar 09 14:04:42   10
Mar 09 14:04:42 but did not.
Mar 09 14:04:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalTimersFromBeingFired(StreamTaskCancellationTest.java:305)
Mar 09 14:04:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired(StreamTaskCancellationTest.java:281)
Mar 09 14:04:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-31370,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 10:32:54 UTC 2023,,,,,,,,,,"0|z1ggxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 09:24;dmvk;master: 0b497c56dbdff0d69a39dcf7048b7aefb5c8b9e4;;;","10/Mar/23 10:32;mapohl;The following test failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47018&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the potential deadlock issue of blocking shuffle,FLINK-31386,13527845,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,09/Mar/23 15:35,23/Apr/23 10:01,04/Jun/24 20:41,10/Mar/23 14:38,1.16.0,1.16.1,,,,,,,,,,1.16.2,1.17.0,,,Runtime / Network,,,,0,pull-request-available,,,,"Currently, the SortMergeResultPartition may allocate more network buffers than the guaranteed size of the LocalBufferPool. As a result, some result partitions may need to wait other result partitions to release the over-allocated network buffers to continue. However, the result partitions which have allocated more than guaranteed buffers relies on the processing of input data to trigger data spilling and buffer recycling. The input data further relies on batch reading buffers used by the SortMergeResultPartitionReadScheduler which may already taken by those blocked result partitions which are waiting for buffers. Then deadlock occurs. We can easily fix this deadlock by reserving the guaranteed buffers on initializing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 23 10:01:35 UTC 2023,,,,,,,,,,"0|z1ggw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 14:38;kevin.cyj;Merged into 1.17 to unblock release. Will pick to master latter.;;;","23/Apr/23 10:01;kevin.cyj;Cherry picked to 1.16 via 4e9516aa855cd5262a8574ecce60768553f0e7cf.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce extended Assertj Matchers for completable futures,FLINK-31385,13527827,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dmvk,dmvk,dmvk,09/Mar/23 14:40,24/Nov/23 06:55,04/Jun/24 20:41,10/Mar/23 17:08,,,,,,,,,,,,1.17.3,1.18.0,,,Tests,,,,0,pull-request-available,,,,"Introduce extended Assertj Matchers for completable futures that don't rely on timeouts.

In general, we want to avoid relying on timeouts in the Flink test suite to get additional context (thread dump) in case something gets stuck.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 06:55:05 UTC 2023,,,,,,,,,,"0|z1ggs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 17:08;dmvk;master: 8424ed2c02e2cc23003919eda53c0fda9e9b353a;;;","24/Nov/23 06:55;yunta;Also picked in release-1.17 2a83c910eee711f8b5f9dd4697de60221f21fb9d for the pick of FLINK-33598.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
If you don't pass a function type to the builders WithSpec method it doesn't error within the Go SDK but the function isn't registered within Flink and doesn't run correctly.,FLINK-31384,13527824,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,pocockn,pocockn,09/Mar/23 14:19,09/Mar/23 14:21,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Stateful Functions,,,,0,,,,,"If you don't pass a function type to the builders `WithSpec` method it doesn't error within the Go SDK but the function isn't registered within Flink and doesn't run correctly. 

You can see the error within the Flink UI but the Go SDK just logs
{code:java}
""registering Stateful Function nil""{code}
And then carries on as usual

I have added a fix here

https://github.com/apache/flink-statefun/pull/325",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-09 14:19:52.0,,,,,,,,,,"0|z1ggrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for documenting additionProperties of the REST API payloads.,FLINK-31383,13527820,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dmvk,dmvk,dmvk,09/Mar/23 13:53,10/May/23 11:20,04/Jun/24 20:41,10/Mar/23 07:11,,,,,,,,,,,,1.18.0,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,,"For implementing the request and response body of the resource requirements endpoint, we need to be able to document ""additionalProperties"" because these payloads have only top-level dynamic properties of the same type.

This affects both classic (HTML docs) and OpenAPI generators.

An example of what we want to be able to document is:
{code:java}
@JsonAnySetter
@JsonAnyGetter
@JsonSerialize(keyUsing = JobVertexIDKeySerializer.class)
@JsonDeserialize(keyUsing = JobVertexIDKeyDeserializer.class)
private final Map<JobVertexID, JobVertexResourceRequirements> jobVertexResourceRequirements;{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 07:11:36 UTC 2023,,,,,,,,,,"0|z1ggqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 07:11;dmvk;master: 84d000cf47b833625fbd5b5f72e48963b3156103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add select query ILIKE predicate support,FLINK-31382,13527807,13526484,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,taoran,taoran,taoran,09/Mar/23 12:17,15/Mar/23 04:11,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,"As FLIP discussed, we will add ILIKE support for select query in order to make consistency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 12:22:51 UTC 2023,,,,,,,,,,"0|z1ggns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:22;taoran;[~jingge]  hi, Jing, can u assign this ticket to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException: Unsupported type when convertTypeToSpec: MAP,FLINK-31381,13527806,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jackylau,jackylau,09/Mar/23 12:17,10/Mar/23 07:27,04/Jun/24 20:41,10/Mar/23 07:27,1.18.0,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"when i fix this https://issues.apache.org/jira/browse/FLINK-31377, and find another bug.

which is not fixed completely https://github.com/apache/flink/pull/18967/files
{code:java}
SELECT array_contains(ARRAY[CAST(null AS MAP<INT, INT>), MAP[1, 2]], MAP[1, 2]); {code}
{code:java}
Caused by: java.lang.UnsupportedOperationException: Unsupported type when convertTypeToSpec: MAP    at org.apache.calcite.sql.type.SqlTypeUtil.convertTypeToSpec(SqlTypeUtil.java:1069)    at org.apache.calcite.sql.type.SqlTypeUtil.convertTypeToSpec(SqlTypeUtil.java:1091)    at org.apache.flink.table.planner.functions.utils.SqlValidatorUtils.castTo(SqlValidatorUtils.java:82)    at org.apache.flink.table.planner.functions.utils.SqlValidatorUtils.adjustTypeForMultisetConstructor(SqlValidatorUtils.java:74)    at org.apache.flink.table.planner.functions.utils.SqlValidatorUtils.adjustTypeForArrayConstructor(SqlValidatorUtils.java:39)    at org.apache.flink.table.planner.functions.sql.SqlArrayConstructor.inferReturnType(SqlArrayConstructor.java:44)    at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:504)    at org.apache.calcite.sql.SqlOperator.deriveType(SqlOperator.java:605)    at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6218)    at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6203)    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:161)    at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1861)    at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1852)    at org.apache.flink.table.planner.functions.inference.CallBindingCallContext$1.get(CallBindingCallContext.java:74)    at org.apache.flink.table.planner.functions.inference.CallBindingCallContext$1.get(CallBindingCallContext.java:69)    at org.apache.flink.table.types.inference.strategies.RootArgumentTypeStrategy.inferArgumentType(RootArgumentTypeStrategy.java:58)    at org.apache.flink.table.types.inference.strategies.SequenceInputTypeStrategy.inferInputTypes(SequenceInputTypeStrategy.java:76)    at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandInference.inferOperandTypesOrError(TypeInferenceOperandInference.java:91)    at org.apache.flink.table. {code}",,,,,,,,,,,,,,,,,,,,,,FLINK-27438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 12:33:45 UTC 2023,,,,,,,,,,"0|z1ggnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:33;Sergey Nuyanzin;That seems to be a duplicate of https://issues.apache.org/jira/browse/FLINK-27438;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support enhanced show catalogs syntax,FLINK-31380,13527805,13526484,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,taoran,taoran,taoran,09/Mar/23 12:14,12/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,stale-assigned,,,"As FLIP discussed. We will support new syntax for some show operations.

To avoid bloat, this ticket supports ShowCatalogs.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:06 UTC 2023,,,,,,,,,,"0|hzzvf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:19;taoran;[~jark] Hi, Jark, please assign this ticket to me. thanks. 
btw. as FLIP discussed, we will add some describe operations and support select with ILIKE predicate. I created sub tickets to prevent too much bloat with ShowOperations.;;;","15/Mar/23 04:20;taoran;show catalogs;
+-----------------+
|   catalog name|

+-----------------+
|       catalog1|
|       catalog2|
|default_catalog|

+-----------------+
3 rows in set

show catalogs like '%log1';
– show catalogs ilike '%log1';
– show catalogs ilike '%LOG1';
+--------------+
|catalog name|

+--------------+
|    catalog1|

+--------------+
1 row in set;;;","15/Mar/23 04:22;taoran;[~jark]  Hi, Jark. I added a lot of test cases and did integration testing for a while. 
Can u help me to review this PR?;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers runs into timeout,FLINK-31379,13527803,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,09/Mar/23 12:06,11/Aug/23 10:24,04/Jun/24 20:41,11/Aug/23 10:24,1.16.1,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,stale-major,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46843&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9655

{code}
Mar 06 12:16:45 ""ForkJoinPool-51-worker-25"" #645 daemon prio=5 os_prio=0 tid=0x00007fe20f633000 nid=0xdd4 waiting on condition [0x00007fe0342c5000]
Mar 06 12:16:45    java.lang.Thread.State: WAITING (parking)
Mar 06 12:16:45 	at sun.misc.Unsafe.park(Native Method)
Mar 06 12:16:45 	- parking to wait for  <0x00000000d213d1f8> (a java.util.concurrent.CompletableFuture$Signaller)
Mar 06 12:16:45 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Mar 06 12:16:45 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Mar 06 12:16:45 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Mar 06 12:16:45 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Mar 06 12:16:45 	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
Mar 06 12:16:45 	at org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers(ZooKeeperMultipleComponentLeaderElectionDriverTest.java:256)
Mar 06 12:16:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 10:24:17 UTC 2023,,,,,,,,,,"0|z1ggmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:07;mapohl;It's the same issue that's observed in FLINK-28078. We might need to harden the test or finally upgrade to curator 5.4.0;;;","09/Mar/23 12:08;mapohl;Another workaround to cover the issue which is closer to reality is that we use separate client for each LeaderElectionService. This will avoid using the same event queue and breaks the strict orderness of events between different LeaderElectionService instances.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","11/Aug/23 10:24;mapohl;I'm gonna close this one. Curator 5.4.0 is added as part of the flink-shaded 17.0 work that was added to Flink 1.18 (FLINK-30772). The error doesn't appear that often (it could still appear in the release-1.17 branch).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation fails to build due to lack of package,FLINK-31378,13527794,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Wencong Liu,loserwang1024,loserwang1024,09/Mar/23 11:26,13/Mar/23 07:19,04/Jun/24 20:41,13/Mar/23 07:18,1.18.0,,,,,,,,,,,1.18.0,,,,Documentation,,,,1,pull-request-available,,,,"In [Project Configuration Section|[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging],] it shows that ""If you want to run your job by simply executing the main class, you will need {{flink-runtime}} in your classpath"". 

However, when I just add flink-runtime in my classPath, an error is thrown like this:""
No ExecutorFactory found to execute the application"".

It seems that flink-clients is also needed to supply an excutor through Java Service Load.

Could you please add this in official article for beginners like me?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 03:19;loserwang1024;image-2023-03-10-11-19-35-773.png;https://issues.apache.org/jira/secure/attachment/13056231/image-2023-03-10-11-19-35-773.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 07:18:55 UTC 2023,,,,,,,,,,"0|z1ggkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:07;Wencong Liu;Hello [~loserwang1024] , the links in your proposal may be not correct? Could you please give the correct link?;;;","10/Mar/23 01:43;loserwang1024;[~Wencong Liu], the link is here: https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging;;;","10/Mar/23 02:56;Wencong Liu;The flink-streaming-java module does not have a dependency on flink-clients module.

[FLINK-15090] Reverse the dependency from flink-streaming-java to flink-client - ASF JIRA (apache.org)

I think the docs should be modified, WDYT? cc [~Weijie Guo] ;;;","10/Mar/23 02:57;xzw0223;I think you should use the table api.

please see this sentence
 *_In case of Table API programs, you will also need {{flink-table-runtime}} and {{{}flink-table-planner-loader{}}}._*

 ;;;","10/Mar/23 03:19;loserwang1024;[~xzw0223] , I have already tried table api without flink-clients. It shows that same error is still thrown.

!image-2023-03-10-11-19-35-773.png!;;;","10/Mar/23 03:28;xzw0223;I think you can upload your pom and describe more details, it will be easier for me to reproduce the problem.;;;","10/Mar/23 03:43;loserwang1024;Ok, [~xzw0223] , you just write any table api programs or stream programs (flink version is 1.16.0) without 
flink-clients dependency, then run by simply executing the main class rather than flink cluster. The problem can reproduce. 
In fact, it's not a problem or bug , just information lacks in official tutorials.;;;","10/Mar/23 03:49;Weijie Guo;If you want to run flink job in the IDE, the dependency of {{flink-clients}} need to be included in your pom.xml. In case of Table API programs, you will also need {{flink-table-runtime}} and {{{}flink-table-planner-loader{}}}. 

In general, we should mark these dependencies to `provided` scope. As a result, to make the applications run within IntelliJ IDEA, it is necessary to tick the {{Include dependencies with ""Provided"" scope}} box in the run configuration. If this option is not available (possibly due to using an older IntelliJ IDEA version), then a workaround is to create a test that calls the application’s {{main()}} method.;;;","10/Mar/23 03:56;xzw0223;I think there is no problem with the documentation, and I can execute it according to the documentation;;;","10/Mar/23 04:00;loserwang1024;[~Weijie Guo] ,if add this in corresponding documentation, it's will be better for beginners like me.;;;","10/Mar/23 04:10;Weijie Guo;[~loserwang1024] Yes, add document for beginners sounds good to me. But before starting this work, we should fix the error in `https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging` first.;;;","10/Mar/23 04:14;Wencong Liu;[~loserwang1024] | have a fix for this document. If you like, you can participate in the review together.;;;","10/Mar/23 04:17;loserwang1024;[~Wencong Liu] , Of course, I’d like to participate in it.;;;","10/Mar/23 05:52;Weijie Guo;[~xzw0223] What version of Flink do you use? I think {{flink-clients}} is necessary for local execution after FLINK-15090.
In addition, why do you think that the {{flink-runtime}} dependency needs to be included in the user's pom, doesn't it need to be modified?;;;","10/Mar/23 05:59;xzw0223;[~Weijie Guo] Sorry, I made a mistake,the test introduces the clients dependency.;;;","10/Mar/23 06:03;Weijie Guo;[~xzw0223] Never mind. If you like, you can participate in the review together.;;;","10/Mar/23 06:04;xzw0223;[~Weijie Guo] No problem.;;;","13/Mar/23 07:18;Weijie Guo;master(1.18) via 231df6276f172434e81a18ad4234557b2d1d711a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryArrayData getArray/getMap should Handle null correctly AssertionError: valueArraySize (-6) should >= 0 ,FLINK-31377,13527765,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jackylau,jackylau,09/Mar/23 08:48,02/Sep/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,"you can reproduce this error below. and reason is in ARRAY_CONTAINS
{code:java}
if the needle is a Map NOT NULL,and the array has null element.

this bellowing will cause getElementOrNull(ArrayData array, int pos) only can handle not null. so it throw exception
/*elementGetter = ArrayData.createElementGetter(needleDataType.getLogicalType());*/,

{code}
 
{code:java}
// code placeholder
Stream<TestSetSpec> getTestSetSpecs() {
    return Stream.of(
            TestSetSpec.forFunction(BuiltInFunctionDefinitions.ARRAY_CONTAINS)
                    .onFieldsWithData(
                            new Map[] {
                                null,
                                CollectionUtil.map(entry(1, ""a""), entry(2, ""b"")),
                                CollectionUtil.map(entry(3, ""c""), entry(4, ""d"")),
                            },
                            null)
                    .andDataTypes(
                            DataTypes.ARRAY(DataTypes.MAP(DataTypes.INT(), DataTypes.STRING())),
                            DataTypes.STRING())
                    .testResult(
                            $(""f0"").arrayContains(
                                            CollectionUtil.map(entry(3, ""c""), entry(4, ""d""))),
                            ""ARRAY_CONTAINS(f0, MAP[3, 'c', 4, 'd'])"",
                            true,
                            DataTypes.BOOLEAN()));
}

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 02 22:35:18 UTC 2023,,,,,,,,,,"0|z1ggeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 11:14;jackylau;hi [~twalthr] ,this is a bug from array_contains, when i develop array_remove i found.
{code:java}
ArrayData.createElementGetter(needleDataType.getLogicalType()) {code}
when the needle is MAP NOT NULL. then ArrayData.createElementGetter will not process null in arrays. and will throw expcetion

 ;;;","09/Mar/23 12:18;jackylau;and i find another problem here https://issues.apache.org/jira/browse/FLINK-31381 ;;;","09/Mar/23 12:50;jackylau;hi [~twalthr] the fix is simple. i guess you accidentally put the needle type, but should element type;;;","09/Mar/23 13:16;twalthr;Thanks for reporting this [~jackylau]. Actually it should not make a difference if element or needle data type are used. The important piece is that the input type strategy has inserted casts to the needle, such that needle and element type are identical.;;;","10/Mar/23 02:38;jackylau;hi [~twalthr] , i don't think needle and element type are identical. because "" a NOT NULL type can be stored in NULL type but not vice versa."" and after dig the code TypeInferenceOperandChecker.insertImplicitCasts, you can see here supportsAvoidingCast.;;;","10/Mar/23 03:10;jackylau;hi [~snuyanzin] , array_contains have another bug, could you also have a look. ;;;","10/Mar/23 07:25;Sergey Nuyanzin;is there a way to reproduce it with sql or any other end-to-end case?
I'm asking since I suspect it will be blocked by https://issues.apache.org/jira/browse/FLINK-27438

;;;","10/Mar/23 07:31;jackylau;[~Sergey Nuyanzin] this unit test code can reproduce it 
{code:java}
// code placeholder
Stream<TestSetSpec> getTestSetSpecs() {
    return Stream.of(
            TestSetSpec.forFunction(BuiltInFunctionDefinitions.ARRAY_CONTAINS)
                    .onFieldsWithData(
                            new Map[] {
                                null,
                                CollectionUtil.map(entry(1, ""a""), entry(2, ""b"")),
                                CollectionUtil.map(entry(3, ""c""), entry(4, ""d"")),
                            },
                            null)
                    .andDataTypes(
                            DataTypes.ARRAY(DataTypes.MAP(DataTypes.INT(), DataTypes.STRING())),
                            DataTypes.STRING())
                    .testResult(
                            $(""f0"").arrayContains(
                                            CollectionUtil.map(entry(3, ""c""), entry(4, ""d""))),
                            ""ARRAY_CONTAINS(f0, MAP[3, 'c', 4, 'd'])"",
                            true,
                            DataTypes.BOOLEAN()));
} {code};;;","10/Mar/23 07:32;jackylau;this https://issues.apache.org/jira/browse/FLINK-27438 just a pure sql level, which is not blocked.;;;","10/Mar/23 07:57;Sergey Nuyanzin;yes i saw the unittest from the description, that's why I was asking namely end-to-end case...
I worry that without fixing https://issues.apache.org/jira/browse/FLINK-27438 it brings no benefit to the end user;;;","10/Mar/23 08:17;jackylau;[~Sergey Nuyanzin] but it impacts on table api and python api. and array_contains example not right logic will impact others to do other function.;;;","26/Jun/23 14:45;jackylau;hi [~Sergey Nuyanzin] [~twalthr] ，does anyone help review the pr again?;;;","25/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","02/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSVReader for streaming does not support splittable,FLINK-31376,13527754,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ram_krish,ram_krish,09/Mar/23 07:40,09/Mar/23 07:40,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"Using CsvReaderFormat, when we create the StreamFormatAdapter it will not support 'splittable'. This task is targetted towards supporting file splits while we create the FileSource over a CSV file. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-09 07:40:52.0,,,,,,,,,,"0|z1ggc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"the values of map<string,string> are truncated by the CASE WHEN function.",FLINK-31375,13527745,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jeff-zou,jeff-zou,09/Mar/23 06:43,23/Mar/23 06:46,04/Jun/24 20:41,,1.15.1,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,0,,,,,"the values of map<string,string> are truncated by the CASE WHEN function.
{code:java}
// sql
create table test (a map<varchar, string>) with ('connector'='print');
insert into test  select * from (values(case when true then map['test','123456789'] else map ['msg_code','0', 'msg_reason', 'abc'] end));{code}
the result:
{code:java}
+I[{test=123}] {code}
We hope the value of result is '123456789', but I get '123', the length is limited by 'abc'.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/23 06:20;qingyue;image-2023-03-22-14-20-18-575.png;https://issues.apache.org/jira/secure/attachment/13056563/image-2023-03-22-14-20-18-575.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 06:46:41 UTC 2023,,,,,,,,,,"0|z1gga0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 02:49;huwh;[~jeff-zou] I was unable to reproduce this in my local environment with the code of master branch.;;;","22/Mar/23 07:10;qingyue;Hi [~jeff-zou], thanks for reporting this issue. I've reproduced it using Flink-1.15.1, 1.15.2, and 1.16.1

!image-2023-03-22-14-20-18-575.png|width=616,height=214!

This is a bug caused by [CALCITE-4603|https://issues.apache.org/jira/browse/CALCITE-4603], and has been fixed in Calcite 1.27.0

As a workaround, you can try manually casting 'abc' as varchar like the following.
{code:sql}
select * from (values(case when true then map['test','123456789'] else map ['msg_code','0', 'msg_reason', cast('abc' as string)] end)); {code}

As release-1.17 has upgraded to Calcite 1.29.0, this issue has been fixed.;;;","23/Mar/23 06:46;jeff-zou;Great，it's ok to use this method, thank u.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProxyStreamPartitioner should implement ConfigurableStreamPartitioner,FLINK-31374,13527711,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,zhangzp,zhangzp,08/Mar/23 23:13,13/Apr/23 03:29,04/Jun/24 20:41,12/Apr/23 06:57,ml-2.2.0,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"In flink-ml-iterations module, we use ProxyStreamPartitioner to wrap StreamPartitioner to deal with records in iterations.

 

However, it did not implement ConfigurableStreamPartitioner interface. Thus that maxParallelism information is lost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 06:57:08 UTC 2023,,,,,,,,,,"0|z1gg2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 06:57;zhangzp;Resolved on master via 990337bf5a0a23b08be85c475043a047703772c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerRoundWrapperOperator should carry epoch information in watermark,FLINK-31373,13527710,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhangzp,zhangzp,08/Mar/23 23:06,19/Aug/23 10:35,04/Jun/24 20:41,,ml-2.2.0,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,auto-deprioritized-major,pull-request-available,,,"Currently we use PerRoundWrapperOperator to wrap the normal flink operators such that they can be used in iterations.

We already contained the epoch information in each record so that we know which iteration each record belongs to.

However, there is no epoch information when the stream element is a watermark. This works in most cases, but fail to address the following use case:
 - In DataStreamUtils#withBroadcast, we will cache the elements (including watermarks) from non-broadcast inputs until the broadcast variables are ready. When the broadcast variables are ready, once we receive a stream element we will process the cached elements first. If the received element is a watermark, the current implementation of iteration module fails (ProxyOutput#collect throws NPE) since there is no epoch  information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:07 UTC 2023,,,,,,,,,,"0|z1gg28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 07:10;zhangzp;As discussed with [~gaoyunhaii] offline, we agree that the watermark is not correctly processed in iteration module.

 

To avoid the above cases for now, we plan to add a java doc to explain that `flink-m-iteration` module cannot deal with watermarks correctly. We will leave it as a TODO here.;;;","20/Apr/23 08:50;Jiang Xin;[~zhangzp] I met the same issue with the following code.
{code:java}
@Test
public void testBoundedIterationWithEndInput() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);
    env.getConfig().enableObjectReuse();

    DataStream<Integer> inputStream = env.fromElements(1, 2, 3);

    DataStreamList outputs =
            Iterations.iterateBoundedStreamsUntilTermination(
                    DataStreamList.of(inputStream),
                    ReplayableDataStreamList.replay(inputStream),
                    IterationConfig.newBuilder().build(),
                    (variableStreams, dataStreams) -> {
                        DataStream<Integer> variables = variableStreams.get(0);
                        DataStream<Integer> result =
                                dataStreams
                                        .<Integer>get(0)
                                        .transform(
                                                ""sum"",
                                                BasicTypeInfo.INT_TYPE_INFO,
                                                new SumOperator());
                        return new IterationBodyResult(
                                DataStreamList.of(variables),
                                DataStreamList.of(result),
                                variables.flatMap(new TerminateOnMaxIter<>(10)));
                    });
    List<Integer> result = IteratorUtils.toList(outputs.get(0).executeAndCollect());
    result.forEach(r -> r.equals(60));
}

private static class SumOperator extends AbstractStreamOperator<Integer>
        implements OneInputStreamOperator<Integer, Integer>, BoundedOneInput {

    private int sum = 0;

    @Override
    public void processElement(StreamRecord<Integer> element) {
        sum += element.getValue();
    }

    @Override
    public void endInput() {
        output.collect(new StreamRecord<>(sum));
    }

    @Override
    public void finish() {
        output.collect(new StreamRecord<>(sum));
    }

    @Override
    public void close() {
        output.collect(new StreamRecord<>(sum));
    }
} {code};;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory Leak in prometheus HTTPMetricHandler when reporting fails,FLINK-31372,13527676,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kdziolak,kdziolak,08/Mar/23 17:29,24/Apr/23 12:20,04/Jun/24 20:41,,1.15.4,1.16.1,1.17.1,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"We've identified a memory leak, that occurs when any of the metric reporters fail with an exception. In such cases HTTPExchanges are not  getting closed properly in io.prometheus.client.exporter.HTTPServer.HTTPMetricHandler

In our case the failure was triggered by usage of incompatible Kafka Client failing metric collection with:

{{Exception in thread ""prometheus-http-1-72873"" java.lang.NoSuchMethodError: 'double org.apache.kafka.common.Metric.value()'}}

Should Prometheus Reporter handle metric collection defensively (by suppressing exceptions) to guarantee metric delivery and avoid similar memory leaks?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-08 17:29:43.0,,,,,,,,,,"0|z1gfuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support subscribing non-existed topics in Pulsar source,FLINK-31371,13527674,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,syhily,enzodechaene,enzodechaene,08/Mar/23 17:19,15/Nov/23 22:15,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,Connectors / Pulsar,,,,1,,,,,"*Describe the bug*
With a Pulsar 2.8.4 server, a Flink stream containing Pulsar sources or sinks will fail at startup if the topic doesn't exist.

 

*To Reproduce*
Create a stream with :
 * Flink 1.15.2
 * Pulsar 2.8.4
 * with a Pulsar source or sink linked to a non existant topic
 * Start the stream

 

*Expected behavior*
If the topic doesn't exist, it should be created at the first connection of the source or sink without error.

 

*Additional context*
In the TopicListSubscriber class of the connector, the method getSubscribedTopicPartitions() try to get the metadata of a topic by doing that :
 
{code:java}
TopicMetadata metadata = queryTopicMetadata(pulsarAdmin, topic);{code}
 

If the topic doesn't exist, I get a NullPointerException on the metadata

We created a previous [ticket|https://github.com/streamnative/pulsar-flink/issues/366] on the Pulsar connector and it was fixed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 06 19:05:19 UTC 2023,,,,,,,,,,"0|z1gfu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 08:47;martijnvisser;Downgraded priority per Flink' Jira process https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process;;;","13/Mar/23 17:52;syhily;Hi [~enzodechaene], the old fix in https://github.com/streamnative/pulsar-flink/pull/369/files is quite hack. I think we can add auto creation logic only if you have enable the related config option in Pulsar broker. Otherwise, we will hang on querying the topic metadata. WDT?;;;","13/Mar/23 17:53;syhily;[~guoweijie] Can you assign this ticket to me?;;;","13/Mar/23 20:06;enzodechaene;Hi [~syhily], yes, it makes sense to do it that way because by default the pulsar broker parameter *allowAutoTopicCreation* is set to true.

Thanks for the answer;;;","14/Mar/23 01:29;syhily;Can we change this JIRA into a feature request with a new title: ""Support subscribing non-existed topics in Pulsar source""?;;;","14/Mar/23 13:02;enzodechaene;This message also appears for the Pulsar Sink, is it possible to make the change for the Sink in this feature ? [~syhily];;;","20/Mar/23 04:16;syhily;[~enzodechaene] Support non-existed topic on Pulsar sink has been support, we will release it in the upcoming release.;;;","06/Oct/23 19:05;enzodechaene;Hi [~syhily], any news for this ticket ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancellation of the StreamTask should prevent more timers from being fired,FLINK-31370,13527643,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,08/Mar/23 13:54,09/Mar/23 17:36,04/Jun/24 20:41,09/Mar/23 13:10,,,,,,,,,,,,1.18.0,,,,Runtime / Task,,,,0,pull-request-available,,,,"If the task is canceled while the watermark progresses, it may be stuck in the Cancelling state for a long time (e.g., when many windows are firing). This is closely related to FLINK-20217, which might bring a more robust solution for checkpoint and cancellation code paths.

As a stopgap solution, we'll introduce a check allowing InternalTimerService to break out of the firing loop if the StreamTask has been marked as canceled.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31387,,,,FLINK-20217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 13:09:21 UTC 2023,,,,,,,,,,"0|z1gfnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 13:09;dmvk;master: 03e4f8eb13d8263a4a1f4947f5d1cb55a3d368e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden modifiers for sql-gateway module,FLINK-31369,13527628,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Mar/23 11:36,13/Mar/23 22:41,04/Jun/24 20:41,13/Mar/23 22:41,,,,,,,,,,,,1.18.0,,,,Table SQL / Gateway,Tests,,,0,pull-request-available,,,,This is a follow up jira issue for https://github.com/apache/flink/pull/22127#discussion_r1129192778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 22:41:51 UTC 2023,,,,,,,,,,"0|z1gfk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 22:41;Sergey Nuyanzin;Merged at: [20dc237f1b6eb32ef5344b5ece0a1e3a008e8bfd|https://github.com/apache/flink/commit/20dc237f1b6eb32ef5344b5ece0a1e3a008e8bfd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move operation execution logic out from TableEnvironmentImpl,FLINK-31368,13527609,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jark,jark,jark,08/Mar/23 09:15,15/Mar/23 09:26,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Currently, {{TableEnvironmentImpl}} is a bit bloated. The implementation of {{TableEnvironmentImpl}} is filled with lots of operation execution logic which makes the class hard to read and maintain. Once you want to add/update an operation, you have to touch the {{TableEnvironmentImpl}}, which is unnecessary and not clean. 

An improvement idea is to extract the operation execution logic (mainly the command operation, which doesn't trigger a Flink job) out from {{TableEnvironmentImpl}} and put it close to the corresponding operation. This is how Spark does with {{RunnableCommand}} and {{V2CommandExec}}. In this way, we only need to add a new class of {{Operation}} without modifying {{TableEnvironmentImpl}} to support a new command.

This is just an internal refactoring that doesn't affect user APIs and is backward-compatible. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 09:26:40 UTC 2023,,,,,,,,,,"0|z1gffs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 09:17;jark;cc [~lincoln.86xy] [~twalthr], what do you think about this? ;;;","08/Mar/23 09:49;zjureel;Hi [~jark] +1 for this proposal. In fact, there are similar problems with the interaction between the sql-gateway and the table environment. Currently sql-gateway parse sql statement to `Operation`s and do different operations in `OperationExecutor` while the similar things are in `TableEnvironmentImpl.executeInternal`. Sometimes I wonder whether the new Operation should be placed in the sql-gateway or `TableEnvironmentImpl`.;;;","08/Mar/23 10:05;jark;Hi [~zjureel], this is another topic. I think if an Operation can be supported in TableEnvironment, we should support it there and call it in SQL Gateway. However, I can see there are some SQL Gateway specific operations or additional work required, I think it's fine to handle the operation in SQL Gateway. ;;;","08/Mar/23 12:30;lincoln.86xy;[~jark] +1 for this refactoring, it will prevent the `TableEnvironmentImpl` from becoming even more bloated and provide benefits for readability and code maintenance.;;;","09/Mar/23 09:27;twalthr;Big +1 on this. We cleaned TableEnvironment during the Blink merge but with every release it grew in size again.

Whenever there is a long list of {{else if (x instanceof ClassX)}} patterns, the implementation should be improved. Ideally there should be just a class like:

{code}
OperationExecutor<O extends Operation> {
  Class<O> supportedOperation();
  Tuple2<Header, List<Row>> apply(O);
}

ShowTablesExecutor<ShowTablesOperation> extends AbstractOperationExecutor {

  ShowTablesExecutor() {
    super(ShowTablesOperation.class);
  }

}
{code}

And a map that lists all executors:
{code}
Map<Class, OperationExecutor<?>> executors = new HashMap<>;
static {
  add(ShowTablesExecutor.class);
}

add(Class c) {
  var i = c.newInstance()
  executors.put(i.supportedOperation(), i);
}
{code};;;","09/Mar/23 15:10;jark;[~twalthr], thank you for your idea. Another idea is colocating the execution logic within the Operation, just like how RunnableCommand and V2CommandExec do in Spark. We can introduce a class like: 

{code:java}
public interface ExecutableOperation {

    TableResultInternal execute(Context ctx);

    interface Context {
        CatalogManager getCatalogManager();

        FunctionCatalog getFunctionCatalog();

        ResourceManager getResourceManager();

        Configuration getConfiguration();
    }
}
{code}

Many base interfaces can extends it ({{AlterOperation}}, {{CreateOperation}}, {{DropOperation}}, etc.). This approach improves code readability (not spread code across different classes) and makes supporting a new statement by just adding an Operation class instead of 3 classes (Operation class, Executor class, and the mapping class). 

What do you think about this?;;;","13/Mar/23 13:10;twalthr;I like the {{ExecutableOperation}} interface. However, a mapping class will be unavoidable as far as I can see. So we can combine our two approaches into one. In any case, we should make sure that downstream implementers extending {{TableEnvironment}} can easily prohibit certain operations or change the behavior slightly.;;;","14/Mar/23 09:50;jark;Thank you all. I will create sub-tasks and work on it. ;;;","15/Mar/23 02:17;luoyuxia;[~jark] Also noticed `SqlToOperationConverter` has a similiar problem. Will it be reasonal that we also refactor it with creating another jira?;;;","15/Mar/23 04:25;jark;[~luoyuxia], you are right. I think we can create another umbrella issue for this. ;;;","15/Mar/23 09:26;luoyuxia;[~jark] I have created FLINK-31464 for `SqlToOperationConverter`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support filter's function push down,FLINK-31367,13527603,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanjialiang,tanjialiang,08/Mar/23 08:19,08/Mar/23 08:19,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Connectors / JDBC,Table SQL / API,,,0,,,,,"Hi teams, as far as i known, source ability support simply filter push down, it may be just push down constant value like this:
{code:java}
CREATE TABLE student (
   id int,
   brithday string
) WITH (
   ...
);

# it can support push down its filter if connector implements SupportsFilterPushDown
# id and birthday will be push down
SELECT * FROM student WHERE id = 1 AND birthday = '1997-04-13';{code}
But it will not push down like this:
{code:java}
CREATE TABLE student (
   id int,
   brithday string
) WITH (
   ...
);

# it will not support push down its filter though connector implements SupportsFilterPushDown
# id and birthday will not push down, so it will be filter in flink task
SELECT * FROM student WHERE id = 1 AND birthday = TO_DATE('1997-04-13 00:00:00');{code}
 

Can we get the flink function in SupportsFilterPushDown, so we can adapt the flink function in every connector?

For example, I can adapt the Flink function TO_DATE to mysql's function STR_TO_DATE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-08 08:19:53.0,,,,,,,,,,"0|z1gfeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception is thrown when s3a and s3p are used together,FLINK-31366,13527599,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,masteryhx,masteryhx,08/Mar/23 08:00,12/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,FileSystems,,,,0,pull-request-available,stale-assigned,,,"h3. Exception

When s3a and s3p plugins exist at the same time, an exception will be thrown as below:
{code:java}
Caused by:java.lang.NoSuchMethodError: org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.<init> (Ljava/lang/String;Lorg/apache/flink/fs/s3presto/common/HadoopConfigLoader;)
{code}
h3. Why

In the construction method of AbstractS3FileSystemFactory, s3a shades HadoopConfigLoader into org.apache.flink.fs.s3hadoop.common.HadoopConfigLoader, and s3p shades HadoopConfigLoader into org. apache.flink.fs.s3presto.common.HadoopConfigLoader when package.

(see shade-plugin)

 

But the AbstractS3FileSystemFactory class will only be loaded once when loading even if there are two plugins.

So it may first uses s3a plugin to load AbstractS3FileSystemFactory, and the construction method is loaded into AbstractS3FileSystemFactory(String name, org.apache. flink.fs.s3hadoop.common.HadoopConfigLoader hadoopConfigLoader), at this time s3p will be abnormal when it is constructed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:06 UTC 2023,,,,,,,,,,"0|z1gfdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 08:47;yunta;I wonder will it break the backward compatibility with your current implementation?;;;","08/Mar/23 09:10;masteryhx;Thanks for the reminder.
It may break in one case: User's s3-plugin extended from s3a or s3p.

In this case, Users also need to relocate their AbstractS3FileSystemFactory otherwise the self-defined s3-plugin may not work. It may need a doc to clarify this.

Or Do you have other better suggestions?;;;","08/Mar/23 09:28;martijnvisser;TBH I'm a bit surprised, as far as I'm aware there are a lot of users that use both plugins at the same time (Presto for checkpointing, Hadoop for file reading) without problems. Are there any specific situations in your case why this breaks for you?;;;","08/Mar/23 10:21;masteryhx;[~martijnvisser] Thanks for the feedback.
I just packaged two plugins and put them into the plugin directory. 
When I started the job, the exception was thrown.

Maybe I missed something. Are there any misunderstaning in my anlysis ?;;;","08/Mar/23 10:41;martijnvisser;[~masteryhx] What do you mean with packaged? You've downloaded them both and put each of them in their own folder in the plugins directory, per https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/plugins/ ?;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify FlinkActionsE2ETest#testMergeInto,FLINK-31365,13527573,13527066,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,08/Mar/23 04:18,08/Mar/23 05:52,04/Jun/24 20:41,08/Mar/23 05:52,,,,,,,,,,,,table-store-0.4.0,,,,,,,,0,pull-request-available,,,,Complicated test may cause failure in docker environment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 05:52:09 UTC 2023,,,,,,,,,,"0|z1gf7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 05:52;lzljs3620320;master: 65335ee90aa0b78a0bd8cf18ae351c7696148e23;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink] add metrics for TableStore,FLINK-31364,13527558,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Ming Li,Ming Li,08/Mar/23 03:08,08/Mar/23 11:33,04/Jun/24 20:41,08/Mar/23 11:33,,,,,,,,,,,,,,,,Table Store,,,,0,,,,,"Currently, relevant metrics are missing in {{{}Table Store{}}}, such as split consumption speed, commit information statistics, etc. We can add metrics for real-time monitoring of the {{{}Table Store{}}}.",,,,,,,,,,,,,,,,,,,,,,FLINK-31224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 11:22:52 UTC 2023,,,,,,,,,,"0|z1gf4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 11:22;nicholasjiang;[~Ming Li], this ticket is duplicated by FLINK-31224.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink failed to commit transactions under EXACTLY_ONCE semantics,FLINK-31363,13527557,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,lightzhao,lightzhao,08/Mar/23 02:55,12/Apr/23 17:13,04/Jun/24 20:41,12/Apr/23 17:13,1.16.1,1.17.0,1.18.0,,,,,,,,,kafka-3.0.0,,,,Connectors / Kafka,,,,1,pull-request-available,,,,"When KafkaSink starts Exactly once and no data is written to the topic during a checkpoint, the transaction commit exception is triggered, with the following exception.

[Transiting to fatal error state due to org.apache.kafka.common.errors.InvalidTxnStateException: The producer attempted a transactional operation in an invalid state.]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 02:54;lightzhao;image-2023-03-08-10-54-51-410.png;https://issues.apache.org/jira/secure/attachment/13056134/image-2023-03-08-10-54-51-410.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 17:13:17 UTC 2023,,,,,,,,,,"0|z1gf48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 09:31;martijnvisser;[~lightzhao] Do you mean when no data is written to a topic that's a source for Flink, or a topic that's a sink for Flink? ;;;","08/Mar/23 09:42;lightzhao;[~martijnvisser] when no data is written to a topic that's a sink for Flink.;;;","10/Mar/23 16:01;tzulitai;Thanks for reporting this [~lightzhao], I think this is a valid issue.

Kafka internally doesn't actually consider a transaction started until the first record is sent to a partition, and then that partition is added to a transaction.
So, when we start new transactions after every checkpoint in the KafkaSink via {{{}producer.beginTransaction(){}}}, there's actually no explicit txn request sent to Kafka until the first {{{}producer.send(){}}}.

In other words, I think the following would return an InvalidTxnStateException from Kafka:

 
{code:java}
producer.beginTransaction();
producer.commitTransaction();

// or

producer.beginTransaction();
producer.abortTransaction();{code}
And this can happen if, for example, within a checkpoint no data has been sent to Kafka at all. Which may be the case if e.g. some upstream filtering operator filtered out all records, all the job simply had no data to process because there was no records written to the Kafka source topic.

It is possible to address this by postponing the {{producer.beginTransaction()}} call until the first record after a checkpoint instead of pre-emptively starting a new transaction after the last checkpoint (like we do now), but that's going to add a redundant if-check on the main record processing loop.

Before deciding on anything, I need to check with our Kafka experts to see if this makes sense semantically.
Perhaps a different kind of exception could be returned from Kafka to indicate that the txn wasn't actually started on the server side, and we can then just safely ignore the exception.

 ;;;","10/Mar/23 22:35;tzulitai;Looking at the KafkaProducer code, the {{TransactionManager}} keeps a {{transactionStarted}} flag that is only set when a record has actually been sent to the transaction. On {{commitTransaction()}} / {{abortTransaction()}} API calls on the Java client, if the flag is false, then the client won't actually send a {{EndTxnRequest}} to the brokers. See here: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L789

So:
{code:java}
producer.beginTransaction();
producer.commitTransaction();

// or

producer.beginTransaction();
producer.abortTransaction(); {code}
the above doesn't throw in normal continuous execution.
It only throws if there was job downtime between the {{beginTransction()}} call and the commit/abort call (because the flag would have been cleared)

So - I think the correct way to fix this (until Kafka changes the transaction protocol to properly support Flink's distributed transaction use case) is that we need to additionally persist the {{transactionStarted}} flag in Flink checkpoints as transaction metadata, and then set that appropriately when creating the recovery producer at restore time. ;;;","11/Mar/23 13:19;lightzhao;[~tzulitai] You are right, it is true that data cannot be consumed in READ_COMMITTED mode, and it is indeed impossible to directly change it to 'false'.;;;","16/Mar/23 21:14;tzulitai;[~lightzhao] I'm preparing a fix for this.

I think the correct fix is to *not* add a txn's metadata into the checkpoint if there were no data written to the transaction.
i.e. the checkpoints will only reflect metadata of txns that actually have data that need to be committed.

This way, creation of recovery producers can always safely set the {{transactionStarted}} flag to {{true}};;;","12/Apr/23 17:13;tzulitai;Merged via flink-connector-kafka a5df3c7c7f4925ff70114f862bca7588b819ac21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to Calcite version to 1.33.0,FLINK-31362,13527535,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,aitozi,aitozi,07/Mar/23 23:38,15/Feb/24 16:25,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"In Calcite 1.33.0, C-style escape strings have been supported. We could leverage it to enhance our string literals usage.

issue: https://issues.apache.org/jira/browse/CALCITE-5305
",,,,,,,,,,,,FLINK-34446,,,,,,,,,,,,,FLINK-29319,,,FLINK-27998,FLINK-31836,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 15 16:25:50 UTC 2024,,,,,,,,,,"0|z1gezc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/24 15:33;jingge;Hi folks, would you like to share the reason why we should upgrade to 1.33.0 instead of the most up-to-date 1.36.0?

A critical issue has been found FLINK-34446. We should consider:
 # downgrade to 1.30.0
 # fix the issue in Calcite
 # wait for the next Calcite release
 # upgrade to Calcite e.g. 1.37.0

[~Sergey Nuyanzin] do you have any concerns?;;;","15/Feb/24 15:43;Sergey Nuyanzin;Calcite upgrade itself is pretty complex task that's why it was already discussed and suggested to go version by version rather than big bang

https://issues.apache.org/jira/browse/FLINK-27998?focusedCommentId=17597797&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17597797
this is the main concern

 

I fear we can not just downgrade it easily, there is already functionality merged which depends on Calcite 1.31.0 e.g. FLINK-24024
Also upgrade brought a number of fixes/improvements which will be rollbacked together with downgrade. 
Plans and parser behavior will be changed again, which means it is definitely a no go for minor version
Let's first see how big is the current issue and what we can do about it

I can have a look closer to it in coming days;;;","15/Feb/24 16:25;jingge;Thanks for the clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"job created by sql-client can't authenticate to kafka, can't find org.apache.kafka.common.security.plain.PlainLoginModule",FLINK-31361,13527530,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,danderson,danderson,07/Mar/23 22:55,27/Mar/24 23:52,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"I'm working with this SQL DDL:
{noformat}
CREATE TABLE pageviews_sink (
  `url` STRING,
  `user_id` STRING,
  `browser` STRING,
  `ts` TIMESTAMP_LTZ(3)
) WITH (
  'connector' = 'kafka',
  'topic' = 'pageviews',
  'properties.bootstrap.servers' = 'xxx.confluent.cloud:9092',
  'properties.security.protocol'='SASL_SSL',
  'properties.sasl.mechanism'='PLAIN',
  'properties.sasl.jaas.config'='org.apache.kafka.common.security.plain.PlainLoginModule required username=""xxx"" password=""xxx"";',
  'key.format' = 'json',
  'key.fields' = 'url',
  'value.format' = 'json'
);
{noformat}
With {{flink-sql-connector-kafka-1.16.1.jar}} in the lib directory, this fails with 
{noformat}
Caused by: javax.security.auth.login.LoginException: No LoginModule found for org.apache.kafka.common.security.plain.PlainLoginModule{noformat}
As a workaround I've found that it does work if I provide both
 
{{flink-connector-kafka-1.16.1.jar}}
{{kafka-clients-3.2.3.jar}}
 
in the lib directory. It seems like the relocation applied in the SQL connector isn't working properly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 06:52:29 UTC 2023,,,,,,,,,,"0|z1gey8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 22:57;danderson;I'm using the sql-client to create the job.;;;","08/Mar/23 09:33;martijnvisser;[~danderson] It's because Flink shades the Kafka Client dependencies. That's outlined in https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/datastream/kafka/#security. 

It will work if you use 'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=""key"" password=""value"";';;;","07/Aug/23 06:52;leonard;I checked the error still exits in https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kafka/#security, we can improve the documentation at least.
[~danderson]  Would you like to improve the documentation ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running HsResultPartitionTest repeatedly causes error with 137 exit code indicating a memory leak ,FLINK-31360,13527491,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,mapohl,mapohl,07/Mar/23 17:11,14/Aug/23 14:23,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,stale-assigned,test-stability,,I ran {{HsResultPartitionTest}} repeatedly locally. It caused errors with an 137 exit consistently indicating that there might be some memory leak.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31359,,,,,"09/Mar/23 12:33;mapohl;Screenshot from 2023-03-09 13-33-37.png;https://issues.apache.org/jira/secure/attachment/13056219/Screenshot+from+2023-03-09+13-33-37.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 14:23:38 UTC 2023,,,,,,,,,,"0|z1gepk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 18:30;Weijie Guo;Hi [~mapohl], I ran this test class many times in my own machine, only encountered exit code 239, which is the same problem as CI. The PR linked to this ticket is also to solve this problem. 
But as for exit code 137, I can't reproduce it locally at present(repeatedly ran this test class 5000 times in my IDEA). Could you please provide more detailed reproduction approach? ;;;","09/Mar/23 12:34;mapohl;I ran the test multiple times in Intellij using its repeat-until-failure feature:
 !Screenshot from 2023-03-09 13-33-37.png|height=400! ;;;","30/May/23 09:32;renqs;Downgraded to major as this is not reproducible and never appears on CI;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Aug/23 12:23;mapohl;[~Weijie Guo] did you manage to look into it? I rerun the test and I can still reproduce the error locally with intellij.;;;","14/Aug/23 14:23;Weijie Guo;Sure, I will take some time to take a look. We have not encountered similar issues in the production environment, so this may be related to testing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsResultPartitionTest fails with fatal error,FLINK-31359,13527486,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,mapohl,mapohl,07/Mar/23 16:51,08/Mar/23 09:11,04/Jun/24 20:41,08/Mar/23 02:59,1.18.0,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46910&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8512

{code}
Mar 07 13:20:39 [ERROR] Process Exit Code: 239
Mar 07 13:20:39 [ERROR] Crashed tests:
Mar 07 13:20:39 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-31346,,,,,,,,,FLINK-31360,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 09:11:14 UTC 2023,,,,,,,,,,"0|z1geog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 16:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46911&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8512;;;","07/Mar/23 17:03;mapohl;[~Weijie Guo] it looks like this is related to FLINK-31346? Can you double-check?;;;","07/Mar/23 17:28;Weijie Guo;Thanks [~mapohl] for reporting this. This is only test issue, caused by FLINK-31346. When I prepared to pick it to release-1.17, I found this problem and fixed it. I will open a pull request to fix this also for master branch.;;;","08/Mar/23 02:59;Weijie Guo;master(1.18) via eff9b16799f162f31058be5acac567943a446dea.
This issue only affect master(1.18).;;;","08/Mar/23 09:11;martijnvisser;Also failed in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46927&view=results - That was started before this fix was merged. Adding it here for completeness reasons;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Display consumed split/partition/queue info on Web UI,FLINK-31358,13527449,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Zhanghao Chen,Zhanghao Chen,07/Mar/23 12:33,13/Mar/23 10:46,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,,"Many data sources have the concept of ""split"", which is a partition of the whole data (e.g. partition in Kafka, queue in RocketMQ), and each Flink source task is allocated with a subset of splits to consume. When a job is lagging on only a few splits, it would be useful for determining whether it is a data source issue or a Flink issue if users can view which source task consumes which split on Web UI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 10:46:01 UTC 2023,,,,,,,,,,"0|z1geg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 13:27;martijnvisser;I'm actually not sure that this information is even captured in metrics, which would be a first requirement before being able to display it in the UI;;;","13/Mar/23 10:46;Wencong Liu;I seems a specific design for source subtasks. It's necessary to consider the unified implementation for different operators. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"A record is deleted before being inserted, it will be deleted",FLINK-31357,13527407,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liufangliang,liufangliang,07/Mar/23 08:49,07/Mar/23 09:06,04/Jun/24 20:41,,1.14.3,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"If you have a record in the database and then delete it and add another record with the same data, the data should be added back to the database, but in practice the data may be deleted directly.

This can be replicated in following unit test
{code:java}
org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest#testJdbcOutputFormat{code}
Enter the following data
{code:java}
Tuple2.of(true, new TestEntry(1001, (""More Java""), (""Mohammad Ali""), 11.11, 11))
Tuple2.of(true, new TestEntry(1001, (""More Java for more dummies""), (""Mohammad Ali""), 33.33, 33)) 
Tuple2.of(false, new TestEntry(1001, (""More Java for more dummies""), (""Mohammad Ali""), 33.33, 33))
Tuple2.of(true, new TestEntry(1001, (""More Java for more dummies""), (""Mohammad Ali""), 33.33, 33)) {code}
The following records are expected to remain in the database, but in fact the data with ID 1001 no longer exists in the database
{code:java}
Tuple2.of(true, new TestEntry(1001, (""More Java for more dummies""), (""Mohammad Ali""), 33.33, 33))  {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 09:06:29 UTC 2023,,,,,,,,,,"0|z1ge6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 09:06;liufangliang;Hi [~lzljs3620320] , [~jark] , Looking forward to your reply.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialize garbled characters at checkpoint,FLINK-31356,13527403,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,seckiller,seckiller,07/Mar/23 08:44,20/Mar/23 12:23,04/Jun/24 20:41,,1.13.6,,,,,,,,,,,,,,,API / Type Serialization System,,,,0,,,,," 
{panel:title=The last checkpoint of the program was successful}
2023-03-07 08:33:16,085 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 39126 (type=CHECKPOINT) @ 1678149196059 for job 8b5720a4a40f50b995c97c6fe5b93079.
2023-03-07 08:33:16,918 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 39126 for job 8b5720a4a40f50b995c97c6fe5b93079 (71251394 bytes in 849 ms).
2023-03-07 08:33:16,918 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 39126 as completed for source Source: kafkaDataStream.
2023-03-07 08:36:10,444 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: mysqlSink (1/2) (898af6700ac9cd087c763cef0b5585d4) switched from RUNNING to FAILED on container_e38_1676011848026_0012_01_000002 @ xxxx (dataPort=44633).
java.lang.RuntimeException: Writing records to JDBC failed.
    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.checkFlushException(JdbcBatchingOutputFormat.java:153) ~[flink-connector-jdbc_2.11-1.13.6.jar:1.13.6]
{panel}
{panel:title=But from this checkpoint restore, it can't be decoded}
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_a1b6a20a1eb2801464c79c8d018a24d1_(1/2) from any of the 1 provided restore options.
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:345) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    ... 10 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:177) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:111) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:131) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:73) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.StateBackend.createKeyedStateBackend(StateBackend.java:136) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:328) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:345) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    ... 10 more
Caused by: java.io.UTFDataFormatException: malformed input around byte 32
    at java.io.DataInputStream.readUTF(DataInputStream.java:656) ~[?:1.8.0_201]
    at java.io.DataInputStream.readUTF(DataInputStream.java:564) ~[?:1.8.0_201]
    at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:379) ~[flink-core-1.13.6.jar:1.13.6]
    at org.apache.flink.api.common.typeutils.base.MapSerializer.deserialize(MapSerializer.java:155) ~[flink-core-1.13.6.jar:1.13.6]
    at org.apache.flink.api.common.typeutils.base.MapSerializer.deserialize(MapSerializer.java:43) ~[flink-core-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.lambda$createV2PlusReader$0(StateTableByKeyGroupReaders.java:79) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.KeyGroupPartitioner$PartitioningResultKeyGroupReader.readMappingsInKeyGroup(KeyGroupPartitioner.java:297) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:258) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:220) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:166) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:62) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:174) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:111) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:131) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:73) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.runtime.state.StateBackend.createKeyedStateBackend(StateBackend.java:136) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:328) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:345) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
{panel}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 12:23:13 UTC 2023,,,,,,,,,,"0|z1ge60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 06:42;Yanfei Lei;Did any POJO fields update when restoring? Is it related to https://issues.apache.org/jira/browse/FLINK-21752?

A common cause of ""java.io.UTFDataFormatException: malformed input around byte""  is the read and write in serializer are not symmetrical.;;;","08/Mar/23 08:22;seckiller;[~Yanfei Lei] 
I have a mapState<String, pojo>. 
It turns out that pojo has only one parameter constructor, I first manually savepoint the program and stop, then I add a parameterless constructor to the pojo, and restore the program from the savepoint just now.   
The program ran normally for a while, and because some sinks reported errors, the program itself tried to restart from the most recent checkpoint , and reported the above error;;;","20/Mar/23 12:23;seckiller;I know the reason through debugging.

My old pojo does not have a parameterless constructor, and the system uses *KryoSerializer* for serialization.

After I add the parameterless constructor, the system uses *PojoSerializer* for serialization, but the property *_subclassSerializerCache_* still retains that my pojo should be deserialized with {*}KryoSerializer{*}, which causes problems.
{code:java}
TypeSerializer<?> getSubclassSerializer(Class<?> subclass) {
    TypeSerializer<?> result = subclassSerializerCache.get(subclass);
    if (result == null) {
        result = createSubclassSerializer(subclass);
        subclassSerializerCache.put(subclass, result);
    }
    return result;
} {code}
normally by removing this cache when restoring, I hope the official can improve this problem later
{code:java}
PojoSerializer(
        Class<T> clazz,
        Field[] fields,
        TypeSerializer<Object>[] fieldSerializers,
        LinkedHashMap<Class<?>, Integer> registeredClasses,
        TypeSerializer<?>[] registeredSerializers,
        Map<Class<?>, TypeSerializer<?>> subclassSerializerCache,
        ExecutionConfig executionConfig) {

    this.clazz = checkNotNull(clazz);
    this.fields = checkNotNull(fields);
    this.numFields = fields.length;
    this.fieldSerializers = checkNotNull(fieldSerializers);
    this.registeredClasses = checkNotNull(registeredClasses);
    this.registeredSerializers = checkNotNull(registeredSerializers);
    this.subclassSerializerCache = checkNotNull(subclassSerializerCache);
    this.subclassSerializerCache.entrySet().removeIf(next -> ""xxx"".equals(next.getKey().getName()));
    this.executionConfig = checkNotNull(executionConfig);
    this.cl = Thread.currentThread().getContextClassLoader();
} {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommonDataStreamTests.test_execute_and_collect failed,FLINK-31355,13527393,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,07/Mar/23 07:56,21/Aug/23 11:04,04/Jun/24 20:41,21/Aug/23 11:04,1.17.0,,,,,,,,,,,,,,,API / Python,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46883&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=32651

{code}
Mar 07 07:20:04 ERROR    root:java_gateway.py:1055 Exception while sending command.
Mar 07 07:20:04 Traceback (most recent call last):
Mar 07 07:20:04   File ""/__w/1/s/flink-python/.tox/py310-cython/lib/python3.10/site-packages/py4j/java_gateway.py"", line 1224, in send_command
Mar 07 07:20:04     raise Py4JNetworkError(""Answer from Java side is empty"")
Mar 07 07:20:04 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-30630,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 11:04:05 UTC 2023,,,,,,,,,,"0|z1ge3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 08:20;mapohl;[~hxbks2ks] can you have a look at it?;;;","07/Mar/23 09:50;dianfu;[~mapohl]  Hey, this seems like test instability issue and should not be a blocker issue. I will update the priority to critical.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 11:04;mapohl;I'm gonna close this one as a duplicate of FLINK-30630. The stacktraces look the same even though the failing tests are different.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyClientServerSslTest.testValidSslConnectionAdvanced timed out,FLINK-31354,13527392,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,07/Mar/23 07:54,22/May/23 16:21,04/Jun/24 20:41,22/May/23 16:20,1.16.1,1.17.0,1.18.0,,,,,,,,,1.16.2,1.17.1,1.18.0,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46883&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8242

{code}
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
05:15:10,904 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 42935, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client threads>
05:15:10,916 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
05:15:12,149 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 1245 ms). Listening on SocketAddress /127.0.0.1:42935.
05:15:12,150 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
05:15:13,249 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 1099 ms).
05:15:14,588 [                main] ERROR org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest [] - 
--------------------------------------------------------------------------------
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) failed with:
org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandshakeTimeoutException: handshake timed out after 1000ms
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$7.run(SslHandler.java:2115)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:153)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 16:20:52 UTC 2023,,,,,,,,,,"0|z1ge3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 10:24;mapohl;I'm lowering the priority for this one to Critical. I compared the runtimes with another recent 1.17 build (see [build 20230307.4|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46885&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7103]):
{code}
================================================================================
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
02:53:03,827 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 40169, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client threads: 1 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
02:53:03,828 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
02:53:04,016 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 188 ms). Listening on SocketAddress /127.0.0.1:40169.
02:53:04,016 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
02:53:04,203 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 187 ms).
02:53:04,309 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful shutdown (took 2 ms).
02:53:04,309 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful shutdown (took 0 ms).
{code}

It appears that the timeout was hit by a slow CI machine. The 1s might be too long for a timeout here.;;;","22/Apr/23 13:46;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48354&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8673;;;","16/May/23 08:14;renqs;[~mapohl] Any updates on this one? Thanks ;;;","16/May/23 08:27;mapohl;I forgot about that one. Thanks for pinging me. I will provide a PR where we increase the timeout.;;;","22/May/23 16:20;mapohl;master: 2805f027e6b207f21fa6224ac8e9f8e258bd49f4
1.17: 553c72a894b25c10e71e54e6567c187876234fd5
1.16: ab836498111d0939e97523585d1549f4fdff86ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge SnapshotEnumerator into TableScan ,FLINK-31353,13527388,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,07/Mar/23 07:36,14/Mar/23 12:57,04/Jun/24 20:41,14/Mar/23 12:57,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"The abilities of SnapshotEnumerator and TableScan are duplicated. Since configurations used to create SnapshotEnumerator also can be fetched in table, it's better to use TableScan to replace SnapshotEnumerator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 12:57:47 UTC 2023,,,,,,,,,,"0|z1ge2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 12:57;lzljs3620320;master: 0271e9cc0795f0b1d2ddeb473098abf9efd4e6fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OverAggregateITCase.testWindowAggregationSumWithoutOrderBy times out on CI,FLINK-31352,13527384,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dmvk,dmvk,07/Mar/23 07:33,19/Aug/23 10:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46880&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15913]

 
{code:java}
Mar 07 00:20:18 ""main"" #1 prio=5 os_prio=0 tid=0x00007f805800b800 nid=0x3a9f waiting on condition [0x00007f8060875000]
Mar 07 00:20:18    java.lang.Thread.State: WAITING (parking)
Mar 07 00:20:18 	at sun.misc.Unsafe.park(Native Method)
Mar 07 00:20:18 	- parking to wait for  <0x00000000a7481ba0> (a java.util.concurrent.CompletableFuture$Signaller)
Mar 07 00:20:18 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Mar 07 00:20:18 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Mar 07 00:20:18 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Mar 07 00:20:18 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Mar 07 00:20:18 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Mar 07 00:20:18 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170)
Mar 07 00:20:18 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Mar 07 00:20:18 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Mar 07 00:20:18 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Mar 07 00:20:18 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Mar 07 00:20:18 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Mar 07 00:20:18 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
Mar 07 00:20:18 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
Mar 07 00:20:18 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
Mar 07 00:20:18 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
Mar 07 00:20:18 	at org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.testWindowAggregationSumWithoutOrderBy(OverAggregateITCase.scala:464) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:07 UTC 2023,,,,,,,,,,"0|z1ge1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2 times out on CI,FLINK-31351,13527328,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,dmvk,dmvk,06/Mar/23 18:53,09/Mar/23 08:19,04/Jun/24 20:41,09/Mar/23 03:16,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46872&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24908]

 
{code:java}
Mar 06 18:28:56 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007ff4b1832000 nid=0x21b2 waiting on condition [0x00007ff3a8c3e000]
Mar 06 18:28:56    java.lang.Thread.State: TIMED_WAITING (sleeping)
Mar 06 18:28:56 	at java.lang.Thread.sleep(Native Method)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.waitUntilJobIsRunning(HiveServer2EndpointITCase.java:1004)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testExecuteStatementInSyncModeWithRuntimeException2$37(HiveServer2EndpointITCase.java:711)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase$$Lambda$2018/2127600974.accept(Unknown Source)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runExecuteStatementInSyncModeWithRuntimeException(HiveServer2EndpointITCase.java:999)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2(HiveServer2EndpointITCase.java:709)
Mar 06 18:28:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 06 18:28:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 06 18:28:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 06 18:28:56 	at java.lang.reflect.Method.invoke(Method.java:498)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 08:19:25 UTC 2023,,,,,,,,,,"0|z1gdpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 07:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46881&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=25249;;;","07/Mar/23 07:10;mapohl;[~luoyuxia] Can you have a look at it? So far, it only happened on {{master}}. But did we do Hive-related {{master}}-only changes or is it also affecting 1.17.0?;;;","07/Mar/23 07:26;luoyuxia;AFAK, we didn't do Hive-related master-only changes. I think it will also affect 1.17.0. Not quite sure, but I suspect it's related to the fix in FLINK-31092. [~fsk119] Could you please take a look?;;;","07/Mar/23 07:50;mapohl;1.16 is also affected: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46882&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=28925;;;","07/Mar/23 16:54;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46912&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24080
;;;","08/Mar/23 06:04;fsk119;I think the main cause is that the Thread#stop is not safe. The problem here is the worker thread is stopped by the canceler thread when the worker thread is still loading classes. However, the class can only be loaded once per classloader. When failed to load the class, the next time Classloader#loadClass will throw exceptions outside. So here I think we just notify the users we can not interrupt thread in time rather than just stop the thread by force.;;;","08/Mar/23 11:55;fsk119;Merged into release-1.16: 6fd3b9b338433d1e8240a1598bda883ef01cc9c4
Merged into release-1.17: 32b370181853f4129fd237c6a57491863a7e8b8c
Merged into master: 384d6b10a2d69b9384052c3d4c3ad82babd201d1;;;","09/Mar/23 08:19;mapohl;The following build failure didn't include the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46971&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27561;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Calcite's  1.30+ UnknownType,FLINK-31350,13527319,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/23 17:21,19/Apr/23 06:58,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"In 1.30 in Calcite https://issues.apache.org/jira/browse/CALCITE-4872 there has been introduced a new type UNKNOWN which is similar to NULL however is not nullable by default and as a result breaks some tests.

 

I guess a similar type should be introduced in Flink to support it",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-06 17:21:16.0,,,,,,,,,,"0|z1gdnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adapt to Calcite's 1.30+ new ROW null semantics,FLINK-31349,13527316,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/23 17:01,19/Apr/23 06:56,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"The change has been introduced at https://issues.apache.org/jira/browse/CALCITE-3627
It leads to unsync behavior of sql and table api in context of nullability for rows.

As a workaround there is a {{SqlRowConstructor}} mimicking Calcite 1.29 behavior.

After resolving this issue {{SqlRowConstructor}} should be removed",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-06 17:01:09.0,,,,,,,,,,"0|z1gdmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation fails to build due to unclosed shortcodes,FLINK-31348,13527310,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,dmvk,dmvk,06/Mar/23 16:27,07/Mar/23 12:52,04/Jun/24 20:41,07/Mar/23 12:51,,,,,,,,,,,,1.15.4,1.16.2,1.17.0,,Documentation,,,,0,pull-request-available,,,,"After migration to HUGO and using Hugo version 0.111.0 or higher, there are a bunch of unclosed shortcodes which prevent the documentation from being served locally.

 

Example:
{code:java}
docker run -v $(pwd):/src -p 1313:1313 jakejarvis/hugo-extended:latest server --buildDrafts --buildFuture --bind 0.0.0.0
 
...

Error: Error building site: ""/src/content.zh/docs/connectors/datastream/formats/parquet.md:111:1"": failed to extract shortcode: unclosed shortcode ""tabs"" {code}
 
This is caused by the new Hugo 0.111.0 version https://github.com/gohugoio/hugo/releases/tag/v0.111.0 which includes ""Throw an error when shortcode is expected to be closed 7d78a49 @bep #10675""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 12:51:20 UTC 2023,,,,,,,,,,"0|z1gdlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:51;martijnvisser;Fixed in:

master: 3ea83baad0c8413f8e1f4a027866335d13789538
release-1.17: bef6f5137645dd135d5db0043b0de4e920cb1253
release-1.16: 49d9ea6e5d3fdf1beca2513ade47a352521b79f3
release-1.15: f11499ec1448d1056ad6af01d91adaefa10a2b98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerClusterITCase.testAutomaticScaleUp times out,FLINK-31347,13527303,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,mapohl,mapohl,06/Mar/23 16:05,31/Aug/23 10:22,04/Jun/24 20:41,07/Mar/23 13:21,1.17.0,,,,,,,,,,,1.17.0,1.18.0,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46850&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10451

{code}
Mar 06 14:11:24 ""main"" #1 prio=5 os_prio=0 tid=0x00007f482800b800 nid=0x6eee waiting on condition [0x00007f48325cd000]
Mar 06 14:11:24    java.lang.Thread.State: TIMED_WAITING (sleeping)
Mar 06 14:11:24 	at java.lang.Thread.sleep(Native Method)
Mar 06 14:11:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Mar 06 14:11:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 06 14:11:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:265)
Mar 06 14:11:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:153)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 09:42:02 UTC 2023,,,,,,,,,,"0|z1gdk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 16:21;mapohl;It looks like all tasks finished successfully before starting the additional TaskManager:
{code}
[...]
13:55:38,791 [Blocking operator (2/4)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Blocking operator (2/4)#1 (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_1_1) switched from RUNNING to FINISHED.
[...]
13:55:38,793 [Blocking operator (3/4)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Blocking operator (3/4)#0 (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_2_0) switched from RUNNING to FINISHED.
13:55:38,793 [Blocking operator (4/4)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Blocking operator (4/4)#0 (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_3_0) switched from RUNNING to FINISHED.
[...]
13:55:38,802 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Blocking operator (3/4) (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_2_0) switched from RUNNING to FINISHED.
13:55:38,803 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Blocking operator (2/4) (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_1_1) switched from RUNNING to FINISHED.
13:55:38,803 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Blocking operator (4/4) (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_3_0) switched from RUNNING to FINISHED.
13:55:38,806 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Unnamed job (cb5c7a7cddf612e4d2af549183152ccb) switched from state RUNNING to FINISHED.
13:55:38,810 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Job cb5c7a7cddf612e4d2af549183152ccb reached terminal state FINISHED.
13:55:38,816 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job cb5c7a7cddf612e4d2af549183152ccb reached terminal state FINISHED.
13:55:38,819 [mini-cluster-io-thread-4] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job cb5c7a7cddf612e4d2af549183152ccb has been registered for cleanup in the JobResultStore after reaching a terminal state.
[...]
13:55:38,846 [                main] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase [] - Start additional TaskManager to scale up to the full parallelism.
[...]
{code};;;","06/Mar/23 16:36;mapohl;[~dmvk] [~chesnay] do you have capacity to look into it? It looks like the lock was released before reaching the taskManager creation, which is odd.

We didn't touch this part of the code in 1.17 as far as I remember, did we?;;;","06/Mar/23 16:37;mapohl;I lowered the priority to Critical because it appears to be a test code issue.;;;","06/Mar/23 16:39;dmvk;I'll give it a ""timeboxed"" shot now.;;;","07/Mar/23 08:51;dmvk;master: babb82553c804c5abbf14353bcca84ad13401676

release-1.17: c02099bcbfd9ec32d1e915bd3efed7dfd23597b3;;;","31/Aug/23 09:42;Sergey Nuyanzin;There is the same case for 1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52872&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=10585;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch shuffle IO scheduler does not throw TimeoutException if numRequestedBuffers is greater than 0,FLINK-31346,13527294,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,06/Mar/23 14:49,09/Mar/23 04:21,04/Jun/24 20:41,09/Mar/23 04:21,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.0,,,Runtime / Network,,,,0,pull-request-available,,,,"We currently rely on throw exception to trigger downstream task failover to avoid read buffer request deadlock. But if {{numRequestedBuffers}} is greater than 0, IO scheduler does not throw {{TimeoutException}}. This will cause a deadlock.
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31359,,,,FLINK-31330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 12:35:34 UTC 2023,,,,,,,,,,"0|z1gdi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:35;Weijie Guo;master(1.18) via 5ad2ae2c24ade2655981f609298978d26329466f.
release-1.17 via 54c67e5e08c11ef9a538abbf14618f9e27be18f7.
release-1.16 via 860ce4f57b2599516cd199a20204c047ca34c1e3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trim autoscaler configMap to not exceed 1mb size limit,FLINK-31345,13527286,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,mxm,mxm,06/Mar/23 14:08,17/Mar/23 12:09,04/Jun/24 20:41,08/Mar/23 16:40,kubernetes-operator-1.4.0,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,,"When the {{autoscaler-<deployment_name>}} ConfigMap which is used to persist scaling decisions and metrics becomes too large, the following error is thrown consistently:

{noformat}
io.fabric8.kubernetes.client.KubernetesClientException: Operation: [replace]  for kind: [ConfigMap]  with name: [deployment]  in namespace: [namespace]  failed.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:159)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:169)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:172)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:113)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:41)
    at io.fabric8.kubernetes.client.extension.ResourceAdapter.replace(ResourceAdapter.java:252)
    at org.apache.flink.kubernetes.operator.autoscaler.AutoScalerInfo.replaceInKubernetes(AutoScalerInfo.java:167)
    at org.apache.flink.kubernetes.operator.autoscaler.JobAutoScalerImpl.scale(JobAutoScalerImpl.java:113)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:178)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:130)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:145)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:103)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:102)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: stream was reset: NO_ERROR
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:514)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:551)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleUpdate(OperationSupport.java:347)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleUpdate(BaseOperation.java:680)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:167)
    ... 21 more
Caused by: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
    at okhttp3.internal.http2.Http2Stream.checkOutNotClosed$okhttp(Http2Stream.kt:646)
    at okhttp3.internal.http2.Http2Stream$FramingSink.emitFrame(Http2Stream.kt:557)
    at okhttp3.internal.http2.Http2Stream$FramingSink.write(Http2Stream.kt:532)
    at okio.ForwardingSink.write(ForwardingSink.kt:29)
    at okhttp3.internal.connection.Exchange$RequestBodySink.write(Exchange.kt:218)
    at okio.RealBufferedSink.emitCompleteSegments(RealBufferedSink.kt:255)
    at okio.RealBufferedSink.write(RealBufferedSink.kt:185)
    at okhttp3.RequestBody$Companion$toRequestBody$2.writeTo(RequestBody.kt:152)
    at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.kt:59)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:34)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at org.apache.flink.kubernetes.operator.metrics.KubernetesClientMetrics.intercept(KubernetesClientMetrics.java:130)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)
    ... 3 more
    Suppressed: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
        ... 31 more
 {noformat}

We should trim the ConfigMap to not exceed the size limit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 16:40:50 UTC 2023,,,,,,,,,,"0|z1gdg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 16:40;gyfora;Merged to main:
f88cbf3fd1b99a574a1ed8b8a2869b96d932e521
70bf6a9d920e9affadb253e7760db12d4e0dd554;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to update nested columns in update statement,FLINK-31344,13527261,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,aitozi,luoyuxia,luoyuxia,06/Mar/23 13:19,12/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,stale-assigned,,,"Currently, it'll throw exception while using update statement to update nested column;

For the following sql:

 
{code:java}
create table (t ROW<`a` INT>) with (xxx);
update t set s.a = 1;{code}
It'll throw the exception:
{code:java}
Caused by: org.apache.flink.sql.parser.impl.ParseException: Encountered ""."" at line 1, column 15.
Was expecting:
    ""="" ...
    
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:46382)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:46190)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlUpdate(FlinkSqlParserImpl.java:14389)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:4121)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:2998)
    at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:306)
    at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:198)
    ... 33 more {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:07 UTC 2023,,,,,,,,,,"0|z1gdao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/23 11:36;aitozi;Hi [~luoyuxia], when solving the FLINK-31301 I notice this ticket. After spending some time investigate on this ticket, I think we can support update nested columns in the following step:
 - Modify the SqlParser to accept the compound identifier for the UPDATE SET clause
 - Modify the interface SupportsRowLevelUpdate, which will be better to work with {{int[][]}} instead of {{Column}} (It will make the control of required column finer).

WDYT ?;;;","10/Apr/23 01:39;luoyuxia;[~aitozi] I think it's fine to make SqlParser to accept the compound identifier. But about the modifcation about {{SupportsRowLevelUpdate}} , I perfer to keep for a while to see whether do we really need such modifcation to avoid over design.;;;","10/Apr/23 03:10;aitozi;Thanks for your reply. Totally agree with you. I have basically verified the first work, can you help assign this ticket to me ?;;;","12/Apr/23 10:54;aitozi;[~luoyuxia] I have finished this ticket, could you help take a look when you are free ?;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove JMH dependency in flink-table-store-micro-benchmark,FLINK-31343,13527253,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,06/Mar/23 12:21,08/Mar/23 02:05,04/Jun/24 20:41,08/Mar/23 02:05,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 02:05:38 UTC 2023,,,,,,,,,,"0|z1gd8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 02:05;lzljs3620320;master: d1c224890dad80c1d7664dc3011534b3ae1679f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientSchemaRegistryITCase timed out when starting the test container,FLINK-31342,13527236,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,06/Mar/23 10:27,16/Oct/23 12:36,04/Jun/24 20:41,16/Oct/23 12:36,1.17.0,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46820&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=11767

{code}
Mar 06 06:53:47 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 1,037.927 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase
Mar 06 06:53:47 [ERROR] org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase  Time elapsed: 1,037.927 s  <<< ERROR!
Mar 06 06:53:47 org.junit.runners.model.TestTimedOutException: test timed out after 10 minutes
Mar 06 06:53:47 	at sun.misc.Unsafe.park(Native Method)
Mar 06 06:53:47 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Mar 06 06:53:47 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Mar 06 06:53:47 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Mar 06 06:53:47 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Mar 06 06:53:47 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Mar 06 06:53:47 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:323)
Mar 06 06:53:47 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1063)
Mar 06 06:53:47 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
Mar 06 06:53:47 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Mar 06 06:53:47 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Mar 06 06:53:47 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Mar 06 06:53:47 	at java.lang.Thread.run(Thread.java:750)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:11 UTC 2023,,,,,,,,,,"0|z1gd54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 10:27;mapohl;Could be related to the OOM in FLINK-31341 because both test failures appeared in the same build close to each other;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemoryError in Kafka e2e tests,FLINK-31341,13527235,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,06/Mar/23 10:24,16/Oct/23 12:15,04/Jun/24 20:41,16/Oct/23 12:15,1.17.0,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-critical,test-stability,,,"We experience a OOM in Kafka e2e tests:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46820&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=11726

{code}
ar 06 06:22:30 [ERROR] Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""ForkJoinPool-1-worker-0""
Exception in thread ""ForkJoinPool-1-worker-0"" Mar 06 06:27:30 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1,094.139 s <<< FAILURE! - in JUnit Jupiter
Mar 06 06:27:30 [ERROR] JUnit Jupiter.JUnit Jupiter  Time elapsed: 947.463 s  <<< ERROR!
Mar 06 06:27:30 org.junit.platform.commons.JUnitException: TestEngine with ID 'junit-jupiter' failed to execute tests
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:153)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Mar 06 06:27:30 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Mar 06 06:27:30 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Mar 06 06:27:30 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
Mar 06 06:27:30 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Mar 06 06:27:30 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Mar 06 06:27:30 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Mar 06 06:27:30 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Mar 06 06:27:30 Caused by: org.junit.platform.commons.JUnitException: Error executing tests for engine junit-jupiter
Mar 06 06:27:30 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:57)
Mar 06 06:27:30 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Mar 06 06:27:30 	... 16 more
Mar 06 06:27:30 Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError
Mar 06 06:27:30 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)
Mar 06 06:27:30 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
Mar 06 06:27:30 	... 17 more
Mar 06 06:27:30 Caused by: java.lang.OutOfMemoryError
Mar 06 06:27:30 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Mar 06 06:27:30 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Mar 06 06:27:30 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Mar 06 06:27:30 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
Mar 06 06:27:30 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:598)
Mar 06 06:27:30 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)
Mar 06 06:27:30 	... 18 more
Mar 06 06:27:30 Caused by: java.lang.OutOfMemoryError: Java heap space
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31134,,,FLINK-31342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:11 UTC 2023,,,,,,,,,,"0|z1gd4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 08:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46970&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=15931

again, FLINK-31342 was affected as well. I guess, we can close FLINK-31134 as a duplicate if we observe this behavior once more.

{code}
Mar 09 03:05:05 [ERROR] Errors: 
Mar 09 03:05:05 [ERROR]   TestEngine with ID 'junit-jupiter' failed to execute tests
Mar 09 03:05:05 [ERROR]   KafkaSinkE2ECase>SinkTestSuiteBase.testScaleDown:223->SinkTestSuiteBase.restartFromSavepoint:291->SinkTestSuiteBase.killJob:552 » Execution
Mar 09 03:05:05 [ERROR]   SQLClientSchemaRegistryITCase.testReading:175->executeSqlStatements:256 » OutOfMemory
Mar 09 03:05:05 [ERROR]   SQLClientSchemaRegistryITCase.testWriting » OutOfMemory Java heap space
Mar 09 03:05:05 [INFO] 
Mar 09 03:05:05 [ERROR] Tests run: 30, Failures: 0, Errors: 4, Skipped: 1
{code};;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceStatisticsReportTest.testFlinkParquetFormatHiveTableSourceStatisticsReport failed with InvalidObjectException,FLINK-31340,13527234,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,mapohl,mapohl,06/Mar/23 10:15,06/Mar/23 11:17,04/Jun/24 20:41,06/Mar/23 11:17,1.17.0,,,,,,,,,,,,,,,Connectors / Hive,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46820&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=22101

{code}
Mar 06 01:33:23 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 124.572 s <<< FAILURE! - in org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest
Mar 06 01:33:23 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkParquetFormatHiveTableSourceStatisticsReport  Time elapsed: 52.608 s  <<< ERROR!
Mar 06 01:33:23 org.apache.flink.table.api.TableException: Could not execute CREATE DATABASE: (catalogDatabase: [{}], catalogName: [hive], databaseName: [db1], ignoreIfExists: [false])
Mar 06 01:33:23 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165)
Mar 06 01:33:23 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:765)
Mar 06 01:33:23 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.setup(HiveTableSourceStatisticsReportTest.java:65)
Mar 06 01:33:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 06 01:33:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 06 01:33:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 06 01:33:23 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 06 01:33:23 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Mar 06 01:33:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Mar 06 01:33:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
Mar 06 01:33:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeEachMethod(TimeoutExtension.java:78)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
Mar 06 01:33:23 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
{code}",,,,,,,,,,,,,,,,,,,,,FLINK-30433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 11:17:09 UTC 2023,,,,,,,,,,"0|z1gd4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 11:17;luoyuxia;Duplicated by FLINK-30433. It happend after we enable test for Hive3 in FLINK-27895. It looks like a bug of Hive3. I have create a Jira HIVE-27108 in Hive community to search help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PlannerScalaFreeITCase.testImperativeUdaf,FLINK-31339,13527231,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,mapohl,mapohl,06/Mar/23 10:05,29/Nov/23 01:53,04/Jun/24 20:41,29/Nov/23 01:53,1.16.2,1.17.1,1.18.0,,,,,,,,,1.17.3,1.18.1,1.19.0,,Table SQL / Planner,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,"{{PlannerScalaFreeITCase.testImperativeUdaf}} failed:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46812&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=15012

{code}
Mar 05 05:55:50 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 62.028 s <<< FAILURE! - in org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase
Mar 05 05:55:50 [ERROR] PlannerScalaFreeITCase.testImperativeUdaf  Time elapsed: 40.924 s  <<< FAILURE!
Mar 05 05:55:50 org.opentest4j.AssertionFailedError: Did not get expected results before timeout, actual result: [{""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":1},""op"":""c""}, {""before"":null,""after"":{""user_name"":""Alice"",""order_cnt"":1},""op"":""c""}, {""before"":{""user_name"":""Bob"",""order_cnt"":1},""after"":null,""op"":""d""}, {""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":2},""op"":""c""}]. ==> expected: <true> but was: <false>
Mar 05 05:55:50 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Mar 05 05:55:50 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Mar 05 05:55:50 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Mar 05 05:55:50 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Mar 05 05:55:50 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
Mar 05 05:55:50 	at org.apache.flink.table.sql.codegen.SqlITCaseBase.checkJsonResultFile(SqlITCaseBase.java:168)
Mar 05 05:55:50 	at org.apache.flink.table.sql.codegen.SqlITCaseBase.runAndCheckSQL(SqlITCaseBase.java:111)
Mar 05 05:55:50 	at org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.testImperativeUdaf(PlannerScalaFreeITCase.java:43)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 01:53:31 UTC 2023,,,,,,,,,,"0|z1gd40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 12:09;mapohl;[~jark] [~Jiangang] can you have a look at it. This test was added as part of FLINK-27790. You both showed up in the commit as author and commiter. That's why I selected you.

In general, this test relies on some timeout (see [SqlITCaseBase:149|https://github.com/apache/flink/blob/469049a4359aea40f083bb2b7e4fbc1f86b65ce9/flink-end-to-end-tests/flink-end-to-end-tests-sql/src/test/java/org/apache/flink/table/sql/codegen/SqlITCaseBase.java#L149] which we shouldn't use in tests anymore (see [Flink Coding Guidelines|https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#avoid-timeouts-in-junit-tests]). I'm not sure whether that's the reason for the test instability.

Can you give us some advice on whether that's a serious issue or whether we can lower the priority?;;;","07/Mar/23 08:21;mapohl;[~qafro] may you have a look at it?;;;","07/Mar/23 10:02;lsy;This test has been introduced since 1.16, so I think it isn't a blocker issue, just unstable now, we can lower its priority. I will take a look.;;;","07/Mar/23 10:28;mapohl;Thanks for the clarification. I reduced the priority accordingly.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Oct/23 17:24;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53504&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=15329;;;","11/Oct/23 07:36;jiabao.sun;I think the reason for this problem is that it exceeds the `mini-batch.allow-latency`.

Other problems caused by the same cause:
* https://issues.apache.org/jira/browse/FLINK-31033
* https://issues.apache.org/jira/browse/FLINK-31141
* https://issues.apache.org/jira/browse/FLINK-32269

MINI-BATCH can have the same results in streaming and batch execution mode, but this will lead to unstable testing.
I agree with [~lsy] that just sending one record for every group key in source is a simple way to fix this problem.;;;","20/Oct/23 06:33;jiabao.sun;Hi [~lsy], could you help review this?
Thanks :);;;","28/Nov/23 06:47;leonard;Fixed in master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52;;;","28/Nov/23 09:05;mapohl;Sorry for reopening this one. But it would be helpful to backport test instability fixes as well if the error also appeared in older releases.;;;","28/Nov/23 09:12;leonard;Np, [~mapohl] ， [~jiabao.sun] would you like to backport the fix to release-1.17 and release-1.18 branch ?;;;","28/Nov/23 09:22;jiabao.sun;Sure. I'm working on this.;;;","28/Nov/23 14:18;jiabao.sun;[BP-1.18] https://github.com/apache/flink/pull/23816
[BP-1.17] https://github.com/apache/flink/pull/23817
[BP-1.16] https://github.com/apache/flink/pull/23818;;;","28/Nov/23 14:26;mapohl;Thanks. :) just as a hint for future backports: 1.16 already reached EOL (with the release of 1.18). There's no need to create test stability fix backports for 1.16.;;;","28/Nov/23 14:29;jiabao.sun;Thanks, I have closed that PR.;;;","29/Nov/23 01:53;leonard;Fixed in

master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52

release-1.18：b3b7240cc34e552273b26d8090d45e492474c9ea

release-1.17: 0053db03772a70c70de0516cc46f7ab363dc74f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support  infer parallelism for flink table store,FLINK-31338,13527230,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhangjun,zhangjun,06/Mar/23 10:00,02/Apr/23 07:16,04/Jun/24 20:41,19/Mar/23 05:36,table-store-0.3.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"When using flink  to query the fts table, we can config the scan parallelism by set the scan.parallelism, but the user may do not know how much parallelism should be used,  setting a too large parallelism will cause resource waste, setting the parallelism too small will cause the query to be slow, so we can add parallelism infer.

The function is enabled by default. the parallelism is equal to the number of read splits. Of course, the user can manually turn off the infer function. In order to prevent too many datafiles from causing excessive parallelism, we also set a max infer parallelism. When the infer parallelism exceeds the setting, use the max parallelism.

In addition, we also need to compare with the limit in the select query statement to get a more appropriate parallelism in the case of limit pushdown, for example we have a sql select * from table limit 1, and finally we infer the parallelism is 10, but we only one parallel is needed , besause we only need one data .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:36:09 UTC 2023,,,,,,,,,,"0|z1gd3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:36;lzljs3620320;Just use github pr/issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedDataStreamBatchTests.test_keyed_co_broadcast_side_output,FLINK-31337,13527223,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Juntao Hu,mapohl,mapohl,06/Mar/23 09:39,06/Mar/23 15:41,04/Jun/24 20:41,06/Mar/23 15:41,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.0,,,API / Python,,,,0,pull-request-available,test-stability,,,"Same build, multiple times:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24566
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24235
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=24545
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24481
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=24757

{code}
Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:743: 
Mar 04 01:21:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:63: in assert_equals_sorted
Mar 04 01:21:35     self.assertEqual(expected, actual)
Mar 04 01:21:35 E   AssertionError: Lists differ: ['0', '1', '2', '4', '5', '5', '6', '6'] != ['0', '1', '2', '3', '5', '5', '6', '6']
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   First differing element 3:
Mar 04 01:21:35 E   '4'
Mar 04 01:21:35 E   '3'
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   - ['0', '1', '2', '4', '5', '5', '6', '6']
Mar 04 01:21:35 E   ?                  ^
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   + ['0', '1', '2', '3', '5', '5', '6', '6']
Mar 04 01:21:35 E   ?   
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-31185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 15:41:42 UTC 2023,,,,,,,,,,"0|z1gd28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46801&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27607;;;","06/Mar/23 09:51;mapohl;All test failures in 1.17 as well: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46802&view=results;;;","06/Mar/23 10:11;mapohl;* master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46809&view=results (all test failures related to this issue)
* 1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46811&view=results
* 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46812&view=results
* master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46817&view=results
* 1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46819&view=results
* 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46820&view=results;;;","06/Mar/23 10:11;mapohl;This test is failing consistently in any stage for master, release-1.17 and release-1.16;;;","06/Mar/23 10:53;mapohl;It appears to be caused by FLINK-31185 which had a green CI build. We can see this on {{master}} as well: It fails consistently for the nightlies (for both the cron and the ci job) but doesn't fail for the per-change CI (even the ci job) (see [master CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary&repositoryFilter=1&branchFilter=2%2C2%2C2%2C2%2C2%2C2%2C2%2C2]). [~dianfu] [~Juntao Hu] why is the test only triggered by CI in the nightlies even for the CI stage?;;;","06/Mar/23 13:26;Juntao Hu;[~mapohl], FLINK-31185 introduces a test case that exposes a hidden bug in PyFlink broadcast process feature since 1.16, which accidently escaped from all test cases with our default test config parallelism=2. The failed test case only fails in Python 3.7, while CI triggered by commit only runs with Python 3.10. Meanwhile, there're some changes in pickle protocol between 3.7 and 3.8+, which produce the exact serialized keys that makes the ""escaping"" possible.

I already created a PR fixing the hidden bug, thx for the report.;;;","06/Mar/23 15:41;dianfu;Fixed in:
- master via 4fd3cf133a22210607038305e97d1a6b1cc4d6c1
- release-1.17 via c43b3d23bde1047e675793bf3e64cfe5c514f088
- release-1.16 via 07f07782ea4fe9be08ef3fb905652c2693a3da4c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
interval type process has problem in table api and sql,FLINK-31336,13527221,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,06/Mar/23 09:25,11/Mar/24 12:43,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"{code:java}
// code placeholder
select typeof(interval '1' day);
 - INTERVAL SECOND(3) NOT NULL {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 12:44:03 UTC 2023,,,,,,,,,,"0|z1gd1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:44;jark;Could you help to investigate what's the behavior in other systems, such as Postgres? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"using sql-gateway to submit job to yarn cluster, sql-gateway don't support kerberos",FLINK-31335,13527219,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,felixzh,felixzh,felixzh,06/Mar/23 09:24,28/Nov/23 13:11,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,1.17.3,,,,Table SQL / Gateway,,,,0,pull-request-available,stale-assigned,,,"when submit job to yarn cluster, sql-gateway don't support kerberos.

1. yarn-per-job mode

-Dexecution.target=yarn-per-job

2. yarn-session mode

-Dexecution.target=yarn-session -Dyarn.application.id=yarnSessionAppID(eg: application_1677479737242_0052)

sql-gateway need to use SecurityUtils Modules.

default use flink-conf.yaml(security.kerberos.login.principal and security.kerberos.login.keytab), also support -Dsecurity.kerberos.login.keytab and -Dsecurity.kerberos.login.principal (eg: 1/2)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 22:35:12 UTC 2023,,,,,,,,,,"0|z1gd1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2e ci fail with unsupported file exception ,FLINK-31334,13527214,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,luoyuxia,luoyuxia,06/Mar/23 09:09,06/Mar/23 09:57,04/Jun/24 20:41,06/Mar/23 09:57,,,,,,,,,,,,,,,,Build System / CI,,,,0,,,,,"The e2e ci throw

""E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline"".

The full exception message is 
{code:java}
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-03-06 07:22:17--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 185.125.190.39, 185.125.190.36, 91.189.91.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|185.125.190.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-03-06 07:22:17 ERROR 404: Not Found.


WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline
##[error]Bash exited with code '100'.
Finishing: Prepare E2E run
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46628&view=logs&j=81be5d54-0dc6-5130-d390-233dd2956037&t=d72874ca-f446-5272-2efd-0705f108dbf6",,,,,,,,,,,,,,,,,,,,,FLINK-30972,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:57:53 UTC 2023,,,,,,,,,,"0|z1gd08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:10;luoyuxia;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46807&view=results]

cc @[~Leonard] ;;;","06/Mar/23 09:12;tison;cc [~chesnay] I guess you're familiar with this part?;;;","06/Mar/23 09:57;martijnvisser;[~luoyuxia] Please make sure that you have rebased onto the latest changes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaPartitionSplitReaderTest.testPendingRecordsGauge failed to create topic,FLINK-31333,13527210,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,06/Mar/23 09:00,16/Oct/23 07:10,04/Jun/24 20:41,16/Oct/23 07:10,1.15.3,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46765&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5
{code}
Mar 03 03:17:58 [INFO] Running org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:175)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:216)
	at org.apache.flink.connector.kafka.testutils.KafkaSourceTestEnv.createTestTopic(KafkaSourceTestEnv.java:217)
	at org.apache.flink.connector.kafka.testutils.KafkaSourceTestEnv.setupTopic(KafkaSourceTestEnv.java:261)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testPendingRecordsGauge(KafkaPartitionSplitReaderTest.java:198)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-06 09:00:33.0,,,,,,,,,,"0|z1gczc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the use of ExecutionConfig on JdbcOutputFormat,FLINK-31332,13527203,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,eskabetxe,eskabetxe,eskabetxe,06/Mar/23 08:38,17/Nov/23 11:14,04/Jun/24 20:41,17/Nov/23 11:14,,,,,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,0,auto-deprioritized-major,pull-request-available,,,"This is for limiting the use of ExecutionConfig on JdbcOutputFormat and centralize the logic of serialisation of records in one place.

Also current the record will be copied at least 2/3 times if isObjectReuse is enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 11:14:22 UTC 2023,,,,,,,,,,"0|z1gcxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","17/Nov/23 11:14;Sergey Nuyanzin;Merged as [a2fd18aab6e63a2f273d185db1afa7b06844aacf|https://github.com/apache/flink-connector-jdbc/commit/a2fd18aab6e63a2f273d185db1afa7b06844aacf];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.16 should implement new LookupFunction,FLINK-31331,13527201,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,06/Mar/23 08:33,06/Mar/23 09:45,04/Jun/24 20:41,06/Mar/23 09:45,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Only implements new LookupFunction, retry lookup join can work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:45:13 UTC 2023,,,,,,,,,,"0|z1gcxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:45;lzljs3620320;master: dfc0d558d2c0d5d299e2da2ffa0819d0c4720919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch shuffle may deadlock for operator with priority input,FLINK-31330,13527196,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,Weijie Guo,Weijie Guo,Weijie Guo,06/Mar/23 08:08,12/Mar/23 15:09,04/Jun/24 20:41,12/Mar/23 15:09,1.16.1,,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,,"For batch job, some operator's input have priority. For example, hash join operator has two inputs called {{build}} and {{probe}} respectively. Only after the build input is finished can the probe input start consuming. Unfortunately, the priority of input will not affect multiple inputs to request upstream data(i.e. request partition). In current implementation, when all states are restored, inputGate will start to request partition. This will enable the upstream {{IO scheduler}} to register readers for all downstream channels, so there is the possibility of deadlock.
Assume that the build and probe input's upstream tasks of hash join are deployed in the same TM. Then the corresponding readers will be registered to an single {{IO scheduler}}, and they share the same {{BatchShuffleReadBufferPool}}.  If the IO thread happens to load too many buffers for the probe reader, but the downstream will not consume the data, which will cause the build reader to be unable to request enough buffers. Therefore, deadlock occurs.
In fact, we realized this problem at the beginning of the design of {{SortMergeShuffle}}, so we introduced a timeout mechanism when requesting read buffers. If this happens, the downstream task will trigger failover to avoid permanent blocking. However, under the default configuration, TPC-DS test with 10T data can easily cause the job to fail because of this reason. It seems that this problem needs to be solved more better.
",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 12 15:09:47 UTC 2023,,,,,,,,,,"0|z1gcw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/23 15:09;Weijie Guo;After some investigation and debugging, it is finally found that the problem has been fixed in FLINK-16536. After that, request partition will comply with the priority of input.
As a result, it is easy to observe that the time of the initialization state becomes very long. Because the {{InputGate}} with the lowest priority will start to restore the channel state and then turn to the running state only after all other inputs are finished. A possible optimization approach is to start the state transition after the channel state data is loaded, rather than being processed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Parquet stats extractor,FLINK-31329,13527173,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,06/Mar/23 05:43,06/Mar/23 09:01,04/Jun/24 20:41,06/Mar/23 09:01,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Some bugs in Parquet stats extractor:
 # Decimal Supports
 # Timestamp Supports
 # Null nullCounts supports",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:01:40 UTC 2023,,,,,,,,,,"0|z1gcr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:01;lzljs3620320;master: 97ca28c4f97ed705b0ae27cd0b30954db1dd6a18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Greedy option on the looping pattern at the end not working,FLINK-31328,13527165,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Juntao Hu,Juntao Hu,06/Mar/23 03:47,06/Mar/23 03:47,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Library / CEP,,,,0,,,,,"If use greedy option on a looping pattern which is at the end of the whole pattern, the matching result is not ""greedy"".

Example1

pattern: A.oneOrMore().consecutive().greedy() (SKIP_TO_NEXT)

sequence: a1, a2, a3

result: [a1] [a2] [a3]

Example2

pattern: B.next(A).oneOrMore().consecutive().greedy() (SKIP_TO_NEXT)

sequence: b1, a1, a2, a3

result: [b1 a1]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-06 03:47:26.0,,,,,,,,,,"0|z1gcpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added conversion method for GenericRowData,FLINK-31327,13527161,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,hunterLiu,hunterLiu,06/Mar/23 03:19,06/Mar/23 05:36,04/Jun/24 20:41,06/Mar/23 05:36,,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"I think when using GenericRowData, it is more difficult to convert to Java pojo type, so I think it is necessary to add a prize in GenericRowData to convert the GenericRowData type to Java pojo type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-06 03:19:03.0,,,,,,,,,,"0|z1gcog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabled source scaling breaks downstream scaling if source busyTimeMsPerSecond is 0,FLINK-31326,13527137,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mateczagany,mateczagany,05/Mar/23 15:20,09/Mar/23 15:05,04/Jun/24 20:41,09/Mar/23 15:05,kubernetes-operator-1.5.0,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,,"In case of 'scaling.sources.enabled'='false' the 'TARGET_DATA_RATE' of the source vertex will be calculated as '(1000 / busyTimeMsPerSecond) * numRecordsOutPerSecond' which currently on the main branch results in an infinite value if 'busyTimeMsPerSecond' is 0. This will also affect downstream operators.

I'm not that familiar with the autoscaler code, but it's in my opinion it's quite unexpected to have such behavioral changes by setting 'scaling.sources.enabled' to false.

 

With PR #543 for FLINK-30575 (https://github.com/apache/flink-kubernetes-operator/pull/543) scaling will happen even with 'busyTimeMsPerSecond' being 0, but it will result in unreasonably high parallelism numbers for downstream operators because 'TARGET_DATA_RATE' will be very high where 0 'busyTimeMsPerSecond' will be replaced with 1e-10.


Metrics from the operator logs (source=e5a72f353fc1e6bbf3bd96a41384998c, sink=51312116a3e504bccb3874fc80d5055e)

'scaling.sources.enabled'='true':
{code:java}
 jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.MAX_PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_UP_RATE_THRESHOLD.Current: 5.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_DOWN_RATE_THRESHOLD.Current: 10.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Current: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Average: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Current: 3.8666666666666667
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Average: 3.8833333333333333

jobVertexID.51312116a3e504bccb3874fc80d5055e.PARALLELISM.Current: 4.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.MAX_PARALLELISM.Current: 12.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Current: 4.827299209321681
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Average: 4.848351269098938
jobVertexID.51312116a3e504bccb3874fc80d5055e.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_UP_RATE_THRESHOLD.Current: 10.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_DOWN_RATE_THRESHOLD.Current: 21.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Current: 7.733333333333333
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Average: 7.766666666666667{code}

'scaling.sources.enabled'='false':
{code:java}
 jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.MAX_PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_UP_RATE_THRESHOLD.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_DOWN_RATE_THRESHOLD.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Current: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Average: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Average: NaN

jobVertexID.51312116a3e504bccb3874fc80d5055e.PARALLELISM.Current: 4.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.MAX_PARALLELISM.Current: 12.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Current: 5.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Average: 4.980555555555556
jobVertexID.51312116a3e504bccb3874fc80d5055e.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_UP_RATE_THRESHOLD.Current: NaN
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_DOWN_RATE_THRESHOLD.Current: NaN
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Current: Infinity
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Average: NaN{code}
 

My guess is 'scaling.sources.enabled' exists to support connectors where `pendingRecords` is not available, but setting this to false also negatively impacts existing Kafka sources for example, and users cannot anticipate this from the documentation.

 

I think it would be worth it to include this in the docs, or if anyone has any suggested solutions I would gladly look into implementing it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 11:17:19 UTC 2023,,,,,,,,,,"0|z1gcj4:",9223372036854775807,Handle new-style and old-style sources equally well and remove option to disable scaling sources,,,,,,,,,,,,,,,,,,,"05/Mar/23 18:26;gyfora;Good catch [~mateczagany] , there are a few oddities around source metrics at the moment when load / incoming data rate is low. After we merge [~mxm] 's work on the pending record / zero scaling improvements, we should revisit this config and simplify the code for the source metrics if possible. ;;;","06/Mar/23 11:17;mxm;Thanks [~mateczagany] for reporting! You are right that the disabled source scaling isn't working properly and has unexpected side effects. It was a testing workaround in the very beginning to work with legacy sources which do not report busy time. I would propose to remove this option altogether and automatically skip scaling of sources which do not report busy time (NaN metric values).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Improve Swing Transformer,FLINK-31325,13527129,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,indyw,indyw,indyw,05/Mar/23 13:18,19/Apr/23 01:37,04/Jun/24 20:41,19/Apr/23 01:37,ml-2.2.0,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Optimize _ComputingSimilarItems_ operator of

_org.apache.flink.ml.recommendation.swing.Swing._ Optimized code can lead to less cpu time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:37:11 UTC 2023,,,,,,,,,,"0|z1gchc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:37;lindong;Merged to apache/flink-ml master branch 2a5991827f8885d7a17023dee43da016be12dd79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken SingleThreadFetcherManager constructor API,FLINK-31324,13527091,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,yunta,yunta,04/Mar/23 15:40,06/Mar/23 10:13,04/Jun/24 20:41,06/Mar/23 10:13,,,,,,,,,,,,1.17.0,1.18.0,,,Connectors / Parent,,,,0,pull-request-available,,,,"FLINK-28853 changed the default constructor of {{SingleThreadFetcherManager}}. Though the {{SingleThreadFetcherManager}} is annotated as {{Internal}}, it actually acts as some-degree public API, which is widely used in many connector projects:
[flink-cdc-connector|https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlSourceReader.java#L93], [flink-connector-mongodb|https://github.com/apache/flink-connector-mongodb/blob/main/flink-connector-mongodb/src/main/java/org/apache/flink/connector/mongodb/source/reader/MongoSourceReader.java#L58] and so on.

Once flink-1.17 is released, all these existing connectors are broken and cannot be used in new release version, and will throw exceptions like:

{code:java}
java.lang.NoSuchMethodError: org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.<init>(Lorg/apache/flink/connector/base/source/reader/synchronization/FutureCompletingBlockingQueue;Ljava/util/function/Supplier;)V
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader.<init>(MySqlSourceReader.java:91) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.MySqlSource.createReader(MySqlSource.java:159) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
	at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:312) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:94) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]
{code}


Thus, I suggest to make the original SingleThreadFetcherManager constructor as depreacted instead of removing it.",,,,,,,,,,,,,,,,,,,,,,,FLINK-28853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 10:13:53 UTC 2023,,,,,,,,,,"0|z1gc8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/23 15:42;yunta;cc [~jark] [~mxm];;;","06/Mar/23 02:29;jark;+1 for keeping compatibility. 

Besides, I think {{SingleThreadFetcherManager}} should be annotated {{@PublicEvolving}} because it is referenced by the public constructor of the  {{@PublicEvolving}} class {{SingleThreadMultiplexSourceReaderBase}}. 

According to FLIP-196,
> Per default, all public members of an API object inherit the stability guarantee of the owning object.

But I think this annotating {{@PublicEvolving}} may need another issue and discussion on the dev ML. 

What do you think [~yunta]?;;;","06/Mar/23 03:31;yunta;I think we can discuss changing the annotation of {{SingleThreadFetcherManager}}. However, the {{Internal}} annotation is introduced in FLINK-22358, which changed a lot of classes. Since I am not so familiar with these changes, I think we need to involve more committers on connector topics for discussion. cc [~becket_qin], [~renqs];;;","06/Mar/23 08:22;becket_qin;I think `SingleThreadFetcherManager` is indeed somewhat public at the moment. Connector implementations extend this class from time to time. So we probably need to make it backwards compatible even though it is marked as internal.

It also looks OK if we make it PublicEvolving. If we do so, the only additional class that we also need to make public is `FutureCompletingBlockingQueue` as PublicEvolving. So it does not pull in much unnecessary class exposures to the users.;;;","06/Mar/23 08:41;yunta;[~becket_qin], thanks for the reply. As the expert on the connector modules, I think you're more suitable to launch the discussion in the dev ML, and we can have a deep discussion in another ticket.;;;","06/Mar/23 10:13;yunta;merged,
master: 4d285753b75157ca078ba0917412c273d147aba9
release-1.17: 8acec2c14a2a56b9293354693008cc9c3e77692e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unstable E2E test for flink actions,FLINK-31323,13527067,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,04/Mar/23 07:44,07/Mar/23 02:33,04/Jun/24 20:41,07/Mar/23 02:33,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently, the flink actions use Data Stream API to do insert job making batch configuration invalid. We should use Table API instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 02:33:43 UTC 2023,,,,,,,,,,"0|z1gc3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 02:33;lzljs3620320;master: 1840de127c453b710d4aa0e1709c0001880f62f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve merge-into action,FLINK-31322,13527066,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yzl,yzl,04/Mar/23 07:41,19/Mar/23 05:36,04/Jun/24 20:41,19/Mar/23 05:36,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,,,,,Umbrella issue for improving flink merge-into action.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-04 07:41:45.0,,,,,,,,,,"0|z1gc3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Yarn-session mode, securityConfiguration supports dynamic configuration",FLINK-31321,13527064,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,felixzh,felixzh,felixzh,04/Mar/23 03:46,17/Mar/23 02:04,04/Jun/24 20:41,17/Mar/23 02:04,1.17.0,,,,,,,,,,,1.17.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"when different tenants submit jobs using the same {_}flink-conf.yaml{_}, the same user is displayed on the Yarn page.
_SecurityConfiguration_ does not support dynamic configuration. Therefore, the user displayed on the Yarn page is the _security.kerberos.login.principal_ in the {_}flink-conf.yaml{_}.

FLINK-29435 only fixed CliFrontend class(Corresponds to flink script).

FlinkYarnSessionCli class(Corresponds to yarn-session.sh script) still exists this question.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 02:04:03 UTC 2023,,,,,,,,,,"0|z1gc2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 02:04;xtsong;- master (1.18): 6f9bca971f69525f9be7779121706bfb5756089d
- release-1.17: 17ab9fda0bcd2687275ea8af2215b939d644cb07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify DATE_FORMAT system (built-in) function to accepts DATEs,FLINK-31320,13527041,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jamesmcguirepro,jamesmcguirepro,03/Mar/23 19:42,24/Jul/23 23:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"The current {{DATE_FORMAT}} function only supports {{{}TIMESTAMP{}}}s. 

(See https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/#temporal-functions)

 

Ideally, it should be able to format {{{}DATE{}}}'s as well as {{TIMESTAMPs}}

 

Example usage:
{noformat}
Flink SQL> CREATE TABLE test_table (
>   some_date DATE,
>   object AS JSON_OBJECT(
>     KEY 'some_date' VALUE DATE_FORMAT(some_date, 'YYYY-MM-dd')
>   )
> )
> COMMENT ''
> WITH (
>   'connector'='datagen'
> )
> ;
> 
[ERROR] Could not execute SQL statement. Reason:
org.apache.calcite.sql.validate.SqlValidatorException: Cannot apply 'DATE_FORMAT' to arguments of type 'DATE_FORMAT(<DATE>, <CHAR(10)>)'. Supported form(s): 'DATE_FORMAT(<TIMESTAMP>, <STRING>)'
'DATE_FORMAT(<STRING>, <STRING>)'{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 23:10:54 UTC 2023,,,,,,,,,,"0|z1gbxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 23:10;jamesmcguirepro;Looks like we have a PR for this one: https://github.com/apache/flink/pull/22353;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka new source partitionDiscoveryIntervalMs=0 cause bounded source can not quit,FLINK-31319,13527030,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,taoran,taoran,taoran,03/Mar/23 17:44,12/Apr/23 17:06,04/Jun/24 20:41,18/Mar/23 09:59,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.0,kafka-3.0.0,,Connectors / Kafka,,,,0,pull-request-available,,,,"As kafka option description, partitionDiscoveryIntervalMs <=0 means disabled.

!image-2023-03-04-01-37-29-360.png|width=781,height=147!

just like start kafka enumerator:

!image-2023-03-04-01-39-20-352.png|width=465,height=311!

but inner 
handlePartitionSplitChanges use error if condition( < 0):

!image-2023-03-04-01-40-44-124.png|width=576,height=237!
 
it will cause noMoreNewPartitionSplits can not be set to true. 
!image-2023-03-04-01-41-55-664.png|width=522,height=610!

Finally cause bounded source can not signalNoMoreSplits, so it will not quit.

Besides，Both ends of the if condition should be mutually exclusive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 17:37;taoran;image-2023-03-04-01-37-29-360.png;https://issues.apache.org/jira/secure/attachment/13056028/image-2023-03-04-01-37-29-360.png","03/Mar/23 17:39;taoran;image-2023-03-04-01-39-20-352.png;https://issues.apache.org/jira/secure/attachment/13056027/image-2023-03-04-01-39-20-352.png","03/Mar/23 17:40;taoran;image-2023-03-04-01-40-44-124.png;https://issues.apache.org/jira/secure/attachment/13056026/image-2023-03-04-01-40-44-124.png","03/Mar/23 17:41;taoran;image-2023-03-04-01-41-55-664.png;https://issues.apache.org/jira/secure/attachment/13056025/image-2023-03-04-01-41-55-664.png",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 18 09:59:10 UTC 2023,,,,,,,,,,"0|z1gbvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 20:29;taoran;Reproduce:  Using bounded kafka, set partitionDiscoveryIntervalMs=0, then job never quit. [~martijnvisser] [~renqs]  WDYT? it's maybe a big bug.;;;","09/Mar/23 08:57;martijnvisser;[~tzulitai] WDYT?;;;","16/Mar/23 14:11;leonard;Both 'partitionDiscoveryIntervalMs=0' and 'partitionDiscoveryIntervalMs<0' are meaningless,  it makes sense to me to do not discovery partitions in these cases. ;;;","16/Mar/23 19:20;ramanverma;[~lemonjing] Can you please link all the back port PRs (1.16 and 1.17) here as well for easier tracking;;;","17/Mar/23 02:19;taoran;[~ramanverma] thanks.
BP-1.16: [https://github.com/apache/flink/pull/22192]
BP-1.17: [https://github.com/apache/flink/pull/22193]
original: [https://github.com/apache/flink-connector-kafka/pull/8];;;","17/Mar/23 07:39;leonard;[~lemonjing] minor tips: the PR links would be automatically created If you used PR title  *FLINK-31319[BP-1.16]xxx*  instead of *[BP-1.16]FLINK-31319xxx. :)*;;;","17/Mar/23 09:27;renqs;1.17: 09fb503f0f3bba2fa0bbab9452baebe07af288cc;;;","18/Mar/23 09:59;leonard;Fixed by: 
 * flink-connector-kafka main : 58f35374b6aec63491623321f4de69a0affa629a
 * flink 1.17: 09fb503f0f3bba2fa0bbab9452baebe07af288cc
 * flink 1.16: a5f085e042be70f45485165c5755171ac5ed8cb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not scale down operators while processing backlog,FLINK-31318,13527029,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,03/Mar/23 17:43,07/Mar/23 16:06,04/Jun/24 20:41,07/Mar/23 16:06,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,,"Currently the autoscaler may try to scale down some operators even when the job is struggling to catch up. 

This can lead to a vicious cycle where the backlog keeps increasing. It makes sense to hold off scale down decisions until the job has caught up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 16:06:54 UTC 2023,,,,,,,,,,"0|z1gbv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 16:06;gyfora;merged to main 8933435ecc647438967e6f522056b8e560d7750b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce data structures for managing resource requirements of a job,FLINK-31317,13527019,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,03/Mar/23 16:17,15/Mar/23 17:12,04/Jun/24 20:41,15/Mar/23 15:59,,,,,,,,,,,,1.18.0,,,,,,,,0,pull-request-available,,,,"We need to introduce new data structures - JobResourceRequirements and JobVertexResource requirements and the corresponding tooling, that we'll propagate throughout the stack in the upcoming issues.",,,,,,,,,,,,,FLINK-31399,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 16:00:58 UTC 2023,,,,,,,,,,"0|z1gbsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 16:00;dmvk;master: a84de2ca4a9239bccb0d28c9bb6f841de1879bf9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-291: Externalized Declarative Resource Management,FLINK-31316,13527017,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dmvk,dmvk,dmvk,03/Mar/23 16:10,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,,,,,,1.20.0,,,,Runtime / Coordination,Runtime / REST,,,0,,,,,"This is an umbrella ticket for [FLIP-291|https://cwiki.apache.org/confluence/display/FLINK/FLIP-291%3A+Externalized+Declarative+Resource+Management].",,,,,,,,,,,,,,,,,,,,FLINK-30773,,,,,,,,FLINK-31996,FLINK-31935,FLINK-31744,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-03 16:10:44.0,,,,,,,,,,"0|z1gbsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkActionsE2eTest.testMergeInto is unstable,FLINK-31315,13527011,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,03/Mar/23 15:45,19/Mar/23 05:36,04/Jun/24 20:41,19/Mar/23 05:36,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"{code:java}
Error:  Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 320.272 s <<< FAILURE! - in org.apache.flink.table.store.tests.FlinkActionsE2eTest
82Error:  testMergeInto  Time elapsed: 111.826 s  <<< FAILURE!
83org.opentest4j.AssertionFailedError: 
84Result is still unexpected after 60 retries.
85Expected: {3, v_3, creation, 02-27=1, 2, v_2, creation, 02-27=1, 6, v_6, creation, 02-28=1, 1, v_1, creation, 02-27=1, 8, v_8, insert, 02-29=1, 11, v_11, insert, 02-29=1, 7, Seven, matched_upsert, 02-28=1, 5, v_5, creation, 02-28=1, 10, v_10, creation, 02-28=1, 9, v_9, creation, 02-28=1}
86Actual: {4, v_4, creation, 02-27=1, 8, v_8, creation, 02-28=1, 3, v_3, creation, 02-27=1, 7, v_7, creation, 02-28=1, 2, v_2, creation, 02-27=1, 6, v_6, creation, 02-28=1, 1, v_1, creation, 02-27=1, 5, v_5, creation, 02-28=1, 10, v_10, creation, 02-28=1, 9, v_9, creation, 02-28=1}
87	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
88	at org.junit.jupiter.api.Assertions.fail(Assertions.java:134)
89	at org.apache.flink.table.store.tests.E2eTestBase.checkResult(E2eTestBase.java:261)
90	at org.apache.flink.table.store.tests.FlinkActionsE2eTest.testMergeInto(FlinkActionsE2eTest.java:355) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-03 15:45:42.0,,,,,,,,,,"0|z1gbr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle may not release result partitions when running multiple jobs in a session cluster,FLINK-31314,13526968,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,tanyuxin,tanyuxin,tanyuxin,03/Mar/23 10:31,19/Jun/23 04:42,04/Jun/24 20:41,19/Jun/23 04:42,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"When I test jobs in a session cluster, I found many result partitions belonging to the finished jobs may not be released.

I have reproduce the issue locally by running HybridShuffleITCase#testHybridFullExchanges repeatedly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-03 10:31:28.0,,,,,,,,,,"0|z1gbhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsupported meta columns in column list of insert statement,FLINK-31313,13526962,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lincoln.86xy,lincoln.86xy,03/Mar/23 09:41,03/Mar/23 12:10,04/Jun/24 20:41,03/Mar/23 12:10,1.16.1,1.17.0,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"Currently an error will be raised when ref meta columns in column list of insert statement, e.g.,
{code}
INSERT INTO sink (a,b,f) -- here `f` is a metadata column of sink table
SELECT ...{code}

{code}
Caused by: org.apache.calcite.runtime.CalciteContextException: At line 1, column 44: Unknown target column 'f'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:932)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:917)
	at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:440)
	at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:428)
	at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:169)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:161)
	at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:72)
{code}

The cause is current PreValidateReWriter in validation phase uses the physical types of sink table which does not include metadata columns",,,,,,,,,,,,,,,,,,,,,,FLINK-30922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 12:08:35 UTC 2023,,,,,,,,,,"0|z1gbg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 10:28;csq;Hi [~lincoln.86xy], I think it is the same issue with FLINK-30922.;;;","03/Mar/23 12:08;lincoln.86xy;[~csq] Thanks for reminding, I've marked this duplicated. Also I left some comments on your pr.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EnableObjectReuse cause different behaviors,FLINK-31312,13526942,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,03/Mar/23 05:44,13/Mar/23 07:00,04/Jun/24 20:41,,,,,,,,,,,,,,,,,API / DataStream,,,,0,,,,,"I have the following test code which fails with the exception `Accessing a field by name is not supported in position-based field mode`, however, if I remove the `enableObjectReuse`, it works. 

The `SourceFunction` generates rows without field names, but the return type info is assigned by `env.addSource(rowGenerator, typeInfo)`.

With object-reuse enabled, rows would be passed to the MapFunction directly, so the exception raises. While if the object-reuse is disabled,  rows would be reconstructed and given field names when passing to the next operator and the test works well.
{code:java}
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);

    // The test fails with enableObjectReuse
    env.getConfig().enableObjectReuse();

    final SourceFunction<Row> rowGenerator =
            new SourceFunction<Row>() {
                @Override
                public final void run(SourceContext<Row> ctx) throws Exception {
                    Row row = new Row(1);
                    row.setField(0, ""a"");
                    ctx.collect(row);
                }

                @Override
                public void cancel() {}
            };

    final RowTypeInfo typeInfo =
            new RowTypeInfo(new TypeInformation[] {Types.STRING}, new String[] {""col1""});

    DataStream<Row> dataStream = env.addSource(rowGenerator, typeInfo);

    DataStream<Row> transformedDataStream =
            dataStream.map(
                    (MapFunction<Row, Row>) value -> Row.of(value.getField(""col1"")), typeInfo);

    transformedDataStream.addSink(new PrintSinkFunction<>());
    env.execute(""Mini Test"");
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 07:00:39 UTC 2023,,,,,,,,,,"0|z1gbbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 08:44;xzw0223;[~Jiang Xin]  Because enableObjectReuse is enabled, the row generated by source is the same as the row object in map. Because objects are reused, the row constructed by source adds field data by location. Therefore, the specified field data needs to be retrieved by location in map,if enableObjectReuse is not enabled, a row object will be re-copied every time you emit. At this time, the row object will construct its fields according to RowTypeInfo, so there is no problem.;;;","03/Mar/23 08:47;xzw0223;I think enableObjectReuse and outputTypeInfo conflict, or enableObjectReuse has a higher priority than outputTypeInfo.;;;","03/Mar/23 09:18;Jiang Xin;[~xzw0223] Thanks for the reply. 
From the perspective of users, the `enableObjectReuse` is an optimization, removing this configuration should only cause a performance issue, rather than an exception. As you said, ""the row object will construct its fields according to RowTypeInfo every time it is emitted"", but this is the implementation detail, users only see inconsistent behaviors.;;;","03/Mar/23 09:33;xzw0223;Yes, so I think that after enablingObjectReuse and TypeInfo are configured, they are in conflict. If you want to add a field, you need to copy a row, which is equivalent to the failure of enableObjectReuse. If you reuse the row, adding TypeInfo is equal to invalid.;;;","09/Mar/23 08:26;kevin.cyj;There is another constructor in Row:

Row(
RowKind kind,
@Nullable Object[] fieldByPosition,
@Nullable Map<String, Object> fieldByName,
@Nullable LinkedHashMap<String, Integer> positionByName);

I guess if this is used, there should be no problem.;;;","09/Mar/23 09:22;Jiang Xin;[~kevin.cyj] Thanks for the reply. Sure, I believe it can be solved by constructing the Row with `positionByName`, but this constructor is not public, furthermore, we can not tell users that they can only use this constructor even if it is public. The main problem is that users are able to access a positioned row by name with object-reuse enabled, while are not able to access it with object-reuse disabled. I want to know is it a bug or feature?;;;","13/Mar/23 07:00;kevin.cyj;Sorry, I did not notice the fact that the constructor is not public. Personally, I do not think it is bug or a feature, I would treat it as a weakness in API design.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports Bounded Watermark streaming read,FLINK-31311,13526941,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,03/Mar/23 05:29,08/Mar/23 04:27,04/Jun/24 20:41,08/Mar/23 04:27,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"There are some bound stream scenarios that require that stream reading can be ended. Generally speaking, the end event time is the better.

So in this ticket, supports writing the watermark to the snapshot and can specify the ending watermark when reading the stream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 04:27:17 UTC 2023,,,,,,,,,,"0|z1gbbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 04:27;lzljs3620320;master: e715ac81a906e8f1b285d3847cc75879bb54d062;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Force clear directory no matter what situation in HiveCatalog.dropTable,FLINK-31310,13526935,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,nicholasjiang,lzljs3620320,lzljs3620320,03/Mar/23 03:13,06/Mar/23 09:27,04/Jun/24 20:41,06/Mar/23 09:27,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Currently, if no table in hive, will not clear the table.

We should clear table directory in any situation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:27:05 UTC 2023,,,,,,,,,,"0|z1gba8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 07:12;nicholasjiang;[~lzljs3620320], could you assign this to me?;;;","03/Mar/23 08:25;lzljs3620320;[~nicholasjiang] Thanks!;;;","04/Mar/23 16:01;nicholasjiang;[~lzljs3620320], the `dropTable` interface is invoked after `getTable`, therefore if no tale in hive, the `dropTable` could not be invoked, because there is a check whether table exist in `getDataTableSchema`. The exist situation occurs in the `getTable`, not in `dropTable`.

IMO, when no table in hive, users could use the FileSystemCatalog to drop the table and clear table directory and HiveCatalog only drops the table in Hive and clear table directory via hive metastore client.;;;","06/Mar/23 09:27;lzljs3620320;[~nicholasjiang] You are right, it is good to keep the files on DFS.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rollback DFS schema if hive sync fail in HiveCatalog.createTable,FLINK-31309,13526934,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,03/Mar/23 03:10,06/Mar/23 09:22,04/Jun/24 20:41,06/Mar/23 09:22,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Avoid schema residue on DFS.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:22:04 UTC 2023,,,,,,,,,,"0|z1gba0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 05:28;zjureel;[~lzljs3620320] Please help to assign to me, thanks;;;","06/Mar/23 09:22;lzljs3620320;master: fa51956e4fd1df18c7cfa87ae0f0042bab2a209b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager's metaspace out-of-memory when submit a flinksessionjobs,FLINK-31308,13526932,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanjialiang,tanjialiang,03/Mar/23 02:35,17/Apr/23 08:48,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,0,,,,,"Hello teams, when i try to recurring submit a flinksessionjobs by flink operator, it will be make JobManager's metaspace OOM. My Job having some flink-sql logic, it is the userclassloader didn't closed? Or may be beacuase of flink-sql's codegen? By the way, it not appear when i using flink-sql-gateway to submit.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 02:34;tanjialiang;image-2023-03-03-10-34-46-681.png;https://issues.apache.org/jira/secure/attachment/13055985/image-2023-03-03-10-34-46-681.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 02:46:55 UTC 2023,,,,,,,,,,"0|z1gb9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 02:46;zjureel;I think it's a same kind of issue in https://issues.apache.org/jira/browse/FLINK-28691 cc [~FrankZou];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB：java.lang.UnsatisfiedLinkError,FLINK-31307,13526931,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wujunzhe,wujunzhe,03/Mar/23 02:30,08/Mar/23 07:37,04/Jun/24 20:41,,1.14.5,,,,,,,,,,,,,,,,,,,0,,,,,"when i use rocksdb like 

!image-2023-03-03-10-27-04-810.png!

 

 I got an unsolvable exception. 

!image-2023-03-03-10-29-27-477.png!

 What can I do to troubleshoot or solve this problem? 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 02:27;wujunzhe;image-2023-03-03-10-27-04-810.png;https://issues.apache.org/jira/secure/attachment/13055983/image-2023-03-03-10-27-04-810.png","03/Mar/23 02:29;wujunzhe;image-2023-03-03-10-29-27-477.png;https://issues.apache.org/jira/secure/attachment/13055982/image-2023-03-03-10-29-27-477.png","03/Mar/23 03:28;wujunzhe;image-2023-03-03-11-28-54-674.png;https://issues.apache.org/jira/secure/attachment/13055986/image-2023-03-03-11-28-54-674.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 07:37:17 UTC 2023,,,,,,,,,,"0|z1gb9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 03:04;yunta;This is the basic usage of RocksDB TTL and the method existed in [FlinkCompactionFilter|https://github.com/ververica/frocksdb/blob/d6f50f33064f1d24480dfb3c586a7bd7a7dbac01/java/src/main/java/org/rocksdb/FlinkCompactionFilter.java#L34 ].

From my understanding, this shall not happen, did your JVM process kick some classes to disk due to very poor memory left?;;;","03/Mar/23 03:28;wujunzhe;How to check my jvm process memory left, what I can be sure is that there is enough memory on the virtual machine, and there is also enough memory for jm and tm. The same configuration parameters can run normally on our test server, but not on the development server. The only difference is that one is a physical machine and the other is a virtual machine. [~yunta] 

!image-2023-03-03-11-28-54-674.png!;;;","06/Mar/23 10:57;yunta;Since the TTL usage of RocksDB is so widely used, I cannot believe there exists such an obvious bug here. What's the environment of your development server, is that a Linux amd64 machine?;;;","07/Mar/23 01:39;wujunzhe;yes，os version is debian9.5， I am also very confused, as long as comment out the code that uses rocksdb, the program can run. :([~yunta] ;;;","08/Mar/23 07:37;yunta;What's the meaning of {{2.2.1}} after {{frocksdbjni-6.20.3-ververica-1.0.jar}}? One possible problem is that your application jar contains an open-source RocksDB which does not have such a method only in Flink version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Servable for PipelineModel,FLINK-31306,13526928,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,03/Mar/23 01:33,13/Mar/23 12:10,04/Jun/24 20:41,13/Mar/23 12:10,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add servable for PipelineModel based on FLIP-289 [1].

[1] https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=240881268",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 12:10:26 UTC 2023,,,,,,,,,,"0|z1gb8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 12:10;lindong;Merged to apache/flink-ml master branch 7a4e5d13a7832b2b411beff6127c923662b67401.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaWriter doesn't wait for errors for in-flight records before completing flush,FLINK-31305,13526920,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mason6345,mason6345,mason6345,02/Mar/23 23:47,17/Apr/23 07:16,04/Jun/24 20:41,12/Apr/23 17:05,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.1,kafka-3.0.0,,Connectors / Kafka,,,,1,pull-request-available,,,,"The KafkaWriter flushing needs to wait for all in-flight records to send successfully. This can be achieved by tracking requests and returning a response from the registered callback from the producer#send() logic.

There is potential for data loss since the checkpoint does not accurately reflect that all records have been sent successfully, to preserve at least once semantics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 07:13:39 UTC 2023,,,,,,,,,,"0|z1gb6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 23:49;mason6345;Hi all, this is a critical regression for deployments who have migrated from the FlinkKafkaProducer to KafkaSink. Can someone with permissions assign this issue to me? Thanks in advance!;;;","09/Mar/23 09:01;martijnvisser;[~tzulitai] FYI;;;","10/Mar/23 07:51;mason6345;Opened a PR! cc: [~stevenz3wu] [~martijnvisser] [~tzulitai] ;;;","10/Mar/23 08:06;martijnvisser;[~mason6345] IIRC this would also affect Flink 1.17, right? If so, we should open the PR in the Flink branch, so it can be fixed for Flink 1.17 (since the connector is still bundled in 1.17) and then later cherry-picked to the external connector repo, and potentially also Flink 1.16? ;;;","10/Mar/23 08:22;mason6345;[~martijnvisser] Yes it does affect 1.17. Makes sense, and my question from the other JIRA ticket is answered here!;;;","10/Mar/23 18:32;mason6345;BTW, I had a mishap with git and so this is the proper Github PR to review: https://github.com/apache/flink/pull/22150;;;","23/Mar/23 03:34;mason6345;[~martijnvisser] [~tzulitai] Have time to review this small change? I need help from a committer;;;","23/Mar/23 04:17;tzulitai;thanks [~mason6345] for the ping! I've actually been discussing your changes with Raman Verma, who has did a review already on the PR.
I'll also do a pass tomorrow and then will try to merge this by end of this week.;;;","27/Mar/23 19:06;tzulitai;Merged via
 * {{apache/flink:main}}  7f47c11da1bbbd6e7650d43742694f2fa0ee8a3f
 * {{{}apache/flink:release-1.16{}}}: df6c4345d2e1b321cd77e74aa2b028b482b69f3c
 * {{{}apache/flink:release-1.17{}}}: b8a9fe5a9942fb6cf11ed56a807d68d375339259

Pending merges for {{apache/flink-connector-kafka}} as well as any necessary backports.;;;","12/Apr/23 17:05;tzulitai;Merged to flink-connector-kafka via:
 * v3.0 - dd09fa9a3d05c7096c85cb14f2a792a66a915547;;;","15/Apr/23 12:22;asardaes;I understand the externalized release of the connector (v3.0) will only be compatible with Flink 1.17.x, but _if_ a Flink 1.16.2 patch is released, will it also include a non-externalized release of the connector? Given the criticality of this, I had hoped the externalized connector would also support 1.16.x so I could immediately use it with 1.16.1.;;;","17/Apr/23 07:13;martijnvisser;[~asardaes] Yes, given that this is backported to the Flink 1.16 branch and (now) has a fixversion set to Flink 1.16.2 (since it exists in the {{apache/flink:release-1.16}} branch this will also be made available with Flink 1.16.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Very slow job start if topic has been used before,FLINK-31304,13526888,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,YordanPavlov,YordanPavlov,02/Mar/23 16:44,02/Mar/23 17:31,04/Jun/24 20:41,,1.15.2,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"We have the following use case. We use KafkaSink with Exactly once semantic, from time to time we would re-start the job clean, in doing so we delete and re-create the output topic and also any Flink checkpoints. In such situation it would take close to an hour for Flink to start. In the the time the job is idling we would see the following log in the Taskmanager:


{code:java}
2023-03-02 16:33:42.004 [Source: Kafka source blocks -> Deduplicate blocks -> Map -> Parse blocks -> Map -> Kafka sink volume: Writer -> Kafka sink volume: Committer (2/5)#0] INFO  o.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-state.clickhouse-0-1-1, transactionalId=state.clickhouse-0-1-1] Invoking InitProducerId for the first time in order to acquire a producer ID
2023-03-02 16:33:42.005 [kafka-producer-network-thread | producer-state.clickhouse-0-2-1] INFO  o.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-state.clickhouse-0-2-1, transactionalId=state.clickhouse-0-2-1] ProducerId set to 31719488 with epoch 8{code}

If we use a brand new output topic name, the job would start straight away. Could you advise if this can be improved?

Such logs would go on and on in what seems forever.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 17:31:23 UTC 2023,,,,,,,,,,"0|z1gb00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 17:31;martijnvisser;[~YordanPavlov] Do you also start with a new transactional ID prefix for your application? If not, these are expected to be unique for each Flink application that you run on the same Kafka cluster. Given that you're basically starting a job from new, that would also require a new transactional ID prefix. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
k8s operator should gather job cpu and memory utilization metrics,FLINK-31303,13526886,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,02/Mar/23 16:11,04/Apr/23 14:15,04/Jun/24 20:41,04/Apr/23 14:15,kubernetes-operator-1.5.0,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,Runtime / Metrics,,,0,pull-request-available,,,,We should extend the operator metrics system to gather this additional information and be able to expose it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 14:15:25 UTC 2023,,,,,,,,,,"0|z1gazk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 14:15;mbalassi;d9ec4b034078a64d08ed01705f5712a45c9a2ca7 and 048c492d09c2208904908d3f15a3a235969a7c19 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split spark modules according to version,FLINK-31302,13526880,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,02/Mar/23 15:29,03/Mar/23 07:46,04/Jun/24 20:41,03/Mar/23 07:46,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 07:46:28 UTC 2023,,,,,,,,,,"0|z1gay8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 07:46;lzljs3620320;master: 77fd8ecfd599c9d6c2b706bee4684558a8ba2f40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsupported nested columns in column list of insert statement,FLINK-31301,13526878,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,aitozi,lincoln.86xy,lincoln.86xy,02/Mar/23 15:22,12/Aug/23 22:35,04/Jun/24 20:41,,1.16.1,1.17.0,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,stale-assigned,,,"Currently an error will be raised when use nested columns in column list of insert statement, e.g.,
{code:java}
INSERT INTO nested_type_sink (a,b.b1,c.c2,f)
SELECT a,b.b1,c.c2,f FROM nested_type_src
{code}
 
{code:java}
java.lang.AssertionError
    at org.apache.calcite.sql.SqlIdentifier.getSimple(SqlIdentifier.java:333)
    at org.apache.calcite.sql.validate.SqlValidatorUtil.getTargetField(SqlValidatorUtil.java:612)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:171)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50)
    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:161)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:281)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.StatementSetImpl.addInsertSql(StatementSetImpl.java:63)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:07 UTC 2023,,,,,,,,,,"0|z1gaxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 15:44;aitozi;Hello [~lincoln.86xy], I'm interested in this ticket and spend some time to look into this. I think we can support insert partial nested column by analysing the targetRow type to derive the RowType from the RelDataType. Then, we can construct the target row by {{row(b, cast(null as int), row(cast(null as varchar), c))}} to make up the complete row.
WDYT ?;;;","05/Apr/23 15:06;aitozi;Hello [~lincoln.86xy], I want to contribute to this ticket and I have basically finished this. Can you help assign this ticket to me ?;;;","06/Apr/23 01:38;lincoln.86xy;[~aitozi] welcome for contributing! assigned to you;;;","11/Apr/23 05:44;aitozi;[~lincoln.86xy] , I opened PR for this, could you help review it, thanks.;;;","12/Apr/23 01:55;lincoln.86xy;[~aitozi] thanks for fixing this! I'll try to find some time to take a look.;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TRY_CAST fails for constructed types,FLINK-31300,13526876,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,02/Mar/23 15:20,09/Mar/23 22:14,04/Jun/24 20:41,09/Mar/23 22:14,1.16.1,1.17.0,,,,,,,,,,1.18.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"In case of problems with cast it is expected to return {{null}}

however for arrays, maps it fails

example of failing queries
{code:sql}
select try_cast(array['a'] as array<int>);
select try_cast(map['a', '1'] as map<int, int>);
{code}

 {noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: 'a'. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at StreamExecCalc$15.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 22:14:10 UTC 2023,,,,,,,,,,"0|z1gaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 22:14;Sergey Nuyanzin;Merged at [5fef51f60085306f2acc8c8d630fe08e64004fc3|https://github.com/apache/flink/commit/5fef51f60085306f2acc8c8d630fe08e64004fc3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PendingRecords metric might not be available,FLINK-31299,13526857,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,02/Mar/23 12:57,06/Mar/23 16:34,04/Jun/24 20:41,06/Mar/23 16:34,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,,"The Kafka pendingRecords metric is only initialized on receiving the first record. For empty topics or checkpointed topics without any incoming data, the metric won't appear.

We need to handle this case in the autoscaler and allow downscaling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-02 12:57:04.0,,,,,,,,,,"0|z1gat4:",9223372036854775807,Work around idle Kafka readers not emitting the pendingRecords metric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionUtilsTest.testFindConnectingAddressWhenGetLocalHostThrows swallows IllegalArgumentException,FLINK-31298,13526830,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,mapohl,mapohl,02/Mar/23 10:01,09/Mar/23 06:01,04/Jun/24 20:41,09/Mar/23 06:01,1.15.3,1.16.1,1.17.0,,,,,,,,,1.16.2,1.17.0,,,Runtime / Network,,,,1,pull-request-available,starter,test-stability,,"FLINK-24156 introduced {{NetUtils.acceptWithoutTimeout}} which caused the test to print a the stacktrace of an {{IllegalArgumentException}}:
{code}
Exception in thread ""Thread-0"" java.lang.IllegalArgumentException: serverSocket SO_TIMEOUT option must be 0
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.util.NetUtils.acceptWithoutTimeout(NetUtils.java:139)
	at org.apache.flink.runtime.net.ConnectionUtilsTest$1.run(ConnectionUtilsTest.java:83)
	at java.lang.Thread.run(Thread.java:750)
{code}

This is also shown in the Maven output of CI runs and might cause confusion. The test should be fixed.",,,,,,,,,,,,,,,,,,,,,,,FLINK-24156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 06:38:00 UTC 2023,,,,,,,,,,"0|z1gan4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 03:03;Wencong Liu;Hello [~mapohl] , I'd like to take this ticket. SocketOptions.SO_TIMEOUT should be set to 0.;;;","06/Mar/23 12:22;mapohl;thanks [~Wencong Liu]. I assigned the ticket to you;;;","08/Mar/23 02:50;Wencong Liu;cc [~mapohl] ;;;","08/Mar/23 06:38;Weijie Guo;master(1.18) via 7c5b7be5bc165a9799f10b5761a6ff15edee43b6.
release-1.17 via 704076a36024d521957e4e2f31820bbad7a102b3.
release-1.16 via b7b1cced495e29075adda10496238f251fe74d53.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager unstable when running it a single time,FLINK-31297,13526828,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,huwh,mapohl,mapohl,02/Mar/23 09:51,06/Mar/23 15:03,04/Jun/24 20:41,06/Mar/23 13:30,1.17.0,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"We noticed a weird test-instability in {{FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager}} when switching to sequential test execution (see FLINK-31278). I couldn't reproduce it in 1.16, therefore, marking it as a blocker for now. But it feels to be more of a test code issue.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46671&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9695

{code}
Mar 01 15:20:17 [ERROR] org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager  Time elapsed: 0.746 s  <<< FAILURE!
Mar 01 15:20:17 java.lang.AssertionError: 
Mar 01 15:20:17 
Mar 01 15:20:17 Expected size: 1 but was: 0 in:
Mar 01 15:20:17 []
Mar 01 15:20:17 	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager(FineGrainedSlotManagerTest.java:209)
Mar 01 15:20:17 
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-18229,,,,,,,,FLINK-31278,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 13:30:04 UTC 2023,,,,,,,,,,"0|z1gamo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 09:53;mapohl;I can reproduce it locally [~huwh] may you have a look at it?;;;","02/Mar/23 10:22;xzw0223;After adding the PendingTaskManager, the pending is cleaned up in the registerTaskManager, I think the expected value should be 0 instead of 1.;;;","02/Mar/23 10:33;Weijie Guo;Thanks [~mapohl] for reporting this, I will take a look at this and contact [~huwh] at the same time.;;;","02/Mar/23 10:37;Weijie Guo;[~xzw0223] Thanks for looking into this, but I do not think directly change the expected value is the right approach. As the comments before the assertion, we can see that `task manager with allocated slot cannot deduct pending task manager`, the expected behavior is the pending taskmanager will not be reduced to 0, right?;;;","02/Mar/23 10:55;Weijie Guo;[~mapohl] By offline discuss with [~huwh], this is just a test code issue, I think we can down the priority.;;;","02/Mar/23 11:05;mapohl;Thanks for your verification :) I downgraded it to Critical. It's kind of blocking some efforts around FLINK-31278, though.;;;","02/Mar/23 16:02;huwh;Thanks [~mapohl] for reporting this bug. Thanks [~Weijie Guo] [~xzw0223] for your attention.

This bug was introduced by FLINK-18229. We release the pending task manager when these is no more resource requirements.

In FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager, we skip the FineGrainedSlotManager and invoke TaskManagerTracker.addPendingTaskManager directly to allocate pending task manager. This make the resource requirements different between SlotManager and TaskManagerTracker. After requirementCheckDelay(50ms by default), the requirement check will release the pending task manager.;;;","06/Mar/23 03:54;huwh;Hi, [~xtsong] [~Weijie Guo] Could you help review this bugfix.;;;","06/Mar/23 13:30;Weijie Guo;master(1.18) via bec6a4589703bb7619cfc04bf69822995b49893f.
release-1.17 via c71d880dd01bddf926b4cebc33beb6c92b9aa25d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JoinConditionEqualityTransferRule to stream optimizer,FLINK-31296,13526804,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,02/Mar/23 05:59,19/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,pull-request-available,,,"I find that {{JoinConditionEqualityTransferRule}} is a common rule for batch and stream mode. So it should be added to the stream optimizer which will bring performance improvement in some case.

Maybe, other rules also need to be reviewed whether can be aligned in batch and stream case.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:07 UTC 2023,,,,,,,,,,"0|z1gahc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 06:30;aitozi;But this will affect the join key pair in the Join operator which may affect the state compatibility, I'm not sure how to handle this implicit breaking;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When kerbero authentication is enabled, hadoop_ Proxy cannot take effect.",FLINK-31295,13526803,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,HunterHunter,HunterHunter,02/Mar/23 05:31,02/Mar/23 14:23,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"When I set ：

security.kerberos.login.keytab: kerbero_user.keytab
security.kerberos.login.principal: kerbero_user

and set  HADOOP_PROXY_USER = proxy_user

Data is still written to hdfs as user kerbero_user.

But : 

When I turn off kerbero authentication.  data is written to hdfs as user proxy_user.

Finally, I found the logic in HadoopModule#install would not be used as a hadoop proxy when kerbero authentication is enabled.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 14:23:01 UTC 2023,,,,,,,,,,"0|z1gah4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 09:32;gaborgsomogyi;Have you tested the latest master? Please see https://issues.apache.org/jira/browse/FLINK-31109;;;","02/Mar/23 14:23;gaborgsomogyi;I can't decide whether you are using tokens or plain UGI authentication. If you accidentally using tokens then proxy user is just not supported.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitterOperator forgot to close Committer when closing.,FLINK-31294,13526800,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,02/Mar/23 05:05,03/Mar/23 11:18,04/Jun/24 20:41,03/Mar/23 11:18,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"{{CommitterOperator}} does not close the {{Committer}} when it closes, which may lead to resource leaks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 11:18:39 UTC 2023,,,,,,,,,,"0|z1gagg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:20;Ming Li;[~lzljs3620320], hi, please help take a look at this issue if you have time.;;;","03/Mar/23 03:06;lzljs3620320;[~Ming Li] Thanks!;;;","03/Mar/23 11:18;lzljs3620320;master: 2e053e445be99dc0e7fc445728c381bbb8e7af37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Request memory segment from LocalBufferPool may hanging forever.,FLINK-31293,13526796,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,02/Mar/23 04:21,04/Apr/23 02:37,04/Jun/24 20:41,04/Apr/23 02:36,1.16.1,1.17.0,1.18.0,,,,,,,,,1.16.2,1.17.1,1.18.0,,Runtime / Network,,,,0,pull-request-available,,,,"In our TPC-DS test, we found that in the case of fierce competition in network memory, some tasks may hanging forever.

From the thread dump information, we can see that the task is waiting for the {{LocalBufferPool}} to become available. It is strange that other tasks have finished and released network memory already. Undoubtedly, this is an unexpected behavior, which implies that there must be a bug in the {{LocalBufferPool}} or {{{}NetworkBufferPool{}}}.

!image-2023-03-02-12-23-50-572.png|width=650,height=153!

By dumping the heap memory, we can find a strange phenomenon that there are available buffers in the {{{}LocalBufferPool{}}}, but it was considered to be un-available. Another thing to note is that it now holds an overdraft buffer.

!image-2023-03-02-12-28-48-437.png|width=520,height=200!

!image-2023-03-02-12-29-03-003.png|width=438,height=84!

TL;DR: This problem occurred in multi-thread race related to the introduction of overdraft buffer.

Suppose we have two threads, called A and B. For simplicity, {{LocalBufferPool}} is called {{LocalPool}} and {{NetworkBufferPool}} is called {{{}GlobalPool{}}}.

Thread A continuously request buffers blocking from the {{{}LocalPool{}}}.
Thread B continuously return buffers to {{{}GlobalPool{}}}.
 # If thread A takes the last available buffer of {{{}LocalPool{}}}, but {{GlobalPool}} does not have a buffer at this time, it will register a callback function with {{{}GlobalPool{}}}.
 # Thread B returns one buffer to {{{}GlobalPool{}}}, but has not started to trigger the callback.
 # Thread A continues to request buffer. Because the {{availableMemorySegments}} of {{LocalPool}} is empty, it requests the overdraftBuffer instead. But there is already a buffer in the {{{}GlobalPool{}}}, it successfully gets the buffer.
 # Thread B triggers the callback. Since there is no buffer in {{GlobalPool}} now, the callback is re-registered.
 # Thread A continues to request buffer. Because there is no buffer in {{{}GlobalPool{}}}, it will block on {{{}CompletableFuture#get{}}}.
 # Thread B continues to return a buffer and triggers the recently registered callback. As a result, {{LocalPool}} puts the buffer into {{{}availableMemorySegments{}}}. Unfortunately, the current logic of {{shouldBeAvailable}} method is: if there is an overdraft buffer, {{LocalPool}} is considered as un-available.
 # Thread A will keep blocking forever.",,,,,,,,,,,,,,,,,,,,FLINK-31104,,,FLINK-26762,,,,,,,,,,,,,"02/Mar/23 04:23;Weijie Guo;image-2023-03-02-12-23-50-572.png;https://issues.apache.org/jira/secure/attachment/13055956/image-2023-03-02-12-23-50-572.png","02/Mar/23 04:28;Weijie Guo;image-2023-03-02-12-28-48-437.png;https://issues.apache.org/jira/secure/attachment/13055957/image-2023-03-02-12-28-48-437.png","02/Mar/23 04:29;Weijie Guo;image-2023-03-02-12-29-03-003.png;https://issues.apache.org/jira/secure/attachment/13055958/image-2023-03-02-12-29-03-003.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 02:36:37 UTC 2023,,,,,,,,,,"0|z1gafk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 02:36;Weijie Guo;master(1.18) via fb6caee13710348a9b53284c2cabbdb2e7aa9739.
release-1.17 via 6a476bee5e452d1f172173ec018939c8a154886c.
release-1.16 via 9582727387d368d1b9e358aedb55c3f2eaae4371.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User HadoopUtils to get Configuration in CatalogContext,FLINK-31292,13526795,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyubin117,lzljs3620320,lzljs3620320,02/Mar/23 04:17,11/Mar/23 03:42,04/Jun/24 20:41,11/Mar/23 03:42,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"At present, if HadoopConf is not passed in the CatalogContext, a new HadoopConf will be directly generated, which may not have the required parameters.

We can refer to HadoopUtils to obtain hadoopConf from the configuration and environment variables.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 03:42:54 UTC 2023,,,,,,,,,,"0|z1gafc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:55;liyubin117;[~lzljs3620320] Good idea! Could you please assign this to me?;;;","03/Mar/23 03:06;lzljs3620320;[~liyubin117]  Thanks!;;;","10/Mar/23 02:07;liyubin117;[~lzljs3620320] I will finish the feature as soon as possible:D;;;","11/Mar/23 03:42;lzljs3620320;master: d2d4dd0bf8d206307eb2c93aa97e7c1cb750f187;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document table.exec.sink.upsert-materialize to none,FLINK-31291,13526794,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianer17,lzljs3620320,lzljs3620320,02/Mar/23 04:03,03/Mar/23 07:11,04/Jun/24 20:41,03/Mar/23 05:34,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"The table store has the ability to correct disorder, such as:

[https://nightlies.apache.org/flink/flink-table-store-docs-master/docs/concepts/primary-key-table/#sequence-field]

But Flink SQL default sink materialize will result strange behavior, In particular, write to the agg table of the fts.

We should document this, set table.exec.sink.upsert-materialize to none always, set 'sequence.field' to table in case of disorder.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 05:34:40 UTC 2023,,,,,,,,,,"0|z1gaf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:35;dianer17;Hi jingsong, I can take this task, would you please assign it to me?;;;","03/Mar/23 05:34;lzljs3620320;master: 2d1da3d8780b46740a1962e118a2dbd780791480;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove features in documentation,FLINK-31290,13526790,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianer17,lzljs3620320,lzljs3620320,02/Mar/23 03:52,06/Mar/23 08:51,04/Jun/24 20:41,06/Mar/23 08:51,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Features is confused in documentation.

Now, there are two pages in features, log system and lookup join.

We can move log system to concepts.

And move lookup join to how-to.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 08:51:24 UTC 2023,,,,,,,,,,"0|z1gae8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:36;dianer17;Hi Jingsong, I can take this task. Would you please assign it to me?;;;","06/Mar/23 08:51;lzljs3620320;master: c139b4baa508d8a8ef35d2013b684e206cffb1b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default aggregate-function for field can be last_non_null_value,FLINK-31289,13526789,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,02/Mar/23 03:50,02/Mar/23 05:36,04/Jun/24 20:41,02/Mar/23 05:36,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"At present, when aggfunc is not configured, NPE will be generated. When the table is oriented to many fields, the configuration will be more troublesome.

We can give the field the default aggfunc, such as last_ non_ null_ Value, which is consistent with the partial-update table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 05:36:06 UTC 2023,,,,,,,,,,"0|z1gae0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:36;lzljs3620320;master: 30f03264bf188df3ccf753cc53da421b3464318c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable overdraft buffer for batch shuffle,FLINK-31288,13526788,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,02/Mar/23 03:49,03/Mar/23 14:50,04/Jun/24 20:41,03/Mar/23 14:50,1.16.1,1.17.0,,,,,,,,,,1.17.0,,,,,,,,0,pull-request-available,,,,"Only pipelined / pipelined-bounded partition needs overdraft buffer. More specifically, there is no reason to request more buffers for non-pipelined (i.e. batch) shuffle. The reasons are as follows:
 # For BoundedBlockingShuffle, each full buffer will be directly released.
 # For SortMergeShuffle, the maximum capacity of buffer pool is 4 * numSubpartitions. It is efficient enough to spill this part of memory to disk.
 # For Hybrid Shuffle, the buffer pool is unbounded. If it can't get a normal buffer, it also can't get an overdraft buffer.",,,,,,,,,,,,,,,,,,,,FLINK-31104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 15:35:00 UTC 2023,,,,,,,,,,"0|z1gads:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 10:08;renqs;Marked as blocker of 1.17 since we need to wait for this one before RC. ;;;","02/Mar/23 15:35;Weijie Guo;master(1.18) via 382148e1229901ab54503c8d9af6a18ea4c078dc.
release-1.17 via 7dd61c31714c1b07790982d21a486f5f803708df.
release-1.16 via 01c8eb59c1be92f1f8c1b81c66073eeb6009eb86.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default value of 'changelog-producer.compaction-interval' can be zero,FLINK-31287,13526787,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xzw0223,lzljs3620320,lzljs3620320,02/Mar/23 03:48,07/Mar/23 02:59,04/Jun/24 20:41,07/Mar/23 02:57,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"At present, the 30-minute interval is too conservative. We can set it to 0 by default, so that each checkpoint will have a full-compaction and generate a changelog.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 02:57:06 UTC 2023,,,,,,,,,,"0|z1gadk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 04:00;xzw0223;[~lzljs3620320]  Can I have a ticket? I think I can do it.;;;","03/Mar/23 03:06;lzljs3620320;[~xzw0223] Thanks!;;;","07/Mar/23 02:57;lzljs3620320;master: 77b4a085424e446863b6d2e4464b4689898d51d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python processes are still alive when shutting down a session cluster directly without stopping the jobs,FLINK-31286,13526778,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,02/Mar/23 02:57,08/Mar/23 17:54,04/Jun/24 20:41,02/Mar/23 06:38,,,,,,,,,,,,1.15.4,1.16.2,1.17.0,,API / Python,,,,0,pull-request-available,,,,"Reproduce steps:
1) start a standalone cluster: ./bin/start_cluster.sh
2) submit a PyFlink job which contains Python UDFs
3) stop the cluster: ./bin/stop_cluster.sh
4) Check if Python process still exists: ps aux | grep -i beam_boot

!image-2023-03-02-10-55-34-863.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 02:55;dianfu;image-2023-03-02-10-55-34-863.png;https://issues.apache.org/jira/secure/attachment/13055946/image-2023-03-02-10-55-34-863.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:38:36 UTC 2023,,,,,,,,,,"0|z1gabk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 06:38;dianfu;Fixed in:
- master: ab4e85e8bda51088cf64d5ddfb9bc0dab1c6e1fd
- 1.17: ecec13a1cdf6622b8f0257f35c24d597f9956f41
- 1.16: ed47440231a75e5de50038919a21a1e914458baa
- 1.15: ada67c0c1bd5b51a2c7cf10624a0e4f2870a9cc5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSource should support reading files in order,FLINK-31285,13526763,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sap1ens,sap1ens,01/Mar/23 23:57,02/Mar/23 03:24,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"Currently, Flink's *FileSource* uses *LocalityAwareSplitAssigner* as a default *FileSplitAssigner* and it doesn't guarantee any order. In many scenarios involving processing historical data, reading files in order can be a requirement, especially when using event-time processing. 

I believe a new FileSplitAssigner should be implemented that supports ordering. FileSourceBuilder should be extended to allow choosing a different FileSplitAssigner.

It's also clear that the files may not be read in _perfect_ order with parallelism > 1. However, in some cases, using parallelism of 1 might be fine.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 03:24:52 UTC 2023,,,,,,,,,,"0|z1ga88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 01:50;jark;I think extending FlinkSourceBuilder to accept user-defined FileSplitAssigner is totally reasonable. Do you have any ideas on how to sort the files? ;;;","02/Mar/23 03:24;sap1ens;I think FileSplitAssigner is the only thing that has to be fixed to support this, so no other ideas. I'd also love to include an implementation that actually performs sorting, so users can just choose it when building a FileSource. Probably a similar approach for the Table API.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase KerberosLoginProvider test coverage,FLINK-31284,13526711,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,01/Mar/23 15:51,03/Mar/23 16:41,04/Jun/24 20:41,03/Mar/23 16:41,1.17.0,,,,,,,,,,,1.18.0,,,,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 16:41:30 UTC 2023,,,,,,,,,,"0|z1g9wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 16:41;gyfora;merged to master 7f5240c9f912ec68c0b18b0022147eaa27992e4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the description of building from source with scala version,FLINK-31283,13526697,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,01/Mar/23 14:14,07/Mar/23 06:19,04/Jun/24 20:41,07/Mar/23 06:19,,,,,,,,,,,,1.15.4,1.16.2,1.17.0,1.18.0,Documentation,,,,0,pull-request-available,,,,"After FLINK-20845, Flink dropped the support of Scala 2.11. However, the doc of ""building from source"" still has the description on scala-2.11.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 06:19:25 UTC 2023,,,,,,,,,,"0|z1g9tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 06:19;yunta;merged
master: 6ac40847d3bd60aa9a373dfb6a390f4c67c6c48d
release-1.17: 07c193b20a89243fd59f068cc64e073b9a0f6f34
release-1.16: 085a70c8c69fef29a2ffe4a08f0a29a6874a3fbd
release-1.15: 4f27e70d317357a9eb2a3f8c6f1dbb6f86779149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create and Close the zookeeper connection frequently,FLINK-31282,13526686,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,serhan123,serhan123,01/Mar/23 12:21,19/Aug/23 10:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,Client / Job Submission,,,,0,auto-deprioritized-major,pull-request-available,,,"When I use sql "" *select * from table* "" , I found there are a lot logs about zookeeper connection Information.  There is *critical* for Zookeeper ，because *the Connection* is *create and close frequently*

 

[

 

2023-03-01 19:38:38.423 INFO  (Thread-262) [   ] o.a.f.r.l.DefaultLeaderRetrievalService Starting DefaultLeaderRetrievalServic
e with ZookeeperLeaderRetrievalDriver\{connectionInformationPath='/leader/rest_server/connection_info'}.
2023-03-01 19:38:38.424 INFO  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn Socket connection
established, initiating session, client: /10.121.65.53:58428, server: node53.hde.com/10.121.65.53:2181
2023-03-01 19:38:38.427 INFO  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn Session establishm
ent complete on server node53.hde.com/10.121.65.53:2181, session id = 0x10000038726412a, negotiated timeout = 60000
2023-03-01 19:38:38.427 INFO  (Thread-262-EventThread) [   ] o.a.f.s.c.o.a.c.f.s.ConnectionStateManager State change: CONNECTE
D
2023-03-01 19:38:38.430 INFO  (Thread-262-EventThread) [   ] o.a.f.s.c.o.a.c.f.i.EnsembleTracker New config event received: {s
erver.1=node53.hde.com:2888:3888:participant, version=0, server.3=node55.hde.com:2888:3888:participant, server.2=node54.hde.co
m:2888:3888:participant}
2023-03-01 19:38:38.431 INFO  (Thread-262-EventThread) [   ] o.a.f.s.c.o.a.c.f.i.EnsembleTracker New config event received: {s
erver.1=node53.hde.com:2888:3888:participant, version=0, server.3=node55.hde.com:2888:3888:participant, server.2=node54.hde.co
m:2888:3888:participant}
2023-03-01 19:38:38.700 INFO  (ForkJoinPool.commonPool-worker-11) [   ] o.a.f.r.l.DefaultLeaderRetrievalService Stopping Defau
ltLeaderRetrievalService.
2023-03-01 19:38:38.700 INFO  (ForkJoinPool.commonPool-worker-11) [   ] o.a.f.r.l.ZooKeeperLeaderRetrievalDriver Closing Zooke
eperLeaderRetrievalDriver\{connectionInformationPath='/leader/rest_server/connection_info'}.
2023-03-01 19:38:38.702 INFO  (Curator-Framework-0) [   ] o.a.f.s.c.o.a.c.f.i.CuratorFrameworkImpl backgroundOperationsLoop ex
iting
2023-03-01 19:38:38.708 WARN  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn An exception was t
hrown while closing send thread for session 0x10000038726412a.
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EndOfStreamException: Unable to read additional data from s
erver sessionid 0x10000038726412a, likely server has closed socket
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77) ~[fli
nk-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:35
0) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290) [flink-shad
ed-zookeeper-3.6.3.jar:3.6.3-15.0]
2023-03-01 19:38:38.810 INFO  (Thread-262-EventThread) [   ] o.a.f.s.z.o.a.z.ClientCnxn EventThread shut down for session: 0x1
0000038726412a
2023-03-01 19:38:38.810 INFO  (ForkJoinPool.commonPool-worker-11) [   ] o.a.f.s.z.o.a.z.ZooKeeper Session: 0x10000038726412a c
losed
2023-03-01 19:38:38.813 INFO  (Thread-262) [   ] o.a.f.r.u.ZooKeeperUtils Enforcing default ACL for ZK connections
2023-03-01 19:38:38.813 INFO  (Thread-262) [   ] o.a.f.r.u.ZooKeeperUtils Using '/flink/application_1677479737242_0025' as Zoo
keeper namespace.
2023-03-01 19:38:38.814 INFO  (Thread-262) [   ] o.a.f.s.c.o.a.c.f.i.CuratorFrameworkImpl Starting
2023-03-01 19:38:38.814 INFO  (Thread-262) [   ] o.a.f.s.z.o.a.z.ZooKeeper Initiating client connection, connectString=node54.
hde.com:2181,node55.hde.com:2181,node53.hde.com:2181 sessionTimeout=60000 watcher=org.apache.flink.shaded.curator5.org.apache.
curator.ConnectionState@74e5bc14
2023-03-01 19:38:38.814 INFO  (Thread-262) [   ] o.a.f.s.z.o.a.z.ClientCnxnSocket jute.maxbuffer value is 1048575 Bytes
2023-03-01 19:38:38.814 INFO  (Thread-262) [   ] o.a.f.s.z.o.a.z.ClientCnxn zookeeper.request.timeout value is 0. feature enab
led=false
2023-03-01 19:38:38.815 INFO  (Thread-262) [   ] o.a.f.s.c.o.a.c.f.i.CuratorFrameworkImpl Default schema
2023-03-01 19:38:38.815 INFO  (Thread-262) [   ] o.a.f.r.l.DefaultLeaderRetrievalService Starting DefaultLeaderRetrievalServic
e with ZookeeperLeaderRetrievalDriver\{connectionInformationPath='/leader/rest_server/connection_info'}.
2023-03-01 19:38:38.817 INFO  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn Opening socket con
nection to server node53.hde.com/10.121.65.53:2181.
2023-03-01 19:38:38.817 INFO  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn SASL config status
: Will not attempt to authenticate using SASL (unknown error)
2023-03-01 19:38:38.817 INFO  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn Socket connection
established, initiating session, client: /10.121.65.53:58432, server: node53.hde.com/10.121.65.53:2181
2023-03-01 19:38:38.821 INFO  (Thread-262-SendThread(node53.hde.com:2181)) [   ] o.a.f.s.z.o.a.z.ClientCnxn Session establishm
ent complete on server node53.hde.com/10.121.65.53:2181, session id = 0x10000038726412b, negotiated timeout = 60000
2023-03-01 19:38:38.821 INFO  (Thread-262-EventThread) [   ] o.a.f.s.c.o.a.c.f.s.ConnectionStateManager State change: CONNECTE
D
2023-03-01 19:38:38.824 INFO  (Thread-262-EventThread) [   ] o.a.f.s.c.o.a.c.f.i.EnsembleTracker New config event received: {s
erver.1=node53.hde.com:2888:3888:participant, version=0, server.3=node55.hde.com:2888:3888:participant, server.2=node54.hde.co
m:2888:3888:participant}



 

]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:07 UTC 2023,,,,,,,,,,"0|z1g9r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonFunctionRunner doesn't extend AutoCloseable but implements close,FLINK-31281,13526678,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,taoran,taoran,01/Mar/23 11:38,28/Mar/23 02:14,04/Jun/24 20:41,28/Mar/23 02:14,,,,,,,,,,,,,,,,API / Python,,,,0,,,,,"The PythonFunctionRunner provides a {{close}} method (see [PythonFunctionRunner|https://github.com/apache/flink/blob/0612a997ddcc791ee54f500fbf1299ce04987679/flink-python/src/main/java/org/apache/flink/python/PythonFunctionRunner.java]) but doesn't implement {{{}AutoCloseable{}}}. However {{AutoCloseable}} would enable us to use Java's try-with-resources feature and more generic utility classes.",,,,,,,,,,,FLINK-31015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 02:13:54 UTC 2023,,,,,,,,,,"0|z1g9pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 11:45;taoran;[~mapohl]  Hi, Matthias. We fix the FLINK-31015. But PythonFunctionRunner & LeaderRetrievalDriver need to address either, although these 2 interfaces are @Internal.  What do u think? ;;;","27/Mar/23 12:59;mapohl;Sorry for the late reply. We're not gaining much from it. In FLINK-31015 was created to improve the usability for SDK users. I would not focus on that one and rather close this issue. WDYT?;;;","28/Mar/23 02:13;taoran;got it. i will close it.[~mapohl] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make core surefire plugin configuration stricter,FLINK-31280,13526658,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,01/Mar/23 09:36,01/Mar/23 09:38,04/Jun/24 20:41,01/Mar/23 09:37,,,,,,,,,,,,,,,,,,,,0,,,,,We should update the {{flink-core}} surefire plugin configuration in order to ease the debuging of FLINK-31278.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 09:37:58 UTC 2023,,,,,,,,,,"0|z1g9kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:37;mapohl;having subtasks for this is overkill.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix MULTIPLY(TIMES) function doesn't support interval types,FLINK-31279,13526652,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,01/Mar/23 09:01,24/Jan/24 04:01,04/Jun/24 20:41,08/Mar/23 09:33,1.18.0,,,,,,,,,,,1.18.0,,,,,,,,0,pull-request-available,,,,"{code:java}
// code placeholder
Flink SQL> select 2 * interval '3'  day;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.planner.codegen.CodeGenException: Interval expression type expected. {code}",,,,,,,,,,,,,,,,,,,,FLINK-31090,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 09:33:02 UTC 2023,,,,,,,,,,"0|z1g9jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:55;martijnvisser;[~jackylau] Why is this is a bug, the documentation shows that you should use INTERVAL like

{code:java}
E.g., INTERVAL ‘10 00:00:00.004’ DAY TO SECOND, INTERVAL ‘10’ DAY, or INTERVAL ‘2-10’ YEAR TO MONTH return intervals.
{code}

https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/;;;","01/Mar/23 10:06;jackylau;[~martijnvisser] i say the times function with args number and interval type;;;","08/Mar/23 09:33;jark;Fixed in master: 75d94108a8e7dc51825ff9063c2c3f4649bf0eb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exit code 137 (i.e. OutOfMemoryError) in core module,FLINK-31278,13526647,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,01/Mar/23 08:48,16/Nov/23 16:46,04/Jun/24 20:41,,1.17.0,1.19.0,,,,,,,,,,,,,,Runtime / Coordination,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,"The following build failed due to a 137 exit code indicating an OutOfMemoryError:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46643&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7847

{code}
[...]
Mar 01 05:29:06 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.65 s - in org.apache.flink.runtime.io.compression.BlockCompressionTest
Mar 01 05:29:06 [INFO] Running org.apache.flink.runtime.dispatcher.DispatcherCachedOperationsHandlerTest
Mar 01 05:29:07 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.142 s - in org.apache.flink.runtime.dispatcher.DispatcherCachedOperationsHandlerTest
Mar 01 05:29:08 [INFO] Running org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStoreTest
##[error]Exit code 137 returned from process: file name '/usr/bin/docker', arguments 'exec -i -u 1001  -w /home/vsts_azpcontainer 5953b171e8ed4caba7af2b326533e249211ed4dcc48640edb3c1b0cbbcdf1a21 /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
Finishing: Test - core
{code}

This build ran on an Azure pipeline machine (Azure Pipelines 9) and, therefore, cannot be caused by FLINK-18356. That said, there was a concurrent 137 exit code build failure happening on agent ""Azure Pipelines 21"" (see [20230301.3|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46643&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7847]) ~10mins later",,,,,,,,,,,,,,,,,,,,,FLINK-33282,,,,FLINK-18356,,,,,,,FLINK-31297,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 13:22:20 UTC 2023,,,,,,,,,,"0|z1g9ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:02;mapohl;There is no heapdump provide due to a failure in the upload step. I extracted the tests that where running while the error happened based on the Maven output:
{code}
$ grep -e "" Tests run: "" -e ""\[INFO\] Running"" 20230301.3.txt | grep -o ""org.apache.flink.[a-zA-Z\.]*"" | sort | uniq -c | sort -n | head -5
      1 org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStoreTest
      1 org.apache.flink.runtime.io.disk.ChannelViewsTest
      1 org.apache.flink.runtime.io.disk.FileChannelManagerImplTest
      1 org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannelTest
      2 org.apache.flink.api.common.accumulators.AverageAccumulatorTest
{code}
Although, that's not necessarily an indication for the cause.

We see that {{ChannelViewsTest}} operates for a bit longer than the rest before the error occurs:
{code}
2023-03-01T05:28:56.0284123Z Mar 01 05:28:56 [INFO] Running org.apache.flink.runtime.io.disk.ChannelViewsTest
2023-03-01T05:29:03.2024639Z Mar 01 05:29:03 [INFO] Running org.apache.flink.runtime.io.disk.FileChannelManagerImplTest
2023-03-01T05:29:03.8510602Z Mar 01 05:29:03 [INFO] Running org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannelTest
2023-03-01T05:29:20.9205409Z Mar 01 05:29:08 [INFO] Running org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStoreTest
{code}

...but {{ChannelViewsTest}} seems to take longer in general (e.g. build [20230301.4|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46644&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7194] lists the test with 36s runtime).;;;","01/Mar/23 09:14;mapohl;[~roman] [~chesnay] can you help with the guessing game based on what was added in 1.17? In the mean time, the only thing I can think of is disabling fork reuse and hoping that we get more insights with future failures.;;;","02/Mar/23 23:45;roman;Given that MemoryExecutionGraphInfoStoreTest was executed the latest, I'd suppose the problem is there (if not in the environment).

Looking at its code, I see that it uses a single-threaded executor per test-class, but creates MemoryExecutionGraphInfoStore per test-method (i.e. shares the executor).

So probably, that executor got stuck in one test, which prevented the cleanup in subsequent tests.

 

But this is just a guess without the logs. I'm thinking about making sure that all the previous processes were stopped at the beginning of the Upload stage, so that the Upload doesn't gets killed by OOMKiller (disabling forks completely might affect test run times).
WDYT?
 
{quote}[~roman] [~chesnay] can you help with the guessing game based on what was added in 1.17? In the mean time, the only thing I can think of is disabling fork reuse and hoping that we get more insights with future failures.
{quote}
Could you elaborate, how disabling fork reuse would help?
 
I looked at the memory available at the [beginning|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46643&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=60]:
{code:java}
Mar 01 05:19:57 Memory information
Mar 01 05:19:57 MemTotal:        7110656 kB
Mar 01 05:19:57 MemFree:          401188 kB
Mar 01 05:19:57 MemAvailable:    6089948 kB
{code}
it doesn't look any smaller than in [successful|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46290&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=60] runs:
{code:java}
Feb 19 03:54:33 MemTotal:        7110656 kB
Feb 19 03:54:33 MemFree:          346696 kB
Feb 19 03:54:33 MemAvailable:    6094648 kB
{code}
So environment (memory) was fine at least at the beginning.;;;","06/Mar/23 13:12;mapohl;{quote}
Could you elaborate, how disabling fork reuse would help?
{quote}
Disabling the fork reuse and disabling parallel execution would enable us to identify the exact test that caused the OOM. It's not necessarily only the test that was started last that could spoil the heap space. You're right with your concern about test runtime. But it looks like the runtime for core will only increase from ~40mins to 1h10mins based on the CI run I did in [this issue's PR|https://github.com/apache/flink/pull/22052]. But I will have another look at the {{MemoryExecutionGraphInfoStoreTest}} as well.;;;","06/Mar/23 16:44;mapohl;Lowering the priority for this one to enable the RC generation for 1.17.0.;;;","07/Mar/23 07:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46883&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7841;;;","30/Mar/23 06:56;mapohl;I had an offline discussion with Chesnay on that matter. He pointed out that the error is actually not directly coming from the JVM but from Docker itself. But even there, 137 exit code usually means (based on historic data) that it's caused by a memory issue. A solution we discussed was the following one:

The unit tests run in 4 JVMs in parallel within docker right now. Docker failing with a 137 exit code probably means that the JVMs took up too much memory. Therefore, a possible solution to provoke such an error again would be to in crease the number of JVMs running parallel without changing there memory setup. 

But Chesnay had another point: Due to it happening quite rarely it could be that it's being caused by certain tests running at the same time. The previously proposed setup wouldn't cover this issue. Alternatively, we could let a concurrent process run in docker while we execute the junit tests in a single JVM. The custom process acquires a specific amount of memory which we can slowly increase. Any test that's requires an unusual amount of memory should fail first. That might be a viable solution to identify memory issues that caused Docker to fail.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","05/Oct/23 09:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53537&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8370;;;","07/Oct/23 17:06;afedulov;https://dev.azure.com/alexanderfedulov/Flink/_build/results?buildId=557&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=8376;;;","07/Oct/23 17:33;afedulov;https://dev.azure.com/alexanderfedulov/Flink/_build/results?buildId=557&view=logs&j=7f652c99-c3cd-5aee-11e2-f8e88140dbea&t=1fe0f51b-b0ac-5dfd-7645-a1ae7486f7da&l=8363;;;","14/Nov/23 07:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54197&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef;;;","14/Nov/23 07:42;mapohl;There is a 137 in the table module as well (which is related to FLINK-18356) but the two errors occurred at different times:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54243&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8384;;;","14/Nov/23 09:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54289&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8257;;;","16/Nov/23 13:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54512&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8353;;;","16/Nov/23 13:11;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54508&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8359;;;","16/Nov/23 13:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54446&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8455;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JM Deployment recovery logic inconsistent with health related restart ,FLINK-31277,13526639,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,01/Mar/23 08:13,02/Mar/23 08:42,04/Jun/24 20:41,02/Mar/23 08:42,kubernetes-operator-1.4.0,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The current JM Deployment logic that restarts missing deployments strictly requires HA metadata event for stateless deployments.

This is inconsistent with how the cluster health check related restarts work which can cause the operator to delete an unhealthy deployment and potentially leave it missing if the first deploy attempt fails.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 08:42:38 UTC 2023,,,,,,,,,,"0|z1g9go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 08:42;gyfora;Merged to main 8a30e9bd770b32e28d2a7c0fe1830f5f6d9ab090;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VectorIndexerTest#testFitAndPredictWithHandleInvalid fails,FLINK-31276,13526638,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,zhangzp,zhangzp,01/Mar/23 08:13,19/Apr/23 03:02,04/Jun/24 20:41,19/Apr/23 03:02,ml-2.2.0,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,,,,,"The unit test fails [1] and the error stack is:

 
Error:  Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.176 s <<< FAILURE! - in org.apache.flink.ml.feature.VectorIndexerTest 
[592|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:593]Error:  testFitAndPredictWithHandleInvalid Time elapsed: 4.178 s <<< FAILURE! 
[593|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:594]java.lang.AssertionError: expected:<The input contains unseen double: 2.0. See handleInvalid parameter for more options.> but was:<null> 
[594|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:595] at org.junit.Assert.fail(Assert.java:89) 
[595|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:596] at org.junit.Assert.failNotEquals(Assert.java:835) 
[596|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:597] at org.junit.Assert.assertEquals(Assert.java:120) 
[597|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:598] at org.junit.Assert.assertEquals(Assert.java:146) 
[598|https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426#step:4:599] at org.apache.flink.ml.feature.VectorIndexerTest.testFitAndPredictWithHandleInvalid(VectorIndexerTest.java:186)
 
 [1] [https://github.com/apache/flink-ml/actions/runs/4300867923/jobs/7497476426]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 08:14:03 UTC 2023,,,,,,,,,,"0|z1g9gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 08:14;zhangzp;I repeated running it for 1000 times and did not reproduce the bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink supports reporting and storage of source/sink tables relationship,FLINK-31275,13526635,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zjureel,zjureel,zjureel,01/Mar/23 07:57,01/Feb/24 02:07,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,FLIP-314 has been accepted https://cwiki.apache.org/confluence/display/FLINK/FLIP-314%3A+Support+Customized+Job+Lineage+Listener,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-6757,,,FLINK-32402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 02:07:35 UTC 2024,,,,,,,,,,"0|z1g9fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 08:01;zjureel;Hi [~jark] [~lzljs3620320] What do you think of this issue? thanks;;;","01/Mar/23 12:54;jark;What's the purpose to ""create relations between source and sink tables for the job""? ;;;","02/Mar/23 09:41;zjureel;[~jark] As described in FLIP-276, we'd like to store the relationship between source/sink tables and etl jobs, and then users can manage their etl jobs and tables. Currently flink get source/sink tables from `CatalogManager` in planner, we need need to assign a planner id to identify the job.;;;","02/Mar/23 10:55;jark;Thank you [~zjureel], I think the lineage information of tables and jobs is very useful and important. Flink should support reporting lineage information to external metadata systems (e.g., DataHub, Atlas), but this may need a dedicated FLIP.
;;;","02/Mar/23 11:38;zjureel;Thanks [~jark], please assign this issue to me and I will create a FLIP for it;;;","02/Mar/23 15:02;jark;Hi [~zjureel], feel free to create FLIP and start a discussion in dev ML. But according to the community contribution guideline[1], issues can be assigned only if there is consensus on the approach. 

[1]: https://flink.apache.org/how-to-contribute/contribute-code/#code-contribution-process;;;","03/Mar/23 00:28;zjureel;[~jark] Get. In order to describe what I want more accurately, I have updated this issue;;;","25/Jul/23 11:51;knaufk;I will mark this as ""Not finished"" for Flink 1.18 and remove the fixVersion from the ticket as the feature freeze has passed. Thanks, Konstantin (one of the release managers for Flink 1.18);;;","05/Oct/23 22:16;ZhenqiuHuang;[~jark]
This JIRA is mentioned in FLIP-314. Is there anyone who is actually working on it? ;;;","07/Oct/23 03:01;zjureel;[~ZhenqiuHuang] Thanks for your attention, FLIP-314 is on voting https://lists.apache.org/thread/dxdqjc0dd8rf1vbdg755zo1n2zj1tj8d;;;","16/Oct/23 04:46;ZhenqiuHuang;[~zjureel]
We have similar requirements. To accelerate the development, I can help on some Jira tickets.;;;","24/Oct/23 01:59;zjureel;[~ZhenqiuHuang] Sorry for the late reply, and please feel free to comment the issues if you have any idea or would like to take it, thanks;;;","31/Oct/23 16:58;mobuchowski;[~zjureel] Sorry, it's probably too late for this comment as FLIP is accepted. But I'll still voice concern here from {{[OpenLineage|https://openlineage.io/] }}as we'd want to implement this interface.

It seems like the intented way for LineageVertex interface is to just provide config context to particular nodes:

```
{{public}} {{interface}} {{LineageVertex {}}
{{    }}{{/* Config for the lineage vertex contains all the options for the connector. */}}
{{    }}{{Map<String, String> config();}}
{{}}}
```
and then in particular case, when the listener understand particular implementation, provide more information:

```

{{// Create kafka source class with lineage vertex}}
{{public}} {{class}} {{KafkaVectorSource }}{{extends}} {{KafkaSource }}{{implements}} {{LineageVertexProvider {}}
{{    }}{{int}} {{capacity;}}
{{    }}{{String valueType;}}
 
{{    }}{{public}} {{LineageVertex LineageVertex() {}}
{{        }}{{return}} {{new}} {{KafkaVectorLineageVertex(capacity, valueType);}}
{{    }}{{}}}
{{}}}
{{```}}
 
I think this is problematic because it strongly couples the listener to particular vertex implementation.
If you want to get list of datasets that are read by particular Flink job, you'll have to understand where the config is coming from and it's structure. Additionally, sometimes config is not everything we need to get lineage - for example, for Kafka connector we could get regex pattern used for reading that we'd need to resolve ourselves.
Or, if the connector subclasses `LineageVector` then another option is to get additional information from the subclass - but still, the connector has to understand it.
Another problem is that the configuration structure for particular connector can have breaking changes between version - so we're tied not only to connector, but also particular version of it.

But if we pushed the responsibility of understanting the datasets that particular vertex of a graph produces to the connector itself, we'd not have this problem.
First, the connector understands where it's reading from and writing to - so providing that information is easy for it. 
Second, the versioning problem does not exist - because the connector can update the code responsible for providing dataset information at same PR that breaks it, which will be transparent for the listener.


I would imagine the interface to be just something like this:


```
{{public}} {{interface}} {{LineageVertex {}}
{{    }}{{/* Config for the lineage vertex contains all the options for the connector. */}}
{{    }}{{Map<String, String> config();}}
{{    /* List of datasets that are consumed by this job */}}{{    }}
{{    }}{{List<Dataset> inputs();}}
{{    /* List of datasets that are produced by this job */}}{{    }}
{{    }}{{List<Dataset> outputs();}}
{{}}}
```
 
What dataset is in this case is debatable: from OL perspective it would be best if this would be something similar to [https://openlineage.io/apidocs/javadoc/io/openlineage/client/openlineage.dataset] - get name (ex. table name) and namespace (ex. standarized database identifier). It also provides extensible list of facets that represent additional information about the dataset that the particular connection wants to expose together with just dataset identifier - ex. something that represents table schema or side of the dataset. It could be something Flink - specific, but should allow particular connections to expose the additional information.
 ;;;","01/Nov/23 06:33;zjureel;Hi [~mobuchowski], thanks for your comments. In the currently FLIP the `LineageVertex` is the top interface for vertexes in lineage graph, it will be used in flink sql jobs and datastream jobs. 

1. For table connectors in sql jobs, there will be `TableLineageVertex` which is generated from flink catalog based table and provides catalog context, table schema for specified connector. The table lineage vertex and edge implementations will be created from dynamic tables for connectors in flink, and they will be updated when the connectors are updated.

2. For customized source/sink in datastream jobs, we can get source and slink `LineageVertex` implementations from `LineageVertexProvider`. When users implement customized lineage vertex and edge, they need to update them when their connectors are updated.

IIUC, do you mean we should give an implementation of `LineageVertex` for datastream jobs and users can provide source/sink information there just like `TableLinageVertex` in sql jobs? Then listeners can use the datastream lineage vertex which is similar with table lineage vertex? 

Due to the flexibility of the source and sink in `DataStream`, we think it's hard to cover all of them, so we just provide `LineageVertex` and `LineageVertexProvider` for them. So we left this flexibility to users and listeners. If a custom connector is a table in `DataStream` job, users can return `TableLineageVertex` in the `LineageVertexProvider`.

And for the following `LineageVertex`
{code:java}
public interface LineageVertex {
    /* Config for the lineage vertex contains all the options for the connector. */
    Map<String, String> config();
    /* List of datasets that are consumed by this job */    
    List<Dataset> inputs();
    /* List of datasets that are produced by this job */    
    List<Dataset> outputs();
}
{code}

We tend to provide independent edge descriptions of connectors in `LineageEdge` for lineage graph instead of adding dataset in `LineageVertex`. The `LineageVertex` here is the `DataSet` you mentioned.

WDYT? Hope to hear from you, thanks




;;;","01/Nov/23 17:30;ZhenqiuHuang;I am recently working on Flink integration in open lineage. In our org, customers mainly use the data stream api for icerberg, kafka, cassandra. As table API is  not even used, so implement these TableLineageVertex is probably not the best way.

I feel the most painful thing is to infer the schema of source/source for lineage perspective. If the schema info can be provided in Flink connector, the integration in open lineage or even other framework will be clean, concise. ;;;","03/Nov/23 08:37;zjureel;[~ZhenqiuHuang] Sounds nice to me, I think we can consider this together. If there are some common connectors and relevant schema for the `DataStream` jobs, we can define these lineage vertex in flink, such as `ColumnsLineageVertex` which has multiple columns with different data type?;;;","03/Nov/23 11:25;mobuchowski;[~zjureel] 
Generally the idea is that the interface should allow for connecting the listener and particular connector in a scenario where they are written by totally separate people and don't even know of each other. For example, I should be able to write an OpenLineage listener (or Datahub, Atlan...) and it should work - and report lineage - from some proprietary, internal connector that implements FLIP-314 interface.

The way of doing it in the currently proposed and implemented interface requires either
 # Listener knowing the LineageVertex subclasses of all possible connectors, or
 # Listener providing it's own specialized subclass that the connectors would implement.

The problem with first idea is that it's not possible to provide extensibility mechanism. If I have to enumerate all the supported connectors, I won't be able to cover some proprietary connector that that I don't know the code of - and the connector author won't be able to do it either.

The problem with second idea is that it's pretty much duplicating effort here. It's hard to get consensus around particular interface, and it's even harder to have implementation of it in open source. Even if this was done, it pretty much forces all the people to use this listener - which I believe is the opposite of the goal of the open interface as FLIP-314.

> 2. For customized source/sink in datastream jobs, we can get source and slink `LineageVertex` implementations from `LineageVertexProvider`. When users implement customized lineage vertex and edge, they need to update them when their connectors are updated.

> IIUC, do you mean we should give an implementation of `LineageVertex` for datastream jobs and users can provide source/sink information there just like `TableLinageVertex` in sql jobs? Then listeners can use the datastream lineage vertex which is similar with table lineage vertex?

In a way, yes. While datastream jobs are flexible, sources and sinks generally read from and write to known data systems, and those connectors know them. On the other hand, I think best possible interface wouldn't have `TableLinageVertex` or `DatasetLineageVertex`, just one unified interface that would allow connectors themselves to describe the list of datasets read from and written to, like the one I've posted in previous comment.

> Due to the flexibility of the source and sink in `DataStream`, we think it's hard to cover all of them, so we just provide `LineageVertex` and `LineageVertexProvider` for them. So we left this flexibility to users and listeners. If a custom connector is a table in `DataStream` job, users can return `TableLineageVertex` in the `LineageVertexProvider`.

My idea is that connector authors provide those - not end users. End users providing those means the job is duplicated - while the Kafka connector always knows it's reading from some topics, or JDBC connector knows it's writing to a particular table in particular database.

However, end users should have the ability to enrich this data with some particular information.

> I feel the most painful thing is to infer the schema of source/source for lineage perspective. If the schema info can be provided in Flink connector, the integration in open lineage or even other framework will be clean, concise. 

Agreed - schema is very important next to actual dataset identifier. And very important for any possible future column level lineage work.;;;","06/Nov/23 09:15;zjureel;Hi [~mobuchowski] Thanks for your reply.

I think our ideas are consistent, just at different levels of abstraction. The interface `LineageVertex` is the top interface for connectors in Flink, and we implement `TableLineageVertex` for tables, because a Table is a complete definition, including the database, schema, etc. We put the options in the `with` into a map, which is consistent with the definition and usage habits of SQL in Flink.

For the official Flink connectors, we will implement the `LineageVertex` for `Source` and `InputFormat` for `DataStream` jobs, such as `KafkaSourceLineage`, etc, as we mentioned in FLINK: `We will implement LineageVertexProvider  for the builtin source and sink such as KafkaSource , HiveSource , FlinkKafkaProducerBase  and etc.`.
End-users don't need to implement them. In order to be consistent with the usage habits of tables, we will put the corresponding information into a map when implementing it, and users can obtain it.

So, I think our current point of divergence is which level of abstraction the user needs to perceive. In the current FLIP, for DataStream jobs, listener developers need to identify whether the `LineageVertex` is a `KafkaSourceLineageVertex` or a `JdbcLineageVertex`. You mean we need to define another layer, such as the `DataSetConfig` interface, and then the listener developer can identify whether it is a `KafkaDataSetConfig` or a `JdbcDataSetConfig`, right?

Our current use of `LineageVertex` is mainly to consider flexibility and facilitate the addition of returned information in the lineage vertex of the `DataStream`, such as the vector type data source information mentioned in the FLIP example. At the same time, connector maintainers can also easily provide lineage vertex for customized connectors. If the connector is in table format, we prefer that users directly provide a TableLineageVertex instance.



;;;","09/Nov/23 14:47;mobuchowski;> So, I think our current point of divergence is which level of abstraction the user needs to perceive.

I would differenciate between ""end user"" - who just writes job code, whether in DataStream or SQL, listener developer and connector developer. So ideally for me, abstraction level for end user who just works on a job-level code is that they would not need to do anything besides configuring the listener and enjoying the lineage graph in their preferred lineage backend.

> In the current FLIP, for DataStream jobs, listener developers need to identify whether the `LineageVertex` is a `KafkaSourceLineageVertex` or a `JdbcLineageVertex`. You mean we need to define another layer, such as the `DataSetConfig` interface, and then the listener developer can identify whether it is a `KafkaDataSetConfig` or a `JdbcDataSetConfig`, right?

For listener developer, I would argue that for transmitting basic lineage - data source, dataset names, possibly schema and column-level lineage - developer should be able to get this data utilizing basic interface buildin for this FLIP. So, basic support would mean just recognizing `DataSetConfig` (or having this data in basic LineageVertex) - without any classes that strongly tie listener to some particular connectors. This is especially important for authors of generic (not only in-house) listeners, like OpenLineageListener or perhaps DatahubListener that would like to support lineage returned from custom connectors.

For connector developer, they should implement this basic interface, and then all implementation of listeners would be able to understand gathered lineage - without even knowledge of this connector.

Basically, instead of N x M problem where there are N connectors and M listeners and every listener has to have specific code for each connector, we should have single intermediate interface, so we'd save everyone's time.

Then, it would be best if there was a standard way for connectors to extend the returned data structure. This could be inheritance, as the FLIP suggests, but I think better, but maybe less type safe way would be to provide something like Map<String, Facet> where Facet is just a self-contained, atomic piece of extension metadata - things like information about output storage system, connector name and version, or perhaps some metrics about job execution - it's up for connector developer. I believe it's better, because lack of knowledge of particular `LineageVertex` subtype doesn't prevent you from getting lineage.

So yes, good comparison is proposed `TableLineageVertex` - I would just extend this concept to DataStream jobs and provide (optionally?) more metadata, with slightly different interface for extension.

 

I want to add that despite some disagreements on this interface, I respect the work you've done on this topic [~zjureel] and I believe even without acknowledging my points, the interface is a big step forward for better observability of Flink jobs.;;;","14/Nov/23 08:09;zjureel;[~mobuchowski] Sorry for the late reply, I think I get your point now and thank you for your valuable suggestions.

Based on our discussion, I think we can add facets in the `LineageVertex` as follows.

{code:java}
public interface LineageVertex {
   /* Facets for the lineage vertex to describe the information of source/sink. */
    Map<String, Facet> facets;
    /* Config for the lineage vertex contains all the options for the connector. */
    Map<String, String> config;
}
{code}

We can implement some common facets such as `ConnectorFacet`, `StreamingFacet`, `SchemaFacet`. We can add new `Facet` implementations as needed in the future.
For `TableLineageVertex` we can create facets from config in table ddl which will make the table options meaningful.
For datastream jobs we can create facts from data source and sink.

{code:java}
/** Connector facet has name and address for the connector, for example, jdbc and url for jdbc connector, kafka and server for kafka connector. */
public interface ConnectorFacet extends Facet {
    String name();
    String address();
}

/** Fact for streaming connector. */
public interface StreamingFacet {
    String topic();
    String group();
    String format();
    String start();
}

/** Fact for schema in connector. */
public interface SchemaFacet {
    Map<String, String> fields();
}
{code}

What do you think of this? Thanks

;;;","16/Nov/23 17:14;mobuchowski;[~zjureel] I think this is a step in the right direction.

For `StreamingFacet`, if the intention is to attach it to connectors interfacing with services like Kafka, Kinesis, Pulsar, PubSub then I believe it needs ability to describe multiple topics - since, at least for Kafka, you can read from either list of topics or wildcard pattern. The same for `SchemaFacet` - the topics can have different schema.

The way that OpenLineage deals with this - and Flink could do too - is associating those with concept of `Dataset`, rather than job itself. So, we'd have roughly
{code:java}
public interface LineageVertex {
    /* List of input (for source) or output (for sink) datasets interacted with by the connector */
    List<Dataset> datasets; 
    /* Facets for the lineage vertex to describe the general information of source/sink. */ 
    Map<String, Facet> facets; 
    /* Config for the lineage vertex contains all the options for the connector. */ 
    Map<String, String> config;
} {code}
{code:java}
public interface Dataset {
    /* Name for this particular dataset. */
    String name;
    /* Unique name for this dataset's datasource. */
    String namespace; 
    /* Facets for the lineage vertex to describe the particular information of dataset. */ 
    Map<String, Facet> facets; 
} {code}
and then those facets could be assigned either for `Dataset` or for `LineageVertex`.;;;","27/Nov/23 02:56;zjureel;[~mobuchowski] Sorry for the late reply.

What does the `DataSet` in `LineageVertex` use for? Why a `LineageVertex` have multiple inputs or outputs? We hope that 'LineageVertex' describes a single source or sink, rather than multiple. So I prefer to add `Map<String, Facet>` to `LineageVertex` directly to describe the particular information. We introduce `LineageEdge` in this FLIP to describe the relation between sources and sinks instead of add `input` or `output` in `LineageVertex`.

;;;","27/Nov/23 13:31;mobuchowski;[~zjureel] 

>Why a `LineageVertex` have multiple inputs or outputs? We hope that 'LineageVertex' describes a single source or sink, rather than multiple.

I have two good counterexamples, for read and write, when one source or sink describes more than one datasets:
 * KafkaSource can read from multiple topics, or even wildcard pattern.
 * Another case is where one company used JDBC connector sink, and they had very large amount of destination tables (1000s), some of them with rather small amounts of data. The database would not work with one-connection-per-table model, so I had a fork of JDBC connector that could dynamically determine to which table the connector should write the data. I tried to contribute that but there was no interest. [https://github.com/apache/flink/pull/15102/files]

Flink is really flexible when it comes to structure of the job, which should be reflected in the API.

>We introduce `LineageEdge` in this FLIP to describe the relation between sources and sinks instead of add `input` or `output` in `LineageVertex`.

I think those are two things are separate, as different datasets in one source can have different output sinks.;;;","07/Dec/23 12:33;mobuchowski;Hey [~zjureel]. Had additional idea - for SQL jobs, would be nice to attach the actual text representation of the query. This would allow data catalogs to accurately show what query created the table. 

Additionally, is there any news about progress? We are excited for this feature, and are willing to help with testing and implementation.;;;","21/Dec/23 10:22;mobuchowski;Hey [~zjureel], is there any news regarding this feature?;;;","16/Jan/24 22:56;ZhenqiuHuang;Hi Everyone, I also want to bump the thread due to the internal needs. I feel open lineage community gives very good suggestion to define an intermediate representation (Dataset) about the metadata of a since/sink. Also LineageVertex could definitely have multiple dataset, for example Hybrid source users who read from Kafka first then switch to iceberg. Given this, I feel the config should be in dataset rather than LineageVertex. On the other hand, we want to make the column lineage possible, so having the query in the dataset will be reason for lineage provide to analysis the column relationship. For input/output schema, we may put it into a facet. It could be optional depends on the connector implementation. How do you think [~zjureel] [~mobuchowski]?


{code:java}
public interface LineageVertex {
    /* List of input (for source) or output (for sink) datasets interacted with by the connector */
    List<Dataset> datasets; 
} 
{code}

{code:java}
public interface Dataset {
    /* Name for this particular dataset. */
    String name;
    /* Unique name for this dataset's datasource. */
    String namespace; 
    /* Query used to generate the dataset If there is */
    String query;
    /* Facets for the lineage vertex to describe the particular information of dataset. */ 
    Map<FacetType, Facet> facets; 
} 
{code}

Facet type could be SchemaFacet and ConfigFacet

;;;","17/Jan/24 10:03;martijnvisser;Looking at the discussions happening here, I think this should be brought back to the Dev mailing list instead of discussing it under Jira ;;;","01/Feb/24 02:07;zjureel;Hi [~mobuchowski] [~ZhenqiuHuang], I feel that there are many details that need to be discussed regarding the interfaces. Would it be convenient for you to send an email to me (zjureel@gmail.com) so that we can schedule an offline meeting to discuss it clearly first, and then initiate a new discussion in the dev mailing list. Hope to hear from you, thanks :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python code examples in documentation are not complete,FLINK-31274,13526630,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,arihuttunen,arihuttunen,01/Mar/23 07:52,01/Mar/23 09:08,04/Jun/24 20:41,,,,,,,,,,,,,,,,,API / Python,Documentation,,,0,,,,,"Because the python examples in the documentation do not contain all the code needed to run the examples, a new user cannot easily run the examples.

This ticket is done when
 * Each documentation page contains all the code needed to run the examples on that page without needing to copy code from other pages
 * Code is not partially left out e.g. 
t_env = TableEnvironment.create(...)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-01 07:52:30.0,,,,,,,,,,"0|z1g9eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Left join with IS_NULL filter be wrongly pushed down and get wrong join results,FLINK-31273,13526624,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,01/Mar/23 07:27,28/Jun/23 14:35,04/Jun/24 20:41,15/Mar/23 10:14,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.1,1.18.0,,Table SQL / Planner,,,,0,pull-request-available,,,,"Left join with IS_NULL filter be wrongly pushed down and get wrong join results. The sql is:
{code:java}
SELECT * FROM MyTable1 LEFT JOIN MyTable2 ON a1 = a2 WHERE a2 IS NULL AND a1 < 10

The wrongly plan is:

LogicalProject(a1=[$0], b1=[$1], c1=[$2], b2=[$3], c2=[$4], a2=[$5])
+- LogicalFilter(condition=[IS NULL($5)])
   +- LogicalJoin(condition=[=($0, $5)], joinType=[left])
      :- LogicalValues(tuples=[[]])
      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) {code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 10:14:02 UTC 2023,,,,,,,,,,"0|z1g9dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 10:14;godfrey;Fixed in

master: 8990822bd77d70f3249e1220a853e16dadd8ef54

1.17.1: 33278628dc599bed8944733efb9495ce77993d4b

1.16.2: f0361c720cb18c4ae7dc669c6a5da5b09bc8f563;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate operators appear in the StreamGraph for Python DataStream API jobs,FLINK-31272,13526622,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,01/Mar/23 07:19,08/Mar/23 17:54,04/Jun/24 20:41,01/Mar/23 13:25,1.15.0,,,,,,,,,,,1.15.4,1.16.2,1.17.0,,API / Python,,,,0,pull-request-available,,,,"For the following job:
{code}
import argparse
import json
import sys
import time
from typing import Iterable, cast

from pyflink.common import Types, Time, Encoder
from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction, EmbeddedRocksDBStateBackend, \
    PredefinedOptions, FileSystemCheckpointStorage, CheckpointingMode, ExternalizedCheckpointCleanup
from pyflink.datastream.connectors.file_system import FileSink, RollingPolicy, OutputFileConfig
from pyflink.datastream.state import ReducingState, ReducingStateDescriptor
from pyflink.datastream.window import TimeWindow, Trigger, TriggerResult, T, TumblingProcessingTimeWindows, \
    ProcessingTimeTrigger


class CountWithProcessTimeoutTrigger(ProcessingTimeTrigger):

    def __init__(self, window_size: int):
        self._window_size = window_size
        self._count_state_descriptor = ReducingStateDescriptor(
            ""count"", lambda a, b: a + b, Types.LONG())

    @staticmethod
    def of(window_size: int) -> 'CountWithProcessTimeoutTrigger':
        return CountWithProcessTimeoutTrigger(window_size)

    def on_element(self,
                   element: T,
                   timestamp: int,
                   window: TimeWindow,
                   ctx: 'Trigger.TriggerContext') -> TriggerResult:
        count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor))
        count_state.add(1)
        # print(""element arrive:"", element, ""count_state:"", count_state.get(), window.max_timestamp(),
        #       ctx.get_current_watermark())

        if count_state.get() >= self._window_size:  # 必须fire&purge！！！！
            print(""fire element count"", element, count_state.get(), window.max_timestamp(),
                  ctx.get_current_watermark())
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE
        if timestamp >= window.end:
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE
        else:
            return TriggerResult.CONTINUE

    def on_processing_time(self,
                           timestamp: int,
                           window: TimeWindow,
                           ctx: Trigger.TriggerContext) -> TriggerResult:
        if timestamp >= window.end:
            return TriggerResult.CONTINUE
        else:
            print(""fire with process_time:"", timestamp)
            count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor))
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE

    def on_event_time(self,
                      timestamp: int,
                      window: TimeWindow,
                      ctx: 'Trigger.TriggerContext') -> TriggerResult:
        return TriggerResult.CONTINUE

    def clear(self,
              window: TimeWindow,
              ctx: 'Trigger.TriggerContext') -> None:
        count_state = ctx.get_partitioned_state(self._count_state_descriptor)
        count_state.clear()


def to_dict_map(v):
    time.sleep(1)
    dict_value = json.loads(v)
    return dict_value


def get_group_key(value, keys):
    group_key_values = []
    for key in keys:
        one_key_value = 'null'
        if key in value:
            list_value = value[key]
            if list_value:
                one_key_value = str(list_value[0])
        group_key_values.append(one_key_value)
    group_key = '_'.join(group_key_values)
    # print(""group_key="", group_key)
    return group_key


class CountWindowProcessFunction(ProcessWindowFunction[dict, dict, str, TimeWindow]):

    def __init__(self, uf):
        self._user_function = uf

    def process(self,
                key: str,
                context: ProcessWindowFunction.Context[TimeWindow],
                elements: Iterable[dict]) -> Iterable[dict]:
        result_list = self._user_function.process_after_group_by_function(elements)
        return result_list


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)
    output_path = known_args.output

    env = StreamExecutionEnvironment.get_execution_environment()
    # write all the data to one file
    env.set_parallelism(1)

    # process time
    env.get_config().set_auto_watermark_interval(0)
    state_backend = EmbeddedRocksDBStateBackend(True)
    state_backend.set_predefined_options(PredefinedOptions.FLASH_SSD_OPTIMIZED)
    env.set_state_backend(state_backend)
    config = env.get_checkpoint_config()
    # config.set_checkpoint_storage(FileSystemCheckpointStorage(""hdfs://ha-nn-uri/tmp/checkpoint/""))
    config.set_checkpoint_storage(FileSystemCheckpointStorage(""file:///Users/10030122/Downloads/pyflink_checkpoint/""))
    config.set_checkpointing_mode(CheckpointingMode.AT_LEAST_ONCE)
    config.set_checkpoint_interval(5000)
    config.set_externalized_checkpoint_cleanup(ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION)

    # define the source
    data_stream1 = env.from_collection(['{""user_id"": [""0""], ""goods_id"": [0,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [1,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [2,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [3,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [4,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [5,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [6,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [7,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [8,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [9,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [10,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [11,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [12,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [13,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [14,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [15,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [16,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [17,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [18,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [19,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [20,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [21,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [22,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [23,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [24,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [25,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [26,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [27,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [28,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [29,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [30,0]}'])

    data_stream2 = env.from_collection(['{""user_id"": [""0""], ""goods_id"": [0,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [1,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [2,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [3,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [4,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [5,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [6,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [7,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [8,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [9,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [10,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [11,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [12,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [13,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [14,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [15,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [16,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [17,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [18,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [19,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [20,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [21,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [22,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [23,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [24,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [25,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [26,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [27,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [28,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [29,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [30,0]}'])

    # group_keys = ['user_id', 'goods_id']
    group_keys = ['user_id']

    sink_to_file_flag = True

    data_stream = data_stream1.union(data_stream2)

    # user_function = __import__(""UserFunction"")

    ds = data_stream.map(lambda v: to_dict_map(v)) \
        .filter(lambda v: v) \
        .map(lambda v: v) \
        .key_by(lambda v: get_group_key(v, group_keys)) \
        .window(TumblingProcessingTimeWindows.of(Time.seconds(12))) \
        .process(CountWindowProcessFunction(lambda v: v), Types.STRING())

    ds = ds.map(lambda v: v, Types.PRIMITIVE_ARRAY(Types.BYTE()))

    base_path = ""/tmp/1.txt""
    encoder = Encoder.simple_string_encoder()
    file_sink_builder = FileSink.for_row_format(base_path, encoder)
    file_sink = file_sink_builder \
        .with_bucket_check_interval(1000) \
        .with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()) \
        .with_output_file_config(
        OutputFileConfig.builder().with_part_prefix(""pre"").with_part_suffix(""suf"").build()) \
        .build()
    ds.sink_to(file_sink)

    # submit for execution
    env.execute()
{code}

The stream graph is as following:
{code}
{
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""Source: Collection Source"",
    ""pact"" : ""Data Source"",
    ""contents"" : ""Source: Collection Source"",
    ""parallelism"" : 1
  }, {
    ""id"" : 2,
    ""type"" : ""Source: Collection Source"",
    ""pact"" : ""Data Source"",
    ""contents"" : ""Source: Collection Source"",
    ""parallelism"" : 1
  }, {
    ""id"" : 9,
    ""type"" : ""TumblingProcessingTimeWindows"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 15,
      ""ship_strategy"" : ""HASH"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 10,
    ""type"" : ""Map"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Map"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 9,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 15,
    ""type"" : ""Map, Filter, Map, _stream_key_by_map_operator"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Map, Filter, Map, _stream_key_by_map_operator"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 1,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    }, {
      ""id"" : 2,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 16,
    ""type"" : ""TumblingProcessingTimeWindows, Map"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 15,
      ""ship_strategy"" : ""HASH"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 18,
    ""type"" : ""Sink: Writer"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Sink: Writer"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 10,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 20,
    ""type"" : ""Sink: Committer"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Sink: Committer"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 18,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  } ]
}
{code}

The plan is incorrect as we can see that TumblingProcessingTimeWindows appears twice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 13:25:29 UTC 2023,,,,,,,,,,"0|z1g9cw:",9223372036854775807,"It may produce duplicate operators for Python DataStream API jobs of versions 1.15.0 ~ 1.15.3 and 1.16.0 ~ 1.16.1. It has addressed this issue since 1.15.4, 1.16.2 and 1.17.0. For jobs which are not affected by this issue, there are no backward compatibility issues. However, for jobs which are affected, it may not be possible to restore from savepoints generated from versions 1.15.0 ~ 1.15.3 and 1.16.0 ~ 1.16.1.",,,,,,,,,,,,,,,,,,,"01/Mar/23 13:25;dianfu;Fixed in:
- master: 753734f0a1a6f44b9e33e84377f63ea6f0a85769
- 1.17: c19c6e5288429bf8df2550e85cefd91fffffb760
- 1.16: 823aa25381ee67673407d581da7fd7259cfbac06
- 1.15: 52cef9e5c6ae747e4f9b1f1d5725bd90db9da7b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce system database for catalog in table store,FLINK-31271,13526618,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,01/Mar/23 06:54,22/Mar/23 02:18,04/Jun/24 20:41,22/Mar/23 02:18,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,2,,,,,"Introduce a system database for each catalog in table store to manage catalog information such as tables dependencies, relations between snapshots and checkpoints for each table",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-01 06:54:31.0,,,,,,,,,,"0|z1g9c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flink jar name in docs for table store,FLINK-31270,13526606,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,01/Mar/23 04:30,01/Mar/23 05:46,04/Jun/24 20:41,01/Mar/23 05:46,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 05:46:01 UTC 2023,,,,,,,,,,"0|z1g99c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 05:46;lzljs3620320;master: 32e0c37e291b239132d8b8959845439c63eeef94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split hive connector to each module of each version,FLINK-31269,13526603,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,01/Mar/23 03:52,14/Mar/23 02:38,04/Jun/24 20:41,14/Mar/23 02:38,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 02:38:50 UTC 2023,,,,,,,,,,"0|z1g98o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 02:38;TsReaper;master: dd2d600f6743ca074a023fdbb1a7a2cbcfbf8ff0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinator.Context#metricGroup will return null when restore from a savepoint,FLINK-31268,13526602,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ruanhang1993,ruanhang1993,ruanhang1993,01/Mar/23 03:47,18/Aug/23 02:23,04/Jun/24 20:41,18/Aug/23 02:23,,,,,,,,,,,,1.18.0,,,,Runtime / Metrics,,,,1,pull-request-available,,,,"The `metricGroup` is initialized lazily in the method `OperatorCoordinatorHandler#initializeOperatorCoordinators`.

This will cause the NullPointerException when we use it in the method like `Source#restoreEnumerator`, which will be invoked through `SchedulerBase#createAndRestoreExecutionGraph` before `OperatorCoordinatorHandler#initializeOperatorCoordinators` in class `SchedulerBase#<init>`.",,,,,,,,,,,,,,FLINK-29801,,,,,,,,FLINK-32754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Aug 18 02:23:25 UTC 2023,,,,,,,,,,"0|z1g98g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 07:24;yunta;[~ruanhang1993] I think this is an actual blocker for releasing flink-1.18. Once restored and using metrics to report, this bug would lead to no more checkpoints could be triggered if implemented operator coordinator metrics. ;;;","18/Aug/23 02:23;renqs;master: eab68f31193f3067ae3b58223e77851b79451df0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fine-Grained Resource Management supports table and sql levels,FLINK-31267,13526601,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,01/Mar/23 03:20,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,1.20.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"Fine-Grained Resource Management supports table and sql levels. Now Fine-Grained Resource can only be used at the datastream api level, and does not support table and sql level settings.",,,,,,,,,,,,,,,,,,,,,FLINK-31115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-01 03:20:38.0,,,,,,,,,,"0|z1g988:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dashboard info error (received and send alway show 0 when having data),FLINK-31266,13526595,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,linqichen178,linqichen178,01/Mar/23 02:04,09/Mar/23 08:54,04/Jun/24 20:41,,1.14.4,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,,!receivedAndSend0.jpg!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/23 02:04;linqichen178;receivedAndSend0.jpg;https://issues.apache.org/jira/secure/attachment/13055915/receivedAndSend0.jpg",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 08:54:56 UTC 2023,,,,,,,,,,"0|z1g96w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 02:08;linqichen178;hey, guys!!  I find that the ""0 recevied and 0 sended "" will mislead us that there is no data in and out, but the true thing is that: it really had data.;;;","01/Mar/23 02:16;Zhanghao Chen;Are you using connectors that implements FLIP-33 [FLINK-11576] FLIP-33: Standardize connector metrics - ASF JIRA (apache.org)? If not, the behavior is expected. Flink won't show received/sent data for soruces/sinks.;;;","01/Mar/23 02:37;linqichen178;No matter what connector, kafka, datagen....

It‘s because that,  all tasks chain to one. ;;;","01/Mar/23 03:05;Weijie Guo;Generally speaking, flink compute the send & received metrics in {{ResultPartition}} and {{InputGate}}, but for the source(without InputGate) / Sink (without ResultPartition), If the connector does not provider some metrics like numBytesIn / numBytesOut, flink can not know this. ;;;","01/Mar/23 07:49;Ming Li;hi, [~Weijie Guo], [~Zhanghao Chen]  It is indeed confusing for new users when there is only one task. Do you think it is necessary for us to provide in/out metrics display at the operator granularity on the dashboard?;;;","09/Mar/23 08:54;kevin.cyj;Something like NaN may be better than 0? BTW, do newer flink versions still have the same behavior?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add smoke test for Pulsar connector,FLINK-31265,13526561,13522153,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,28/Feb/23 17:04,28/Feb/23 17:04,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"At present, pulsar connector only supports datastream job. We should take the following two steps:

1. Add smoke test for datastream uber jar.

2. During the code review process for SQL/Table API support, ensure that the corresponding smoke test is not missing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-28 17:04:28.0,,,,,,,,,,"0|z1g8zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add smoke test for HBase connector,FLINK-31264,13526560,13522153,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,28/Feb/23 16:54,28/Feb/23 16:54,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / HBase,Tests,,,0,,,,,"1.Sync `SQLClientHBaseITCase` to flink-connector-hbase and re-write it by test {{TestContainer}}. 
2.Introduce smoke test for habse datastream job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-28 16:54:54.0,,,,,,,,,,"0|z1g8z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add smoke test for ElasticSearch connector,FLINK-31263,13526559,13522153,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,28/Feb/23 16:45,28/Feb/23 16:52,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,0,,,,, We should introducing smoke test for ElasticSearch connector to test Datastream&SQL job fat jar at the same time.,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30359,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-28 16:45:09.0,,,,,,,,,,"0|z1g8yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move kafka sql connector fat jar test to SmokeKafkaITCase,FLINK-31262,13526553,13522153,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,28/Feb/23 16:02,16/Oct/23 07:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / Kafka,Tests,,,0,pull-request-available,stale-assigned,,,"{{SmokeKafkaITCase}} only covered the application packaging test of datastream job, we should also bring this kind of test to sql job submitted by sql-client to cover the case of sql connector fat jar.
In fact, we already have similar test class with the same purpose, that is {{SQLClientKafkaITCase}}, but this test is ignored since 2021/4/20. It depend on the 
{{LocalStandaoneKafkaResource}} which download {{kafka}} and sets up a local cluster, but  we often encounter download failures in CI environments. Fortunately, now we prefer to use {{TestContainer}} in E2E test. So I suggest integrating the corresponding test logic into {{SmokeKafkaITCase}}, and then removing it.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:07 UTC 2023,,,,,,,,,,"0|z1g8xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make AdaptiveScheduler aware of the (local) state size,FLINK-31261,13526527,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,roman,roman,roman,28/Feb/23 13:06,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,stale-assigned,,,"FLINK-21450 makes the Adaptive Schulder aware of Local Recovery.

Each slot-group pair is assigned a score based on a keyGroupRange size.

That score isn't always optimlal - it could be improved by computing the score based on the actual state size on disk.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21450,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 22:35:08 UTC 2023,,,,,,,,,,"0|z1g8rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushLocalHashAggIntoScanRule should also work with union RelNode,FLINK-31260,13526525,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,28/Feb/23 12:53,10/Mar/23 12:02,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"As discussed in [comments|https://github.com/apache/flink/pull/22001#discussion_r1119652784] Currently, {{PushLocalHashAggIntoScanRule}} match for the Exchange -> LocalHashAggregate -> Scan. As a result, the following pattern can not be optimized


{code:java}
      +- Union(all=[true], union=[type, sum$0])
         :- Union(all=[true], union=[type, sum$0])
         :  :- LocalHashAggregate(groupBy=[type], select=[type, Partial_SUM(price) AS sum$0])
         :  :  +- TableSourceScan(table=[[default_catalog, default_database, table1, project=[type, price], metadata=[]]], fields=[type, price])
         :  +- LocalHashAggregate(groupBy=[type], select=[type, Partial_SUM(price) AS sum$0])
         :     +- TableSourceScan(table=[[default_catalog, default_database, table2, project=[type, price], metadata=[]]], fields=[type, price])
         +- LocalHashAggregate(groupBy=[type], select=[type, Partial_SUM(price) AS sum$0])
            +- TableSourceScan(table=[[default_catalog, default_database, table3, project=[type, price], metadata=[]]], fields=[type, price])

{code}

We should extend the rule to support this pattern.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 12:02:40 UTC 2023,,,,,,,,,,"0|z1g8rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 09:44;zhengyiweng;Hi,[~aitozi] .I think that the exchange in the rules may can be removed.I tried to remove it and had passed the 

PushLocalAggIntoTableSourceScanRuleTest and 

LocalAggregatePushDownITCase. But I'm not sure if it will affect others and result to incorrect results.WDYT?;;;","10/Mar/23 12:02;aitozi;[~zhengyiweng] Thanks for your attention, I think the exchange in rule pattern can be removed. I'd like revisit this issue after [https://github.com/apache/flink/pull/22001] merged. Since in that PR will generate the pattern above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gateway supports initialization of catalog at startup,FLINK-31259,13526520,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,28/Feb/23 12:16,14/Aug/23 05:51,04/Jun/24 20:41,14/Aug/23 05:51,1.18.0,,,,,,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,stale-assigned,,,Support to initializing catalogs in gateway when it starts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 05:51:40 UTC 2023,,,,,,,,,,"0|z1g8q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 02:09;jark;Could you share more details of the purpose and design? ;;;","01/Mar/23 03:02;zjureel;Hi [~jark] We'd like to initialize catalogs in session when it is created in gateway, for example, create mysql and hive catalog. Currently, business users can initialize them by an initialization file in sql client. There may be some sensitive data in the catalog script, such as username/password/hive uri which we don't want to show to them. So we want to add this ability to sql gateway too.

When we start gateway with an initialization file, it will create catalogs from the file for each new session. Users can perform query on them directly and cannot drop these catalogs. What do you think?;;;","01/Mar/23 06:40;jark;Thank you [~zjureel], do you mean supporting the initialization file in SQL Gateway, that the init file is a global catalog setting for every session? ;;;","01/Mar/23 07:31;zjureel;[~jark] I'm not sure whether my understanding of `global catalog` is correct. Currently each `Session` in gateway has its own `CatalogManager`,  I'd like to create independent catalog from the initialization for each `CatalogManager` in `Session` when the session is created. The initialized `Catalog` instance will not be shared between sessions.

Similar to this, I discussed with [~fsk119] whether it is necessary for all sessions to share a `CatalogManager`, we think the demand for this is still uncertain at present;;;","01/Mar/23 12:53;jark;Sorry, for the ""global catalog"", I mean the catalog definition is shared for every session, and every session will initialize the catalog instance for the global catalog definition once the session is created. ;;;","02/Mar/23 08:23;zjureel;Thanks [~jark], I get it. If there is demand, I think we can consider to share `CatalogManager` instead of `Catalog` between sessions in the future :);;;","16/Mar/23 13:18;lintingbin;[~jark]We also have the same usage scenario, sql-gateway needs to be able to add catalog after startup, instead of adding catalog for each user who uses sql-gateway. Just like Trino can define a catalog in its configuration file, sometimes the catalog contains a lot of sensitive information. Even if there is no sensitive information, it is very cumbersome for users to add it by themselves. The same logic applies to vvp, as long as the catalog is created on the vvp platform, all users can use it.;;;","17/Mar/23 02:31;jark;[~lintingbin] yes, I think this is a valuable feature, and I think [FLIP-295|https://cwiki.apache.org/confluence/display/FLINK/FLIP-295%3A+Introduce+Pluggable+Catalog+Management] can address this. (Note that FLIP-295 is still under drafting and I haven't reviewed it yet).;;;","17/Mar/23 03:15;lintingbin;[~jark] [FLIP-295|https://cwiki.apache.org/confluence/display/FLINK/FLIP-295%3A+Introduce+Pluggable+Catalog+Management] may indeed be able to solve this problem, but I feel that it is more biased towards catalog storage problems. If sql-gateway can support a "" -i, --init <initialization file>"" parameter similar to sql-client or support a similar ""sql-gateway-defaults.yaml"" configuration file similar to [ververica-flink-sql-gateway|https://github.com/ververica/flink-sql-gateway/blob/master/conf] can solve this problem very well. Because it is not just catalogs, it may also involve some udf initialization and so on.;;;","17/Mar/23 03:55;jark;[~lintingbin] I see. What do you think about this? [~fsk119];;;","20/Mar/23 03:47;zjureel;[~jark] I think this feature is a little different to FLIP-295. As [~lintingbin] mentioned, we don't want to implement a custom `CatalogProvider`, but only register some existing catalogs to `CatalogManager`. Sorry to see the comments late and I have created a PR to support "" -i, --init <initialization file>"" in `SqlGateway` cc [~fsk119];;;","21/Mar/23 02:24;fsk119;I apologize for the delay in my response. After discussing with [~jark]  and [~lincoln.86xy] offline, we have come to the following agreements:
 # The -i parameter in the gateway side lacks a clear meaning. The init script allows CREATE TABLE statements that may create a real table in external storage. However, since the CREATE TABLE statement can only be executed once, it is not suitable for the gateway side. Thus, the init script in the gateway differs significantly from its counterpart in the client side.
 # FLIP-295 serves the same purpose as the -i parameter. It enables users to specify a special CatalogManager with built-in catalogs. The Gateway builds the special catalog manager for every session using the new CatalogManager API. For example, a platform developer can specify the following arguments in the flink-conf.yaml:

{code:java}
catalogmanager.type: mysql
catalogmanager.mysql.url: <URL> 
catalogmanager.mysql.username: flink 
catalogmanager.mysql.password: flink2023! {code}
With these configurations, the Gateway uses the SPI mechanism to load the special catalog manager, which is responsible for loading the built-in catalogs registered in the MySql database. I believe this also works for UDF.

In contrast, the -i parameter only works for SQL Gateway users, whereas FLIP-295 is useful for Table API users as well.

WDYT [~zjureel] [~lintingbin]? Please corret me if I am wrong.;;;","21/Mar/23 02:42;zjureel;Thanks [~fsk119] I found some differences between your description here and [FLIP-295|https://cwiki.apache.org/confluence/display/FLINK/FLIP-295%3A+Introduce+Pluggable+Catalog+Management].

I don't quite understand the meaning of the mysql `catalog manager` you provided above. It's a `Catalog` or `CatalogManager`?

In FLIL-295 users can config `CatalogStore` as follows
```
sql.catalog.store.type: file
sql.catalog.store.file.path: file:///xxxx/xxx
```
Since no details are described here, I don't know if it is possible to configure multiple catalogs in the store file, such as mysql catalog, hive catalog, paimon catalog, etc. 
If it can, and the file catalog store is built in flink, I think it can meet the needs to config multiple catalogs in the file.

;;;","21/Mar/23 03:02;fsk119;[~zjureel]  Thanks for pointing out the correct configuration. I don't discuss with the FLIP proposer as well... But from the description, the FLIP should achieve the goal we discuss above. I will try to ping the author of the FLIP-295 to share more thoughts.;;;","21/Mar/23 03:03;lintingbin;[~fsk119] I generally agree with your conclusions. I see that in the latest version, sql client already supports connecting to sql gateway (./bin/sql-client.sh gateway --endpoint <gateway address>), which may cause the problem of point 1 you mentioned.;;;","21/Mar/23 03:38;hackergin;[~zjureel] [~fsk119]  Sorry, the description in the document is not very clear.  Introducing catalogStore is to access and save different catalog configurations through different systems, such as file, mysql, hdfs..  .  One catalogStore can store multiple catalogs.;;;","21/Mar/23 16:24;jark;I agree with [~fsk119]. FLIP-295 should be a standard way for Table/SQL to support lazy initialization of catalogs and persistence of catalog configurations. Looking forward to the discussion of FLIP-295. ;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Aug/23 05:51;zjureel;See https://issues.apache.org/jira/browse/FLINK-32427;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not get kerberos keytab in flink operator,FLINK-31258,13526515,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,zhangjun,zhangjun,28/Feb/23 12:02,02/Mar/23 16:07,04/Jun/24 20:41,02/Mar/23 16:07,,,,,,,,,,,,1.17.0,,,,Kubernetes Operator,,,,0,,,,,"env:

flink k8s operator 1.4

flink 1.14.6 :

the conf
{code:java}
  flinkConfiguration:
    security.kerberos.login.keytab=/path/your/user.keytab 
   security.kerberos.login.principal=your@HADOOP.COM  {code}
and I get an exception:

 
{code:java}
Status:
  Cluster Info:
  Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""basic-example\""."",""throwableList"":[{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""basic-example\"".""},{""type"":""org.apache.flink.configuration.IllegalConfigurationException"",""message"":""Kerberos login configuration is invalid: keytab [/path/your/user.keytab] doesn't exist!""}]} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 16:07:12 UTC 2023,,,,,,,,,,"0|z1g8p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 16:07;mbalassi;This is actually due to the KerberosConfigMount in the flink-kubernetes module.

We introduced a config to disable this in 1.17:
https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#kubernetes-decorator-kerberos-mount-enabled

It is straight-forward to backport it if you need it in previous versions on your end, I would not cut a new Apache release for this of older versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix errors in “CSV Formats"" page ",FLINK-31257,13526513,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhengyiweng,zhengyiweng,zhengyiweng,28/Feb/23 11:57,01/Mar/23 06:35,04/Jun/24 20:41,01/Mar/23 06:35,,,,,,,,,,,,1.18.0,,,,chinese-translation,,,,0,pull-request-available,,,,"As shown in the picture,I find some errors in Format Options.There are also some punctuation errors that can be corrected together.

 

The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/connectors/table/formats/csv/|http://example.com/]

 

The markdown file is located in docs/content.zh/docs/connectors/table/formats/csv.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 11:53;zhengyiweng;csv-1.png;https://issues.apache.org/jira/secure/attachment/13055905/csv-1.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 06:35:43 UTC 2023,,,,,,,,,,"0|z1g8oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 12:06;zhengyiweng;Hi,[~jark].Could you please assign it to me? I'd like to working on it.;;;","28/Feb/23 12:08;jark;What error do you mean? Could you share the screenshot?;;;","28/Feb/23 12:33;zhengyiweng;Hi，[~jark] ,I have put the screenshot into the attachment。Can you see it?;;;","28/Feb/23 12:42;zhengyiweng;I think it's better to change it to ""指定将 null 值转换成的字符串(默认禁用)。"".What do you think?;;;","01/Mar/23 02:14;jark;You can translate it into “指定识别成 null 值的字符串，在输入端会将该字符串转为 null 值，在输出端会将null值转成该字符串。默认禁用。”
;;;","01/Mar/23 03:48;zhengyiweng;Thanks [~jark] ,I have pull the pr for this ticket.;;;","01/Mar/23 06:35;jark;Fixed in master: 33da59089554f74de6330cd9455d34bc8269a580;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] FLIP-297: Improve Auxiliary Sql Statements,FLINK-31256,13526484,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,taoran,taoran,taoran,28/Feb/23 08:47,12/Jun/23 10:42,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,The FLIP design doc can be found at page https://cwiki.apache.org/confluence/display/FLINK/FLIP-297%3A+Improve+Auxiliary+Sql+Statements.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 10:42:35 UTC 2023,,,,,,,,,,"0|z1g8ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 03:22;luoyuxia;[~lemonjing] Thanks for kicking it off. I'm wondering any plan for implementing `SHOW [USER] FUNCTIONS [ ( FROM | IN ) [catalog_name.]database_name ] [ [NOT] (LIKE | ILIKE) <sql_like_pattern> ]` ? If so, can we consider increase the priority?  I can help review.

The background is in [FLIP-311|[https://cwiki.apache.org/confluence/display/FLINK/FLIP-311%3A+Support+Call+Stored+Procedure],] we also propose syntax to show procedures. I'm thinking show procedure can reuse some code or logic of show function.;;;","12/Jun/23 10:42;taoran;[~luoyuxia]  yes. i will open a ticket for solving this. 
The current progress of this FLIP is relatively slow mainly because I can't find someone to help review in time.
i'm grateful you can help to review it. thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorUtils#createWrappedOperatorConfig should update input and sideOutput serializers,FLINK-31255,13526478,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,zhangzp,zhangzp,28/Feb/23 08:15,18/Apr/23 05:50,04/Jun/24 20:41,18/Apr/23 05:50,ml-2.0.0,ml-2.1.0,ml-2.2.0,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Currently we use operator wrapper to enable using normal operators in iterations. However, the operatorConfig is not correctly unwrapped. For example, the following code fails because of wrong type serializer.

 
{code:java}
@Test
public void testIterationWithMapPartition() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    DataStream<Long> input =
        env.fromParallelCollection(new NumberSequenceIterator(0L, 5L), Types.LONG);
    DataStreamList result =
        Iterations.iterateBoundedStreamsUntilTermination(
            DataStreamList.of(input),
            ReplayableDataStreamList.notReplay(input),
            IterationConfig.newBuilder()
                .setOperatorLifeCycle(OperatorLifeCycle.PER_ROUND)
                .build(),
            new IterationBodyWithMapPartition());

    List<Integer> counts = IteratorUtils.toList(result.get(0).executeAndCollect());
    System.out.println(counts.size());
}

private static class IterationBodyWithMapPartition implements IterationBody {

    @Override
    public IterationBodyResult process(
        DataStreamList variableStreams, DataStreamList dataStreams) {
        DataStream<Long> input = variableStreams.get(0);

        DataStream<Long> mapPartitionResult =
            DataStreamUtils.mapPartition(
                input,
                new MapPartitionFunction <Long, Long>() {
                    @Override
                    public void mapPartition(Iterable <Long> iterable, Collector <Long> collector)
                        throws Exception {
                        for (Long iter: iterable) {
                            collector.collect(iter);
                        }
                    }
                });

        DataStream<Integer> terminationCriteria =
            mapPartitionResult.<Long>flatMap(new TerminateOnMaxIter(2)).returns(Types.INT);

        return new IterationBodyResult(
            DataStreamList.of(mapPartitionResult), variableStreams, terminationCriteria);
    }
} {code}
The error stack is:

Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.flink.iteration.IterationRecord
    at org.apache.flink.iteration.typeinfo.IterationRecordSerializer.serialize(IterationRecordSerializer.java:34)
    at org.apache.flink.iteration.datacache.nonkeyed.FileSegmentWriter.addRecord(FileSegmentWriter.java:79)
    at org.apache.flink.iteration.datacache.nonkeyed.DataCacheWriter.addRecord(DataCacheWriter.java:107)
    at org.apache.flink.iteration.datacache.nonkeyed.ListStateWithCache.add(ListStateWithCache.java:148)
    at org.apache.flink.ml.common.datastream.DataStreamUtils$MapPartitionOperator.processElement(DataStreamUtils.java:445)
    at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:69)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 18 05:48:41 UTC 2023,,,,,,,,,,"0|z1g8h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 08:17;zhangzp;It seems that the bug comes from not unwrapping all the streamconfig of the wrapped operator in OperatorUtils#createWrappedOperatorConfig.;;;","18/Apr/23 05:48;lindong;Merged to apache/flink-ml master branch 894685455d1c26fd45198857b7a96ee850725a59.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the read performance for files table,FLINK-31254,13526470,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,28/Feb/23 07:49,28/Feb/23 12:00,04/Jun/24 20:41,28/Feb/23 12:00,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"At present, the reading performance of the Files table is very poor. Even every data read will read the schema file. We can optimize the reading performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 12:00:45 UTC 2023,,,,,,,,,,"0|z1g8fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 12:00;lzljs3620320;master: 0a8c3a1c426454258b663ffd52de5bcdc1e8ed20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port itcases to Flink 1.15 and 1.14,FLINK-31253,13526467,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tyrantlucifer,lzljs3620320,lzljs3620320,28/Feb/23 07:39,17/Mar/23 02:01,04/Jun/24 20:41,17/Mar/23 02:01,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,starter,,,"At present, only common has tests. We need to copy a part of itcase to 1.14 and 1.15 to ensure normal work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 02:01:53 UTC 2023,,,,,,,,,,"0|z1g8eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 08:19;tyrantlucifer;I want to try it. Coule you please assign it to me? Thanks! cc [~lzljs3620320] ;;;","01/Mar/23 03:12;lzljs3620320;[~tyrantlucifer] Assigned thanks!;;;","17/Mar/23 02:01;lzljs3620320;master: 7663615366a16d3481c67aa093a5f95973ae552e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve StaticFileStoreSplitEnumerator to assign batch splits,FLINK-31252,13526455,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,28/Feb/23 06:08,03/Mar/23 07:01,04/Jun/24 20:41,03/Mar/23 07:01,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"The following batch assignment operation is for two things:
1. It can be evenly distributed during batch reading to avoid scheduling problems (for example, the current resource can only schedule part of the tasks) that cause some tasks to fail to read data.
2. Read with limit, if split is assigned one by one, it may cause the task to repeatedly create SplitFetchers. After the task is created, it is found that it is idle and then closed. Then, new split coming, it will create SplitFetcher and repeatedly read the data of the limit number (the limit status is in the SplitFetcher).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 07:01:02 UTC 2023,,,,,,,,,,"0|z1g8c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 07:01;lzljs3620320;master: 4296d7c1cca7ff8fb5525401b1ef1659aae5879a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate deserialize method in DeserializationSchema,FLINK-31251,13526454,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,28/Feb/23 06:02,28/Feb/23 06:02,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"Deprecate method `T deserialize(byte[] message)` and use `void deserialize(byte[] message, Collector<T> out)` instead in `DeserializationSchema`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-28 06:02:57.0,,,,,,,,,,"0|z1g8bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet format supports MULTISET type,FLINK-31250,13526453,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,28/Feb/23 05:45,28/Feb/23 09:44,04/Jun/24 20:41,28/Feb/23 09:44,1.18.0,,,,,,,,,,,1.18.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"Parquet format supports ARRAY, MAP and ROW type, doesn't support MULTISET type. Parquet format should support MULTISET type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 09:44:08 UTC 2023,,,,,,,,,,"0|z1g8bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 09:44;lzljs3620320;master: 161014149e803bfd1d3653badb230b2ed36ce3cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint timeout mechanism fails when finalizeCheckpoint is stuck,FLINK-31249,13526441,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhourenxiang,zhourenxiang,28/Feb/23 03:40,11/Mar/24 12:44,04/Jun/24 20:41,,1.11.6,1.16.0,,,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,,,,,"When jobmanager receives all ACKs of tasks, it will finalize the pending checkpoint to a completed checkpoint. Currently JM finalizes the pending checkpoint with holding the checkpoint coordinator lock.

When a DFS failure occurs, the {{jobmanager-future}} thread may be blocked at finalizing the pending checkpoint.

!image-2023-02-28-12-17-19-607.png|width=1010,height=244!

And then the next checkpoint is triggered, the {{Checkpoint Timer}} thread waits for the lock to be released. 

!image-2023-02-28-11-25-03-637.png|width=1144,height=248!

If the previous checkpoint times out, the {{Checkpoint Timer}} will not execute the timeout event since it is blocked at waiting for the lock. As a result, the previous checkpoint cannot be cancelled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 03:25;zhourenxiang;image-2023-02-28-11-25-03-637.png;https://issues.apache.org/jira/secure/attachment/13055873/image-2023-02-28-11-25-03-637.png","28/Feb/23 04:04;zhourenxiang;image-2023-02-28-12-04-35-178.png;https://issues.apache.org/jira/secure/attachment/13055874/image-2023-02-28-12-04-35-178.png","28/Feb/23 04:17;zhourenxiang;image-2023-02-28-12-17-19-607.png;https://issues.apache.org/jira/secure/attachment/13055875/image-2023-02-28-12-17-19-607.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Mon Mar 06 08:45:19 UTC 2023,,,,,,,,,,"0|z1g88w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 14:11;mayuehappy;[~roman] [~yunta] could you please take a look at this ticket ? 

;;;","02/Mar/23 12:22;zhourenxiang;[~Yanfei Lei] [~masteryhx] Count you please have a look at this ticket ? :P;;;","02/Mar/23 22:30;roman;[~mayuehappy] , [~zhourenxiang] ,

on the images I see that CheckpointCoordinator.chooseRequestToExecute is waiting for the last checkpoint to be finalized. This is intentional to avoid concurrency issues.

IIUC, checkpoint finalization is paused artificially.

 

Are you observing any issues with that in non-mocked setup?;;;","03/Mar/23 03:43;zhourenxiang;[~roman] 

When it takes too long to finalize the last checkpoint, should we cancel the last checkpoint by checkpoint timeout function？

Currently I haven't observed this issue in non-mocked setup, but I think it could happen when finalizing checkpoint gets stuck in writing metadata to DFS due to a DFS failure, like namenode failure of HDFS.;;;","03/Mar/23 09:32;roman;[~zhourenxiang] , the timeout is applied to a checkpoint that is already started (i.e. RPC sent out to sources). Long checkpoint might potentially accumulate too much data (with Unaligned checkpoints) or block progress (with Aligned checkpoints).

But here, the checkpoint was not yet triggered, so the tasks aren't even aware of it. What would be the benefit of timing it out?;;;","03/Mar/23 11:25;zhourenxiang;[~roman] If it takes too long to finalize the checkpoint metadata, it usually means that there is a problem with the external storage service (in HDFS, it could happen when writing to a slow DataNode). In this case, we can retry writing a new metadata to DFS or just discard this checkpoint and make another one, rather than leaving the checkpoint stuck. What do you think of it ?;;;","03/Mar/23 17:08;roman;Allowing to trigger a new checkpoint without unblocking the other (main) thread doesn't make much sense to me: at least to process the ACKs for that new checkpoint, the main thread is required.

 

Ideally, all IO should be done in a separate thread, but we're not there yet. I don't see a way to interrupt writing metadata generically (for any filesystem).

Rather, specific FS implementations can be configured to tinder out too long requests.

 

Besides that, the same filesystem usually stores state backend snapshots and this metadata. When overloaded, it's more likely that state backend snapshots will time out first.;;;","06/Mar/23 07:58;zhourenxiang;Thanks for your reply.

The following checkpoints are all blocked when this case occurred, and the checkpoint-related metrics can not report, so the user may not realize that his job's checkpoint has blocked for a long time. This is important for tasks with high real-time requirements. So could we think about failing this checkpoint?User can aware that the checkpoint is stuck by the failure checkpoints. ;;;","06/Mar/23 08:45;roman;That's doable by writing metadata in a separate (IO) thread and waiting for a result with a timeout.

But I'm not sure whether that wouldn't do more harm than good:
 * most of the work was already done by this point (snapshotting the tasks), and timing out writing the metadata file (usually small) will discard and start it over; that essentially delays the checkpoint
 * and if the timeout is caused by the overload then that next checkpoint is much less likely to succeed (because it needs to discard the state written, upload it again, write metadata again)
 * in a more narrow case, when it's the IO thread pool that is overloaded (but not the IO) - it will be a pure regression

 So I'd avoid such a change without a real world use case.

 

Could you elaborate why the above proposal
{quote}Rather, specific FS implementations can be configured to tinder out too long requests.
{quote}
doesn't work in your case?

 

As for the alerts, it should also possible to have them when there are no datapoints about recent checkpoints.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve documentation for append-only table,FLINK-31248,13526430,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,28/Feb/23 02:30,02/Mar/23 06:11,04/Jun/24 20:41,02/Mar/23 06:11,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:11:35 UTC 2023,,,,,,,,,,"0|z1g86g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 06:11;TsReaper;master: b8a700082a6032dfed7cee4273f3f76ce0483b5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for fine-grained resource allocation (slot sharing groups) at the TableAPI level,FLINK-31247,13526408,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,sergiosp,sergiosp,27/Feb/23 22:42,16/Mar/23 08:54,04/Jun/24 20:41,16/Mar/23 08:54,1.16.1,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"Currently Flink allows for fine-grained resource allocation at the DataStream API level (please see [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/finegrained_resource/)]

 

This ticket is an enhancement request to support the same API at the Table level:

 

org.apache.flink.table.api.Table.setSlotSharingGroup(String ssg)

org.apache.flink.table.api.Table.setSlotSharingGroup(SlotSharingGroup ssg)

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 09:06:13 UTC 2023,,,,,,,,,,"0|z1g81k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 09:05;xtsong;I'm afraid this is not feasible. SlotSharingGroup defines which operators / transformations should be colocated. However, Table API is declarative and does not have the concept of operators / transformations. ;;;","09/Mar/23 09:06;martijnvisser;+1 for what [~xtsong] just said. I think we can close this ticket as a Won't Do;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beautify the SpecChange message,FLINK-31246,13526384,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,27/Feb/23 20:00,05/Mar/23 14:31,04/Jun/24 20:41,05/Mar/23 14:31,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Currently the Spec Change message contains the full PodTemplate twice.
This makes the message seriously big and also contains very little useful information.

We should abbreviate the message",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 05 14:31:12 UTC 2023,,,,,,,,,,"0|z1g7w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 08:20;Wencong Liu;Hello [~pvary], I'm quite interested in this issue could you please provide the code position?;;;","28/Feb/23 08:56;pvary;Talked with [~gyfora] about this, and he is concerned that this would be a breaking change for some users and could cause issues for them;;;","05/Mar/23 14:31;gyfora;merged to main ddada0e835a6078be70f0828434d5fada7f335b1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive scheduler does not reset the state of GlobalAggregateManager when rescaling,FLINK-31245,13526337,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,27/Feb/23 13:03,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,,,,,"*Problem*

GlobalAggregateManager is used to share state amongst parallel tasks in a job and thus coordinate their execution. It maintains a state (the _accumulators_ field in JobMaster) in JM memory. The accumulator state content is defined in user code, in my company, a user stores task parallelism in the accumulator, assuming task parallelism never changes. However, this assumption is broken when using adaptive scheduler.

*Possible Solutions*
 # Mark GlobalAggregateManager as deprecated. It seems that operator coordinator can completely replace GlobalAggregateManager and is a more elegent solution. Therefore, it is fine to deprecate GlobalAggregateManager and leave this issue there. If that's the case, we can open another ticket for doing that.
 # If we decide to continue supporting GlobalAggregateManager, then we need to reset the state when rescaling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 13:21:44 UTC 2023,,,,,,,,,,"0|z1g7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 13:07;Zhanghao Chen;[~dmvk] Looking forward to your opinions on this. Personally, I think we can deprecate the use of GlobalAggregateManager.;;;","27/Feb/23 13:19;dmvk;Hi [~Zhanghao Chen] , thanks for the report!

 

As far as I'm aware, this was only used in the Kinesis source [1] for an early version of watermark alignment. I'd favor deprecating it, but we need to run this through the dev mailing list first in case there are other use cases I'm unaware of.

 

[1] https://github.com/apache/flink-connector-aws/blob/d0817fecdcaa53c4bf039761c2d1a16e8fb9f89b/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/util/JobManagerWatermarkTracker.java;;;","27/Feb/23 13:21;Zhanghao Chen;Thanks [~dmvk]. I'll open a discussion in dev mailing list for that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffHeapUnsafeMemorySegmentTest.testCallCleanerOnceOnConcurrentFree prints IllegalStateException,FLINK-31244,13526335,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,27/Feb/23 12:57,19/Aug/23 10:35,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Runtime / Network,Tests,,,0,auto-deprioritized-major,starter,test-stability,,"We're observing strange IllegalStateException stacktrace output in {{OffHeapUnsafeMemorySegmentTest.testCallCleanerOnceOnConcurrentFree}} in CI like:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=5584] 
{code:java}
Feb 18 03:58:47 [INFO] Running org.apache.flink.core.memory.OffHeapUnsafeMemorySegmentTest
Exception in thread ""Thread-13"" java.lang.IllegalStateException: MemorySegment can be freed only once!
    at org.apache.flink.core.memory.MemorySegment.free(MemorySegment.java:244)
    at java.lang.Thread.run(Thread.java:748)
Exception in thread ""Thread-15"" java.lang.IllegalStateException: MemorySegment can be freed only once!
    at org.apache.flink.core.memory.MemorySegment.free(MemorySegment.java:244)
    at java.lang.Thread.run(Thread.java:748)
Exception in thread ""Thread-17"" java.lang.IllegalStateException: MemorySegment can be freed only once!
    at org.apache.flink.core.memory.MemorySegment.free(MemorySegment.java:244)
    at java.lang.Thread.run(Thread.java:748){code}
This is caused by FLINK-21798. The corresponding system property is enabled as part of the CI run (see [tools/ci/test_controller.sh:108|https://github.com/apache/flink/blob/7e37d59f834bca805f5fbee99db87eb909d1814f/tools/ci/test_controller.sh#L108]) which makes the {{IllegalStateException}} to be thrown.

AFAIU, the intention of this test was to make sure that the cleaner logic is only called once even if the free method is called multiple times. ",,,,,,,,,,,,,,,,,,,,,,,FLINK-21798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:08 UTC 2023,,,,,,,,,,"0|z1g7lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 12:58;mapohl;[~xtsong] what's the intention of throwing the {{IllegalStateException}} in the first place here? AFAIU, checking the cleanup method is the actual reason for this test.;;;","27/Feb/23 15:25;Weijie Guo;IIUC, {{flink.tests.check-segment-multiple-free}} is just to help us find out whether there is potential double free case in flink through test.
However, this test({{testCallCleanerOnceOnConcurrentFree}}) exactly needs to create the special case of double free. Maybe we can disable this check for this test?;;;","28/Feb/23 10:51;xtsong;IIRC, the conclusions of previous discussions were:
1. A memory segment should not be freed multiple times, because that indicates design flaws that the ownership of the memory segment is unclear.
2. It's hard to tell whether there are existing cases that may lead to a memory segment being freed multiple times, even with the CI tests. So we did not throw an exception for it in production. The exception will only be thrown for CI tests, in order to catch and fix such multiple freeing cases.
3. Ensuring the cleaner is only called once even if the segment is freed multiple times is more like a safety net.

In this test case, the exception is thrown from another thread and is uncaught, thus the test did not fail. Obviously, this is undesired and should be fixed. In fact, I wonder if we should change it to through the exception for multiple freeing of segment by default, and preserve the switch for falling back in case of bad cases.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KryoSerializer when loaded from user code classloader cannot load Scala extensions from app classloader,FLINK-31243,13526327,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,amitgurd,amitgurd,27/Feb/23 12:00,02/Mar/23 15:53,04/Jun/24 20:41,,1.15.3,1.16.1,,,,,,,,,,,,,,API / Core,,,,1,,,,,"The [KryoSerializer|https://github.com/apache/flink/blob/9bf0d9f2c2bcb2bc0c8ab6228bb0a9e76e10ad70/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/kryo/KryoSerializer.java] uses Class.forName() to dynamically load Scala extensions by name. This seems to imply that it references only its own classloader to find these extensions. By default, as the application classloader is favored for KryoSerializer, this implies that unless the flink-scala artifact is available to the application classloader, the Scala extensions cannot be loaded. Scala applications that include flink-scala are therefore unable to benefit from the Scala extensions to the Kryo Serializer.

Exception looks like this:
{noformat}
java.lang.ClassNotFoundException: org.apache.flink.runtime.types.FlinkScalaKryoInstantiator
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:315)
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:486)
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:521)
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo(KryoSerializer.java:720)
    at software.amazon.kinesisanalytics.kryotest.Main.main(Main.java:16)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
    at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:84)
    at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:70)
    at org.apache.flink.runtime.webmonitor.handlers.JarRunOverrideHandler.lambda$handleRequest$3(JarRunOverrideHandler.java:239)
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829){noformat}
Example code resulting in this issue:

Main class for Flink application:
{noformat}
package software.amazon.kinesisanalytics.kryotest;

import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.io.Serializable;

public class Main {
    private static class Something implements Serializable {
        public static long serialVersionUID = 289034745902347830L;
    }

    public static void main(String... args) {
        StreamExecutionEnvironment executionEnvironment = new StreamExecutionEnvironment();
        KryoSerializer<Something> serializer = new KryoSerializer<>(Something.class, executionEnvironment.getConfig());
        serializer.getKryo();
    }
}
{noformat}
build.gradle for Flink application:
{code:java}
plugins {
    id 'application'
    id 'java'
    id 'com.github.johnrengelman.shadow' version '7.1.2'
}

group 'software.amazon.kinesisanalytics'
version '0.1'

repositories {
    mavenCentral()
}

dependencies {
    compileOnly 'org.apache.flink:flink-core:1.15.2'
    compileOnly 'org.apache.flink:flink-streaming-java:1.15.2'
    implementation 'org.apache.flink:flink-scala_2.12:1.15.2'
}

shadowJar {
    dependencies {
        exclude(dependency('com.esotericsoftware.kryo:.*:.*'))
        exclude(dependency('com.esotericsoftware.minlog:.*:.*'))
        exclude(dependency('com.twitter:.*:.*'))
        exclude(dependency('org.apache.flink:flink-core:.*'))
        exclude(dependency('org.apache.flink:flink-streaming-java:.*'))
        exclude(dependency('org.scala-lang:.*:.*'))
    }
}

mainClassName = 'software.amazon.kinesisanalytics.kryotest.Main'
 {code}
Note that the application jar does not include Kryo itself, nor flink-core, but does include flink-scala.

Placing flink-scala in the application classpath eliminates the error, but as I understand it, the [point of eliminating Scala|https://flink.apache.org/2022/02/22/scala-free-in-one-fifteen/] from the Flink application classloader was to allow the only Scala dependencies to be loaded by the user code classloader. This issue prevents that from being achieved for the Scala extensions to the Kryo Serializer.","OS: Amazon Linux 2

JVM: Amazon Corretto 11

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 15:53:30 UTC 2023,,,,,,,,,,"0|z1g7jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 13:46;martijnvisser;[~amitgurd] Given that https://nightlies.apache.org/flink/flink-docs-master/docs/dev/configuration/overview/#which-dependencies-do-you-need lists that you need to include {{flink-streaming-scala_2.12}} and not {{flink-streaming-java}} if you want to use the DataStream API with Scala, can you check if this still occurs when you use the correct dependencies? ;;;","27/Feb/23 16:51;amitgurd;[~martijnvisser] I can confirm that I still see the error using the flink-streaming-scala_2.12 dependency. Revised build.gradle:
{noformat}
plugins {
    id 'application'
    id 'java'
    id 'com.github.johnrengelman.shadow' version '7.1.2'
}

group 'software.amazon.kinesisanalytics'
version '0.1'

repositories {
    mavenCentral()
}

dependencies {
    compileOnly 'org.apache.flink:flink-core:1.15.3'
    implementation 'org.apache.flink:flink-streaming-scala_2.12:1.15.3'
}

shadowJar {
    dependencies {
        exclude(dependency('com.esotericsoftware.kryo:.*:.*'))
        exclude(dependency('com.esotericsoftware.minlog:.*:.*'))
        exclude(dependency('com.twitter:.*:.*'))
        exclude(dependency('org.apache.flink:flink-core:.*'))
        exclude(dependency('org.scala-lang:.*:.*'))
    }
}

mainClassName = 'software.amazon.kinesisanalytics.kryotest.Main'
 {noformat}
I still see ""Kryo serializer scala extensions are not available."" with the exception stack trace in the Description of this Jira issue.;;;","02/Mar/23 15:44;martijnvisser;| Placing flink-scala in the application classpath eliminates the error, but as I understand it, the point of eliminating Scala from the Flink application classloader was to allow the only Scala dependencies to be loaded by the user code classloader. 

The point in the blog post is that you can now have a Scala-free user-code classpath (because you only use the Java APIs or because you want to build your application in Scala with the Java APIs) if you want to, by removing the flink-scala-* from the lib folder. 

If you want to use Scala (as you want), that is not an option. ;;;","02/Mar/23 15:48;amitgurd;Is the intention then to only allow a particular Flink runtime setup to support applications using a single Scala version? The blog post cites wrappers for Scala 3 – does this mean that the same Flink runtime cannot be expected to support running different jobs that involve Scala 2.12, 2.13 and 3 wrappers?;;;","02/Mar/23 15:53;martijnvisser;[~amitgurd] The intention was to unblock users from being stuck with Scala 2.12.7 forever (which is the latest version that Flink can support with Scala). You can achieve that by not using Flink's Scala APIs/runtime, but build your application in your desired Scala version while using Flink's Java APIs. If you want to use Flink's Scala APIs, you must use Scala 2.12.7. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the definition of creating functions in the SQL client documentation,FLINK-31242,13526323,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Duplicate,,yesorno,yesorno,27/Feb/23 11:35,08/Nov/23 07:09,04/Jun/24 20:41,08/Nov/23 07:09,1.17.0,,,,,,,,,,,,,,,Documentation,,,,0,auto-deprioritized-minor,pull-request-available,,,"The definition of creating functions is wrong in the SQL Client section:
{code:java}
-- Define user-defined functions here.

CREATE FUNCTION foo.bar.AggregateUDF AS myUDF;{code}
The correct one should be:
{code:java}
-- Define user-defined functions here. 

CREATE FUNCTION myUDF AS 'foo.bar.AggregateUDF'; {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 03 22:35:09 UTC 2023,,,,,,,,,,"0|z1g7io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dependency cannot be resolved,FLINK-31241,13526313,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,27/Feb/23 10:13,21/Aug/23 08:58,04/Jun/24 20:41,21/Aug/23 08:58,1.17.0,,,,,,,,,,,,,,,Connectors / Hive,,,,0,auto-deprioritized-critical,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46535&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf]
{code:java}
Feb 25 01:24:49 [ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.17-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-service:jar:3.1.3 -> org.apache.hive:hive-llap-server:jar:3.1.3 -> org.apache.hbase:hbase-server:jar:2.0.0-alpha4 -> org.glassfish.web:javax.servlet.jsp:jar:2.3.2 -> org.glassfish:javax.el:jar:3.0.1-b06-SNAPSHOT: Failed to read artifact descriptor for org.glassfish:javax.el:jar:3.0.1-b06-SNAPSHOT: Could not transfer artifact org.glassfish:javax.el:pom:3.0.1-b06-SNAPSHOT from/to apache.snapshots (https://repository.apache.org/snapshots): transfer failed for https://repository.apache.org/snapshots/org/glassfish/javax.el/3.0.1-b06-SNAPSHOT/javax.el-3.0.1-b06-SNAPSHOT.pom, status: 502 Proxy Error -> [Help 1] {code}
I would imagine this to be a network issue.",,,,,,,,,,,,,,,,,,,,,,FLINK-30104,,,FLINK-19469,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:11 UTC 2023,,,,,,,,,,"0|z1g7gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce the overhead of conversion between DataStream and Table,FLINK-31240,13526307,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,Jiang Xin,Jiang Xin,27/Feb/23 09:34,25/May/23 13:12,04/Jun/24 20:41,25/May/23 13:11,,,,,,,,,,,,1.18.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"In some cases, users may need to convert the underlying DataStream to Table and then convert it back to DataStream(e.g. some Flink ML libraries accept a Table as input and convert it to DataStream for calculation.). This would cause unnecessary overhead because of data conversion between the internal data type and the external data type.

We can reduce the overhead by checking if there are paired `fromDataStream`/`toDataStream` function call without any transformation, if so using the source datastream directly.

 The performance of Flink ML's Bucketizer algorithm[1] is used to demonstrate the impact of this optimization. The execution time is obtained by taking the median execution time across 5 runs for each setup.

Before optimization: 40746ms
After optimization: 12972ms
Thus this optimization reduces the total execution time of Flink ML's Bucketizer algorithm to about 1/3.

[1] https://github.com/apache/flink-ml/blob/master/flink-ml-benchmark/src/main/resources/bucketizer-benchmark.json",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 13:12:53 UTC 2023,,,,,,,,,,"0|z1g7f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 01:17;lindong;[~yunfengzhou] Can you explain the impact of this performance optimizing in the JIRA description?;;;","30/Mar/23 03:42;yunfengzhou;It seems that I do not have permission to modify this JIRA's description. I have added a performance benchmark result to the description of the following PR.

https://github.com/apache/flink/pull/22262;;;","25/May/23 13:12;lindong;Merged to master branch 6b6df3db466d6a030d5a38ec786ac3297cb41c38.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix sum function can't get the corrected value when the argument type is string,FLINK-31239,13526299,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,27/Feb/23 09:06,03/Mar/23 12:35,04/Jun/24 20:41,03/Mar/23 12:35,1.17.0,,,,,,,,,,,1.17.0,1.18.0,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 12:35:18 UTC 2023,,,,,,,,,,"0|z1g7dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 09:13;lsy;Currently, for the following case:
{code:java}
tableEnv.executeSql(""create table test_sum_dec(a int, x string, z decimal(10, 5))"");
tableEnv.executeSql(
                ""insert into test_sum_dec values (1, 'b', null), ""
                        + ""(1, 'b', 1.2), ""
                        + ""(2, 'b', null), ""
                        + ""(2, 'b', null),""
                        + ""(4, '1', null),""
                        + ""(4, 'b', null)"")
        .await();

List<Row> result =
        CollectionUtil.iteratorToList(
                tableEnv.executeSql(""select a, sum(x) from test_sum_dec group by a"")
                        .collect());
assertThat(result.toString()).isEqualTo(""[+I[1,null], +I[2, null], +I[4, 1.0]]"");
 {code}
The native sum function return `[+I[1,null], +I[2, null], +I[4, 1.0]]`, but hive sum function return `[+I[1,0.0], +I[2,0.0], +I[4, 1.0]]`. The native function return result is not consistent with hive, so we should fix it.;;;","03/Mar/23 12:35;godfrey;Fixed in

1.18.0: 263555c9adcca0abe194e9a6c1d85ec591c304e4..62a3b99d23229b39c798a0b657cb11218a5bc940

1.17.0: 3bdb50513ddbbf6c67560a078da3f9506e5cd611..ac2eb5b977de47fc5550d2ee9f30fff4dcaca2b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use IngestDB to speed up Rocksdb rescaling recovery ,FLINK-31238,13526295,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mayuehappy,mayuehappy,mayuehappy,27/Feb/23 08:56,13/Feb/24 16:07,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Runtime / State Backends,,,,3,pull-request-available,stale-assigned,,,"（The detailed design is in this document
[https://docs.google.com/document/d/10MNVytTsyiDLZQSR89kDkVdmK_YjbM6jh0teerfDFfI|https://docs.google.com/document/d/10MNVytTsyiDLZQSR89kDkVdmK_YjbM6jh0teerfDFfI]）

There have been many discussions and optimizations in the community about optimizing rocksdb scaling and recovery.

https://issues.apache.org/jira/browse/FLINK-17971

https://issues.apache.org/jira/browse/FLINK-8845

https://issues.apache.org/jira/browse/FLINK-21321

We hope to discuss some of our explorations under this ticket

The process of scaling and recovering in rocksdb simply requires two steps
 # Insert the valid keyGroup data of the new task.
 # Delete the invalid data in the old stateHandle.

The current method for data writing is to specify the main Db first and then insert data using writeBatch.In addition, the method of deleteRange is currently used to speed up the ClipDB. But in our production environment, we found that the speed of rescaling is still very slow, especially when the state of a single Task is large. 

 

We hope that the previous sst file can be reused directly when restoring state, instead of retraversing the data. So we made some attempts to optimize it in our internal version of flink and frocksdb.

 

We added two APIs *ClipDb* and *IngestDb* in frocksdb. 
 * ClipDB is used to clip the data of a DB. Different from db.DeteleRange and db.Delete, DeleteValue and RangeTombstone will not be generated for parts beyond the key range. We will iterate over the FileMetaData of db. Process each sst file. There are three situations here. 
If all the keys of a file are required, we will keep the sst file and do nothing 
If all the keys of the sst file exceed the specified range, we will delete the file directly. 
If we only need some part of the sst file, we will rewrite the required keys to generate a new sst file。
All sst file changes will be placed in a VersionEdit, and the current versions will LogAndApply this edit to ensure that these changes can take effect
 * IngestDb is used to directly ingest all sst files of one DB into another DB. But it is necessary to strictly ensure that the keys of the two DBs do not overlap, which is easy to do in the Flink scenario. The hard link method will be used in the process of ingesting files, so it will be very fast. At the same time, the file number of the main DB will be incremented sequentially, and the SequenceNumber of the main DB will be updated to the larger SequenceNumber of the two DBs.

When IngestDb and ClipDb are supported, the state restoration logic is as follows
 * Open the first StateHandle as the main DB and pause the compaction.
 * Clip the main DB according to the KeyGroup range of the Task with ClipDB
 * Open other StateHandles in sequence as Tmp DB, and perform ClipDb  according to the KeyGroup range
 * Ingest all tmpDb into the main Db after tmpDb cliped
 * Open the Compaction process of the main DB
!screenshot-1.png|width=923,height=243!

We have done some benchmark tests on the internal Flink version, and the test results show that compared with the writeBatch method, the expansion and recovery speed of IngestDb can be increased by 5 to 10 times as follows 

(SstFileWriter means uses the recovery method of generating sst files through SstFileWriter in parallel）
 * parallelism changes from 4 to 2

|*TaskStateSize*|*Write_Batch*|*SST_File_Writer*|*Ingest_DB*|
|500M|Iteration 1: 8.018 s/op
Iteration 2: 9.551 s/op
Iteration 3: 7.486 s/op|Iteration 1: 6.041 s/op
Iteration 2: 5.934 s/op
Iteration 3: 6.707 s/o|{color:#ff0000}Iteration 1: 3.922 s/op{color}
{color:#ff0000}Iteration 2: 3.208 s/op{color}
{color:#ff0000}Iteration 3: 3.096 s/op{color}|
|1G|Iteration 1: 19.686 s/op
Iteration 2: 19.402 s/op
Iteration 3: 21.146 s/op|Iteration 1: 17.538 s/op
Iteration 2: 16.933 s/op
Iteration 3: 15.486 s/op|{color:#ff0000}Iteration 1: 6.207 s/op{color}
{color:#ff0000}Iteration 2: 7.164 s/op{color}
{color:#ff0000}Iteration 3: 6.397 s/op{color}|
|5G|Iteration 1: 244.795 s/op
Iteration 2: 243.141 s/op
Iteration 3: 253.542 s/op|Iteration 1: 78.058 s/op
Iteration 2: 85.635 s/op
Iteration 3: 76.568 s/op|{color:#ff0000}Iteration 1: 23.397 s/op{color}
{color:#ff0000}Iteration 2: 21.387 s/op{color}
{color:#ff0000}Iteration 3: 22.858 s/op{color}|
 * parallelism changes from 4 to 8

|*TaskStateSize*|*Write_Batch*|*SST_File_Writer*|*Ingest_DB*|
|500M|Iteration 1: 3.477 s/op
Iteration 2: 3.515 s/op
Iteration 3: 3.433 s/op|Iteration 1: 3.453 s/op
Iteration 2: 3.300 s/op
Iteration 3: 3.313 s/op|{color:#ff0000}Iteration 1: 0.941 s/op{color}
{color:#ff0000}Iteration 2: 0.963 s/op{color}
{color:#ff0000}Iteration 3: 1.102 s/op{color}|
|1G|IIteration 1: 7.571 s/op
Iteration 2: 7.352 s/op
Iteration 3: 7.568 s/op|Iteration 1: 5.032 s/op
Iteration 2: 4.689 s/op
Iteration 3: 6.883 s/op|{color:#ff0000}Iteration 1: 2.130 s/op{color}
{color:#ff0000}Iteration 2: 2.110 s/op{color}
{color:#ff0000}Iteration 3: 2.034 s/op{color}|
|5G|Iteration 1: 91.870 s/op
Iteration 2: 94.229 s/op
Iteration 3: 93.271 s/op|Iteration 1: 25.845 s/op
Iteration 2: 25.571 s/op
Iteration 3: 25.685 s/op|{color:#ff0000}Iteration 1: 11.154 s/op{color}
{color:#ff0000}Iteration 2: 10.732 s/op{color}
{color:#ff0000}Iteration 3: 10.622 s/op{color}|
 * parallelism changes from 4 to 6

|*TaskStateSize*|*Write_Batch*|*SST_File_Writer*|*Ingest_DB*|
|500M|Iteration 1: 8.209 s/op
Iteration 2: 9.893 s/op
Iteration 3: 9.150 s/op|Iteration 1: 6.041 s/op
Iteration 2: 5.934 s/op
Iteration 3: 6.707 s/o|{color:#ff0000}Iteration 1: 2.622 s/op{color}
{color:#ff0000}Iteration 2: 2.545 s/op{color}
{color:#ff0000}Iteration 3: 2.573 s/op{color}|
|1G|Iteration 1: 21.206 s/op
Iteration 2: 26.214 s/op
Iteration 3: 20.269 s/op|Iteration 1: 10.043 s/op
Iteration 2: 10.744 s/op
Iteration 3: 10.461 s/op|{color:#ff0000}Iteration 1: 4.400 s/op{color}
{color:#ff0000}Iteration 2: 4.340 s/op{color}
{color:#ff0000}Iteration 3: 6.234 s/op{color}|
|5G|IIteration 1: 170.606 s/op
Iteration 2: 160.576 s/op
Iteration 3: 159.425 s/op|IIteration 1: 52.537 s/op
Iteration 2: 50.576 s/op
Iteration 3: 50.823 s/op|{color:#ff0000}Iteration 1: 19.053 s/op{color}
{color:#ff0000}Iteration 2: 18.504 s/op{color}
{color:#ff0000}Iteration 3: 18.249 s/op{color}|
 * parallelism changes from 4 to 3

|*TaskStateSize*|*Write_Batch*|*SST_File_Writer*|*Ingest_DB*|
|500M|Iteration 1: 6.330 s/op
Iteration 2: 5.614 s/op
Iteration 3: 5.736 s/op|Iteration 1: 4.083 s/op
Iteration 2: 5.655 s/op
Iteration 3: 3.998 s/op|{color:#ff0000}Iteration 1: 2.157 s/op{color}
{color:#ff0000}Iteration 2: 2.201 s/op{color}
{color:#ff0000}Iteration 3: 3.212 s/op{color}|
|1G|Iteration 1: 13.814 s/op
Iteration 2: 12.852 s/op
Iteration 3: 13.480 s/op|Iteration 1: 9.619 s/op
Iteration 2: 9.197 s/op
Iteration 3: 8.694 s/op|{color:#ff0000}Iteration 1: 4.227 s/op{color}
{color:#ff0000}Iteration 2: 4.234 s/op{color}
{color:#ff0000}Iteration 3: 4.177 s/op{color}|
|5G|Iteration 1: 136.621 s/op
Iteration 2: 127.097 s/op
Iteration 3: 139.694 s/op|Iteration 1: 39.612 s/op
Iteration 2: 38.809 s/op
Iteration 3: 39.125 s/op|{color:#ff0000}Iteration 1: 16.691 s/op{color}
{color:#ff0000}Iteration 2: 16.599 s/op{color}
{color:#ff0000}Iteration 3: 16.726 s/op{color}|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 08:41;mayuehappy;image-2023-02-27-16-41-18-552.png;https://issues.apache.org/jira/secure/attachment/13055851/image-2023-02-27-16-41-18-552.png","27/Feb/23 08:57;mayuehappy;image-2023-02-27-16-57-18-435.png;https://issues.apache.org/jira/secure/attachment/13055852/image-2023-02-27-16-57-18-435.png","07/Mar/23 06:27;mayuehappy;image-2023-03-07-14-27-10-260.png;https://issues.apache.org/jira/secure/attachment/13056084/image-2023-03-07-14-27-10-260.png","09/Mar/23 07:23;mayuehappy;image-2023-03-09-15-23-30-581.png;https://issues.apache.org/jira/secure/attachment/13056188/image-2023-03-09-15-23-30-581.png","09/Mar/23 07:26;mayuehappy;image-2023-03-09-15-26-12-314.png;https://issues.apache.org/jira/secure/attachment/13056189/image-2023-03-09-15-26-12-314.png","09/Mar/23 07:28;mayuehappy;image-2023-03-09-15-28-32-363.png;https://issues.apache.org/jira/secure/attachment/13056190/image-2023-03-09-15-28-32-363.png","09/Mar/23 07:41;mayuehappy;image-2023-03-09-15-41-03-074.png;https://issues.apache.org/jira/secure/attachment/13056191/image-2023-03-09-15-41-03-074.png","09/Mar/23 07:41;mayuehappy;image-2023-03-09-15-41-08-379.png;https://issues.apache.org/jira/secure/attachment/13056192/image-2023-03-09-15-41-08-379.png","09/Mar/23 07:45;mayuehappy;image-2023-03-09-15-45-56-081.png;https://issues.apache.org/jira/secure/attachment/13056193/image-2023-03-09-15-45-56-081.png","09/Mar/23 07:46;mayuehappy;image-2023-03-09-15-46-01-176.png;https://issues.apache.org/jira/secure/attachment/13056194/image-2023-03-09-15-46-01-176.png","09/Mar/23 07:50;mayuehappy;image-2023-03-09-15-50-04-281.png;https://issues.apache.org/jira/secure/attachment/13056195/image-2023-03-09-15-50-04-281.png","29/Mar/23 07:25;mayuehappy;image-2023-03-29-15-25-21-868.png;https://issues.apache.org/jira/secure/attachment/13056880/image-2023-03-29-15-25-21-868.png","17/Jul/23 06:37;mayuehappy;image-2023-07-17-14-37-38-864.png;https://issues.apache.org/jira/secure/attachment/13061361/image-2023-07-17-14-37-38-864.png","17/Jul/23 06:38;mayuehappy;image-2023-07-17-14-38-56-946.png;https://issues.apache.org/jira/secure/attachment/13061362/image-2023-07-17-14-38-56-946.png","22/Jul/23 06:16;mayuehappy;image-2023-07-22-14-16-31-856.png;https://issues.apache.org/jira/secure/attachment/13061545/image-2023-07-22-14-16-31-856.png","22/Jul/23 06:19;mayuehappy;image-2023-07-22-14-19-01-390.png;https://issues.apache.org/jira/secure/attachment/13061546/image-2023-07-22-14-19-01-390.png","08/Aug/23 13:32;mayuehappy;image-2023-08-08-21-32-43-783.png;https://issues.apache.org/jira/secure/attachment/13061988/image-2023-08-08-21-32-43-783.png","08/Aug/23 13:34;mayuehappy;image-2023-08-08-21-34-39-008.png;https://issues.apache.org/jira/secure/attachment/13061989/image-2023-08-08-21-34-39-008.png","08/Aug/23 13:39;mayuehappy;image-2023-08-08-21-39-39-135.png;https://issues.apache.org/jira/secure/attachment/13061990/image-2023-08-08-21-39-39-135.png","28/Mar/23 12:10;mayuehappy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13056860/screenshot-1.png",,20.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 07 22:35:11 UTC 2023,,,,,,,,,,"0|z1g7cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 11:02;Yanfei Lei;[~mayuehappy] Great work!  I also tried to use ingest() to speed up rescaling, but I didn't find a way to handle multiple rescaling(like 1->2->1).

As you mentioned, ""it is necessary to strictly ensure that the keys of the two DBs do not overlap, which is easy to do in the Flink scenario."" Could you please share how clipDB() rewrites the required keys to generate a new sst file to ensure non-overlap?

 ;;;","27/Feb/23 12:27;mayuehappy;[~Yanfei Lei] thanks a lot for reply.  Let me give an example to illustrate ClipDb. 
If a DB contains 4 files, the key ranges are  f1[1,3]  f2[4,6] f3[7,9] f4[10,12] 。After rescaling, we need the key range to be 1 to 5, so we need to call the method db.ClipDB(1, 5).
During the execution of clipDb, first we will get all file meta information (including key range). Then do the following with these files.
* For f1, all the keys are what we need. So we won't do anything with f1
* For f2, we only need k4 ~ k5. So we will built a TableReader in rocksdb to traverse the files of f2, and then use TableBuilder to create a new sst file f5. In the process of traversing f2, we will rewrite the data of k4~k5 into f5 (including kTypeDeletion, etc. ValueType) and then rebuild the meta information of f5. After f5 is built, delete f2 and add f5 to the original level of f2.  The whole process is similar to a forced compaction on f2
* For f3 and f4, there is no key we need, so these two files can be deleted directly.

After these steps, we can ensure that db only contains the data of key 1 ~ 5.

In level compaction, L0 files may be rewritten because they are out of order. For other levels, because they are in order, only boundary files will be rewritten at most. So the number of files that may be traversed is

Level_0_File_Num + Num_Of_Level * 2


When the state is relatively large, most files will be kept or discarded directly.








;;;","28/Feb/23 08:43;Yanfei Lei;Thanks for your sharing, this drastically reduces the number of files that need to be traversed.  I am also curious about how seqno is handled. After rewriting, is the seqno in the new SST file consistent with the original?

Is there any difference between ingestDB() and [IngestExternalFile()|https://rocksdb.org/blog/2017/02/17/bulkoad-ingest-sst-file.html] ?

BTW, are you planning to push clipDB() and ingestDB() to the RocksDB community?;;;","28/Feb/23 10:18;mayuehappy;[~Yanfei Lei]
1. IngestExternalFile will use global_seq_no, but when we rewrite, the seqno of each key will be consistent with the original. This can solve the problem of repeated rescale.

2. After ingestDb, seq_no will be updated with the maximum value of the two db. For example after db1 ingestDb(db2). The last_seq_no of db1 will be updated to Max(db_1last_seq_no, db2_last_seq_no)

3. In our preliminary implementation, the level of each file will not change after ingest. If a file is in l3 then it is still in l3 after ingest. But I'm not sure this is the best way, because it may cause the number of level files to change after two db ingests or increase the number of levels in the db. But on the whole, it seems these problems may not be very serious.

4. I'm not sure how the RocksDB community will thinks about this issue, because it seems that these two APIs may only be useful in some specific scenarios like flink
;;;","01/Mar/23 02:16;masteryhx;Thank you for sharing the design. The duration of rescaling is a big concern, when the state size of one parallelism becomes large. The idea in this design is a good solution. In a nutshell, reuse the sst files stored in the latest checkpoint.

However, I have some concerns about this design.

1. Does the IngestDb API rely on the ingestExternalFile() provided by RocksDB?
 * If yes. The restriction on the ingestExternalFile() API makes the design difficult to realize. I think the procedure of reusing the sst files needs to ingest sst files from one RocksDB to another one. However, as far as I know, RocksDB currently forbids this behavior. Though some people have been asking for this feature and removing the restriction, e.g., [https://github.com/facebook/rocksdb/pull/5602], the RocksDB community does not accept this feature right now.
 * If no. I think the IngestDb API needs to manipulate the VersionSet of RocksDB, thereby relying on the API of VersionSet. If RocksDB refactors the API of VersionSet, the IngestDb API also needs to change accordingly. We need to consider the maintainence cost in FRocksDB.

    So I think it's better to push forward this feature in the RocksDB community.

2. Both Downloading sst files and restoring DB are time-consuming, and the optimization of RocksDB rescaling need to take both into consideration. Local recovery is unavailable during rescaling recovery. From my experience, the duration of downloading sst files is often higher than rebuilding DB. especially when the bandwidth of users' remote DFS is limited. In this case, it's better to also consider the duration of downloading sst files.;;;","01/Mar/23 03:37;mayuehappy;[~masteryhx] Thanks for reply.
1. In our current implementation, IngestDb is a new API in rocksdb and does not depend on ingestExternalFile. Therefore, the API of VersionSet needs to be used to add and delete rocksdb sst files.
2. I think the Verset is the core API of rocksdb. If it is changed, all operations such as compaction\flush\ingestFile need to be changed, so I think the migration will not be complicated. I alse can try to push forword this feature to the rocksdb community
3. This design is mainly considered to reduce the time of rebuildDB. I think it is possible to reduce the number of downloaded sst files and only download the required sst files. We can continue to optimize in the feature, thus currently in our production environment, especially when the state is relatively large, the time-consuming of rebuilding db is still much longer than downloading files. For example, for a task with a single task of 10G state, it may only take 1 minute to download the files, but takes more than 15 minutes to rebuild DB.;;;","07/Mar/23 06:31;mayuehappy;[~yunta] [~klion26] [~ym] [~roman] 
Can you guys take a look at the tickets when you are free? Do you have any thoughts on this?;;;","08/Mar/23 08:57;masteryhx;[~mayuehappy] , Thanks for your detailed explanation. 

Flink can benefit from extending RocksDB's functions, but should comform to the public RocksDB's API that are designed for the users of RocksDB. For example, the TTL state in Flink extends the function of RocksDB compaction filter. 

To my best knowledge, VersionSet API is tightly coupled with the implementation of MVCC mechanism inside RocksDB. The implementation of MVCC in RocksDB has some complicated engineering techniques and design principles. I am not sure about the safety of the two new APIs. That's why I'd like to suggest to push forward it in the RocksDB community firstly.

Of course, I'm also very open to hear the idea of other developers.;;;","08/Mar/23 09:49;yunta;[~mayuehappy] Thanks for the great work!
If my understanding is correct, the operations of ClipDb and IngestDb happened during the #initialization phase?
Actually, I also implemented a similar ability in another LSM-like storage but running in the background instead of blocking the #initialization phase. Do you think we can also achieve similar effects?;;;","09/Mar/23 07:24;mayuehappy;[~masteryhx]  Thanks for the reply, I understand your concerns. It's a good suggestion to push this to rocksdb community;;;","09/Mar/23 07:54;mayuehappy;[~yunta] Thanks for replying ~

Yes, in the current implementation both Clip and Ingest happen in #initialization phase . But compared to the previous way, its blocking time will be much reduced.

I have also thought about asynchronous recovery during rescaling. For example, delete data that does not belong to the Task KeyGroup through a Special Compaction, and ensure that the compaction can be executed at the first checkpoint.  Or make some special marks for invalid data like DeteleRange and then clean it up asynchronously.

Considering that most of our online jobs use rocksdb statebackend , we need to support these feature on rocksdb. It seems that some of other solutions will bring more changes to rocksdb. Both consider the risks and benefits,  we chose the above method;;;","11/Mar/23 05:08;yunta;[~mayuehappy] Thanks for the reply. 
Considering the efforts to maintain the patch on top of RocksDB across versions, I think it's fine for current solutions.
However, I feel it might be very difficult and slow to push these features to the RocksDB community based on my previous knowledge. But I think we can get some advice from them. 
;;;","29/Mar/23 07:28;mayuehappy;[~yunta]

Thanks for the reply, I have written the detailed design plan in this document, please take a look when you have time

[https://docs.google.com/document/d/10MNVytTsyiDLZQSR89kDkVdmK_YjbM6jh0teerfDFfI] 



For rocksdb related changes, I created an issue on rocksdb community. Unfortunately, Rocksdb Community seems to be not very active, and no one has responded to me yet.;;;","19/Jun/23 13:20;srichter;[~mayuehappy] Thanks for working on this topic! I saw that your PR against RocksDB was merged a few days ago. With your code in RocksDB, are you planning to continue with this JIRA and is there any planned release?;;;","25/Jun/23 13:56;mayuehappy;[~srichter] 

Thanks very much for the reply, I hope to continue this JIRA and it will be my honor to introduce this feature in the Flink community in the future.
After the above discussion, in order to promote this feature to Flink, in the past two months, I introduced *ClipColumnFamily* and *CreateColumnFamilyWithImport* in the rocksdb for splitting and merging multiple DBs respectively ([#11378|https://github.com/facebook/rocksdb/pull/11378]、[#11379|https://github.com/facebook/rocksdb/pull/11379]、[#11381|https://github.com/facebook/rocksdb/pull/11381]、[#11372|https://github.com/facebook/rocksdb/pull/11372]). Using these features can help Flink Speed up recovery when using rocksdb.

But after that there are still some issues worth discussing and some work needs to be done (for example, should we upgrade the version of Frocksdb to the latest version, or backport the functions related to rocksdb to frocksdb).
I drafted a Flip before ([(https://docs.google.com/document/d/10MNVytTsyiDLZQSR89kDkVdmK_YjbM6jh0teerfDFfI|https://docs.google.com/document/d/10MNVytTsyiDLZQSR89kDkVdmK_YjbM6jh0teerfDFfI] )), but it hasn’t been updated after the rocksdb code merged in.

I've been busy with my company projects recently, sorry for didn't follow up the Jira here. But I'll update this Flip as soon as possible and start a discussion on the community mailing list.;;;","07/Jul/23 12:25;pnowojski;[~mayuehappy] thanks for your work on this! Really nice and important improvement, that I'm looking forward to. In the mean time, could you publish a draft pr for the Flink side of your changes?;;;","17/Jul/23 09:34;yunta;Since the feature freeze date for flink-1.18 is July 24th, and no updated FLIP yet. I have marked this item would not make in [flink-1.18|https://cwiki.apache.org/confluence/display/FLINK/1.18+Release]. And we can still move on this feature in the next release.;;;","22/Jul/23 06:17;mayuehappy;[~pnowojski] Sorry for missing your comment. The newly added api in rocksdb may not be available to flink at present, because the related JNI method has not been supported yet, but I will support it as soon as possible. And I will draft a pr for the code changes on the Flink side next week. 
Thanks for following up;;;","22/Jul/23 06:19;mayuehappy;[~yunta]  ok , sorry for the delay:(;;;","02/Aug/23 06:34;masteryhx;[~mayuehappy] Thanks a lot for your efforts!
If the pr/FLIP is ready, I'd also like to help to take a review.;;;","08/Aug/23 13:42;mayuehappy;[~pnowojski] [~srichter] [~yunta] [~masteryhx] 
Sorry for the late update . I've submitted a draft pr （[https://github.com/apache/flink/pull/23169]）. I haven't published the frocksdb jni in this PR to the public maven repository, so this code can only run on my local machine. I have passed all the statebackend unit tests . The frocksdb branch used in the PR is [https://github.com/mayuehappy/rocksdb/tree/Frocksdb-8.4.0-ingest-db]. (Depends on rocksdb-8.4.0) .This is just a preview version, in order to facilitate everyone to see flink-related changes and further discussions.;;;","07/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix possible bug of array_distinct,FLINK-31237,13526289,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,27/Feb/23 08:40,27/Feb/23 14:10,04/Jun/24 20:41,27/Feb/23 14:10,1.18.0,,,,,,,,,,,1.18.0,,,,,,,,0,pull-request-available,,,,"as talked here [https://github.com/apache/flink/pull/19623,] we should use builtin expressions/functions. because the sql semantic is different from  java equals",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 14:10:26 UTC 2023,,,,,,,,,,"0|z1g7b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 14:10;Sergey Nuyanzin;Merged to master: [6797d6f2592373b2606ddd8c8aad316d677c1cc6|https://github.com/apache/flink/commit/6797d6f2592373b2606ddd8c8aad316d677c1cc6];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit pushdown should not open useless RecordReader,FLINK-31236,13526288,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Feb/23 08:26,27/Feb/23 10:21,04/Jun/24 20:41,27/Feb/23 10:21,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:21:12 UTC 2023,,,,,,,,,,"0|z1g7aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 10:21;lzljs3620320;master: ed454b8f909a85cdd43fac70c4280b24d0c0e34f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Jdbc Connector can not push down where condition,FLINK-31235,13526285,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,leo.zhi,leo.zhi,27/Feb/23 08:04,09/Mar/23 10:55,04/Jun/24 20:41,,1.14.0,,,,,,,,,,,,,,,,,,,0,,,,,"when we use flink 1.13/1.14/1.15, I found out that every time I query tidb(mysql) , it will load the whole table without the where condiditon.

 

Below table has 1 milion records, it takes 15 minuetes to load and return one record.

I dont know why, and it is very appreciated for the help :)

For example:

val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

env.setRuntimeMode(RuntimeExecutionMode.BATCH)

val tEnv: StreamTableEnvironment = StreamTableEnvironment.create(env)

tEnv.executeSql(
s""""""
|CREATE TABLE table(
| ID varchar(50) NOT NULL,
| CreateTime Timestamp NOT NULL
|) with (
| 'connector' = 'jdbc',
| 'url' = 'jdbc:mysql://xxxx:3306/xx?tinyInt1isBit=false&transformedBitIsBoolean=false',
| 'username' = '',
| 'password' = '',
| 'table-name' = 'Service',
| 'driver' = 'com.mysql.cj.jdbc.Driver'
|)
"""""".stripMargin)

val query: Table = tEnv.sqlQuery(""select * from table where ID = '00011'"")

query.print()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 01:03;leo.zhi;image-2023-02-28-09-03-16-225.png;https://issues.apache.org/jira/secure/attachment/13055868/image-2023-02-28-09-03-16-225.png","28/Feb/23 06:31;leo.zhi;image-2023-02-28-14-31-43-387.png;https://issues.apache.org/jira/secure/attachment/13055879/image-2023-02-28-14-31-43-387.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 10:55:30 UTC 2023,,,,,,,,,,"0|z1g7a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 11:12;libenchao;[~leo.zhi] JDBC filter pushdown has been supported in the latest version, see FLINK-16024. Since this is a new feature, we won't port it back to older versions.;;;","28/Feb/23 01:06;leo.zhi;Hi [~libenchao] ,
Thanks for the quick reply, do you mean the jdbc push down feature is effective in the version flink-connector-jdbc:1.16.0?

I am not sure what is the  fix version of jdbc-3.1.0 in the FLINK-16024. ,maybe  jdbc-3.1.0 will be released in the feature?

!image-2023-02-28-09-03-16-225.png!

 ;;;","28/Feb/23 02:06;libenchao;jdbc connector has been moved to a separate repo, see https://github.com/apache/flink-connector-jdbc.

jdbc-3.1.0 is an unreleased version now.;;;","28/Feb/23 06:33;leo.zhi;Since jdbc push down feature which will effective at the unreleased version jdbc-3.1.0, I dont know when it will be released,  for example we can push down where condition at 1.17/1.18 ?

May I now is there any way can helps us not scan the whole table before flink 1.16 ?
I tried below configs, not useful.

(CreateTime is Date Type, and lower-bound and upper-bound should be Integer, my way is right? )

!image-2023-02-28-14-31-43-387.png!;;;","09/Mar/23 09:07;martijnvisser;[~libenchao] I'm thinking that it makes sense to release JDBC 3.10 when Flink 1.17 is out, with this new feature. WDYT? ;;;","09/Mar/23 10:55;libenchao;[~martijnvisser] +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an option to redirect stdout/stderr for flink on kubernetes,FLINK-31234,13526274,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,27/Feb/23 06:51,29/Jun/23 12:19,04/Jun/24 20:41,29/Jun/23 12:12,1.17.0,,,,,,,,,,,1.18.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,,"Flink on Kubernetes does not support redirecting stdout/stderr to files. This is to allow users to get logs via ""kubectl logs"".

But for our internal scenario, we use a kubernetes user to submit all jobs to the k8s cluster and provide a platform for users to submit jobs. Users can't access kubernetes directly. so we need to display logs/stdout in flink webui.

Because the web ui retrieves the stdout file by filename, which has the same prefix as \{taskmanager}.log (such as flink--kubernetes-taskmanager-0-my-first-flink-cluster-taskmanager-1-4.log). We can't support this with a simple custom image.

IMO, we should add an option to redirect stdout/stderr to files. When this setting is enabled.
1. flink-console.sh will redirect stdout/err to files.
2. avoid logs twices in the log file and the stdout file. we could do this by using ThresholdFilter ([log4j|https://logging.apache.org/log4j/2.x/manual/filters.html#ThresholdFilter] and [logback|https://logback.qos.ch/manual/filters.html#thresholdFilter]) with a system property.

Of course, this option is false by default.",,,,,,,,,,,,,,,,,,,,FLINK-31947,,,,,FLINK-15792,FLINK-17166,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 12:15:36 UTC 2023,,,,,,,,,,"0|z1g77s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 07:47;huwh;Hi, [~wangyang0918] [~chesnay] Looking forward to your opinion ;;;","16/Jun/23 12:11;huwh;[~wangyang0918] [~gyfora]  I think it's worth adding this option. We support it in YARN and standalone resource providers. We also have a related page on the Flink web UI.;;;","20/Jun/23 08:34;wangyang0918;Given that the YARN/Standalone mode could access the stdout/stderr logs in the Flink webUI, it also certainly makes sense to have it in the K8s deployment. And I think the limitation that stdout/stderr files could not be rolled based file size is acceptable.;;;","29/Jun/23 12:11;wangyang0918;Fixed via:

master: dfe6bdad0b9493eb35b98ac8326a097a945badf2;;;","29/Jun/23 12:15;wangyang0918;Thanks [~huwh] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no error should be logged  when retrieving the task manager's stdout if it does not exist,FLINK-31233,13526271,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,huwh,huwh,huwh,27/Feb/23 06:05,14/Jun/23 23:48,04/Jun/24 20:41,14/Jun/23 23:48,1.17.0,,,,,,,,,,,1.18.0,,,,Runtime / REST,,,,0,pull-request-available,,,,"When running Flink on Kubernetes, the stdout logs is not redirected to files so it will not shown in WEB UI. This is as expected.

But It returns “500 Internal error” in REST API and produces an error log in jobmanager.log. This is confusing and misleading.

 

I think this API should return “404 Not Found” without any error logs, similar to how jobmanager/stdout works. 

 

!image-2023-02-27-13-57-27-190.png!

!image-2023-02-27-13-56-40-718.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 05:56;huwh;image-2023-02-27-13-56-40-718.png;https://issues.apache.org/jira/secure/attachment/13055839/image-2023-02-27-13-56-40-718.png","27/Feb/23 05:57;huwh;image-2023-02-27-13-57-27-190.png;https://issues.apache.org/jira/secure/attachment/13055838/image-2023-02-27-13-57-27-190.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 23:47:41 UTC 2023,,,,,,,,,,"0|z1g774:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 07:47;huwh;Hi, [~wangyang0918] [~chesnay] Looking forward to your opinion ;;;","23/Mar/23 03:31;Weijie Guo;Thanks [~huwh] for reporting this. 

> This API should return “404 Not Found” without any error logs
Does this mean that the log is not printed only for 404 errors?
;;;","23/Mar/23 03:54;huwh;[~Weijie Guo] Thanks for your attention.

Yes, the stdout file not exists is the expected behavior. So we should return the ""404 Not found"" response and not print log in jobmanager/taskmanager(or maybe print debug level), just like AbstractJobManagerFileHandler#respondToRequest did;;;","06/Apr/23 12:50;huwh;[~xtsong] Could you take a look at this;;;","08/Jun/23 06:07;huwh;After offline discussion with [~guoyangze], 

1. This change should only apply to TaskManagerStdoutFileHandler and should not affect processing of other files.
2. Users need to be informed on the web UI that the stdout file does not exist, probably because in K8S mode, they can use 'kubectl logs' to get the stdout context.

;;;","08/Jun/23 06:11;guoyangze;Thanks for reporting this. I've assigned it to you.;;;","14/Jun/23 23:47;guoyangze;master: c418b57870aaf5b4f4f9d0f69a70366b583ae166
a9e38d29811edb2bd35a97a2ff47f6519316c092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet format supports MULTISET type for Table Store,FLINK-31232,13526270,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,27/Feb/23 06:01,27/Feb/23 07:43,04/Jun/24 20:41,27/Feb/23 07:43,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Parquet format should support MULTISET type for Table Store.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 07:43:51 UTC 2023,,,,,,,,,,"0|z1g76w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 07:43;lzljs3620320;master: 0f393d4bf373081a26a8b9c09893136973cd6e21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-playgrounds has permission errors in mounted files,FLINK-31231,13526254,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhongpu314,zhongpu314,27/Feb/23 04:23,27/Feb/23 04:53,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"In flink-playgrounds [1], we mount the folders under ""/tmp` for both jobmanager and taskmanager:
{code:java}
- /tmp/flink-checkpoints-directory:/tmp/flink-checkpoints-directory
- /tmp/flink-savepoints-directory:/tmp/flink-savepoints-directory {code}
Docker mounts host volumes preserving the host UUID and GUID (in Linux, they are both 1000). However, the default user of flink image is ""flink"" who does not have the writable permission. As a result, it will raise an error.

 

There are two workaround solutions:
 * Grant the world writable for those two folders (i.e., `chmod o+wx /tmp/flink-checkpoints-directory`)
 * Specify the user in docker-compose (i.e., ""user: 1000:1000"")

But the second method may not be applied in Windows host. 

 [1] https://github.com/apache/flink-playgrounds",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 04:53:32 UTC 2023,,,,,,,,,,"0|z1g73k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 04:53;zhongpu314;It seems that this issue only occurs in Mac/Linux, because When sharing files from Windows, Docker Desktop sets permissions on shared volumes to a default value of 0777 [1].

 [1] https://docs.docker.com/desktop/troubleshoot/topics/#volumes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve YarnClusterDescriptor memory unit display,FLINK-31230,13526232,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,slfan1989,slfan1989,slfan1989,27/Feb/23 01:32,07/Apr/23 13:33,04/Jun/24 20:41,23/Mar/23 02:13,1.17.1,,,,,,,,,,,1.18.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"When we use the yarn-session.sh -q command, the basic information of the NM will be printed, and the memory unit is MB. If we have 128GB of memory, it will display 131072 MB. At the same time, if we have multiple NMs, finally totalMemeory doesn't show units.

This jira will use hadoop's StringUtil to format unit output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 02:12:46 UTC 2023,,,,,,,,,,"0|z1g6yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 02:12;fanrui;master commit: 753da61bb366df35df4bc2c2ebbbe7f75e8237d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement a Pub/Sub Lite source and sink,FLINK-31229,13526217,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dpcollins-google,dpcollins-google,26/Feb/23 18:18,26/Feb/23 18:18,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,0,,,,,"Pub/Sub Lite is a low-cost google cloud streaming system with more familiar semantics to users of OSS streaming systems. I've implemented and performance tested to GiB/s scale a connector that I would like to upstream into flink.

https://github.com/googleapis/java-pubsublite-flink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-26 18:18:01.0,,,,,,,,,,"0|z1g6vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-playgrounds is using a buggy zookeeper,FLINK-31228,13526206,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhongpu314,zhongpu314,26/Feb/23 14:04,12/May/24 17:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,Docker,pull-request-available,,"The flink-playgrounds [1] is using a buggy zookeeper image from docker hub, and thus the kafka can never connect to the zookeeper server.
----
How to reproduce the bug?

For example, after running ""docker-compose up -d"" in operations-playground [2], we can observe in Flink web UI that the job will in ""restarting"" state soon, and both ""zookeeper"" and ""kafka"" are stopped.
----
I noticed that the used ""wurstmeister/zookeeper:3.4.6"" image was published in 7 years ago, and it is badly documented. In my test, switching to ""bitnami/zookeeper:3.7.1"" can solve this problem.

Also, I think it would be better to discard images published by wurstmeister, since they are not updated frequently (and there buggy), and we should switch to bitnami.

 

 [1][https://github.com/apache/flink-playgrounds|https://github.com/apache/flink-playgrounds)]

 [2] [https://github.com/apache/flink-playgrounds/tree/master/operations-playground)]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:08 UTC 2023,,,,,,,,,,"0|z1g6t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove 'scala version' from file sink modules,FLINK-31227,13526201,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,djkooks,djkooks,26/Feb/23 12:26,14/Mar/23 10:20,04/Jun/24 20:41,13/Mar/23 08:12,,,,,,,,,,,,1.16.2,1.17.0,,,Documentation,,,,0,pull-request-available,,,,"Hello,
In case of using FileSink for parquet/orc, for version 1.16+, seems 'withScalaVersion' should not be attached to artifactId.

[https://mvnrepository.com/artifact/org.apache.flink/flink-orc/1.16.0]
[https://mvnrepository.com/artifact/org.apache.flink/flink-parquet/1.16.0]

I think in [https://github.com/apache/flink/blob/master/docs/content/docs/connectors/datastream/filesystem.md], 'withScalaVersion' should be removed.
{noformat}
...
{{< artifact flink-parquet withScalaVersion >}}
...
{{< artifact flink-orc withScalaVersion >}}
...
{noformat}

How do you think?
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31442,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 10:20:08 UTC 2023,,,,,,,,,,"0|z1g6s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 08:12;martijnvisser;Fixed in 

master: a8848f8d6aa700e0325e79a8f7b44f611d287cb3
release-1.17: a8d7ec97ed02799882da2ca0416a811efef7f7b2
release-1.16: f200775b88b055c804365a8dc6ffb3ef4a268bf9
release-1.15: cb5dbf5b9070dc1bf4e7465458cd9db5e50ea996;;;","14/Mar/23 10:20;martijnvisser;Reverted ""FLINK-31227[docs] Remove Scala suffix for ORC and Parquet format download links on the the FileSystem documentation. This closes #22039""

78bae43288ad64511c298a067d66fa37667771d3


Decided not to backport this to release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the FRocksDB version in table-store's lookup module,FLINK-31226,13526198,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,26/Feb/23 09:42,28/Feb/23 11:15,04/Jun/24 20:41,28/Feb/23 11:15,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Current FRocksDB version in flink-table-store is still 6.20.3-ververica-1.0, which cannot run with Apple's silicon chips. 
Newly released 6.20.3-ververica-2.0 FRocksDB could run well with Apple's silicon chips and is compatible with the previous version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 11:15:38 UTC 2023,,,,,,,,,,"0|z1g6rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 11:15;lzljs3620320;master: 3f56f3b8cf5bd4075be84690bb8e8c677506e8ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rocksdb max open file can lead to oom ,FLINK-31225,13526192,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,26/Feb/23 04:04,27/Feb/23 10:54,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,"the default value for 

state.backend.rocksdb.files.open

is -1

 

[https://github.com/facebook/rocksdb/issues/4112] this issue told us the rocksdb will not close fd , so this can lead to oom issue.

 

also I can reproduce the situation in my enviroment. left part(2/21- 2/24) is leave max open file to -1, right part(2/24 till now) is set the max open file to 300. the memory grow is very differnt.

!image-2023-02-26-12-08-49-717.png|width=616,height=285!

 

I have also attached a jeprof for 2/21-2/24 instance, the tm memory size is about 8GB, heap memory is about 2.6GB, the native part in leak_test is about 1GB. I think the remaining part (8-2.6-1)is occupied by fd .

 

I suggest set this to a default value like 500.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/23 04:08;zhoujira86;image-2023-02-26-12-08-49-717.png;https://issues.apache.org/jira/secure/attachment/13055826/image-2023-02-26-12-08-49-717.png","26/Feb/23 04:11;zhoujira86;leak_test(2).png;https://issues.apache.org/jira/secure/attachment/13055827/leak_test%282%29.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:53:37 UTC 2023,,,,,,,,,,"0|z1g6q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 10:16;yunta;Before moving to the discussion on results, I wonder why you could have more than 300 SST files within one RocksDB instance. Does the instance have more than 18GB disk space usage?;;;","27/Feb/23 10:53;zhoujira86;[~yunta] Master

I logged onto the container,  do a list and find out:

/tmp/flink-io-b7483fcd-50aa-45d2-bd8e-a2b0862c6323/job_00000000000000000000000000000000_op_LegacyKeyedCoProcessOperator_a3bf5557de0062839a60e12819947e17_{_}12_50{_}_uuid_b91b9766-b8af-4de3-9a01-b5dcce29a7bf/db# ll | grep sst
{-}rw-r{-}{-}r{-}- 1 flink flink 30662931 Feb 27 18:20 002512.sst
{-}rw-r{-}{-}r{-}- 1 flink flink  1788364 Feb 27 18:30 002513.sst
{-}rw-r{-}{-}r{-}- 1 flink flink  3209306 Feb 27 18:40 002515.sst
{-}rw-r{-}{-}r{-}- 1 flink flink 13443570 Feb 27 18:40 002517.sst
{-}rw-r{-}{-}r{-}- 1 flink flink  1694438 Feb 27 18:50 002518.sst
{-}rw-r{-}{-}r{-}- 1 flink flink  1509487 Feb 27 18:50 002519.sst

 

I think the LSM tree merge process will delete L0 files.

 

And as mentioned above [https://github.com/facebook/rocksdb/issues/4112] . some one mentioned this. 

 

this is not exactly a leak
but a lot of memory is allocated
but not released
const int table_cache_size = (mutable_db_options_.max_open_files == -1)
? TableCache::kInfiniteCapacity
: mutable_db_options_.max_open_files - 10;
table_cache_ = NewLRUCache(table_cache_size,
immutable_db_options_.table_cache_numshardbits);
all allocated records are stored in this cache
mutable_db_options_.max_open_files is equal 1
so table_cache_size= 4 mb

 

seems this mutable_db_options_.max_open_files  = -1 configuration will save TableReader(even if the sst file has already been deleted?) in memory, which will cause memory keep growing problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics for flink table store,FLINK-31224,13526167,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhangjun,zhangjun,25/Feb/23 14:07,29/Mar/23 02:05,04/Jun/24 20:41,29/Mar/23 02:05,table-store-0.3.1,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"We can divide the metrics of the flink table store into three categories: SourceMetrics, WriterMetrics, and CompactionMetrics

Some metrics  are as follows :
{code:java}
public class WriterMetrics {

    private final Counter deltaManifestCount;
    private final Counter deltaTotalFileSize;
    private final Counter deltaTotalRowCount;
    private final Counter changelogManifestCount;
    private final Counter changelogTotalFileSize;
    private final Counter changelogTotalRowCount;
} 

public class SourceMetrics {

    private final Counter assignedSplits;
    private final Counter finishedSplits;
    private final Counter splitReaderFetchCalls;
}

public class CompactionMetrics {

    private final Counter beforeCompactionFileCount;
    private final Counter beforeCompactionDeltaTotalFileSize;
    private final Counter beforeCompactionDeltaTotalRowCount;
    private final Counter afterCompactionFileCount;
    private final Counter afterCompactionDeltaTotalFileSize;
    private final Counter afterCompactionDeltaTotalRowCount;
    private final Counter compactChangelogFileCount;
    private final Counter compactChangelogTotalFileSize;
    private final Counter compactChangelogTotalRowCount;
}

{code}
 

My idea is that we may not know which metrics users really need at first. We can add some core metrics first, and then we can collect users' needs and add some user metrics later.

 

[~lzljs3620320] [~nicholasjiang] do you have any advice? thanks

 ",,,,,,,,,,,,,,,,,,,,,FLINK-31364,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 10:14:04 UTC 2023,,,,,,,,,,"0|z1g6kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/23 15:31;nicholasjiang;[~zhangjun], could you firstly add the design of the metric group and corresponding metric of writer into the description? After the design aggrement of the design, you could push a pull request.;;;","06/Mar/23 09:34;lzljs3620320;Hi [~zhangjun] , thanks for your contribution.

My main concern is, do we really need these metrics, like file and rowCount, some of them are already in snapshot meta. 

I prefer to wait until the user needs are clear before providing these metrics. What do you think?;;;","06/Mar/23 10:14;zhangjun;[~lzljs3620320] ok，I agree with you，let us add the metrics later.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql-client.sh fails to start with ssl enabled,FLINK-31223,13526150,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,macdoor615,macdoor615,25/Feb/23 07:13,28/May/24 08:50,04/Jun/24 20:41,25/Apr/24 16:08,1.17.0,,,,,,,,,,,1.19.1,1.20.0,,,Table SQL / Client,,,,0,pull-request-available,stale-assigned,,,"*Version:* 1.17-SNAPSHOT *Commit:* c66ef25 

1. ssl disabled 

sql-client.sh works properly

2. ssl enabled

web ui can access with [https://url|https://url/]

The task can be submitted correctly through sql-gateway. I can confirm that sql-gateway exposes the http protocol, not https.

But sql-client.sh fails to start with the following exceptions. It seems that sql-client.sh expect https protocol

 
{code:java}
2023-02-25 14:43:19,317 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-02-25 14:43:19,343 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Starting rest endpoint.
2023-02-25 14:43:19,713 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Rest endpoint listening at localhost:44922
2023-02-25 14:43:19,715 INFO  org.apache.flink.table.client.SqlClient                      [] - Start embedded gateway on port 44922
2023-02-25 14:43:20,040 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Shutting down rest endpoint.
2023-02-25 14:43:20,088 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Shut down complete.
2023-02-25 14:43:20,089 ERROR org.apache.flink.table.client.SqlClient                      [] - SQL Client must stop.
org.apache.flink.table.client.SqlClientException: Failed to create the executor.
        at org.apache.flink.table.client.gateway.ExecutorImpl.<init>(ExecutorImpl.java:170) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.gateway.ExecutorImpl.<init>(ExecutorImpl.java:113) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.gateway.Executor.create(Executor.java:34) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:110) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228) [flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179) [flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Failed to get response.
        at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:427) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:416) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.gateway.ExecutorImpl.negotiateVersion(ExecutorImpl.java:447) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.table.client.gateway.ExecutorImpl.<init>(ExecutorImpl.java:132) ~[flink-sql-client-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        ... 5 more
Caused by: org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: org.apache.flink.shaded.netty4.io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 485454502f312e3120343034204e6f7420466f756e640d0a636f6e74656e742d747970653a206170706c69636174696f6e2f6a736f6e3b20636861727365743d5554462d380d0a6163636573732d636f6e74726f6c2d616c6c6f772d6f726967696e3a202a0d0a636f6e74656e742d6c656e6774683a2033380d0a0d0a7b226572726f7273223a5b224e6f7420666f756e643a202f6261642d72657175657374225d7d
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:489) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:280) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]
Caused by: org.apache.flink.shaded.netty4.io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 485454502f312e3120343034204e6f7420466f756e640d0a636f6e74656e742d747970653a206170706c69636174696f6e2f6a736f6e3b20636861727365743d5554462d380d0a6163636573732d636f6e74726f6c2d616c6c6f772d6f726967696e3a202a0d0a636f6e74656e742d6c656e6774683a2033380d0a0d0a7b226572726f7273223a5b224e6f7420666f756e643a202f6261642d72657175657374225d7d
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1215) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1285) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:519) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:458) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:280) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 11:48:59 UTC 2024,,,,,,,,,,"0|z1g6go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/23 16:55;Weijie Guo;Thanks [~macdoor615] for reporting this. When we create the {{RestClient}}, we load all the configuration options, but when we create the rest server, we only load the {{EndpointOptions}}. As a result, the client will enable SSL, but the server does not. I will fix this asap.;;;","27/Feb/23 02:06;fsk119;Hello. In the embedded mode, we always assume the started gateway is only visible to the current client. I think it's not a big problem to enable SSL or not. ;;;","27/Feb/23 06:09;macdoor615;[~fsk119] you are right. it is not a big problem in embedded mode without ssl. but i need to enable ssl to communicate with remote caller. when ssl enabled, sql-client can not work;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","24/Apr/24 12:49;davidradl;[~Weijie Guo] we urgently need this fixed. What is you current outlook on this? Many thanks. We are looking at testing your pr locally to confirm it resolves the problem for us.   ;;;","25/Apr/24 16:07;Weijie Guo;master(1.20) via 259aef0b66b3e55013f02ac31d4cff61202b78c5.
release-1.19 via a5efed2c06c42186e376dd2ede18dc09986c0387.;;;","29/Apr/24 11:48;davidradl;I have raised backport prs:

#24741 for 1.18

[#24742|https://github.com/apache/flink/pull/24742] for 1.19 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove usage of deprecated ConverterUtils.toApplicationId,FLINK-31222,13526145,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,25/Feb/23 04:19,02/Mar/23 08:51,04/Jun/24 20:41,02/Mar/23 06:14,1.17.1,,,,,,,,,,,1.18.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"When reading the code, I found that we use ConverterUtils.toApplicationId to convert applicationId, this method is deprecated, we should use ApplicationId.fromString",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:14:36 UTC 2023,,,,,,,,,,"0|z1g6fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 06:14;fanrui;Thanks for the fix:)

 

apache:master: 95dd5423aad847ec41b81676deaf4cb94d9e11d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Typo in YarnConfigOptions,FLINK-31221,13526124,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,24/Feb/23 21:33,28/Feb/23 08:28,04/Jun/24 20:41,28/Feb/23 08:28,1.17.1,,,,,,,,,,,1.18.0,,,,Deployment / YARN,,,,0,pull-request-available,,,,"I found a typo in YarnConfigOptions, I will fix it.

willl -> will
{code:java}
public static final ConfigOption<String> LOCALIZED_KEYTAB_PATH =
     .....
                        ""Local (on NodeManager) path where kerberos keytab file will be""
                                + "" localized to. If ""
                                + SHIP_LOCAL_KEYTAB.key()
                                + "" set to ""
                                + ""true, Flink willl ship the keytab file as a YARN local ""
                                + ""resource. In this case, the path is relative to the local ""
                                + ""resource directory. If set to false, Flink""
                                + "" will try to directly locate the keytab from the path itself.""); 


{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 08:28:11 UTC 2023,,,,,,,,,,"0|z1g6aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 08:28;fanrui;Hi [~slfan1989] , thanks for your fix:)

apache:master commit : 150f2765c967aad7c15d2ea6517f03a5eea4e8db ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace Pod with PodTemplateSpec for the pod template properties,FLINK-31220,13526092,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,24/Feb/23 17:09,07/Feb/24 15:39,04/Jun/24 20:41,06/Feb/24 20:49,,,,,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,1,pull-request-available,,,,"The current podtemplate fields in the CR use the Pod object for schema. This doesn't make sense as status and other fields should never be specified and they take no effect.

We should replace this with PodTemplateSpec and make sure that this is not a breaking change even if users incorrectly specified status before.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 20:49:51 UTC 2024,,,,,,,,,,"0|z1g63s:",9223372036854775807,"Deprecation warning:

We deprecate Pod fields that are not part of the PodTemplateSpec:

podTemplate.apiVersion
podTemplate.kind
podTemplate.status

These fields should be removed from the FlinkDeployment CR podTemplate fields as they will become unsupported from version 1.9.0.",,,,,,,,,,,,,,,,,,,"29/Mar/23 02:11;xxxinli1;Hi [~gyfora], this heard like importing the pod spec in the controller code and needs to keep all optional fields for a while (at least serval versions before announcing the shift). What's the opinion on when to remove some fields like status mentioned?;;;","29/Mar/23 05:26;gyfora;Seems to me that this will be a backward compatible change. Kubernetes (and the Flink Operator) should simply ignore the removed fields (like status, apiVersion etc.). ;;;","29/Mar/23 05:27;gyfora;The key here is the additionalProperties field that captures all unknown fields. [https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-model-generator/kubernetes-model-core/src/generated/java/io/fabric8/kubernetes/api/model/PodTemplateSpec.java#L42-L43]

This would capture anything that was in Pod but is not in the PodTemplateSpec anymore.;;;","29/Mar/23 07:43;xxxinli1;Will add another Spec field or CR kind like `Ingress`/`IngressRoute` be better without carry on all backward issues?;;;","29/Mar/23 08:23;gyfora;Sorry I don't understand the question and how it is related to this Jira;;;","31/Jan/24 10:22;gyfora;Seems like this is indeed a backward incompatible change that may be tricky to apply. While it's not a problem for the already running jobs, but users won't be able to submit the same custom resources without modifications (removing the unsupported/not required fields);;;","06/Feb/24 20:49;gyfora;merged to main 24643c8c6d9d734732ed2cb7e3112c4452675f40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for canary resources,FLINK-31219,13526085,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,24/Feb/23 16:09,25/Mar/23 22:20,04/Jun/24 20:41,25/Mar/23 22:20,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,While the current health probe mechanism is able to detect different types of errors like startup / informer issues it can be generally beneficial to allow a simply canary mechanism that can verify that the operator recieives updates and reconciles resources in a timely manner.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 25 22:20:42 UTC 2023,,,,,,,,,,"0|z1g628:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/23 22:20;gyfora;merged to main 0eb04d66537618dd5d2f03cbdbfcbadf6d1525ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve health probe to recognize informers already failed at start,FLINK-31218,13526084,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,24/Feb/23 16:07,29/Mar/23 15:24,04/Jun/24 20:41,29/Mar/23 15:24,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,The current healthprobe will report unhealthy if any informers cannot start. This is in contradiction with the setting that can allow the operator to start while some informers are unhealthy (and keep trying to start them),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 15:24:46 UTC 2023,,,,,,,,,,"0|z1g620:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 15:24;gyfora;merged to main 3899050ae09ee0590d9810a858703d8d6a9ca602;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update netty to current,FLINK-31217,13526079,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,satanicmechanic,satanicmechanic,24/Feb/23 15:05,27/Feb/23 07:59,04/Jun/24 20:41,27/Feb/23 07:59,,,,,,,,,,,,,,,,,,,,0,security,,,,Netty 3.10.6 has several vulnerabilities and needs updating to the current release.,,,,,,,,,,,,,,,,,,,,,FLINK-29065,FLINK-24736,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-24 15:05:52.0,,,,,,,,,,"0|z1g60w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update kryo to current,FLINK-31216,13526078,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,satanicmechanic,satanicmechanic,24/Feb/23 15:04,27/Feb/23 07:56,04/Jun/24 20:41,27/Feb/23 07:56,,,,,,,,,,,,,,,,,,,,0,security,,,,"kryo 2.24 is several years out of date and has a [deserialization vulnerability|https://github.com/EsotericSoftware/kryo/issues/942] associated with it.  Please update to current.",,,,,,,,,,,,,,,,,,,,,FLINK-3154,FLINK-24736,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-24 15:04:43.0,,,,,,,,,,"0|z1g60o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backpropagate processing rate limits from non-scalable bottlenecks to upstream operators,FLINK-31215,13526077,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gyfora,gyfora,gyfora,24/Feb/23 15:01,27/Feb/23 10:28,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,,"The current algorithm scales operators based on input data rates by propagating it forward through the graph.

However there are cases where a certain operators processing capacity is limited either because it has a set maxParallelism or the users excludes it from scaling (or otherwise the capacity doesnt increase with scaling).

In these cases it doesn't make sense to scale upstream operators to the target data rate if the job is going to be bottlenecked by a downstream operator. But instead we should backpropagate the limit based on the non-scalable bottleneck.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:28:28 UTC 2023,,,,,,,,,,"0|z1g60g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 09:56;gaurav726;[~gyfora]  

can you assigned it to me,  i want to work on it;;;","27/Feb/23 10:01;gyfora;[~gaurav726] I have already started working on this;;;","27/Feb/23 10:28;gaurav726;cool, earlier it was unassigned, that's why asked you, as me and Michael had a discussion about it as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for new command line option -py.pythonpath,FLINK-31214,13526074,13509222,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,samrat007,samrat007,24/Feb/23 14:54,05/Apr/23 15:54,04/Jun/24 20:41,05/Apr/23 15:54,,,,,,,,,,,,1.18.0,,,,API / Python,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 05 15:54:15 UTC 2023,,,,,,,,,,"0|z1g5zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 06:21;samrat007;I would like to work in this issue. 

 

can you please assign this task to me [~hxbks2ks] . 

 ;;;","05/Apr/23 15:54;samrat007;master :- f8b0834fb83b7d17ac9eeaf09f2f6cea070c978a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregation merge engine supports retract inputs,FLINK-31213,13526068,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Feb/23 13:36,01/Mar/23 02:39,04/Jun/24 20:41,27/Feb/23 11:25,,,,,,,,,,,,table-store-0.3.1,table-store-0.4.0,,,Table Store,,,,0,pull-request-available,,,,"For sum, it can support retracts.
For others, which do not support retraction (`UPDATE_BEFORE` and `DELETE`). If the user allow some functions to ignore retraction messages, the user can configure: `'fields.${field_name}.ignore-retract'='true'`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 11:25:07 UTC 2023,,,,,,,,,,"0|z1g5yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 11:25;lzljs3620320;master: a441f56e49ffb2fc70cef60f43bbf2c45ad83e60

release-0.3: 31ee641803d7bc4e0b3fcd47167723083ebc34cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data lost on interval left join with window group,FLINK-31212,13526047,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zicat,zicat,24/Feb/23 10:03,21/Mar/23 08:46,04/Jun/24 20:41,21/Mar/23 08:45,1.16.1,,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,," 

I have a case in [^test.sql] that records in table_1 left join fail will be discard by group window.

I check the interval join operator implements. If one record in left table join right table fail, the record will not be emitted realtime but emitted waiting for additional half join bound time. In the test.sql, table_1 left join table_2 in 5 minute bound,  and the output will delay 2.5 minute this will cause window discard the records.
h2. testing
h4. input:

!image-2023-02-24-17-58-44-461.png!

{""n"":""n1"",""ts"":""2023-02-24 14:00:00""}

{""n"":""n2"",""ts"":""2023-02-24 14:00:00""}

{""n"":""n1"",""ts"":""2023-02-24 14:06:01""}

!image-2023-02-24-17-58-57-238.png!

{""n"":""n1"",""ts"":""2023-02-24 14:00:00"",""v"":111}

{""n"":""n1"",""ts"":""2023-02-24 14:06:01"",""v"":111}
h4. output:

expect:

!image-2023-02-24-18-00-52-891.png!

real:

!image-2023-02-24-17-59-25-179.png!

I remove this logic in [https://github.com/apache/flink/pull/22014]  Please help to review this PR.",,,,,,,,,,,,,,,,,,,,,,FLINK-18996,,,,,,,,,,,,,,"24/Feb/23 09:58;zicat;image-2023-02-24-17-58-44-461.png;https://issues.apache.org/jira/secure/attachment/13055805/image-2023-02-24-17-58-44-461.png","24/Feb/23 09:58;zicat;image-2023-02-24-17-58-57-238.png;https://issues.apache.org/jira/secure/attachment/13055804/image-2023-02-24-17-58-57-238.png","24/Feb/23 09:59;zicat;image-2023-02-24-17-59-25-179.png;https://issues.apache.org/jira/secure/attachment/13055803/image-2023-02-24-17-59-25-179.png","24/Feb/23 10:00;zicat;image-2023-02-24-18-00-52-891.png;https://issues.apache.org/jira/secure/attachment/13055802/image-2023-02-24-18-00-52-891.png","24/Feb/23 09:48;zicat;test.sql;https://issues.apache.org/jira/secure/attachment/13055806/test.sql",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 08:46:21 UTC 2023,,,,,,,,,,"0|z1g5ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 10:32;martijnvisser;[~zicat] You've selected affected version 1.8.4, which is old and not supported. Can you please first verify this with Flink 1.16 before opening a PR?;;;","24/Feb/23 10:43;zicat;[~martijnvisser]  I select the wrong version and I have changed to 1.16,  I verify this case effect 1.16 ;;;","24/Feb/23 10:45;martijnvisser;Ah good, thanks. 

[~godfreyhe] [~lincoln.86xy] WDYT?;;;","21/Mar/23 08:45;lincoln.86xy;This is a duplicate issue of FLINK-18996;;;","21/Mar/23 08:46;lincoln.86xy;let's move further discussions to FLINK-18996;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink chk files not delete automic after 1.13,FLINK-31211,13526021,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,leo.zhi,leo.zhi,24/Feb/23 08:22,07/Mar/23 01:09,04/Jun/24 20:41,06/Mar/23 08:47,1.14.0,1.15.0,1.16.0,,,,,,,,,,,,,,,,,0,,,,,"Checkpoint chk files can be deleted automic when the version of flink 1.13, but we upgert to 1.14/1.15/1.16 , it failed, every chk file retained.
chk-1
chk-2

chk-3

...

By the way, we use flink on k8s.",Flink 1.14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/23 08:22;leo.zhi;11.png;https://issues.apache.org/jira/secure/attachment/13055793/11.png","28/Feb/23 07:07;leo.zhi;image-2023-02-28-15-07-38-155.png;https://issues.apache.org/jira/secure/attachment/13055882/image-2023-02-28-15-07-38-155.png","28/Feb/23 07:07;leo.zhi;image-2023-02-28-15-07-51-406.png;https://issues.apache.org/jira/secure/attachment/13055881/image-2023-02-28-15-07-51-406.png","03/Mar/23 05:48;leo.zhi;image-2023-03-03-13-48-30-510.png;https://issues.apache.org/jira/secure/attachment/13055988/image-2023-03-03-13-48-30-510.png","03/Mar/23 05:49;leo.zhi;image-2023-03-03-13-49-07-262.png;https://issues.apache.org/jira/secure/attachment/13055989/image-2023-03-03-13-49-07-262.png","03/Mar/23 05:50;leo.zhi;image-2023-03-03-13-50-32-795.png;https://issues.apache.org/jira/secure/attachment/13055990/image-2023-03-03-13-50-32-795.png","07/Mar/23 01:09;leo.zhi;image-2023-03-07-09-09-28-083.png;https://issues.apache.org/jira/secure/attachment/13056081/image-2023-03-07-09-09-28-083.png",,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 01:09:29 UTC 2023,,,,,,,,,,"0|z1g5o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 09:17;martijnvisser;Please see https://flink.apache.org/2022/05/06/improvements-to-flink-operations-snapshots-ownership-and-savepoint-formats/ for more details on what has changed on this;;;","27/Feb/23 07:31;leo.zhi;Thanks Martijn, it helps me a lot.;;;","28/Feb/23 07:06;leo.zhi;Hi [~martijnvisser] ,

Sorry for the trouble, I really can not figure out how to use this config:

-restoreMode claim

I only wish flink can delete the chk files automatically when I start/restart a job like flink 1.13.

And there is no need to restore from the savepoint, so no need to add the -s savepointPath configuration, I think.

Now I am use flink on K8s, I use the claim configuration and flink can not start the job and error occurs like below.

Can you help me? Many thanks.

!image-2023-02-28-15-07-38-155.png!!image-2023-02-28-15-07-51-406.png!;;;","28/Feb/23 07:56;leo.zhi;I checked the flink source code of  version 1.14 and 1.15, only 1.15 has CLAIM feature, maybe 

-restoreMode only effective above 1.15?

So flink 1.14 chk files can not delete the chk files automatically? 

There is only one way that I need upgrate to 1.15.

Am I right?;;;","28/Feb/23 09:01;martijnvisser;[~leo.zhi] Yes, you need to upgrade;;;","01/Mar/23 06:34;leo.zhi;Thanks, I am trying to upgrade to 1.16;;;","03/Mar/23 05:51;leo.zhi;[~martijnvisser]  Hi Martijn, I upgrade to 1.16 now ,and use the claim confirguration, but it is not useful..

The checkpoint files are still retained.

I have no idea about it ,can you do me a favor?

Thanks

!image-2023-03-03-13-50-32-795.png!

!image-2023-03-03-13-49-07-262.png!;;;","06/Mar/23 00:53;leo.zhi;unfortunately, there is no solution :(;;;","06/Mar/23 08:47;martijnvisser;[~leo.zhi] As mentioned in the blog post

| Flink keeps around a configured number of checkpoints.

So I do think that this option does provide a solution. Checkpoints under CLAIM are under Flinks control, so Flink nows when the delete or not delete a checkpoint. ;;;","07/Mar/23 01:09;leo.zhi;Sorry，I dont think this option can be useful, there are thousands of chk files now in flink 1.16 , if CLAIM is the right way, maybe our docker image is not correct that makes this CLIAM not effective.

!image-2023-03-07-09-09-28-083.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Azure Pipelines report warning on ""no space left on device"" in Restore Docker images step",FLINK-31210,13526018,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,24/Feb/23 08:09,20/Mar/23 08:15,04/Jun/24 20:41,20/Mar/23 08:15,1.16.1,,,,,,,,,,,,,,,Test Infrastructure,,,,0,test-stability,,,,"We're experiencing ""no space left on device"" issues lately when restoring the Docker images. This doesn't make the builds fail, though. It's just reported as warning
{code}
Loaded image: docker.elastic.co/elasticsearch/elasticsearch:6.8.20
ApplyLayer exit status 1 stdout:  stderr: write /usr/share/elasticsearch/modules/ingest-geoip/GeoLite2-City.mmdb: no space left on device
##[error]Bash exited with code '1'.
{code}

All of this happens on Azure machines (not the Alibaba VMs). Therefore, I'm not sure whether there's much to do on our side.

* [20230224.3|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46491&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=728e59c6-8078-53a8-7bbe-bb7b0b1f2c63]
* [20230223.18|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46486&view=results]
* [20230223.6|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46441&view=results]
* [20230223.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46434&view=results]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 08:15:20 UTC 2023,,,,,,,,,,"0|z1g5nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 10:34;mapohl;* [20230225.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46534&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13415]
 * [20230226.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46543&view=results]
 * [20230227.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46559&view=results]
 * [20230227.9|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46571&view=results]
* [20230228.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46610&view=results]
* [20230227.15|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46599&view=results]
* [20230228.9|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46633&view=results];;;","28/Feb/23 16:31;mapohl;Ok, the overall pattern suggests that it's only affecting e2e_2 stages. Additionally, it only seems to appear on the {{release-1.16}} branch.

It started appearing with {{5770182e}} quite consistently with [20230223.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46434&view=results] on Thu at 1:15 AM.

It's not happening on newer branches because we removed elasticsearch in 1.17. I would have assumed it to happen on {{release-1.15}}, though, because we had [20230224.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46490&view=results] and [20230223.17|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46477&view=results] being triggered after Thu 1:15am. 

1.15 and 1.16 use the same Docker image versions.;;;","01/Mar/23 08:34;mapohl;* [20230301.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46642&view=results]
* [20230302.3|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46686&view=results]
* [20230302.24|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46753&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=841082b6-1a93-5908-4d37-a071f4387a5f]
* [20230303.03|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46766&view=results]
* [20230304.03|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46811&view=results]
* [20230305.03|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46811&view=results]
* [20230306.03|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46819&view=results]
* [20230306.21|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46869&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10381]
* [20230307.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46882&view=results]
* ...
* [20230310.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47009&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8459];;;","20/Mar/23 08:15;mapohl;I'm closing this issue because it seems to have been resolved upstream. We haven't encoutered this issue on the 1.16 branch lately.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce creation time to files table,FLINK-31209,13526017,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Feb/23 08:08,27/Feb/23 10:10,04/Jun/24 20:41,27/Feb/23 10:10,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:10:21 UTC 2023,,,,,,,,,,"0|z1g5n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 10:10;lzljs3620320;master: aeede4accec105892600271f42de161692bad984;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceReader overrides meaninglessly a method(pauseOrResumeSplits),FLINK-31208,13526007,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,loserwang1024,loserwang1024,loserwang1024,24/Feb/23 06:37,09/Aug/23 04:27,04/Jun/24 20:41,14/Mar/23 09:14,kafka-3.0.0,,,,,,,,,,,kafka-3.0.1,,,,Connectors / Kafka,,,,0,pull-request-available,starter,,,"KafkaSourceReader overrides meaninglessly a method(pauseOrResumeSplits) ，which is no difference with its Parent class (SourceReaderBase). why not remove this override method?

 

Relative code is here, which we can see is no difference?
{code:java}
//org.apache.flink.connector.kafka.source.reader.KafkaSourceReader#pauseOrResumeSplits
@Override
public void pauseOrResumeSplits(
        Collection<String> splitsToPause, Collection<String> splitsToResume) {
    splitFetcherManager.pauseOrResumeSplits(splitsToPause, splitsToResume);
} 

//org.apache.flink.connector.base.source.reader.SourceReaderBase#pauseOrResumeSplits
@Override
public void pauseOrResumeSplits(
        Collection<String> splitsToPause, Collection<String> splitsToResume) {
    splitFetcherManager.pauseOrResumeSplits(splitsToPause, splitsToResume);
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 04:27:53 UTC 2023,,,,,,,,,,"0|z1g5kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 13:31;Wencong Liu;It looks like some redundant code. cc [~renqs] WDYT?;;;","01/Mar/23 09:21;renqs;Thanks for reporting this [~loserwang1024] ! Yeah this overriding is indeed redundant. Would you like to take over the issue? 

Also I marked this issue as an starter. It should be very easy to fix;;;","01/Mar/23 12:01;loserwang1024;Of course, I’d like to take over this issue . [~renqs] ;;;","14/Mar/23 09:16;renqs;Fixed on master: 6149d156a23244bea3ae50a678b35159ba53dcfb;;;","16/Mar/23 08:21;martijnvisser;We should move this commit to the flink-connector-kafka repo, since we'll remove the Kafka connector and won't be released with 1.18 anymore. ;;;","05/May/23 07:42;loserwang1024;I'd like to re-commit this to  flink-connector-kafka repo.;;;","09/Aug/23 04:27;renqs;Merged to apache/flink-connector-kafka:main

d6525c1481fc2d2821f361d2d5ce48f97221dd74;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports high order function like other engine,FLINK-31207,13526000,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,24/Feb/23 05:23,05/Feb/24 19:45,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"spark [https://spark.apache.org/docs/latest/api/sql/index.html#transform]

transform/transform_keys/transform_values 

after calcite  https://issues.apache.org/jira/browse/CALCITE-3679s upports high order functions, we should supports many high order funcsions like spark/presto",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-3679,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 19:45:15 UTC 2024,,,,,,,,,,"0|z1g5jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 16:25;twalthr;[~jackylau] I would propose to make this a top-level issue. It goes beyond adding a built-in function but adds a new category of functions.;;;","05/Feb/24 19:45;martijnvisser;And I also think this requires a FLIP;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links on flink.apache.org,FLINK-31206,13525993,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,martijnvisser,tison,tison,24/Feb/23 04:02,24/Feb/23 11:09,04/Jun/24 20:41,24/Feb/23 09:29,,,,,,,,,,,,,,,,Project Website,,,,0,,,,,"Previously page link https://flink.apache.org/contribute/code-style-and-quality/preamble/ is broken, new link is https://flink.apache.org/how-to-contribute/code-style-and-quality-preamble/.

Shall we set up a redirection or just let those broken links wait for maintainers fixing?

cc [~martijnvisser]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 11:09:03 UTC 2023,,,,,,,,,,"0|z1g5hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 09:29;martijnvisser;[~tison] Thanks for flagging it! https://flink.apache.org/contribute/code-style-and-quality/preamble/ was an incorrect redirect, now https://flink.apache.org/contributing/code-style-and-quality-preamble.html correctly redirects again to https://flink.apache.org/how-to-contribute/code-style-and-quality-preamble/

Fixed in asf-site: 9dc7f0cc1954ff4f845a34f9aeaa2723b345ba74;;;","24/Feb/23 11:09;tison;[~martijnvisser] Thanks for your follow-ups! See https://github.com/apache/flink-web/commit/9dc7f0cc1954ff4f845a34f9aeaa2723b345ba74#r101925551.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
do optimize for multi sink in a single relNode tree ,FLINK-31205,13525976,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,24/Feb/23 02:04,24/Feb/24 03:20,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Flink supports multi sink usage, but it optimize the each sink in a individual RelNode tree, this will miss some opportunity to do some cross tree optimization, eg: 


{code:java}
create table newX(
  a int,
  b bigint,
  c varchar,
  d varchar,
  e varchar
) with (
  'connector' = 'values'
  ,'enable-projection-push-down' = 'true'


insert into sink_table select a, b from newX
insert into sink_table select a, 1 from newX
{code}

It will produce the plan as below, this will cause the source be consumed twice


{code:java}
Sink(table=[default_catalog.default_database.sink_table], fields=[a, b])
+- TableSourceScan(table=[[default_catalog, default_database, newX, project=[a, b], metadata=[]]], fields=[a, b])

Sink(table=[default_catalog.default_database.sink_table], fields=[a, b])
+- Calc(select=[a, 1 AS b])
   +- TableSourceScan(table=[[default_catalog, default_database, newX, project=[a], metadata=[]]], fields=[a])

{code}

In this ticket, I propose to do a global optimization for the multi sink by 
* Megre the multi sink(with same table) into a single relNode tree with an extra union node
* After optimization, split the merged union back to the original multi sink

In my poc, after step 1, it will produce the plan as below, I think it will do good for the global performacne


{code:java}
Sink(table=[default_catalog.default_database.sink_table], fields=[a, b])
+- Union(all=[true], union=[a, b])
   :- TableSourceScan(table=[[default_catalog, default_database, newX, project=[a, b], metadata=[]]], fields=[a, b])(reuse_id=[1])
   +- Calc(select=[a AS $f0, CAST(1 AS BIGINT) AS $f1])
      +- Reused(reference_id=[1])
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 24 03:20:00 UTC 2024,,,,,,,,,,"0|z1g5e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 07:25;aitozi;After some research, I found that there are better choices than using a union to get a single tree. {{Union}} can only cover the use case of multi-sink to the same table because the {{Union}} enforces the type consistency.

We can add a new ""virtual"" RelNode, accepting the multi-sink as input. It can work as packing the multi-tree together so that, from the perspective of the optimizer, it can have the ability to do global optimization.

In my POC, I add a new type RelNode named {{MultiSink}} before passing it to the calcite optimizer. 
The MultiSink does not do any transformation on the inputs.

After logical optimization, the plan is

{code:java}
LogicalMultiSink
:- LogicalSink(table=[default_catalog.default_database.sink_table], fields=[a, b])
:  +- LogicalProject(inputs=[0..1])
:     +- LogicalTableScan(table=[[default_catalog, default_database, newX]])
+- LogicalSink(table=[default_catalog.default_database.sink_table], fields=[a, b])
   +- LogicalProject(inputs=[0], exprs=[[1:BIGINT]])
      +- LogicalTableScan(table=[[default_catalog, default_database, newX]])
{code}

After physical optimization, the plan is 

{code:java}
MultiSink
:- Sink(table=[default_catalog.default_database.sink_table], fields=[a, b])
:  +- TableSourceScan(table=[[default_catalog, default_database, newX, project=[a, b], metadata=[]]], fields=[a, b])
+- Sink(table=[default_catalog.default_database.sink_table], fields=[$f0, $f1])
   +- Calc(select=[a AS $f0, 1:BIGINT AS $f1])
      +- TableSourceScan(table=[[default_catalog, default_database, newX, project=[a, b], metadata=[]]], fields=[a, b])
{code}

Before transforming to the ExecNode, we remove the {{MultiSink}} (which is only intended to work during the optimizing phase), then the final result can be 

{code:java}
TableSourceScan(table=[[default_catalog, default_database, newX, project=[a, b], metadata=[]]], fields=[a, b])(reuse_id=[1])

Sink(table=[default_catalog.default_database.sink_table], fields=[a, b])
+- Reused(reference_id=[1])

Sink(table=[default_catalog.default_database.sink_table], fields=[$f0, $f1])
+- Calc(select=[a AS $f0, 1 AS $f1])
   +- Reused(reference_id=[1])
{code}

With the new RelNode, single-tree optimization is possible. We can do more things during the single tree optimization, e.g., introduce the cost model for the CTE to decide whether to inline/reuse and so on.;;;","28/Feb/23 07:37;aitozi;looking forward to your opinion CC [~godfreyhe] [~twalthr] [~snuyanzin];;;","24/Feb/24 02:53;tjbanghart;I happened upon this ticket looking for issues related to multi-query optimization. CALCITE-1440 looks like generalization of this problem. The proposed {{Combine}} rel node looks similar to {{{}MultiSink{}}}.

 

Do you have a PR for this change? Would love to see it and hopefully get something similar in Calcite.;;;","24/Feb/24 03:20;aitozi;[~tjbanghart] I have not prepare a PR for this ticket. In my previous poc, the {{MultiSink}} rel node can construct a single tree then feed it to calcite optimizer. So, IMO the {{MultiSink}} solution can do multi-query optimization.

However, it still have a problem that the calcite optimzer do not recognize the CTE during the optimization. So this solution do not have significant advantage than the current solution based on {{RelNodeBlock}} in Flink. So I have not prepared a PR for this.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogITCase fails due to avro conflict in table store,FLINK-31204,13525970,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,24/Feb/23 00:25,24/Feb/23 04:00,04/Jun/24 20:41,24/Feb/23 04:00,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Test fails in IDEA

	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.NoSuchMethodError: org.apache.avro.Schema.isNullable()Z
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.nullableSchema(AvroSchemaConverter.java:203)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:172)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:147)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:147)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:55)
	at org.apache.flink.table.store.format.avro.AvroFileFormat$AvroGenericRecordBulkFormat.<init>(AvroFileFormat.java:95)
	at org.apache.flink.table.store.format.avro.AvroFileFormat.createReaderFactory(AvroFileFormat.java:80)
	at org.apache.flink.table.store.format.FileFormat.createReaderFactory(FileFormat.java:71)
	at org.apache.flink.table.store.format.FileFormat.createReaderFactory(FileFormat.java:67)
	at org.apache.flink.table.store.file.manifest.ManifestList$Factory.create(ManifestList.java:130)
	at org.apache.flink.table.store.file.operation.AbstractFileStoreScan.<init>(AbstractFileStoreScan.java:95)
	at org.apache.flink.table.store.file.operation.KeyValueFileStoreScan.<init>(KeyValueFileStoreScan.java:57)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:118)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:71)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:38)
	at org.apache.flink.table.store.file.AbstractFileStore.newCommit(AbstractFileStore.java:116)
	at org.apache.flink.table.store.file.AbstractFileStore.newCommit(AbstractFileStore.java:43)
	at org.apache.flink.table.store.table.AbstractFileStoreTable.newCommit(AbstractFileStoreTable.java:121)
	at org.apache.flink.table.store.connector.sink.FileStoreSink.lambda$createCommitterFactory$63124b4e$1(FileStoreSink.java:69)
	at org.apache.flink.table.store.connector.sink.CommitterOperator.initializeState(CommitterOperator.java:104)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:283)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 04:00:42 UTC 2023,,,,,,,,,,"0|z1g5co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 04:00;lzljs3620320;master: 306a9ededbebc1d825cbb02c18338f5accf7faca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application upgrade rollbacks failed in Flink Kubernetes Operator,FLINK-31203,13525921,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hjw,hjw,23/Feb/23 16:33,31/Aug/23 06:18,04/Jun/24 20:41,31/Aug/23 06:18,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"I make a test on the Application upgrade rollback feature, but this function fails.The Flink application mode job cannot roll back to  last stable spec.
As shown in the follow example, I declare a error pod-template without a container named flink-main-container to test rollback feature.
However, only the error of deploying the flink application job failed without rollback.
 
Error:
org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster ""basic-example"".
 at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployClusterInternal(KubernetesClusterDescriptor.java:292)
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://*/k8s/clusters/c-fwkxh/apis/apps/v1/namespaces/test-flink/deployments. Message: Deployment.apps ""basic-example"" is invalid: [spec.template.spec.containers[0].name: Required value, spec.template.spec.containers[0].image: Required value]. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=spec.template.spec.containers[0].name, message=Required value, reason=FieldValueRequired, additionalProperties={}), StatusCause(field=spec.template.spec.containers[0].image, message=Required value, reason=FieldValueRequired, additionalProperties={})], group=apps, kind=Deployment, name=basic-example, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Deployment.apps ""basic-example"" is invalid: [spec.template.spec.containers[0].name: Required value, spec.template.spec.containers[0].image: Required value], metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).
 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:673)
 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:612)
 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)
 
Env:
Flink version:Flink 1.16
Flink Kubernetes Operator:1.3.1
 
*Last* ** *stable  spec:*
apiVersion: [flink.apache.org/v1beta1|http://flink.apache.org/v1beta1]
kind: FlinkDeployment
metadata:
  name: basic-example
spec:
  image: flink:1.16
  flinkVersion: v1_16
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""2""
    kubernetes.operator.deployment.rollback.enabled: true
    state.savepoints.dir: s3://flink-data/savepoints
    state.checkpoints.dir: s3://flink-data/checkpoints
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    high-availability.storageDir: s3://flink-data/ha
  serviceAccount: flink
  *podTemplate:*
    *spec:*
      *containers:*
        *- name: flink-main-container*      
          *env:*
          *- name: TZ*
            *value: Asia/Shanghai*
  jobManager:
    resource:
      memory: ""2048m""
      cpu: 1
  taskManager:
    resource:
      memory: ""2048m""
      cpu: 1
  job:
    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
    parallelism: 2
    upgradeMode: stateless
 
*new Spec:*
apiVersion: [flink.apache.org/v1beta1|http://flink.apache.org/v1beta1]
kind: FlinkDeployment
metadata:
  name: basic-example
spec:
  image: flink:1.16
  flinkVersion: v1_16
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""2""
    kubernetes.operator.deployment.rollback.enabled: true
    state.savepoints.dir: s3://flink-data/savepoints
    state.checkpoints.dir: s3://flink-data/checkpoints
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    high-availability.storageDir: s3://flink-data/ha
  serviceAccount: flink
  *podTemplate:*
    *spec:*
      *containers:*
        *-   env:*
          *- name: TZ*
            *value: Asia/Shanghai*
  jobManager:
    resource:
      memory: ""2048m""
      cpu: 1
  taskManager:
    resource:
      memory: ""2048m""
      cpu: 1
  job:
    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
    parallelism: 2
    upgradeMode: stateless",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 06:18:48 UTC 2023,,,,,,,,,,"0|z1g51s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 08:00;hjw;[Gyula Fora|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gyfora]  Could you help to look at this problem? thx.

 ;;;","31/Aug/23 06:18;gyfora;I think this is fixed in 1.6.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for reading Parquet files containing Arrays with complex types.,FLINK-31202,13525882,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,KristoffSC,KristoffSC,23/Feb/23 11:36,23/Feb/23 11:50,04/Jun/24 20:41,,1.16.0,1.16.1,1.16.2,1.17.0,1.17.1,,,,,,,,,,,,,,,2,,,,,"reading complex types to Parquet is possible since Flink 1.16 after implementing https://issues.apache.org/jira/browse/FLINK-24614

However this implementation lacks support for reading complex nested types such as
* Array<Array>
* Array<Map>
* Array<Row>

This ticket is about to add support for reading below types from Parquet format files.

Currently when trying to read Parquet file containing column which such a type, below exception is thrown:


{code:java}
Caused by: java.lang.RuntimeException: Unsupported type in the list: ROW<`f1` INT>
	at org.apache.flink.formats.parquet.vector.reader.ArrayColumnReader.readPrimitiveTypedRow(ArrayColumnReader.java:175)
	at org.apache.flink.formats.parquet.vector.reader.ArrayColumnReader.fetchNextValue(ArrayColumnReader.java:113)
	at org.apache.flink.formats.parquet.vector.reader.ArrayColumnReader.readToVector(ArrayColumnReader.java:81)
{code}

OR:


{code:java}
Caused by: java.lang.RuntimeException: Unsupported type in the list: ARRAY<INT>
	at org.apache.flink.formats.parquet.vector.reader.ArrayColumnReader.readPrimitiveTypedRow(ArrayColumnReader.java:175)
	at org.apache.flink.formats.parquet.vector.reader.ArrayColumnReader.fetchNextValue(ArrayColumnReader.java:113)
	at org.apache.flink.formats.parquet.vector.reader.ArrayColumnReader.readToVector(ArrayColumnReader.java:81)

{code}

Parquet files and reproducer code is attached to the ticket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 11:50;KristoffSC;ParquetSourceArrayOfArraysIssue.java;https://issues.apache.org/jira/secure/attachment/13055760/ParquetSourceArrayOfArraysIssue.java","23/Feb/23 11:50;KristoffSC;ParquetSourceArrayOfRowIssue.java;https://issues.apache.org/jira/secure/attachment/13055761/ParquetSourceArrayOfRowIssue.java","23/Feb/23 11:50;KristoffSC;arrayOfArrayOfInts.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13055762/arrayOfArrayOfInts.snappy.parquet","23/Feb/23 11:50;KristoffSC;arrayOfrows.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13055763/arrayOfrows.snappy.parquet",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-23 11:36:32.0,,,,,,,,,,"0|z1g4t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provides option to sort partition for full stage in streaming read,FLINK-31201,13525881,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Feb/23 11:35,27/Feb/23 09:52,04/Jun/24 20:41,27/Feb/23 09:52,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"The overall order may be out of order due to the writing of the old partition. We can provide an option to sort the full reading stage by partition fields to avoid the disorder.
(Actually, Currently, it is out of order for partitions. Because HashMap is used, we may be able to sort according to the creation time of the first file?)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 09:52:21 UTC 2023,,,,,,,,,,"0|z1g4sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 09:52;lzljs3620320;master: d6fade86710ce4d7be9c456440c233326e0b5179
release-0.3: c07482c3e64c576ae53322b4511733e2328cd818;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add MAP_VALUES supported in SQL & Table API,FLINK-31200,13525880,13076759,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jackylau,jackylau,23/Feb/23 11:30,30/Mar/23 05:08,04/Jun/24 20:41,21/Mar/23 06:59,1.18.0,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,,,,,"Returns an unordered array containing the values of the map.

Syntax:
map_values(map)

Arguments:
map An Map to be handled.

Returns:

An Map. If value is NULL, the result is NULL. 
Examples:
{code:sql}
> SELECT map_values(map(1, 'a', 2, 'b'));
 - [""a"",""b""]{code}
See also
spark https://spark.apache.org/docs/latest/api/sql/index.html#map_values",,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 11:35:32 UTC 2023,,,,,,,,,,"0|z1g4so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 11:35;Sergey Nuyanzin;There is an existing PR for that https://github.com/apache/flink/pull/15797;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add MAP_KEYS supported in SQL & Table API,FLINK-31199,13525879,13076759,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jackylau,jackylau,23/Feb/23 11:27,23/Feb/23 12:25,04/Jun/24 20:41,23/Feb/23 12:06,1.18.0,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Returns an unordered array containing the keys of the map.

Syntax:
map_keys(map)

Arguments:
map An Map to be handled.

Returns:

An Map. If value is NULL, the result is NULL. 
Examples:
{code:sql}
> SELECT map_keys(map(1, 'a', 2, 'b'));
 [1,2] {code}
See also
spark https://spark.apache.org/docs/latest/api/sql/index.html#map_keys

 
h4.  ",,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 12:25:08 UTC 2023,,,,,,,,,,"0|z1g4sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 11:35;Sergey Nuyanzin;There is an existing PR for that https://github.com/apache/flink/pull/15797;;;","23/Feb/23 12:25;jackylau;[~Sergey Nuyanzin] I just see it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class loader problem by incorrect classloader in flink sql 1.16,FLINK-31198,13525877,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stayrascal,stayrascal,23/Feb/23 11:17,23/Feb/23 12:55,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"Hi, I meet a wired problem about using Flink connector via `./bin/sql-client.sh -j connector.jar` in Flink 1.16.1. it seems that using incorrect class loader to load user classes, but it's working in Flink 1.15. Otherwise, we will meet ClassNotFound problem.

In Flink 1.16, I have to put the connector jar into lib folder as the system jar, but it might not a good practice.

 

The detail information as follow, I tried test this issue in a standlone cluster, and try test hudi/iceberg/flink table store cases, flink table store case worked well, but others failed. 

 

1. start a standlone cluster without native lib jar

`./bin/start-cluster.sh`

 

2. create a flink table store catalog succeed. 

```

./bin/sql-client.sh embedded -j flink-table-store-dist-0.3.0.jar
 
CREATE CATALOG fts WITH (
'type'='table-store',
'warehouse'='file:///Users/xxxxx/Downloads/flink-1.16.1/data/fts'
);
```

!image-2023-02-23-20-04-01-936.png!

3. create a iceberg hadoop catalog failed

```

./bin/sql-client.sh embedded -j iceberg-flink-runtime-1.16-1.1.0.jar


CREATE CATALOG hadoop_catalog WITH (
'type'='iceberg',
'catalog-type'='hadoop',
'warehouse'='file:///Users/xxxxx/Downloads/flink-1.16.1/data/iceberg/',
'property-version'='1'
);
```

!image-2023-02-23-20-08-54-885.png!

but the iceberg FlinkCatalogFactory class is loaded by flink user class loader, it's wried that why the classes from same jar been loaded by different class loader, both the classes are not in system classpath.  !image-2023-02-23-20-10-27-274.png!

4. create hudi catalog succeed, but insert data failed.

```

./bin/sql-client.sh embedded -j hudi-flink1.16-bundle-0.13.0.jar

 
CREATE CATALOG dfs_catalog WITH (
'type'='hudi',
'catalog.path'='file:///Users/xxxx/Downloads/flink-1.16.1/data/'
);
 
USE CATALOG {*}dfs_catalog{*};
CREATE DATABASE hudi_dfs_db;
USE hudi_dfs_db;
 
CREATE TABLE flink_hudi_mor_tbl(
 uuid VARCHAR(20) PRIMARY KEY NOT ENFORCED,
 name VARCHAR(10),
 age INT,
 ts TIMESTAMP(3),
 `partition` VARCHAR(20)
)
PARTITIONED BY (`partition`)
WITH (
'connector' = 'hudi',
'table.type' = 'MERGE_ON_READ',
'hoodie.datasource.write.recordkey.field' = 'uuid',
'precombine.field' = 'ts'
);
 
INSERT INTO `dfs_catalog`.`hudi_dfs_db`.`flink_hudi_mor_tbl` VALUES ('id31','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1');```
As similar with Iceberg case, using flink hudi connector can create dfs catalog succeed, but insert data failed as HoodieRecord not found. but flink 1.15.1 works.

!image-2023-02-23-20-14-24-895.png!

!image-2023-02-23-20-21-52-113.png!

After investigating the reason, I found that there is a change from https://issues.apache.org/jira/browse/FLINK-28451, which change some logic about parse.parse(statement), In flink 1.15, the parse oepration was been wrapped by `context.wrapClassLoader(() -> parser.parse(statement))`, which set the flink classloader in the thread, so it can loader the connector classes succeed, but Flink 1.16 doesn't work, not sure it's the root cause?

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 11:58;stayrascal;image-2023-02-23-19-58-48-273.png;https://issues.apache.org/jira/secure/attachment/13055764/image-2023-02-23-19-58-48-273.png","23/Feb/23 11:59;stayrascal;image-2023-02-23-19-59-37-233.png;https://issues.apache.org/jira/secure/attachment/13055765/image-2023-02-23-19-59-37-233.png","23/Feb/23 12:04;stayrascal;image-2023-02-23-20-04-01-936.png;https://issues.apache.org/jira/secure/attachment/13055766/image-2023-02-23-20-04-01-936.png","23/Feb/23 12:08;stayrascal;image-2023-02-23-20-08-54-885.png;https://issues.apache.org/jira/secure/attachment/13055767/image-2023-02-23-20-08-54-885.png","23/Feb/23 12:10;stayrascal;image-2023-02-23-20-10-27-274.png;https://issues.apache.org/jira/secure/attachment/13055768/image-2023-02-23-20-10-27-274.png","23/Feb/23 12:14;stayrascal;image-2023-02-23-20-14-24-895.png;https://issues.apache.org/jira/secure/attachment/13055769/image-2023-02-23-20-14-24-895.png","23/Feb/23 12:22;stayrascal;image-2023-02-23-20-21-52-113.png;https://issues.apache.org/jira/secure/attachment/13055770/image-2023-02-23-20-21-52-113.png",,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 12:55:27 UTC 2023,,,,,,,,,,"0|z1g4s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 12:08;martijnvisser;[~stayrascal] Please add a correct description and information in the ticket and not only images, since they can't be searched. Else we'll have to close the ticket;;;","23/Feb/23 12:26;stayrascal;[~martijnvisser] thanks for reminder, i'm providing the detail information.;;;","23/Feb/23 12:32;martijnvisser;[~stayrascal] There have been changes in classloading / UDFs in 1.16, see https://issues.apache.org/jira/browse/FLINK-29890?focusedCommentId=17636291&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17636291 for more details. ;;;","23/Feb/23 12:49;stayrascal;Thanks [~martijnvisser] for your quick response, I'm not sure if it's same problem, because FLINK-29890 is more relate to load UDF jars, the solution mentioned in FLINK-29890  about using 'add jar' or 'create function ... using jar' might can do in UDF situation, but for connector cases, it might not a good behavior, does it mean the user have to run an additional sql ('add jar') before submit their sql jobs? and the '-j' parameter of sql-client.sh might not need any more?;;;","23/Feb/23 12:55;stayrascal;I also tried using 'add jar xxxx.jar' instead of sql-client -j xxxx.jar, the problem still exist.

```

ADD JAR '/Users/xxxxxxx/Downloads/flink-1.16.1/hudi-flink1.16-bundle-0.13.0.jar';

 

Flink SQL> INSERT INTO `dfs_catalog`.`hudi_dfs_db`.`flink_hudi_mor_tbl` VALUES
>   ('id31','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1'),
>   ('id32','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),
>   ('id33','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par2'),
>   ('id34','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par2'),
>   ('id35','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par3'),
>   ('id36','Emma',20,TIMESTAMP '1970-01-01 00:00:06','par3'),
>   ('id37','Bob',44,TIMESTAMP '1970-01-01 00:00:07','par4'),
>   ('id38','Han',56,TIMESTAMP '1970-01-01 00:00:08','par4');
[INFO] Submitting SQL update statement to the cluster...
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: org.apache.hudi.common.model.HoodieRecord

```;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to write Parquet files containing Arrays with complex types.,FLINK-31197,13525864,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,KristoffSC,KristoffSC,23/Feb/23 10:21,24/Nov/23 20:49,04/Jun/24 20:41,,1.15.0,1.15.1,1.15.2,1.15.3,1.15.4,1.15.5,1.16.0,1.16.1,1.16.2,1.17.0,1.17.1,,,,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,2,,,,,"After https://issues.apache.org/jira/browse/FLINK-17782 It should be possible to write complex types with File sink using Parquet format. 

However it turns out that still it is impossible to write types such as:
* Array<Arrays>
* Array<Map>
* Array<Row> 

When trying to write a Parquet row with such types, the below exception is thrown:
{code:java}
Caused by: java.lang.RuntimeException: org.apache.parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
	at org.apache.flink.formats.parquet.row.ParquetRowDataBuilder$ParquetWriteSupport.write(ParquetRowDataBuilder.java:91)
	at org.apache.flink.formats.parquet.row.ParquetRowDataBuilder$ParquetWriteSupport.write(ParquetRowDataBuilder.java:71)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)
	at org.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:310)
	at org.apache.flink.formats.parquet.ParquetBulkWriter.addElement(ParquetBulkWriter.java:52)
	at org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.write(BulkPartWriter.java:51)
	at org.apache.flink.connector.file.sink.writer.FileWriterBucket.write(FileWriterBucket.java:191)

{code}


The exception is misleading, not showing the real problem. 
The reason why those complex types are still not working is that during developemnt of https://issues.apache.org/jira/browse/FLINK-17782

code paths for those types were left without implementation, no Unsupported Exception no nothing, simply empty methods. In https://github.com/apache/flink/blob/release-1.16.1/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/row/ParquetRowDataWriter.java
You will see 
{code:java}
@Override
public void write(ArrayData arrayData, int ordinal) {}
{code}

for MapWriter, ArrayWriter and RowWriter.

I see two problems here:
1. Writing those three types is still not possible.
2. Flink is throwing an exception that gives no hint about the real issue here. It could throw ""Unsupported operation"" for now. Maybe this should be item for a different ticket?


The code to reproduce this issue is attached to the ticket. It tries to write to Parquet file a single row with one column of type Array<Array<int>>",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31811,,,,,,,,,,,"23/Feb/23 10:21;KristoffSC;ParquetSinkArrayOfArraysIssue.java;https://issues.apache.org/jira/secure/attachment/13055758/ParquetSinkArrayOfArraysIssue.java",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 20:49:57 UTC 2023,,,,,,,,,,"0|z1g4p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 07:38;martijnvisser;[~sujun1020] [~lzljs3620320] Any thoughts on this? ;;;","24/Nov/23 20:49;bjarke_tornager;[~martijnvisser] any update on this issue? I am having the same problem when trying to write complex nested data to parquet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink on YARN honors env.java.home,FLINK-31196,13525856,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,prabhujoseph,prabhujoseph,23/Feb/23 09:48,24/Feb/23 05:13,04/Jun/24 20:41,24/Feb/23 05:12,1.16.1,,,,,,,,,,,,,,,Deployment / YARN,,,,0,,,,,"Flink on YARN, honor env.java.home, and launch JobManager and TaskMananger containers with a configured env.java.home.

One option to set the JAVA_HOME for Flink JobManager, TaskManager running on YARN
{code:java}
containerized.master.env.JAVA_HOME: /usr/lib/jvm/java-11-openjdk
containerized.taskmanager.env.JAVA_HOME: /usr/lib/jvm/java-11-openjdk {code}
 ",,,,,,,,,,,,,,,,,,,,,,FLINK-22091,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 05:11:38 UTC 2023,,,,,,,,,,"0|z1g4nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 10:03;samrat007;We are working on this fix.
cc: [~prabhujoseph] ;;;","24/Feb/23 01:48;zlzhang0122;Maybe duplicate with this? [Flink-22091|https://issues.apache.org/jira/browse/FLINK-22091];;;","24/Feb/23 05:11;prabhujoseph;Thanks [~zlzhang0122]  for the update. Have missed to search properly in Flink Jira List before raising the ticket. We will close this as a duplicate and provide a patch for the original one, Flink-22091;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FullChangelogStoreSinkWrite bucket writer conflicts with rescale,FLINK-31195,13525841,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,23/Feb/23 08:22,29/Mar/23 03:07,04/Jun/24 20:41,29/Mar/23 03:07,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"At present, this operator relies on ListState, Flink distributes data according to round-robin when rescaling, which may be different from the distribution rules of our bucket after rescaling.

We need to change the mode of UnionListState, broadcast to each node, and finally decide whether it belongs to the task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-23 08:22:03.0,,,,,,,,,,"0|z1g4k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduces savepoint mechanism of Table Store,FLINK-31194,13525834,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nicholasjiang,nicholasjiang,23/Feb/23 07:03,29/Mar/23 03:08,04/Jun/24 20:41,29/Mar/23 03:08,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Disaster Recovery is very much mission critical for any software. Especially when it comes to data systems, the impact could be very serious leading to delay in business decisions or even wrong business decisions at times. Flink Table Store could introduce savepoint mechanism to assist users in recovering data from a previous state.

As the name suggest, ""savepoint"" saves the table as of the snapshot, so that it lets you restore the table to this savepoint at a later point in snapshot if need be. Care is taken to ensure cleaner will not clean up any files that are savepointed. On similar lines, savepoint cannot be triggered on a snapshot that is already cleaned up. In simpler terms, this is synonymous to taking a backup, just that we don't make a new copy of the table, but just save the state of the table elegantly so that we can restore it later when in need.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-23 07:03:48.0,,,,,,,,,,"0|z1g4ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The option `table.exec.hive.native-agg-function.enabled` should work at job level when using it in SqlClient side,FLINK-31193,13525832,13488604,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,23/Feb/23 06:47,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,1.20.0,,,,Connectors / Hive,,,,0,,,,,"Sink Flink 1.17, we have implemented the native hive aggregation function, users can enable this optimization by the option `table.exec.hive.native-agg-function.enabled` per-job. The option works well in Table API & 

HiveServer2Endpoint, but in SQL client, this option can't work per-job, it works at the module level. 
On the SqlClient side, if we want to use the native hive aggregation function, we need to enable the option first and then load HiveModule, only by doing that we can load the hive native aggregation function. This behavior is inconsistent with the definition of the option itself, and also is inconsistent with the behavior of the Table API and HiveServerEndpoint2, which is a bug. we cannot align the behavior of the option at the moment due to code implementation reasons. If we want to align the behavior, we need to modify the `Module` interface, which is a Public interface.  In 1.17 we didn't have time to complete this work, so we need to fix this in 1.18 and make the parameter per-job effective on the SqlClient side as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-23 06:47:52.0,,,,,,,,,,"0|z1g4i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dataGen takes too long to initialize under sequence,FLINK-31192,13525827,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xzw0223,xzw0223,xzw0223,23/Feb/23 06:09,25/Feb/24 11:17,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,"The SequenceGenerator preloads all sequence values in open. If the totalElement number is too large, it will take too long.
[https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/datagen/SequenceGenerator.java#L91]



The reason is that the capacity of the Deque will be expanded twice when the current capacity is full, and the array copy is required, which is time-consuming.

 

Here's what I think : 
 do not preload the full amount of data on Sequence, and generate a piece of data each time next is called to solve the problem of slow initialization caused by loading full amount of data.

  record the currently sent Sequence position through the checkpoint, and continue to send data through the recorded position after an abnormal restart to ensure fault tolerance",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 22:35:14 UTC 2023,,,,,,,,,,"0|z1g4gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 06:14;xzw0223;[~mapohl] I created a new issue to discuss the problem of slow datagen initialization. We can discuss how this problem can be better solved.;;;","23/Feb/23 06:18;Weijie Guo;[~xzw0223] Thanks for reporting this, does this problem only exist in 1.16? If not, you'd better adjust the affected version.;;;","23/Feb/23 06:28;xzw0223;[~Weijie Guo] All versions, it seems, have been around since birth.;;;","23/Feb/23 06:28;xzw0223;I'd like to solve this problem. May I have a ticket [~Weijie Guo] ;;;","23/Feb/23 06:41;xzw0223;Add one thing : It is added that during snapshot restoration and snapshot, it is still necessary to traverse all data in Deque for saving and restoration;;;","23/Feb/23 07:06;Weijie Guo;[~xzw0223] flink community no longer supports versions lower than 1.15. There is no need to add so many affected versions. BTW, you are assigned.;;;","23/Feb/23 07:41;xzw0223;[~Weijie Guo] thank you;;;","27/Feb/23 02:11;xzw0223;[~Weijie Guo]  It's taken care of. Can you take a look at it? Thank you.;;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VectorIndexer should check whether doublesByColumn is null before snapshot,FLINK-31191,13525823,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,23/Feb/23 06:04,01/Mar/23 10:10,04/Jun/24 20:41,01/Mar/23 10:10,ml-2.2.0,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Currently VectorIndexer would lead to NPE when doing checkpoint. It should check whether `doublesByColumn` is null before calling snapshot.

 

logview: [https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039]

details:
 
 
[735|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:736]Caused by: java.lang.NullPointerException 
[736|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:737] at org.apache.flink.ml.feature.vectorindexer.VectorIndexer$ComputeDistinctDoublesOperator.convertToListArray(VectorIndexer.java:232) 
[737|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:738] at org.apache.flink.ml.feature.vectorindexer.VectorIndexer$ComputeDistinctDoublesOperator.snapshotState(VectorIndexer.java:228) 
[738|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:739] at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222) 
[739|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:740] ... 33 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 10:10:45 UTC 2023,,,,,,,,,,"0|z1g4g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 10:10;lindong;Merged to flink-ml master branch 10c08277fd7c03faef8ffb9fadd5008f2b101a19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports Spark call procedure command on Table Store,FLINK-31190,13525819,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,23/Feb/23 04:54,10/Aug/23 05:39,04/Jun/24 20:41,29/Mar/23 03:09,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,At present Hudi and Iceberg supports the Spark call procedure command to execute the table service action etc. Flink Table Store could also support Spark call procedure command to run compaction etc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 04:55:39 UTC 2023,,,,,,,,,,"0|z1g4f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 04:55;nicholasjiang;[~lzljs3620320], could you assign this ticket to me? I would like to support Spark call procedure command which refers to the implementation of Hudi and Iceberg.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow special handle of less frequent values in StringIndexer,FLINK-31189,13525810,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,hongfanxo,hongfanxo,23/Feb/23 03:30,03/Apr/23 06:31,04/Jun/24 20:41,03/Apr/23 06:31,,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Real-world datasets often contain categorical features with millions of distinct values, some of which may only appear a few times. To maximize the performance of certain algorithms, it is important to treat these less frequent values properly. A popular approach is to put them to a special index, as is done in sklearn's OneHotEncoder [1].

 

[1] https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 06:31:35 UTC 2023,,,,,,,,,,"0|z1g4d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 06:31;zhangzp;Resolved via 560532d99c4330949d4da2c94d0e5228bdfbc9cd on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose kubernetes scheduler configOption when running flink on kubernetes,FLINK-31188,13525806,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,legendtkl,legendtkl,23/Feb/23 02:33,23/Feb/23 03:01,04/Jun/24 20:41,23/Feb/23 03:01,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"Now when we deploy Flink job on kubernetes, the scheduler is kubernetes scheduler by default. But the custom kubernetes scheduler setting sometimes is needed by users.

 

So can we add the config option for kubernetes scheduler setting? 

Thanks.",,,,,,,,,,FLINK-28825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 02:59:33 UTC 2023,,,,,,,,,,"0|z1g4c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 02:52;wangyang0918;This might be a duplicate of FLINK-28825.;;;","23/Feb/23 02:59;legendtkl;thanks [~wangyang0918] . Close this for duplication.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone HA mode does not work if dynamic properties are supplied,FLINK-31187,13525768,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mateczagany,mateczagany,mateczagany,22/Feb/23 18:09,18/Jun/23 15:29,04/Jun/24 20:41,01/Mar/23 15:26,kubernetes-operator-1.4.0,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"With FLINK-30518 '--host $(POD_IP)' has been added to the arguments of the JMs which fixes the issue with HA on standalone mode, but it always gets appended to the end of the final JM arguments: https://github.com/usamj/flink-kubernetes-operator/blob/72ec9d384def3091ce50c2a3e2a06cded3b572e6/flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/decorators/CmdStandaloneJobManagerDecorator.java#L107

But this will not be parsed properly in case any dynamic properties were set in the arguments, e.g.:
{code:java}
 Program Arguments:
   --configDir
   /opt/flink/conf
   -D
   jobmanager.memory.off-heap.size=134217728b
   -D
   jobmanager.memory.jvm-overhead.min=201326592b
   -D
   jobmanager.memory.jvm-metaspace.size=268435456b
   -D
   jobmanager.memory.heap.size=469762048b
   -D
   jobmanager.memory.jvm-overhead.max=201326592b
   --job-classname
   org.apache.flink.streaming.examples.statemachine.StateMachineExample
   --test
   test
   --host
   172.17.0.11{code}
You can verify this bug by using the YAML I've attached and in the JM logs you can see this line: 
{code:java}
Remoting started; listening on addresses :[akka.tcp://flink@flink-example-statemachine.flink:6123]{code}
Without any program arguments supplied this would correctly be:
{code:java}
Remoting started; listening on addresses :[akka.tcp://flink@172.17.0.8:6123]{code}

I believe this could be easily fixed by appending the --host parameter before JobSpec.args and if a committer can assign this ticket to me I can create a PR for this.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30518,,,,,,,,,,,"22/Feb/23 18:06;mateczagany;standalone-ha.yaml;https://issues.apache.org/jira/secure/attachment/13055738/standalone-ha.yaml",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 15:26:13 UTC 2023,,,,,,,,,,"0|z1g43s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 15:26;gyfora;merged to main 2af2c99d251bb84c77200f2896260bbe72a3ee6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing topic from kafka source does nothing,FLINK-31186,13525743,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,exidex,exidex,22/Feb/23 12:53,23/Feb/23 09:19,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,,,,,0,,,,,"As far as I can tell, there is no good way to remove topic from the list of topic that kafka source consumes from. 

We use {{StreamExecutionEnvironment.fromSource}} api with {{KafkaSource.setTopics}} which accepts list of topics. but when we remove the topic from list after some time the flink kafka source still consumes from it. 

My guess is that it relates to this TODO in code:
[GitHub|https://github.com/apache/flink/blob/cc66d4855e6f8ee9986809a18f68a458bcfe3c12/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/enumerator/KafkaSourceEnumerator.java#L305]

You can kind of workaroud this by removing whole job state or changing uid of kafka source but that affects either whole job or whole source. The other way is to use state processor api but it doesn't expose source operator state, which in turn can be worked around using reflection and copying code from SourceCoordinator. None of those are satisfactory",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 09:19:54 UTC 2023,,,,,,,,,,"0|z1g3y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 01:41;luoyuxia;cc [~renqs] ;;;","23/Feb/23 09:19;martijnvisser;Wouldn't this be considered by design, given that you're breaking the state of your job by removing a topic, so it requires an action for the user to deal with that (either start from a clean state or change the UID)? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python BroadcastProcessFunction not support side output,FLINK-31185,13525738,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,22/Feb/23 12:01,06/Mar/23 10:37,04/Jun/24 20:41,03/Mar/23 12:10,1.16.1,,,,,,,,,,,1.16.2,1.17.0,,,API / Python,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 10:37:09 UTC 2023,,,,,,,,,,"0|z1g3x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 12:10;dianfu;Fixed in:
- master: 8d52415a05bdc67eb67a59bbc2e53f48762da374
- 1.17: 7040af5b7933905798ff6af0b35ac364b5fbe432
- 1.16: 8713b176abc5c9f5267d7559ace0b6bd8afc6d3f;;;","06/Mar/23 10:14;martijnvisser;[~dianfu] Why was this added to Flink 1.17 or Flink 1.16? This doesn't seem like a bugfix, but it adds a new feature. Given that 1.17 is in featue freeze, this should only have been added to 1.17? This is now causing FLINK-31337;;;","06/Mar/23 10:28;dianfu;[~martijnvisser] The PR title seems like a feature, however it is trying to fix a bug reported in the slack channel: [https://apache-flink.slack.com/archives/C03G7LJTS2G/p1676950459225789?thread_ts=1676924426.566459&cid=C03G7LJTS2G];;;","06/Mar/23 10:30;dianfu;[~martijnvisser] Thanks for the reminder.  I have pinged [~Juntao Hu] to fix this issue ASAP.;;;","06/Mar/23 10:31;martijnvisser;[~dianfu] TBH that Slack thread still kind of reads like something that just wasn't supported before (""Okay it must just be not supported in the python layer. I can make a simple process function to get around it, but sad that the broadcast function can't emit side outputs."") - Since this is now blocking Flink 1.17, will this be a simple fix or do we need to revert this change?;;;","06/Mar/23 10:32;martijnvisser;[~dianfu] Thank you.;;;","06/Mar/23 10:37;dianfu;[~martijnvisser] It seems like an instable test(still not reproduced locally). I will disable this test for now if we cannot fix it in one or two hours later to not block the release. What do you think?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to get python udf runner directory via running GET_RUNNER_DIR_SCRIPT ,FLINK-31184,13525735,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhongwei,zhongwei,22/Feb/23 11:32,22/Feb/23 11:32,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,API / Python,,,,0,,,,,"The following exception is thrown when using python udf in user job:

 
{code:java}
Caused by: java.io.IOException: Cannot run program ""ERROR: ld.so: object '/usr/lib64/libjemalloc.so.1' from LD_PRELOAD cannot be preloaded: ignored.
/mnt/ssd/0/yarn/nm-local-dir/usercache/flink/appcache/application_1670838323719_705777/python-dist-fe870981-4de7-4229-ad0b-f51881e80d90/python-archives/pipeline_venv_v5.tar.gz/lib/python3.7/site-packages/pyflink/bin/pyflink-udf-runner.sh"": error=2, No such file or directory
  at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
  at org.apache.beam.runners.fnexecution.environment.ProcessManager.startProcess(ProcessManager.java:147)
  at org.apache.beam.runners.fnexecution.environment.ProcessManager.startProcess(ProcessManager.java:122)
  at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:106)
  at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:252)
  at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:231)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
  at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)
  ... 19 more
  Suppressed: java.lang.NullPointerException: Process for id does not exist: 1-1
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)
    at org.apache.beam.runners.fnexecution.environment.ProcessManager.stopProcess(ProcessManager.java:172)
    at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:126)
    ... 29 more
Caused by: java.io.IOException: error=2, No such file or directory
  at java.lang.UNIXProcess.forkAndExec(Native Method)
  at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
  at java.lang.ProcessImpl.start(ProcessImpl.java:134)
  at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
  ... 32 more {code}
 

 

This is because SRE introduce a environment param 

 
{code:java}
LD_PRELOAD=/usr/lib64/libjemalloc.so.1 {code}
The logic of the python process itself can be executed normally, but an extra error message will be printed. So the whole output looks like:
{code:java}
ERROR: ld.so: object '/usr/lib64/libjemalloc.so.1' from LD_PRELOAD cannot be preloaded: ignored.
/mnt/ssd/0/yarn/nm-local-dir/usercache/flink/appcache/application_1670838323719_705777/python-dist-fe870981-4de7-4229-ad0b-f51881e80d90/python-archives/pipeline_venv_v5.tar.gz/lib/python3.7/site-packages/pyflink/bin/{code}
And the whole output is treated as a command, which caused the exception.

It seems the output is not very reliable. Maybe we need to find another way to transfer data, or filter the output before using.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-22 11:32:40.0,,,,,,,,,,"0|z1g3wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kinesis EFO Consumer can fail to stop gracefully,FLINK-31183,13525726,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,22/Feb/23 10:36,23/Feb/23 17:06,04/Jun/24 20:41,23/Feb/23 17:06,1.15.3,1.16.1,aws-connector-4.0.0,,,,,,,,,1.15.4,1.16.2,aws-connector-4.1.0,,Connectors / Kinesis,,,,0,pull-request-available,,,,"*Background*

When stopping a Flink job using the stop-with-savepoint API the EFO Kinesis source can fail to close gracefully.

 

Sample stack trace
{code:java}
2023-02-16 20:45:40
org.apache.flink.runtime.checkpoint.CheckpointException: Task has failed.
	at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1013)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task name with subtask : Source: vas_source_stream (38/48)#0 Failure reason: Task has failed.
	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1395)
	at org.apache.flink.runtime.taskmanager.Task.lambda$triggerCheckpointBarrier$3(Task.java:1338)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:343)
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: event executor terminated
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	... 3 more
Caused by: java.util.concurrent.RejectedExecutionException: event executor terminated
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:923)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:350)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:343)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:825)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:815)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher$ChannelSubscription.cancel(HandlerPublisher.java:502)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.cancel(DelegatingSubscription.java:37)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2ResetSendingSubscription.cancel(Http2ResetSendingSubscription.java:41)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.cancel(DelegatingSubscription.java:37)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler$OnCancelSubscription.cancel(ResponseHandler.java:409)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber$1.cancel(FlatteningSubscriber.java:98)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber.handleStateUpdate(FlatteningSubscriber.java:170)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber.access$100(FlatteningSubscriber.java:29)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber$1.request(FlatteningSubscriber.java:93)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber$FanOutShardSubscription.requestRecord(FanOutShardSubscriber.java:401)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.consumeAllRecordsFromKinesisShard(FanOutShardSubscriber.java:355)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:189)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:169)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:124)
	at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23528,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 12:41:36 UTC 2023,,,,,,,,,,"0|z1g3ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 12:41;dannycranmer;Merged commit [{{a8c34db}}|https://github.com/apache/flink/commit/a8c34db3d601c534f92e68a2709a6467eb94276e] into apache:release-1.15 

Merged commit [{{cd7b049}}|https://github.com/apache/flink/commit/cd7b0495bcdadc3a9808a475be819c9808d5f17e] into apache:release-1.16 

Merged commit [{{fdfe982}}|https://github.com/apache/flink-connector-aws/commit/fdfe9821b36027e9afd8db4d32ac8eff080dad2d] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy,FLINK-31182,13525719,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,qingyue,qingyue,22/Feb/23 10:17,06/Mar/23 14:33,04/Jun/24 20:41,06/Mar/23 14:33,1.17.0,1.17.1,1.18.0,,,,,,,,,1.16.2,1.17.0,1.18.0,,Table SQL / Planner,,,,0,pull-request-available,,,,"This issue is reported from the [user mail list|https://lists.apache.org/thread/y6fgzyx330omhkr40376knw8k4oczz3s].

The stacktrace is 
{code:java}
Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) {code}
The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is ""Missing""; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.
!screenshot-1.png|width=675,height=295!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/23 10:38;qingyue;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13055730/screenshot-1.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 14:33:13 UTC 2023,,,,,,,,,,"0|z1g3sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 12:03;qingyue;Correct me if I'm wrong, but for all BultinFunctionDefinition with output type strategy as TypeStrategies.MISSING, the deserialization will fail. By removing the check, the execution of the deserialized plan is successful. However, I'm not very sure about the purpose of the check. Can anyone shed some light?;;;","22/Feb/23 19:51;Sergey Nuyanzin;it looks this code is present for a long time... Doesn't it work with older versions as well?
Or if it works with e.g. 1.16.x or 1.15.x then probably the reason in some other changes;;;","23/Feb/23 02:24;qingyue;[~Sergey Nuyanzin] Yes, the check on the output type inference strategy was introduced in FunctionCatalogTableOperator in FLINK-15487 for FLIP-65 a long time ago.

I guess it worked well before FLIP-190 was introduced because`LogicalUnnestRule` will infer the output type and convert UNNEST to EXPLODE. (see [LogicalUnnestRule L#99|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/LogicalUnnestRule.scala#L99]). As a result, although the BuiltinFunctionDefinition for UNNEST always has a ""MISSING"" type inference, it does not affect the execution.

However, after CompiledPlan is introduced, RexNodeJsonDeserializer relies on the OperatorTable to lookup functions, which always get a static placeholder (i.e. MISSING) for UNNEST, and due to this check, the deserialization failed. 

Could you help to take a look, cc [~godfreyhe] and [~twalthr] ;;;","27/Feb/23 13:44;twalthr;[~qingyue] I opened a PR. Would be great if you can help me with a review.;;;","05/Mar/23 07:40;qingyue;Hi [~twalthr], thanks for the fix!  LGTM, and feel free to merge.;;;","06/Mar/23 14:33;twalthr;Fixed in master: 86e0a0b384291f9d8bacc3bbef3c58fcfb79bb04
Fixed in 1.17: 5c1ec1980b0adf24a1557119e6b28efa7fffe93e
Fixed in 1.16: d71a87dc722e2b311d929623d5715321a003353b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support LIKE operator pushdown,FLINK-31181,13525698,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,grzegorz.kolakowski,grzegorz.kolakowski,22/Feb/23 08:18,28/Feb/23 13:12,04/Jun/24 20:41,28/Feb/23 13:12,,,,,,,,,,,,jdbc-3.1.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,,"Filter pushdown has been introduced with [FLINK-16024|https://issues.apache.org/jira/browse/FLINK-16024] for selected unary and binary operators. We can extend the functionality to support pushdown for LIKE operator as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 13:12:02 UTC 2023,,,,,,,,,,"0|z1g3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 13:12;libenchao;fixed via https://github.com/apache/flink-connector-jdbc/commit/aa75d6476a381c4c80bb39364944891c514b8002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail early when installing minikube and check whether we can retry,FLINK-31180,13525691,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Duplicate,,mapohl,mapohl,22/Feb/23 07:43,23/Oct/23 08:49,04/Jun/24 20:41,23/Oct/23 08:49,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Test Infrastructure,,,,0,auto-deprioritized-minor,pull-request-available,starter,test-stability,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46367&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4726

We experienced a build failure where Minikube couldn't be installed due to some network issues. Two things which we could do here:
* check whether we can add a retry loop (maybe also to other resource)
* fail early if CI didn't manage to install the binaries",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 23 08:49:35 UTC 2023,,,,,,,,,,"0|z1g3mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","29/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Oct/23 08:49;mapohl;I'm going to close this issue in favor of FLINK-32107 which covers similar issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make data structures serializable,FLINK-31179,13525688,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Feb/23 07:15,19/Mar/23 05:37,04/Jun/24 20:41,19/Mar/23 05:37,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-22 07:15:06.0,,,,,,,,,,"0|z1g3m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Public Writer API,FLINK-31178,13525686,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Feb/23 07:02,02/Mar/23 06:11,04/Jun/24 20:41,02/Mar/23 06:11,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:11:47 UTC 2023,,,,,,,,,,"0|z1g3lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 06:11;TsReaper;master: 9b8dc4e2f3cf8bc28a95d5381c8544868bd5b688;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
To introduce a formatter for Markdown files,FLINK-31177,13525685,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhongpu314,zhongpu314,22/Feb/23 06:55,22/Feb/23 09:03,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"Currently, markdown files in *docs* are maintained and updated by many contributors, and different people have varying code style taste. By the way, as the syntax of markdown is not really strict, the styles tend to be inconsistent.

To name a few,
 * Some prefer `*` to make a list item, while others may prefer `-`.
 * It is common to  leave many unnecessary blank lines and spaces.
 * To make a divider, the number of `-` can be varying.

To this end, I think it would be nicer to encourage or demand contributors to format their markdown files before making a pull request.  Personally, I think Prettier ([https://prettier.io/)] is a good candidate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 09:03:49 UTC 2023,,,,,,,,,,"0|z1g3lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 08:30;martijnvisser;[~zhongpu314] Please open a discussion thread on the Dev mailing list for a topic like this, especially taking https://flink.apache.org/contributing/contribute-documentation.html and https://flink.apache.org/contributing/docs-style.html into account.;;;","22/Feb/23 09:03;zhongpu314;[~martijnvisser] Thanks, I have opened a thread here (https://lists.apache.org/thread/wdsf2n0pffzss9sr5wsqz2xgd2jdts9s).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correct the description of sql gateway configuration,FLINK-31176,13525676,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangkang,wangkang,wangkang,22/Feb/23 03:22,23/Feb/23 04:15,04/Jun/24 20:41,23/Feb/23 04:15,1.16.0,,,,,,,,,,,1.18.0,,,,Documentation,Table SQL / Gateway,,,0,pull-request-available,,,,"correct the description of sql gateway configuration:
1.sql-gateway.session.idle-timeout 、sql-gateway.session.check-interval description in SqlGatewayServiceConfigOptions
2.GetSessionConfigHeaders and TriggerSessionHeartbeatHeaders class description

!image-2023-02-22-11-17-08-611.png|width=717,height=289!

when setting  sql-gateway.session.idle-timeout  to  negative value,SqlGateway will throw NumberFormatException,beacause the TimeUtils.pasDuration method doesn't support the negative value,so we should remove the 'or negative value' description",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/23 03:17;wangkang;image-2023-02-22-11-17-08-611.png;https://issues.apache.org/jira/secure/attachment/13055708/image-2023-02-22-11-17-08-611.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 04:15:05 UTC 2023,,,,,,,,,,"0|z1g3jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 04:32;Wencong Liu;Thanks [~wangkang] ! I'll take a look.;;;","23/Feb/23 04:15;fsk119;Merged into master: b252abe179b5b4458e5306f7bdf8e484ac61d515;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix link doesn't work in hive_reaad_write doc,FLINK-31175,13525675,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,22/Feb/23 03:20,22/Feb/23 09:49,04/Jun/24 20:41,22/Feb/23 09:49,1.17.0,,,,,,,,,,,1.17.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 09:20:45 UTC 2023,,,,,,,,,,"0|z1g3j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 09:20;fsk119;Merged into master: 3a64195c495fe70e44c034309c37b238b4203c1c
Merged into release-1.17: fb901d19ebc392bbb29336c752e6c18776ba9036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Doc] Inconsistent data format between flink-training-repo and learn-flink doc,FLINK-31174,13525674,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,zhongpu314,zhongpu314,22/Feb/23 03:05,31/Aug/23 22:35,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Documentation / Training,,,,0,auto-deprioritized-minor,pull-request-available,,,"The data format specified in [flink-training-repo|[https://github.com/apache/flink-training/tree/release-1.16]] shows that a TaixRide is either a start or an end one with the eventTime.

However, the Java code in [Data Pipelines & ETL|[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/learn-flink/etl/]]  tried to get both ""startTime"" and ""endTime"". But in fact, those are not defined in flink-training-repo.
{code:java}
Interval rideInterval = new Interval(ride.startTime, ride.endTime); {code}
I think such inconsistency would puzzle new comers of Flink.

According to the mail list discussion, it is due to a new [pr|[https://github.com/apache/flink-training/pull/36]] and the doc forgets to update. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 22:35:10 UTC 2023,,,,,,,,,,"0|z1g3iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","31/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix several bugs in flink-ml-iteration module,FLINK-31173,13525668,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,22/Feb/23 02:31,20/Apr/23 09:48,04/Jun/24 20:41,20/Apr/23 08:40,ml-2.0.0,ml-2.1.0,ml-2.2.0,,,,,,,,,ml-2.3.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"In flink-ml-iteration, there are several bugs as follows:
 # TailOperator should have one input operator. We have added a Tail operator to increment the epoch watermark at each iteration. We have made an assumption that each Tail operator have only one input and did not align the epoch watermarks from different inputs. This assumption might not be true if the input is an `union`.
 # ProxyOperatorStateBackend does not correctly initialize the state descriptor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 08:40:20 UTC 2023,,,,,,,,,,"0|z1g3hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 08:40;lindong;Merged to apache/flink-ml master branch:
- 92ecb0e591f30ff7dc4bd4db027350ad4edf4000
- 0c852da7a1b87bc632e21a19148dbb8d19a8bd30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support coGroup in Python DataStream API,FLINK-31172,13525662,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,22/Feb/23 01:36,22/Feb/23 01:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,,API / Python,,,,0,,,,,"The aim of this ticket is support [coGroup|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/datastream/operators/overview/#window-cogroup] in Python DataStream API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-22 01:36:13.0,,,,,,,,,,"0|z1g3g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support iterate in Python DataStream API,FLINK-31171,13525661,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,22/Feb/23 01:34,22/Feb/23 01:34,04/Jun/24 20:41,,,,,,,,,,,,,,,,,API / Python,,,,0,,,,,"The aim of this ticket is to support [iterate|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/datastream/operators/overview/#iterate] in Python DataStream API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-22 01:34:11.0,,,,,,,,,,"0|z1g3g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The spelling error of the document word causes sql to fail to execute,FLINK-31170,13525564,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhangboyi,zhangboyi,21/Feb/23 13:14,22/Feb/23 00:38,04/Jun/24 20:41,,1.14.0,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"The spelling error of the document word causes sql to fail to execute



!image-2023-02-21-21-14-18-486.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 13:14;zhangboyi;image-2023-02-21-21-14-18-486.png;https://issues.apache.org/jira/secure/attachment/13055689/image-2023-02-21-21-14-18-486.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 00:38:54 UTC 2023,,,,,,,,,,"0|z1g2ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 00:38;zhangboyi;I'll fix that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesResourceManagerDriverTest.testOnPodDeleted fails fatally due to 239 exit code,FLINK-31169,13525560,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,mapohl,mapohl,21/Feb/23 12:57,22/Feb/23 10:06,04/Jun/24 20:41,22/Feb/23 09:56,1.17.0,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46341&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=27329

{code}
[...]
Feb 21 04:44:11 [ERROR] Process Exit Code: 239
Feb 21 04:44:11 [ERROR] Crashed tests:
Feb 21 04:44:11 [ERROR] org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest
Feb 21 04:44:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
[...]
{code}

{code}
[...]
Test org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.testOnPodDeleted[testOnPodDeleted()] is running.
--------------------------------------------------------------------------------
04:43:57,681 [ForkJoinPool-4-worker-1] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Recovered 0 pods from previous attempts, current attempt id is 1.
04:43:57,701 [testing-rpc-main-thread] INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
04:43:57,705 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Creating new TaskManager pod with name testing-flink-cluster-taskmanager-1-1 and resource <704,0.0>.
04:43:57,708 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Received new TaskManager pod: testing-flink-cluster-taskmanager-1-1
04:43:57,708 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Pod testing-flink-cluster-taskmanager-1-1 is created.
04:43:57,708 [testing-rpc-main-thread] WARN  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Pod testing-flink-cluster-taskmanager-1-1 is terminated before being scheduled.
04:43:57,709 [testing-rpc-main-thread] ERROR org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Error completing resource request.
org.apache.flink.util.FlinkException: Pod is terminated.
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.onPodTerminated(KubernetesResourceManagerDriver.java:379) ~[classes/:?]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$handlePodEventsInMainThread$2(KubernetesResourceManagerDriver.java:347) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
04:43:57,724 [testing-rpc-main-thread] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'testing-rpc-main-thread' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Pod is terminated.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.onPodTerminated(KubernetesResourceManagerDriver.java:379) ~[classes/:?]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$handlePodEventsInMainThread$2(KubernetesResourceManagerDriver.java:347) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Pod is terminated.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$requestResource$1(KubernetesResourceManagerDriver.java:233) ~[classes/:?]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
        ... 12 more
Caused by: org.apache.flink.util.FlinkException: Pod is terminated.
        ... 9 more
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-30908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 10:06:37 UTC 2023,,,,,,,,,,"0|z1g2tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 13:02;mapohl;release-1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46343&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=27329;;;","21/Feb/23 13:11;mapohl;[~xtsong] may you have a look at this? Looks like it's related to FLINK-30908;;;","22/Feb/23 02:59;xtsong;This is indeed caused by FLINK-30908. The ""pod termination before being scheduled"" is mis-treated as fatal error. I have provided a fix. Without the fix, I can reproduce the problem locally in about 200~300 runs. After the fix, I cannot reproduce the problem in 100k run.;;;","22/Feb/23 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46382&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26468;;;","22/Feb/23 09:56;xtsong;- master (1.18): d7cae5365d730272a4089988c235c2038eafab53
- release-1.17: bfadd9c69223765d485ed2371fa108b25756c1fc;;;","22/Feb/23 10:06;mapohl;The following build failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46385&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManagerHAProcessFailureRecoveryITCase failed due to job not being found,FLINK-31168,13525559,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,21/Feb/23 12:45,02/Aug/23 11:20,04/Jun/24 20:41,02/Aug/23 11:20,1.15.3,1.16.1,1.18.0,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46342&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=12706

We see this build failure because a job couldn't be found:
{code}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized
	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319)
	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1061)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:958)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:942)
	at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testJobManagerFailure(JobManagerHAProcessFailureRecoveryITCase.java:235)
	at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase$4.run(JobManagerHAProcessFailureRecoveryITCase.java:336)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1056)
	... 4 more
Caused by: java.lang.RuntimeException: Error while waiting for job to be initialized
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160)
	at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.lambda$execute$2(AbstractSessionClusterExecutor.java:82)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73)
	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
	at java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:479)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job 865dcd87f4828dbeb3d93eb52e2636b1 not found
	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99)
	at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986)
	at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:252)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)
	at akka.dispatch.OnComplete.internal(Future.scala:299)
	at akka.dispatch.OnComplete.internal(Future.scala:297)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (865dcd87f4828dbeb3d93eb52e2636b1)
	at org.apache.flink.runtime.dispatcher.Dispatcher.requestExecutionGraphInfo(Dispatcher.java:840)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 5 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25585,FLINK-15661,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 11:20:07 UTC 2023,,,,,,,,,,"0|z1g2tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 12:46;mapohl;I'm closing FLINK-25585 and FLINK-15561 and create this one as a follow-up because the other two issue seem to be related but lack logs from the documented builds.;;;","21/Feb/23 12:48;mapohl;This one was documented in FLINK-15561 which is still valid and covers the same issue:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42038&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7466

The CI build is related to the [FLINK-25554 1.16 backport PR|https://github.com/apache/flink/pull/21067]. Therefore, I'm gonna add 1.16 as an affected version as well.;;;","21/Feb/23 12:51;mapohl;There is a [user ML thread|https://lists.apache.org/thread/tws3r1oqwpc2n3v1dw5qvkzjzm27fgyd] that raises a similar stacktrace.;;;","21/Feb/23 16:52;mapohl;Based on the stacktrace listed in the description, the job is executed (see [JobManagerHAProcessFailureRecoveryITCase:235|https://github.com/apache/flink/blob/489827520b1a53db04a94346c98327d0d42301c5/flink-tests/src/test/java/org/apache/flink/test/recovery/JobManagerHAProcessFailureRecoveryITCase.java#L235] which submits the job and then waits for the initialization to be over. This is implemented in using {{{}ClientUtils.waitUntilJobInitializationFinished{}}} in [AbstractSessionClusterExecutor:82ff|https://github.com/apache/flink/blob/f4b59f615438e76c2b42999fc0a8ebce6a543b07/flink-clients/src/main/java/org/apache/flink/client/deployment/executors/AbstractSessionClusterExecutor.java#L82] utilizing a {{{}RestClusterClient{}}}'s {{getJobStatus}} and {{requestJobResult}} methods.

The actual error happened in [Dispatcher:480f|https://github.com/apache/flink/blob/9e908567ecc06b607d08fca8e2e781b463a38de6/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L840] where {{JobManagerRunner}} is already deregistered. Therefore, the {{requestExecutionGraphInfo}} fails returning an exceptionally completed future with the {{FlinkJobNotFoundException}}.

{{ClientUtils.waitUntilJobInitializationFinished}} uses {{new ExponentialWaitStrategy(50, 2000)}} as a {{WaitStrategy}}. My theory is, that the 2s of the wait strategy are too long and the job starts and finishes between two {{getJobStatus}} calls causing the corresponding {{JobManagerRunner}} to be removed and the {{getJobStatus}} call to fail as observed.;;;","22/Feb/23 16:27;mapohl;My previous conclusion doesn't hold because the {{requestExecutionGraphInfo}} shouldn't fail but rely on the {{ExecutionGraphInfoStore}} as a backup. A local test which increased the interval for the {{waitUntilJobInitializationFinished}} also succeeded as expected.;;;","22/Feb/23 16:35;mapohl;What is odd, though, is the location of the inital cause in the stacktrace:
{code}
[...]
Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (865dcd87f4828dbeb3d93eb52e2636b1)
	at org.apache.flink.runtime.dispatcher.Dispatcher.requestExecutionGraphInfo(Dispatcher.java:840)
[...]
{code}
The build mentioned in the Jira issue's description is based on {{c6b649bf}}. [Dispatcher:840 in c6b649bf|https://github.com/apache/flink/blob/c6b649bf/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L840] doesn't point to a code line that throws the {{FlinkJobNotFoundException}}. Line 841 would be a valid location. It looks like I'm missing something here. ;;;","26/Apr/23 09:32;Sergey Nuyanzin;master(1.18): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48442&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=13428;;;","16/May/23 08:14;renqs;[~mapohl] are you still working on this issue? Thanks;;;","16/May/23 08:40;mapohl;I didn't continue with that issue, no.;;;","13/Jun/23 08:23;renqs;Downgraded to major as the latest case happened two months ago. ;;;","29/Jun/23 10:50;Sergey Nuyanzin;new case 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50607&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=10348;;;","11/Jul/23 07:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51165&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=11042;;;","17/Jul/23 09:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51299&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=11285;;;","27/Jul/23 16:50;mapohl;The error appears in the [FileExecutionGraphInfoStore#getAvailableJobDetails(JobID)|https://github.com/apache/flink/blob/d78d52b27af2550f50b44349d3ec6dc84b966a8a/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/FileExecutionGraphInfoStore.java#L237] where the JobDetails are not cached. The {{FileSystemExecutionGraphInfoStore}} uses two caches:
* {{jobDetailsCache}} holds the {{JobDetails}} of each job in cache. It has a default expiration time of 3600s (see [jobstore.expiration-time default|https://github.com/apache/flink/blob/f3598c50c0d3dcdf8058b01f13b7eb9fc5954f7c/flink-core/src/main/java/org/apache/flink/configuration/JobManagerOptions.java#L340]). The size of the cache is limited to {{Integer.MAX_VALUE}} entries (see [jobstore.max-capacity default|https://github.com/apache/flink/blob/f3598c50c0d3dcdf8058b01f13b7eb9fc5954f7c/flink-core/src/main/java/org/apache/flink/configuration/JobManagerOptions.java#L350]). Removing entries from this cache will trigger the removal of the file and invalidates the corresponding entries in both caches. Removal can happen even earlier, though
* {{executionGraphInfoCache}} holds the {{ExecutionGraphInfo}} in the cache. It's limited to 50MB (see [jobstore.cache-size default|https://github.com/apache/flink/blob/f3598c50c0d3dcdf8058b01f13b7eb9fc5954f7c/flink-core/src/main/java/org/apache/flink/configuration/JobManagerOptions.java#L331]). The ExecutionGraphInfo will be reloaded from disk in case it was removed from the cache before.

This error can be simulated by reducing either the expiry time of the {{jobDetailsCache}} or decreasing the entry limit of that cache.

One suspicion is that the entry gets removed earlier. The [CacheBuilder#maximumSize JavaDoc|https://guava.dev/releases/19.0/api/docs/com/google/common/cache/CacheBuilder.html#maximumSize(long)] states that this can happen:
{quote}
Note that the cache may evict an entry before this limit is exceeded.
{quote}

We should investigate whether the error started to appear with the {{flink-shaded}} update (FLINK-30772, FLINK-32032) where the update might have included a change in how the cache operates. I will continue investigating this tomorrow.;;;","28/Jul/23 06:45;mapohl;My suspicion doesn't seem to hold: I couldn't identify any change between guava 30.1 and guava 31.1 (I checked the sources and the release notes) that would have been an explanation for different eviction behavior. Additionally, the [CacheBuilder#maximumSize JavaDoc|https://guava.dev/releases/19.0/api/docs/com/google/common/cache/CacheBuilder.html#maximumSize(long)] states that entries are only evicted if the cache is getting close to its limit (which is {{Integer.MAX_VALUE}}). The test itself only runs a single job which would lead to one entry in the cache.;;;","28/Jul/23 07:20;mapohl;For the record: The reported cases do not always have the same cause.
||Build||Branch||Comment||Description||
|[20230221.2 (#46342)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46342&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=12706]|release-1.15 (c6b649bf)|Jira issue description|Job was not found during initialization phase of the job|
|[20221014.20 (#42038)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42038&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9922]|1.16 PR|1st report in comments|Error after job finished|
|[20232504.1 (#48442)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48442&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=12170]|master (1.18)|2nd report in comments|Error after job finished|
|[20230629.1 (#50607)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50607&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9363]|master (74ae4b24)|3rd report in comments|Ask timeout|
|[20230711.1 (#51165)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51165&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9898]|master (4cf2124d)|4th report in comments|Error while restarting the dispatcher (job not finished)|
|[20230717.1 (#51299)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51299&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=10039]|master (4690dc29)|5th report in comments|Error while restarting the dispatcher (job not finished)|

[20230711.1 (#51165)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51165&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9898] is the first failure that included the flink-shaded 17.0 upgrade (FLINK-32032).;;;","28/Jul/23 08:45;mapohl;The most-recent failures seem to have been caused by the job recovery not being successful:
* [20230711.1 (#51165) Dispatcher #1 output in line 8688|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51165&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8688]
* [20230717.1 (#51299) Dispatcher #1 output in line 9918|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51299&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9918]

I'm gonna raise the priority of this issue to blocker because it could be related to the leader election changes.

The job is not picked up anymore and therefore, cannot be saved in the {{ExecutionGraphInfoStore}}. The job client will wait for the initialization phase to be over and then requests the JobResult which calls {{Dispatcher.requestJobStatus}}. {{requestJobStatus}} won't find a {{JobManagerRunner}} in {{Dispatcher#jobManagerRunnerRegistry}} and non in the {{Dispatcher#executionGraphInfoStore}} (see [Dispatcher#requestJobStatus|https://github.com/apache/flink/blob/ab9445aca56e7d139e8fd9bcc23e5f7e06288e66/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L902]). Therefore, the response future will complete exceptionally with a {{FlinkJobNotFoundException}} causing the error which we're seeing in the last two CI failures.;;;","28/Jul/23 09:43;mapohl;It appears that the leader election is not causing the problem: The JobGraph is written properly to the JobGraphStore under {{flink/default/jobgraphs}} in the JM run #0 (see [log line in build #51299|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51299&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9106]):
{code}
Jul 17 01:09:58 01:09:47,954 10630 [flink-akka.actor.default-dispatcher-20] INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore [] - Added JobGraph(jobId: 10ea837b5ff5916e57e0f49298d952d3) to ZooKeeperStateHandleStore{namespace='flink/default/jobgraphs'}.
{code}

A leader is picked in JM run #1 that tries to recover the data from the correct ZNode {{flink/default/jobgraphs}} (see [log line in build #51299|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51299&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9917]). But it appears that the ZNode is empty:
{code}
Jul 17 01:09:59 01:09:55,657 6184 [cluster-io-thread-2] INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore [] - Retrieved job ids [] from ZooKeeperStateHandleStore{namespace='flink/default/jobgraphs'}
Jul 17 01:09:59 01:09:55,657 6184 [cluster-io-thread-2] INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
{code};;;","28/Jul/23 09:49;mapohl;So far, I wasn't able to reproduce it locally with up to now 300 test runs.

Update: The test runs happened with JDK11. I noticed that the two test failures in question happened with JDK17. I'm gonna continue looking into the issue in that direction.;;;","28/Jul/23 12:19;mapohl;(y) I was able to reproduce this test failure using JDK17 (after 3 runs and after 74 runs). I'm gonna go ahead and investigate it further.;;;","29/Jul/23 10:09;mapohl;the suspicion is now that the process destruction is being prevented by JDK17. There's an entry in the logs that suggests that:
{code}
01:09:49,250 [                main] ERROR org.apache.flink.runtime.testutils.TestJvmProcess            [] - Failed to forcibly destroy process
java.lang.reflect.InaccessibleObjectException: Unable to make public java.lang.Process java.lang.ProcessImpl.destroyForcibly() accessible: module java.base does not ""opens java.lang"" to unnamed module @19dc67c2
        at java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354) ~[?:?]
        at java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297) ~[?:?]
        at java.lang.reflect.Method.checkCanSetAccessible(Method.java:199) ~[?:?]
        at java.lang.reflect.Method.setAccessible(Method.java:193) ~[?:?]
        at org.apache.flink.runtime.testutils.TestJvmProcess.destroy(TestJvmProcess.java:214) ~[flink-runtime-1.18-SNAPSHOT-tests.jar:1.18-SNAPSHOT]
        at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure(JobManagerHAProcessFailureRecoveryITCase.java:367) ~[test-classes/:?]
        at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
        [...]
{code}
The {{TestJvmProcess}} class actually tries to destroy the process forcefully first through reflection (see [apache/flink:org.apache.flink.runtime.testutils.TestJvmProcess:214ff|https://github.com/apache/flink/blob/2940c02c986e3d70708187091bf006806bb90dff/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/TestJvmProcess.java#L214]) and calls {{destroy}} afterwards if the forced approach didn't work without waiting for the process to finish. That could mean that the process gets destroyed eventually but has enough time to finish the job in the first run (i.e. cleaning the jobgraph up at the end) before the second JM process is started.

Two interesting observations, though:
* Running the test under JDK17 locally multiple times seems to create daemon child processes under the intellij process (which means that some process is still not properly destroyed)
* The dispatcher logs of the first run don't reveal the finishing of the job. I have to check whether the {{Process}} class stops sending the logs to the pipe before triggering the process destruction (that would explain that the logs are not complete);;;","31/Jul/23 07:39;mapohl;jdk17 again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51817&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=11234;;;","31/Jul/23 08:44;mapohl;I'm lowering the priority of this issue again. We should wait for the JM process to finish before triggering the follow-up JM. The job occassionally finishing before the JM process is destroyed seems to be the cause of this test instability.;;;","02/Aug/23 11:20;mapohl;master: 476d68814ab606a86e376b1649713a7661fb12e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify that no exclusions were erroneously added to the japicmp plugin,FLINK-31167,13525543,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,21/Feb/23 11:08,20/Mar/23 14:41,04/Jun/24 20:41,20/Mar/23 14:41,,,,,,,,,,,,,,,,,,,,0,,,,,"Verify that no exclusions were erroneously added to the japicmp plugin that break compatibility guarantees. Check the exclusions for the japicmp-maven-plugin in the root pom (see [apache/flink:pom.xml:2175ff|https://github.com/apache/flink/blob/3856c49af77601cf7943a5072d8c932279ce46b4/pom.xml#L2175] for exclusions that:
* For minor releases: break source compatibility for {{@Public}} APIs
* For patch releases: break source/binary compatibility for {{@Public}}/{{@PublicEvolving}}  APIs
Any such exclusion must be properly justified, in advance.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27919,FLINK-30798,FLINK-29807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 14:41:09 UTC 2023,,,,,,,,,,"0|z1g2ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 11:15;mapohl;FLINK-29807 created a exclusion that was necessary due to [9bf0d9f2|https://github.com/apache/flink/commit/9bf0d9f2]. Reasoning:
{quote}
UnionSerializerConfigSnapshot was a PublicEvolving and Deprecated class that has been removed, embedded inside a Public CoGroupedStreams class, triggering this false failure
{quote};;;","21/Feb/23 11:18;mapohl;FLINK-27919 introduced a new method {{SourceReaderContext.currentParallelism}} with a default implementation in [a5667e82|https://github.com/apache/flink/commit/a5667e82#diff-a247a24ccd1afc07c5d690a8a58b1f6584329925fdf0d7dc89361b90d621b7f2R72]. This is actually not a breaking change because a new method is added with a default implementation. [~afedulov] can you confirm the reasoning?;;;","21/Feb/23 11:28;mapohl;FLINK-30798 introduced a few exclusions:
* [642d28ec|https://github.com/apache/flink/commit/642d28ec] for {{FinalizeOnMaster}}: why do we add an empty default implementation for {{FinalizeOnMaster.finalizeGlobal(int)}}. Isn't only the {{finalizeGlobal(FinalizationContext)}} default implementation necessary?
* [74ef6be4|https://github.com/apache/flink/commit/74ef6be4] for {{OutputFormat}}: the same question applies here for the deprecated {{OutputFormat.open(int, int)}}.

[~SleePy] I guess it is to make new implementations easier?
;;;","22/Feb/23 07:14;SleePy;Hi [~mapohl] ,

As you mentioned, the new method with context were introduced to replace the old one.

Let's take the {{finalizeGlobal}} as an example. If we do not provide the default implementation for {{finalizeGlobal(int)}} and someone wants to use the new method with context, he has to implement both {{finalizeGlobal(int)}} and {{finalizeGlobal(FinalizationContext)}} even the former one means nothing to him.

That's not we expect users to do. The {{finalizeGlobal(int)}} works for the legacy codes, and {{finalizeGlobal(FinalizationContext)}} works for the new implementation. Users only need to implement one of them. It's sort of like how SinkFunction is handled in https://issues.apache.org/jira/browse/FLINK-7552.

I think it's not a breaking change however it could not pass the japicmp checking.;;;","22/Feb/23 15:44;mapohl;[~SleePy] thanks for your explanation and the link to FLINK-7552. That makes actually sense. I'm still trying to get my head around the API backwards compatibility. That's why I'm asking to be on the safe side. One more thing: wouldn't we be more stricter by throwing an {{UnsupportedOperationException}} as the default behavior of the deprecated method implementation? Or am I missing some points here?;;;","03/Mar/23 08:40;SleePy;Hi [~mapohl] , sorry for the late response.
{quote} wouldn't we be more stricter by throwing an UnsupportedOperationException as the default behavior of the deprecated method implementation?
{quote}
I think it's considerable choice. There is only one scenario i'm worrying about. If someone implements a new {{{}OutputFormat{}}}, and the IDE does not generate the {{open}} method automatically somehow. The codes would pass the compiling but fail in runtime. It's not so friendly. So I prefer the less stricter choice a bit. What do you think?;;;","20/Mar/23 14:41;mapohl;I guess it's reasonable considering that we used the same approach in the past. I consider this issue to be resolved since the same argument can be applied to {{SourceReaderContext}} change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
array_contains does NOT work when haystack elements are not nullable and needle is nullable,FLINK-31166,13525538,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,21/Feb/23 10:41,09/Mar/23 23:43,04/Jun/24 20:41,09/Mar/23 23:43,1.18.0,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"{{ARRAY_CONTAINS}} works ok for the case when both haystack elements and needle are not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], 0);{code}
it works ok when both haystack elements and needle are nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));{code}
it works ok when haystack elements are nullable and needle is not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], 1);{code}
and it does NOT work when haystack elements are not nullable and needle is nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));{code}
 

!image-2023-02-22-09-56-59-257.png!

 

!image-2023-02-21-18-41-19-385.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 10:37;jackylau;image-2023-02-21-18-37-45-202.png;https://issues.apache.org/jira/secure/attachment/13055681/image-2023-02-21-18-37-45-202.png","21/Feb/23 10:41;jackylau;image-2023-02-21-18-41-19-385.png;https://issues.apache.org/jira/secure/attachment/13055679/image-2023-02-21-18-41-19-385.png","22/Feb/23 01:56;jackylau;image-2023-02-22-09-56-59-257.png;https://issues.apache.org/jira/secure/attachment/13055702/image-2023-02-22-09-56-59-257.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 23:43:14 UTC 2023,,,,,,,,,,"0|z1g2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 12:20;martijnvisser;[~jackylau] Can you please add a reproducible example for this?;;;","22/Feb/23 01:56;jackylau;[~martijnvisser] 

select array_contains(array[1, 2, 3], cast(null as int));;;;","22/Feb/23 07:58;martijnvisser;[~jackylau] So this is specifically about the behaviour when the needle (given ARRAY_CONTAINS(haystack, needle) is the function) is null? ;;;","09/Mar/23 10:22;Sergey Nuyanzin;[~jackylau] can you please update jira description with some example e.g. from the PR.
Having screenshots in description does not help much since it is impossible to copy&paste and run on Flink.

if i understand correctly from the PR:
{{ARRAY_CONTAINS}} works ok for the case when both haystack elements and needle are not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], 0);{code}
it works ok when both haystack elements and needle are nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));{code}
it works ok when haystack elements are nullable and needle is not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], 1);{code}
and it does NOT work when haystack elements are not nullable and needle is nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));{code}
I'm asking since for me 
in fact it was impossible to get to this from jira description reading and I still want to double check if I understand it correctly;;;","09/Mar/23 11:04;jackylau;hi [~Sergey Nuyanzin] thanks for your description. yeap, you are right. and i will describe the Jira will later ;;;","09/Mar/23 11:06;jackylau;have fixed this pr info [~Sergey Nuyanzin] , thanks for your detail review very much and i have learned a lot ;;;","09/Mar/23 23:43;Sergey Nuyanzin;Merged as [10dce7cf0a04b80d7416a5760e1a6dbc430d9f88|https://github.com/apache/flink/commit/10dce7cf0a04b80d7416a5760e1a6dbc430d9f88];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Over Agg: The window rank function without order by error in top N query,FLINK-31165,13525522,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,rohankrao,rohankrao,21/Feb/23 08:54,10/Apr/23 13:06,04/Jun/24 20:41,30/Mar/23 02:00,1.16.0,,,,,,,,,,,1.17.1,1.18.0,,,Table SQL / API,,,,0,pull-request-available,,,," 
{code:java}
val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

val tableEnv = StreamTableEnvironment.create(env)


val td = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""NAME"", DataTypes.VARCHAR(2147483647))
    .column(""ROLLNO"", DataTypes.DECIMAL(5, 0))
    .column(""DOB"", DataTypes.DATE())
    .column(""CLASS"", DataTypes.DECIMAL(2, 0))
    .column(""SUBJECT"", DataTypes.VARCHAR(2147483647))
    .build())
  .build()

val table = tableEnv.from(td)


tableEnv.createTemporaryView(""temp_table"", table)

val newTable = tableEnv.sqlQuery(""select temp_table.*,cast('2022-01-01' as date) SRC_NO from temp_table"")

tableEnv.createTemporaryView(""temp_table2"", newTable)


val newTable2 = tableEnv.sqlQuery(""select * from (select NAME,ROLLNO,row_number() over (partition by NAME ORDER BY SRC_NO) AS rownum  from temp_table2 a) where rownum <= 1"")

tableEnv.toChangelogStream(newTable2).print()

env.execute()
 {code}
 

 

I am getting the below error if I run the above code.

I have already provided an order by column.

If I change the order by column to some other column, such as ""SUBJECT"", then the job runs fine.

 

 
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL), args [rel#245:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#244,window#0=window(partition {0} rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()]))]
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
    at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
    at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.scala:160)
    at org.example.OverAggregateBug$.main(OverAggregateBug.scala:39)
    at org.example.OverAggregateBug.main(OverAggregateBug.scala)
Caused by: org.apache.flink.table.api.ValidationException: Over Agg: The window rank function without order by. please re-check the over window statement.
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$2(FlinkLogicalOverAggregate.scala:95)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$2$adapted(FlinkLogicalOverAggregate.scala:92)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$1(FlinkLogicalOverAggregate.scala:92)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$1$adapted(FlinkLogicalOverAggregate.scala:89)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.convert(FlinkLogicalOverAggregate.scala:89)
    at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:167)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
    ... 27 more {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 02:00:47 UTC 2023,,,,,,,,,,"0|z1g2l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 10:07;martijnvisser;[~godfrey] [~lincoln.86xy] Any thoughts on this one?;;;","22/Mar/23 03:42;qingyue;Hi [~rohankrao] , thanks for reporting this issue.

Actually, the order by field SRC_NO is a constant and is folded during query rewrite. See [LogicalWindow.java#L371|https://github.com/apache/flink/blob/268fc1a46f8af171c7102229a010af71c56623d0/flink-table/flink-table-planner/src/main/java/org/apache/calcite/rel/logical/LogicalWindow.java#L371], when applying Calcite's ProjectToWindowRule.
{code:java}
LogicalProject(inputs=[0..2])
+- LogicalFilter(condition=[<=($2, 1)])
   +- LogicalProject(inputs=[0..1], exprs=[[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY 2022-01-01 NULLS FIRST)]])
      +- LogicalTableScan(table=[[default_catalog, default_database, temp_table]]) {code}
From the perspective of SQL semantics, using a constant as the order by key for row_number has no meaning, because the constant will not change the sorting result of row_number, and each row will get the same rank. As a current workaround, please try to specify another field to ensure that row_number is sorted in the correct order.

While I tested the query against MySQL, PostgreSQL, SQLServer, Spark, and Hive. SQLServer will throw an exception that ""Windowed functions and NEXT VALUE FOR functions do not support constants as ORDER BY clause expressions.""; the rest do allow this to happen, and just output the first inserted row. Do you think we need to align this behavior? Or at least throwing a more meaningful error. cc [~godfreyhe]  [~lincoln.86xy] 

 

 ;;;","22/Mar/23 06:01;qingyue;I rethink it, from the streaming semantics the result might be non-deterministic if supporting order by constants. So I suggest throwing a meaningful error to indicate users not to use constants as the order by key. WDYT? [~godfrey] [~lincoln.86xy] ;;;","22/Mar/23 12:23;lincoln.86xy;[~qingyue] can you verify the behavior of this case under flink batch? Personally I prefer to keep a unified behavior on streaming and batch, non-determinism should not be the only reason to reject the query, because similar proctime based computations on streaming are also mostly non-deterministic, WDYT?;;;","22/Mar/23 13:05;qingyue;The current behavior under the Flink batch is the same as under streaming mode since this rewrite rule is applied during the LogicalWindow creation. 

My concern mainly comes from the implementation aspect. The reason that caused this problem is the use of constant folding optimization when creating LogicalWindow, which leads to the orderByKey being empty when passed to FlinkLogicalOverAggregateConverter.

There are two possible solutions. The first one is to remove the constant folding optimization or add some judgment here, such as giving up optimization when orderByKey becomes empty after optimization. The second one is to remove the check of orderByKey in FlinkLogicalOverAggregateConverter, but then the problem becomes how to distinguish between order by constants and no order by clause. ;;;","23/Mar/23 03:01;lincoln.86xy;[~qingyue] If there is a large cost on implementation, then I would prefer to optimize the current error message to prompt user for a clearer indication. [~godfreyhe] WDYT?;;;","29/Mar/23 10:12;godfrey;[~lincoln.86xy] [~qingyue] I prefer to just improve the error message in FlinkLogicalOverAggregateConverter, and it can be: The window rank function requires the order by with variable column.;;;","29/Mar/23 10:23;qingyue;I agree with the error msg improvement, and I'd like to do this task. Cc [~lincoln.86xy] [~godfrey] ;;;","29/Mar/23 10:25;lincoln.86xy;[~qingyue] thanks for driving this! assigned to you.;;;","30/Mar/23 02:00;lincoln.86xy;fixed in master: eeb446c0ed07c38175efdadf7e2e21702ff02b70
release-1.17: 7cf6150f1b7660198fb03aea31d13030a01744d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected correlate variable $cor0 in the plan error in where clause,FLINK-31164,13525512,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,rohankrao,rohankrao,21/Feb/23 07:45,21/Feb/23 08:34,04/Jun/24 20:41,21/Feb/23 08:34,1.16.0,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"{code:java}
val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

val tableEnv = StreamTableEnvironment.create(env)


val accountsTd = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""account_num"", DataTypes.VARCHAR(2147483647))
    .column(""acc_name"", DataTypes.VARCHAR(2147483647))
    .column(""acc_phone_num"", DataTypes.VARCHAR(2147483647))
    .build())
  .build()

val accountsTable = tableEnv.from(accountsTd)

tableEnv.createTemporaryView(""accounts"", accountsTable)


val transactionsTd = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""account_num"", DataTypes.VARCHAR(2147483647))
    .column(""transaction_place"", DataTypes.VARCHAR(2147483647))
    .column(""transaction_time"", DataTypes.BIGINT())
    .column(""amount"", DataTypes.INT())
    .build())
  .build()

val transactionsTable = tableEnv.from(transactionsTd)

tableEnv.createTemporaryView(""transaction_data"", transactionsTable)




val newTable = tableEnv.sqlQuery(""select   acc.account_num,  (select count(*) from transaction_data where transaction_place = trans.transaction_place and account_num = acc.account_num)  from  accounts acc,transaction_data trans"")

tableEnv.toChangelogStream(newTable).print()

env.execute() {code}
I get the following error if I run the above code.

 
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: unexpected correlate variable $cor0 in the plan
    at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.checkCorrelVariableExists(FlinkDecorrelateProgram.scala:59)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.optimize(FlinkDecorrelateProgram.scala:42)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1(FlinkGroupProgram.scala:56)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1$adapted(FlinkGroupProgram.scala:51)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.immutable.Range.foreach(Range.scala:158)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:51)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
    at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.scala:160)
    at org.example.WhereClauseBug$.main(WhereClauseBug.scala:50)
    at org.example.WhereClauseBug.main(WhereClauseBug.scala)
{code}",,,,,,,,,,,,,,,,,,,,,FLINK-31163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-21 07:45:46.0,,,,,,,,,,"0|z1g2iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected correlate variable $cor0 in the plan error in where clause,FLINK-31163,13525511,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rohankrao,rohankrao,21/Feb/23 07:45,10/Apr/24 20:43,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"{code:java}
val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

val tableEnv = StreamTableEnvironment.create(env)


val accountsTd = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""account_num"", DataTypes.VARCHAR(2147483647))
    .column(""acc_name"", DataTypes.VARCHAR(2147483647))
    .column(""acc_phone_num"", DataTypes.VARCHAR(2147483647))
    .build())
  .build()

val accountsTable = tableEnv.from(accountsTd)

tableEnv.createTemporaryView(""accounts"", accountsTable)


val transactionsTd = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""account_num"", DataTypes.VARCHAR(2147483647))
    .column(""transaction_place"", DataTypes.VARCHAR(2147483647))
    .column(""transaction_time"", DataTypes.BIGINT())
    .column(""amount"", DataTypes.INT())
    .build())
  .build()

val transactionsTable = tableEnv.from(transactionsTd)

tableEnv.createTemporaryView(""transaction_data"", transactionsTable)




val newTable = tableEnv.sqlQuery(""select   acc.account_num,  (select count(*) from transaction_data where transaction_place = trans.transaction_place and account_num = acc.account_num)  from  accounts acc,transaction_data trans"")

tableEnv.toChangelogStream(newTable).print()

env.execute() {code}
I get the following error if I run the above code.

 
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: unexpected correlate variable $cor0 in the plan
    at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.checkCorrelVariableExists(FlinkDecorrelateProgram.scala:59)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.optimize(FlinkDecorrelateProgram.scala:42)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1(FlinkGroupProgram.scala:56)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1$adapted(FlinkGroupProgram.scala:51)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.immutable.Range.foreach(Range.scala:158)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:51)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
    at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.scala:160)
    at org.example.WhereClauseBug$.main(WhereClauseBug.scala:50)
    at org.example.WhereClauseBug.main(WhereClauseBug.scala)
{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-31164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 20:43:17 UTC 2024,,,,,,,,,,"0|z1g2io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 08:33;martijnvisser;[~lincoln.86xy] [~godfrey] WDYT?;;;","17/May/23 15:36;Sergey Nuyanzin;Partially it is fixed within CALCITE-4913 and could come with Calcite 1.31.0 upgrade FLINK-28744
Partially need to debug/fix a bit correlation vars namespace resolution (probably on Calcite level);;;","12/Sep/23 16:50;Sergey Nuyanzin;--[~rohankrao]   it seems after upgrade to Calcite 1.32.0 and some other fixes it is not an issue anymore for 1.18-
-Could you please double check it?--

yes, seems still an issue, will have a look;;;","10/Apr/24 20:43;jeyhunkarimov;When I tried to reproduce the issue (as of 3590c2d86f4186771ffcd64712f756d31306eb88), the given query in the issue executes and exits without any exceptions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid setting private tokens to AM container context when kerberos delegation token fetch is disabled,FLINK-31162,13525502,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,21/Feb/23 06:20,23/Feb/23 12:12,04/Jun/24 20:41,23/Feb/23 12:12,1.16.1,,,,,,,,,,,1.16.2,,,,Deployment / YARN,,,,0,pull-request-available,,,,"In our internal env, we have enabled [Consistent Reads from HDFS Observer NameNode|https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html]. With this, some of the _ObserverReadProxyProvider_ implementation clone the delegation token for HA service and mark those tokens private so that they won't be accessible through _ugi.getCredentials()._

But Flink internally uses _currUsr.getTokens()_ [here|https://github.com/apache/flink/blob/release-1.16.1/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java#L222] to get the current user credentials tokens to be set in AM context for submitting the YARN app to RM.

This fails with the following error:
{code:java}
Unable to add the application to the delegation token renewer.
java.io.IOException: Failed to renew token: Kind: HDFS_DELEGATION_TOKEN, Service: test01-ha4.abc:9000, Ident: (HDFS_DELEGATION_TOKEN token 151335106 for john)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:495)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$900(DelegationTokenRenewer.java:79)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:939)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:916)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby. Visit https://s.apache.org/sbnn-error
at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:108)
at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:2044)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1451)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:5348)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:733)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:1056)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:525)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:495)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1038)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1003)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:931)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1905)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2856)

at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
at org.apache.hadoop.ipc.Client.call(Client.java:1445)
at org.apache.hadoop.ipc.Client.call(Client.java:1342)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
at com.sun.proxy.$Proxy87.renewDelegationToken(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewDelegationToken(ClientNamenodeProtocolTranslatorPB.java:986)
at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy88.renewDelegationToken(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$Renewer.renew(DFSClient.java:761)
at org.apache.hadoop.security.token.Token.renew(Token.java:466)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:629)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:626)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1905)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:625)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:481)
... 6 more
{code}
Based on the [code comment here in HAUtilClient.java|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HAUtilClient.java#L128], it seems like the user credentials should be obtained using _ugi.getCredentials()_ instead of {_}ugi.getTokens(){_}. Also Spark seems to use _ugi.getCredentials()_ [here|https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L348] to set the credentials obtained to AM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 12:12:28 UTC 2023,,,,,,,,,,"0|z1g2go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 12:12;martijnvisser;Fixed in release-1.16: d5c0944b8eacdb5d842c2aa443621442f6684244;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade MojoHaus Versions Maven Plugin to 2.14.2,FLINK-31161,13525496,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,21/Feb/23 04:19,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,1.20.0,,,,Build System,,,,0,pull-request-available,,,,"when we use multiple project, the parrent struct like this [https://stackoverflow.com/questions/39449275/update-parent-version-in-a-maven-projects-module.]

when i use 

mvn org.codehaus.mojo:versions-maven-plugin:2.8.1:update-parent  -DparentVersion=[1.15.2.1] -DallowSnapshots

could not change parrent version.

!image-2023-02-21-12-17-20-195.png!

 

it is fixed added by skipResolution by upgrading to 2.14.2  [https://www.mojohaus.org/versions/versions-maven-plugin/update-parent-mojo.html]
{code:java}
 mvn org.codehaus.mojo:versions-maven-plugin:2.14.2:update-parent  -DparentVersion=[1.15.2.1] -DallowSnapshots -DskipResolution{code}
[https://github.com/mojohaus/versions/tree/2.14.2]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 04:17;jackylau;image-2023-02-21-12-17-20-195.png;https://issues.apache.org/jira/secure/attachment/13055655/image-2023-02-21-12-17-20-195.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 03:04:42 UTC 2023,,,,,,,,,,"0|z1g2fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 04:27;jackylau;hi [~chesnay] , could you help to review it?;;;","23/Feb/23 03:04;taoran;hi, guys. i got same issue. in my company, we have a extended project inherit from flink, we can not use mvn versions:set to change version cause by 

[ERROR] Failed to execute goal org.codehaus.mojo:versions-maven-plugin:2.15.0:set (default-cli) on project flink-plus: Project version is inherited from parent. -> [Help 1].



because current plugin can not change inherited parent version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support join/cogroup in BroadcastUtils.withBroadcastStream,FLINK-31160,13525484,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,21/Feb/23 01:51,19/Apr/23 09:05,04/Jun/24 20:41,19/Apr/23 09:05,ml-2.2.0,,,,,,,,,,,ml-2.3.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Currently BroadcastUtils#withBroadcastStream does not support cogroup/join, since we restricted that users can only introduce one extra operator in the specified lambda function.

 

To support using join/cogroup in BroadcastUtils#withBroadcastStream, we would like to relax the restriction such that users can introduce more than more extra operator, but only the output operator can access the broadcast variables.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 09:05:38 UTC 2023,,,,,,,,,,"0|z1g2co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 09:05;lindong;Merged to apache/flink-ml master branch e843b300b47a1ee3446296a359528a6b39566eed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-30583 Provide the flame graph to the subtask level,FLINK-31159,13525477,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Hong Teoh,liangtl,liangtl,20/Feb/23 21:20,21/Feb/23 08:23,04/Jun/24 20:41,21/Feb/23 08:23,1.17.0,,,,,,,,,,,1.17.0,,,,Runtime / REST,Runtime / Web Frontend,,,0,,,,,"The issue aims to verify FLINK-30583.

Please verify:
 # When below conditions are met:
 ** Job has more than 1 parallel subtask
 ** Some subtasks are busy, whilst others are not (e.g. some receive records, some don't)
 ** The FlameGraph accurately reflects the busy subtask",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/23 21:30;liangtl;Map_record_distribution.png;https://issues.apache.org/jira/secure/attachment/13055648/Map_record_distribution.png","20/Feb/23 21:32;liangtl;all_subtasks.png;https://issues.apache.org/jira/secure/attachment/13055649/all_subtasks.png","20/Feb/23 21:32;liangtl;subtask0.png;https://issues.apache.org/jira/secure/attachment/13055651/subtask0.png","20/Feb/23 21:32;liangtl;subtask3.png;https://issues.apache.org/jira/secure/attachment/13055650/subtask3.png",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 08:23:02 UTC 2023,,,,,,,,,,"0|z1g2b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 21:34;liangtl;h2. Test Outline

We want to submit a job that has parallelism > 1 and only 1 subtask is busy.

We want to verify FlameGraph correctly reflects the busy subtask.

 
h2. Test Setup

Flink job does the following:
 * Source from a Kinesis stream
 * Map with busy loop to calculate factorials
 * Discarding sink

 
{code:java}
public static long factorialUsingStreams(int n) {
     return LongStream.rangeClosed(1, n)
        .reduce(1, (long x, long y) -> x * y);
}

public static void main(String[] args) throws Exception {
  // ...
  env.addSource(new FlinkKinesisConsumer<>(
        STREAM, new SimpleStringSchema(), consumerConfig))
    .map((MapFunction<String, String>) value -> {
        for (int i = 0; i < 10; i++) {
            factorialUsingStreams(25);
        }
        return value;
     })
    .addSink(new DiscardingSink<>());
  env.execute(""Busy loop"");
}{code}
Flink job has parallelism of 4

Kinesis source stream only has 1 shard, so only 1 subtask will be reading records.

This means only 1 subtask will be busy.

 
h2. Results

Verified that the Flame graph accurately reflects this. See snapshots below
 - Only subtask 3 has records

!Map_record_distribution.png|width=1090,height=550!
 * Flame graph with ""all"" setting

!all_subtasks.png|width=1083,height=562!
 * FlameGraph with subtask 0 shows nothing on CPU

!subtask0.png|width=1080,height=550!
 * FlameGraph from subtask 3 shows CPU usage as shown on ""all""

!subtask3.png|width=1080,height=572!

 

 ;;;","21/Feb/23 02:09;qingyue;Thanks [~liangtl] for creating this, I'd like to do this testing work.;;;","21/Feb/23 06:52;renqs;[~qingyue] Thanks for taking the issue! I've assigned this one to you.;;;","21/Feb/23 08:23;liangtl;This issue has been completed! Sorry for the churn [~qingyue] + [~renqs] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vote on the release candidate,FLINK-31158,13525473,13525466,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,mapohl,mapohl,20/Feb/23 19:33,23/Mar/23 07:22,04/Jun/24 20:41,23/Mar/23 07:22,1.17.0,,,,,,,,,,,1.17.0,,,,,,,,0,,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 07:21:48 UTC 2023,,,,,,,,,,"0|z1g2a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 07:21;leonard;The vote passed in the mail list: 

https://lists.apache.org/thread/cgz83tktlg25klf9fm9m3bp0n1dzwm5n;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propose a pull request for website updates,FLINK-31157,13525471,13525466,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,mapohl,mapohl,20/Feb/23 19:24,23/Mar/23 08:08,04/Jun/24 20:41,23/Mar/23 07:14,1.17.0,,,,,,,,,,,1.17.0,,,,,,,,0,pull-request-available,,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 07:12:42 UTC 2023,,,,,,,,,,"0|z1g29s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 07:12;leonard;Release note: 
flink master(1.18) : 103e5f9fd0fe057b7861876e141203169fda8197
flink 1.17: 2a38aab51715b7b4be6d9293eb7574bc908dc2e4

Release announcement:

flink-web asf-site: ffe20805aa6a70cd5f6b8c5b7046f5e0c80bef3f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage source and binary releases on dist.apache.org,FLINK-31156,13525469,13525466,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,mapohl,mapohl,20/Feb/23 19:20,10/Mar/23 03:09,04/Jun/24 20:41,10/Mar/23 03:09,,,,,,,,,,,,,,,,,,,,0,,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 03:09:30 UTC 2023,,,,,,,,,,"0|z1g29c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 03:09;renqs;* Source and binary: [https://dist.apache.org/repos/dist/dev/flink/flink-1.17.0-rc1/]
 * Tag {{{}release-1.17.0-rc1{}}}: [https://github.com/apache/flink/releases/tag/release-1.17.0-rc1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build and stage Java and Python artifacts,FLINK-31155,13525468,13525466,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,mapohl,mapohl,20/Feb/23 19:12,10/Mar/23 03:09,04/Jun/24 20:41,10/Mar/23 03:07,,,,,,,,,,,,,,,,,,,,0,,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 03:07:33 UTC 2023,,,,,,,,,,"0|z1g294:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 03:07;renqs;* [Key|https://dist.apache.org/repos/dist/release/flink/KEYS] fingerprint: A1BD477F79D036D2C30CA7DBCA8AEEC2F6EB040B
 * Maven central deployment:  [https://repository.apache.org/content/repositories/orgapacheflink-1591]
 * Tag {{{}release-1.17.0-rc1{}}}: [https://github.com/apache/flink/releases/tag/release-1.17.0-rc1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.17.0-rc1,FLINK-31154,13525466,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,mapohl,mapohl,20/Feb/23 18:57,27/Feb/24 15:22,04/Jun/24 20:41,23/Mar/23 07:28,1.17.0,,,,,,,,,,,1.17.0,,,,,,,,0,,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.

h4. Prerequisites
Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code}
RC_NUM=""1""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,FLINK-31146,FLINK-31562,,,FLINK-34530,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 09:40:51 UTC 2023,,,,,,,,,,"0|z1g28o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 07:28;leonard;Resolved as all sub-tasks have been finished;;;","23/Mar/23 09:40;renqs;For the record, 1.17.0 RC1 is canceled because of:
 * Blocker FLINK-31386
 * Wrong signature of {{flink-1.17.0-bin-scala_2.12.tgz}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a release branch,FLINK-31153,13525465,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,mapohl,mapohl,20/Feb/23 18:32,21/Feb/23 11:29,04/Jun/24 20:41,21/Feb/23 10:13,1.17.0,,,,,,,,,,,1.17.0,,,,,,,,0,,,,,"If you are doing a new minor release, you need to update Flink version in the following repositories and the [AzureCI project configuration|https://dev.azure.com/apache-flink/apache-flink/]:
 * [apache/flink|https://github.com/apache/flink]
 * [apache/flink-docker|https://github.com/apache/flink-docker]
 * [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks]

Patch releases don't require the these repositories to be touched. Simply checkout the already existing branch for that version:
{code:java}
$ git checkout release-$SHORT_RELEASE_VERSION
{code}
h4. Flink repository

Create a branch for the new version that we want to release before updating the master branch to the next development version:
{code:bash}
$ cd ./tools
tools $ releasing/create_snapshot_branch.sh
tools $ git checkout master
tools $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$NEXT_SNAPSHOT_VERSION releasing/update_branch_version.sh
{code}
In the {{master}} branch, add a new value (e.g. {{v1_16(""1.16"")}}) to [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] as the last entry:
{code:java}
// ...
v1_12(""1.12""),
v1_13(""1.13""),
v1_14(""1.14""),
v1_15(""1.15""),
v1_16(""1.16"");
{code}
The newly created branch and updated {{master}} branch need to be pushed to the official repository.
h4. Flink Docker Repository

Afterwards fork off from {{dev-master}} a {{dev-x.y}} branch in the [apache/flink-docker|https://github.com/apache/flink-docker] repository. Make sure that [apache/flink-docker:.github/workflows/ci.yml|https://github.com/apache/flink-docker/blob/dev-master/.github/workflows/ci.yml] points to the correct snapshot version; for {{dev-x.y}} it should point to {{{}x.y-SNAPSHOT{}}}, while for {{dev-master}} it should point to the most recent snapshot version (\{[$NEXT_SNAPSHOT_VERSION}}).

After pushing the new minor release branch, as the last step you should also update the documentation workflow to also build the documentation for the new release branch. Check [Managing Documentation|https://cwiki.apache.org/confluence/display/FLINK/Managing+Documentation] on details on how to do that. You may also want to manually trigger a build to make the changes visible as soon as possible.

h4. Flink Benchmark Repository
First of all, checkout the {{master}} branch to {{dev-x.y}} branch in [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks], so that we can have a branch named {{dev-x.y}} which could be built on top of (${{CURRENT_SNAPSHOT_VERSION}}).

Then, inside the repository you need to manually update the {{flink.version}} property inside the parent *pom.xml* file. It should be pointing to the most recent snapshot version ($NEXT_SNAPSHOT_VERSION). For example:
{code:xml}
<flink.version>1.18-SNAPSHOT</flink.version>
{code}

h4. AzureCI Project Configuration
The new release branch needs to be configured within AzureCI to make azure aware of the new release branch. This matter can only be handled by Ververica employees since they are owning the AzureCI setup.
 
----
h3. Expectations (Minor Version only if not stated otherwise)
 * Release branch has been created and pushed
 * Changes on the new release branch are picked up by [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary]
 * {{master}} branch has the version information updated to the new version (check pom.xml files and 
 * [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum)
 * New version is added to the [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum.
 * Make sure [flink-docker|https://github.com/apache/flink-docker/] has {{dev-x.y}} branch and docker e2e tests run against this branch in the corresponding Apache Flink release branch (see [apache/flink:flink-end-to-end-tests/test-scripts/common_docker.sh:51|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/common_docker.sh#L51])
 * [apache-flink:docs/config.toml|https://github.com/apache/flink/blob/release-1.17/docs/config.toml] has been updated appropriately in the new Apache Flink release branch.
 * The {{flink.version}} property (see [apache/flink-benchmarks:pom.xml|https://github.com/apache/flink-benchmarks/blob/master/pom.xml#L48] of Flink Benchmark repo has been updated to the latest snapshot version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 10:13:25 UTC 2023,,,,,,,,,,"0|z1g28g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 23:12;jingge;[~mapohl]  Thanks for creating this task and the umbrella task. It seems those tasks contain more or different information. Are you planning to build a template for the future release?;;;","20/Feb/23 23:14;jingge;BTW, you mentioned in the description: _you need to update Flink version in ""two"" repositories_ followed with three repos. Did you mean ""three""?;;;","21/Feb/23 08:47;mapohl;{quote}
BTW, you mentioned in the description: you need to update Flink version in ""two"" repositories followed with three repos. Did you mean ""three""?
 {quote}
Thanks for the hint. I updated the description accordingly. The content in the description is coming from the [Flink docs on how to create a release|https://cwiki.apache.org/confluence/display/FLINK/Flink+Release+Management]. I copied it and tried to align it but must have missed that bit. (y)

{quote}
It seems those tasks contain more or different information. Are you planning to build a template for the future release?
{quote}
We're experimenting a bit with improving the documentation. One idea is to move the documentation into Jira to enable release managers to document their efforts (see [the release sync protocol from February 14, 2023 week|https://cwiki.apache.org/confluence/display/FLINK/1.17+Release#id-1.17Release-2023-02-14]). For now, this is meant as a test. If it works well, we will create an improved Jira issue collection which can serve as templates. AFAIU, the Apache Jira doesn't provide templates, though. But having a dedicated set of FLINK Jira issues that can be cloned might be good enough.;;;","21/Feb/23 09:57;jingge;Make sense. Thanks [~mapohl] for the clarification. Looking forward to the structure Flink Release Task Collection Template - FRACT ? ;);;;","21/Feb/23 10:13;mapohl;Closing this issue. All the expected artifacts are created:
* (/) Release branch has been created and pushed
 * (/) Changes on the new release branch are picked up by [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary]
 * {{(/) master}} branch has the version information updated to the new version (check pom.xml files)
 ** Related commit is [923361a3|https://github.com/apache/flink/commit/923361a3006c7c4ce8acbc74a9b756f93c93020a]
 * (/) New version is added to the [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum.
 ** Related commit is [d76c49b4|https://github.com/apache/flink/commit/d76c49b4]
 * (/) Make sure [flink-docker|https://github.com/apache/flink-docker/] has {{dev-x.y}} branch
 * (/) Docker e2e tests run against the new flink-docker branch in the corresponding Apache Flink release branch (see [apache/flink:flink-end-to-end-tests/test-scripts/common_docker.sh:51|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/common_docker.sh#L51])
 ** Related commit is [df4d2ff7|https://github.com/apache/flink/commit/df4d2ff7]
 * [(/) apache-flink:docs/config.toml|https://github.com/apache/flink/blob/release-1.17/docs/config.toml] has been updated appropriately in the new Apache Flink release branch.
 ** Related commit is [df4d2ff7|https://github.com/apache/flink/commit/df4d2ff7]
 * The {{flink.version}} property (see [apache/flink-benchmarks:pom.xml|https://github.com/apache/flink-benchmarks/blob/master/pom.xml#L48] of Flink Benchmark repo has been updated to the latest snapshot version.
 ** Related commit is [79fdbaf6|https://github.com/apache/flink-benchmarks/commit/79fdbaf6f7ae2da63d08374b3052422afcec34ea];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select executing Release Manager,FLINK-31152,13525464,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,20/Feb/23 18:20,14/Dec/23 15:17,04/Jun/24 20:41,23/Mar/23 06:12,1.17.0,,,,,,,,,,,1.17.0,,,,Release System,,,,0,,,,,"h4. GPG Key

You need to have a GPG key to sign the release artifacts. Please be aware of the ASF-wide [release signing guidelines|https://www.apache.org/dev/release-signing.html]. If you don’t have a GPG key associated with your Apache account, please create one according to the guidelines.

Determine your Apache GPG Key and Key ID, as follows:
{code:java}
$ gpg --list-keys
{code}
This will list your GPG keys. One of these should reflect your Apache account, for example:
{code:java}
--------------------------------------------------
pub   2048R/845E6689 2016-02-23
uid                  Nomen Nescio <anonymous@apache.org>
sub   2048R/BA4D50BE 2016-02-23
{code}
In the example above, the key ID is the 8-digit hex string in the {{pub}} line: {{{}845E6689{}}}.

Now, add your Apache GPG key to the Flink’s {{KEYS}} file in the [Apache Flink release KEYS file|https://dist.apache.org/repos/dist/release/flink/KEYS] repository at [dist.apache.org|http://dist.apache.org/]. Follow the instructions listed at the top of these files. (Note: Only PMC members have write access to the release repository. If you end up getting 403 errors ask on the mailing list for assistance.)

Configure {{git}} to use this key when signing code by giving it your key ID, as follows:
{code:java}
$ git config --global user.signingkey 845E6689
{code}
You may drop the {{--global}} option if you’d prefer to use this key for the current repository only.

You may wish to start {{gpg-agent}} to unlock your GPG key only once using your passphrase. Otherwise, you may need to enter this passphrase hundreds of times. The setup for {{gpg-agent}} varies based on operating system, but may be something like this:
{code:bash}
$ eval $(gpg-agent --daemon --no-grab --write-env-file $HOME/.gpg-agent-info)
$ export GPG_TTY=$(tty)
$ export GPG_AGENT_INFO
{code}
h4. Access to Apache Nexus repository

Configure access to the [Apache Nexus repository|https://repository.apache.org/], which enables final deployment of releases to the Maven Central Repository.
 # You log in with your Apache account.
 # Confirm you have appropriate access by finding {{org.apache.flink}} under {{{}Staging Profiles{}}}.
 # Navigate to your {{Profile}} (top right drop-down menu of the page).
 # Choose {{User Token}} from the dropdown, then click {{{}Access User Token{}}}. Copy a snippet of the Maven XML configuration block.
 # Insert this snippet twice into your global Maven {{settings.xml}} file, typically {{{}${HOME}/.m2/settings.xml{}}}. The end result should look like this, where {{TOKEN_NAME}} and {{TOKEN_PASSWORD}} are your secret tokens:
{code:xml}
<settings>
   <servers>
     <server>
       <id>apache.releases.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
     <server>
       <id>apache.snapshots.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
   </servers>
 </settings>
{code}

h4. Website development setup

Get ready for updating the Flink website by following the [website development instructions|https://flink.apache.org/contributing/improve-website.html].
h4. GNU Tar Setup for Mac (Skip this step if you are not using a Mac)

The default tar application on Mac does not support GNU archive format and defaults to Pax. This bloats the archive with unnecessary metadata that can result in additional files when decompressing (see [1.15.2-RC2 vote thread|https://lists.apache.org/thread/mzbgsb7y9vdp9bs00gsgscsjv2ygy58q]). Install gnu-tar and create a symbolic link to use in preference of the default tar program.
{code:bash}
$ brew install gnu-tar
$ ln -s /usr/local/bin/gtar /usr/local/bin/tar
$ which tar
{code}
 
----
h3. Expectations
 * Release Manager’s GPG key is published to [dist.apache.org|http://dist.apache.org/]
 * Release Manager’s GPG key is configured in git configuration
 * Release Manager's GPG key is configured as the default gpg key.
 * Release Manager has {{org.apache.flink}} listed under Staging Profiles in Nexus
 * Release Manager’s Nexus User Token is configured in settings.xml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 06:12:26 UTC 2023,,,,,,,,,,"0|z1g288:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 06:12;renqs;* Release Manager’s GPG key is published to [dist.apache.org|http://dist.apache.org/]
 * Release Manager’s GPG key is configured in git configuration
 * Release Manager's GPG key is configured as the default gpg key.
 * Release Manager has {{org.apache.flink}} listed under Staging Profiles in Nexus
 * Release Manager’s Nexus User Token is configured in settings.xml;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review Release Notes in JIRA,FLINK-31151,13525462,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,20/Feb/23 18:04,23/Mar/23 06:11,04/Jun/24 20:41,23/Mar/23 06:11,,,,,,,,,,,,,,,,,,,,0,,,,,"JIRA automatically generates Release Notes based on the {{Fix Version}} field applied to issues. Release Notes are intended for Flink users (not Flink committers/contributors). You should ensure that Release Notes are informative and useful.

Open the release notes from the version status page by choosing the release underway and clicking Release Notes.

You should verify that the issues listed automatically by JIRA are appropriate to appear in the Release Notes. Specifically, issues should:
 * Be appropriately classified as {{{}Bug{}}}, {{{}New Feature{}}}, {{{}Improvement{}}}, etc.
 * Represent noteworthy user-facing changes, such as new functionality, backward-incompatible API changes, or performance improvements.
 * Have occurred since the previous release; an issue that was introduced and fixed between releases should not appear in the Release Notes.
 * Have an issue title that makes sense when read on its own.

Adjust any of the above properties to the improve clarity and presentation of the Release Notes.

Ensure that the JIRA release notes are also included in the release notes of the documentation (see section ""Review and update documentation"").
h4. Content of Release Notes field from JIRA tickets 

To get the list of ""release notes"" field from JIRA, you can ran the following script using JIRA REST API (notes the maxResults limits the number of entries):
{code:bash}
curl -s https://issues.apache.org/jira//rest/api/2/search?maxResults=200&jql=project%20%3D%20FLINK%20AND%20%22Release%20Note%22%20is%20not%20EMPTY%20and%20fixVersion%20%3D%20${RELEASE_VERSION} | jq '.issues[]|.key,.fields.summary,.fields.customfield_12310192' | paste - - -
{code}
{{jq}}  is present in most Linux distributions and on MacOS can be installed via brew.

 
----
h3. Expectations
 * Release Notes in JIRA have been audited and adjusted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 06:11:06 UTC 2023,,,,,,,,,,"0|z1g27s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 06:11;renqs;* Followed the guide to create [this page|https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&version=12351585] on JIRA web UI, which collects all issues with fix version = 1.17.0 so some CI instabilities were included.
 * Used {{curl}} to export all issues with non-empty {{release note}} field. Adjusted the format and opened [this PR|https://github.com/apache/flink/pull/22146] to add release note in doc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross team testing,FLINK-31150,13525461,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,20/Feb/23 18:02,27/Feb/23 10:39,04/Jun/24 20:41,27/Feb/23 10:39,,,,,,,,,,,,,,,,,,,,0,,,,,"For user facing features that go into the release we'd like to ensure they can actually _be used_ by Flink users. To achieve this the release managers ensure that an issue for cross team testing is created in the Apache Flink Jira. This can and should be picked up by other community members to verify the functionality and usability of the feature.
The issue should contain some entry points which enables other community members to test it. It should not contain documentation on how to use the feature as this should be part of the actual documentation. The cross team tests are performed after the feature freeze. Documentation should be in place before that. Those tests are manual tests, so do not confuse them with automated tests.
To sum that up:
 * User facing features should be tested by other contributors
 * The scope is usability and sanity of the feature
 * The feature needs to be already documented
 * The contributor creates an issue containing some pointers on how to get started (e.g. link to the documentation, suggested targets of verification)
 * Other community members pick those issues up and provide feedback
 * Cross team testing happens right after the feature freeze

 
----
h3. Expectations
 * Jira issues for each expected release task according to the release plan is created and labeled as {{{}release-testing{}}}.
 * All the created release-testing-related Jira issues are resolved and the corresponding blocker issues are fixed.",,,,,,,,,,,,,,,FLINK-30926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 08:55:13 UTC 2023,,,,,,,,,,"0|z1g27k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 08:55;mapohl;[~renqs] created FLINK-30926 as an umbrella ticket for the 1.17 release-testing efforts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review and update documentation,FLINK-31149,13525460,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,mapohl,mapohl,20/Feb/23 18:01,23/Mar/23 07:18,04/Jun/24 20:41,23/Mar/23 07:18,1.17.0,,,,,,,,,,,1.17.0,,,,,,,,0,pull-request-available,,,,"There are a few pages in the documentation that need to be reviewed and updated for each release.
 * Ensure that there exists a release notes page for each non-bugfix release (e.g., 1.5.0) in {{{}./docs/release-notes/{}}}, that it is up-to-date, and linked from the start page of the documentation.
 * Upgrading Applications and Flink Versions: [https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html]
 * ...

 
----
h3. Expectations
 * Update upgrade compatibility table ([apache-flink:./docs/content/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content/docs/ops/upgrading.md#compatibility-table] and [apache-flink:./docs/content.zh/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content.zh/docs/ops/upgrading.md#compatibility-table]).
 * Update [Release Overview in Confluence|https://cwiki.apache.org/confluence/display/FLINK/Release+Management+and+Feature+Plan]
 * (minor only) The documentation for the new major release is visible under [https://nightlies.apache.org/flink/flink-docs-release-$SHORT_RELEASE_VERSION] (after at least one [doc build|https://github.com/apache/flink/actions/workflows/docs.yml] succeeded).
 * (minor only) The documentation for the new major release does not contain ""-SNAPSHOT"" in its version title, and all links refer to the corresponding version docs instead of {{{}master{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 07:00:10 UTC 2023,,,,,,,,,,"0|z1g27c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 07:00;leonard;master(1.18): 1009196d1ffe2d83ccab46608c3581e25d12fcc3

1.17：2bd324c175e342801faea2453ee5c52a4309dec6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triage release-blocking issues in JIRA,FLINK-31148,13525459,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,20/Feb/23 17:59,23/Mar/23 06:07,04/Jun/24 20:41,23/Mar/23 06:07,,,,,,,,,,,,,,,,,,,,0,,,,,"There could be outstanding release-blocking issues, which should be triaged before proceeding to build a release candidate. We track them by assigning a specific Fix version field even before the issue resolved.

The list of release-blocking issues is available at the version status page. Triage each unresolved issue with one of the following resolutions:
 * If the issue has been resolved and JIRA was not updated, resolve it accordingly.
 * If the issue has not been resolved and it is acceptable to defer this until the next release, update the Fix Version field to the new version you just created. Please consider discussing this with stakeholders and the dev@ mailing list, as appropriate.
 ** When using ""Bulk Change"" functionality of Jira
 *** First, add the newly created version to Fix Version for all unresolved tickets that have old the old version among its Fix Versions.
 *** Afterwards, remove the old version from the Fix Version.
 * If the issue has not been resolved and it is not acceptable to release until it is fixed, the release cannot proceed. Instead, work with the Flink community to resolve the issue.

 
----
h3. Expectations
 * There are no release blocking JIRA issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 06:07:24 UTC 2023,,,,,,,,,,"0|z1g274:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 06:07;renqs;All blockers of 1.17.0 has been swiped out. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a new version in JIRA,FLINK-31147,13525458,13525457,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,mapohl,mapohl,20/Feb/23 17:59,21/Feb/23 11:30,04/Jun/24 20:41,21/Feb/23 11:30,,,,,,,,,,,,,,,,,,,,0,,,,,"When contributors resolve an issue in JIRA, they are tagging it with a release that will contain their changes. With the release currently underway, new issues should be resolved against a subsequent future release. Therefore, you should create a release item for this subsequent release, as follows:
 # In JIRA, navigate to the [Flink > Administration > Versions|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions].
 # Add a new release: choose the next minor version number compared to the one currently underway, select today’s date as the Start Date, and choose Add.
(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

 
----
h3. Expectations
 * The new version should be listed in the dropdown menu of {{fixVersion}} or {{affectedVersion}} under ""unreleased versions"" when creating a new Jira issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 11:30:49 UTC 2023,,,,,,,,,,"0|z1g26w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 11:30;mapohl;I verified that the 1.18.0 version was created.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepare Flink 1.17 Release,FLINK-31146,13525457,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,mapohl,mapohl,20/Feb/23 17:58,30/Jan/24 09:48,04/Jun/24 20:41,23/Mar/23 07:23,1.17.0,,,,,,,,,,,1.17.0,,,,Release System,,,,0,,,,,"This umbrella issue is meant as a test balloon for moving the [release documentation|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release] into Jira.
h3. Prerequisites
h4. Environment Variables

Commands in the subtasks might expect some of the following enviroment variables to be set accordingly to the version that is about to be released:
{code:bash}
RELEASE_VERSION=""1.5.0""
SHORT_RELEASE_VERSION=""1.5""
CURRENT_SNAPSHOT_VERSION=""$SHORT_RELEASE_VERSION-SNAPSHOT""
NEXT_SNAPSHOT_VERSION=""1.6-SNAPSHOT""
SHORT_NEXT_SNAPSHOT_VERSION=""1.6""
{code}
h4. Build Tools

All of the following steps require to use Maven 3.2.5 and Java 8. Modify your PATH environment variable accordingly if needed.
h4. Flink Source
 * Create a new directory for this release and clone the Flink repository from Github to ensure you have a clean workspace (this step is optional).
 * Run {{mvn -Prelease clean install}} to ensure that the build processes that are specific to that profile are in good shape (this step is optional).

The rest of this instructions assumes that commands are run in the root (or {{./tools}} directory) of a repository on the branch of the release version with the above environment variables set.",,,,,,,,,,,,,FLINK-31154,,,FLINK-34275,FLINK-32921,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 07:23:31 UTC 2023,,,,,,,,,,"0|z1g26o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 07:23;leonard;Mark as resolved as all sub-tasks have been resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stabilize Kafka-related tests,FLINK-31145,13525453,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,mapohl,mapohl,20/Feb/23 17:30,16/Oct/23 07:10,04/Jun/24 20:41,16/Oct/23 07:10,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,,This is an umbrella ticket that collects all the Kafka infrastructure related test instabilities.,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26115,,,FLINK-32101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 16 07:10:32 UTC 2023,,,,,,,,,,"0|z1g25s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 07:10;martijnvisser;With the Flink Kafka connector being externalized, I took a pass among all reported test instabilities and see that they haven't been reported anymore in the last 3-6 months. Therefore I've closed them, I'll also close this ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slow scheduling on large-scale batch jobs ,FLINK-31144,13525451,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,jto,jto,20/Feb/23 16:59,03/Apr/23 09:02,04/Jun/24 20:41,10/Mar/23 09:11,1.15.3,1.16.1,1.17.0,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"When executing a complex job graph at high parallelism `DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs` can get slow and cause long pauses where the JobManager becomes unresponsive and all the taskmanagers just wait. I've attached a VisualVM snapshot to illustrate the problem.[^flink-1.17-snapshot-1676473798013.nps]

At Spotify we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes and make the overall execution 30% slower or more.
More importantly this prevent us from running said jobs on larger cluster as adding resources to the cluster worsen the issue.

We have successfully tested a modified Flink version where `DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs` was completely commented and simply returns an empty collection and confirmed it solves the issue.

In the same spirit as a recent change ([https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L98-L102)] there could be a mechanism in place to detect when Flink run into this specific issue and just skip the call to `getInputLocationFutures`  [https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L105-L108.]

I'm not familiar enough with the internals of Flink to propose a more advanced fix, however it seems like a configurable threshold on the number of consumer vertices above which the preferred location is not computed would do. If this  solution is good enough, I'd be happy to submit a PR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/23 13:22;martijnvisser;Screenshot 2023-03-13 at 14.22.27.png;https://issues.apache.org/jira/secure/attachment/13056281/Screenshot+2023-03-13+at+14.22.27.png","20/Feb/23 16:54;jto;flink-1.17-snapshot-1676473798013.nps;https://issues.apache.org/jira/secure/attachment/13055642/flink-1.17-snapshot-1676473798013.nps","21/Feb/23 09:29;jto;image-2023-02-21-10-29-49-388.png;https://issues.apache.org/jira/secure/attachment/13055672/image-2023-02-21-10-29-49-388.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 24 09:28:55 UTC 2023,,,,,,,,,,"0|z1g25c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 07:08;huwh;Thanks for reporting this issue. [~jto] 

DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs will go over all the input locations which may cost much time when the topology is complex. Maybe make PreferredLocations configurable is an option to solve this problem.

I have some more questions about this issue:

1. Is the slow scheduling or the sheduled result of lolcation preferred make your job slow?
2. ""we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes""      What does ""pause"" meaning? Is the getPreferredLocationsBasedOnInputs take more than 40+ minutes? Could you provide the topology of the complex job.;;;","21/Feb/23 09:30;jto;Hi [~huwh],


Thank you for the quick reply :)


{quote}when the topology is complex.
{quote}

Indeed. For the issue to be noticeable, the jobgraph has to be fairly complex, feature all-to-all distributions and execute with a high parallelism.

 
{quote}1. Is the slow scheduling or the scheduled result of location preferred make your job slow?
{quote}

Yes it very much does. We have a job that takes ~2h30 (after many many tweaks to get the best possible perf.). It's impossible to get it to run in less time because adding more taskmanagers make the scheduling slow and overall the execution gets longer. Removing preferred location makes it possible to run it in less that 2h (We're aiming at ~1h45min).

 
{quote}2. ""we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes""  What does ""pause"" meaning? Is the getPreferredLocationsBasedOnInputs take more than 40+ minutes?
{quote}

By ""pause"" I mean that at the beginning of the execution, the taskmanagers will wait for the JobManager for ~40min and then will start processing. With Flink 1.17 and no preferred location, the ""pause"" is down to ~5min.

I should also mention the JM is very unresponsive and the web console struggles the show anything. 


{quote}Could you provide the topology of the complex job.
{quote}

I can but not sure what format to use. The graph is quite big and a simple screenshot is unreadable: !image-2023-02-21-10-29-49-388.png!

I can maybe share the archived execution json file (~500Mb) if that's helpful ?

 ;;;","22/Feb/23 03:33;huwh;{quote}however it seems like a configurable threshold on the number of consumer vertices above which the preferred location is not computed would do
{quote}
IMO, It's too hard to decide the threshold for user. Introduce a config to disable input location preferences is more understandable.

[~zhuzh] WDYT;;;","22/Feb/23 04:24;zhuzh;Thanks for reporting this issue! [~jto]
What's the the parallelism of the job vertices? Unfortunately I cannot tell it from the attached image.

IIUC, the problem is caused by vertices with large parallelism and ALL-to-ALL edges. If so, changing the `if` condition of [this logic|https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L98-L102)] to {{consumedPartitionGroup.size() > MAX_DISTINCT_CONSUMERS_TO_CONSIDER}} may help. Would you give a try?;;;","22/Feb/23 17:13;jto;{quote}IMO, It's too hard to decide the threshold for user. Introduce a config to disable input location preferences is more understandable
{quote}
Yes. My suggestion was to have a reasonable default value that would work for almost everyone but still make it configurable for advanced edge cases under [advanced-scheduling-options.|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#advanced-scheduling-options]. A simple enable / disabled config also work for me.


Hey [~zhuzh]!

Thank you for comment. The parallelism on all the vertices is 4000.
I did not have the time to test the change in the condition but I think I can try it tomorrow and I'll keep you posted :);;;","23/Feb/23 07:06;zhuzh;I suspect the slowness is caused by the N^2 complexity to compute the input locations when there are N upstream task and N downstream tasks.
If so, as long as N is not too large, e.g. not larger than MAX_DISTINCT_CONSUMERS_TO_CONSIDER=8, the cost of input location computation should be acceptable. Also, if there are too many distinct consumers, input locality would make none sense.

As [~huwh] mentioned, it's hard for users to decides a proper threshold for each job. It's also inconvenient if users had to decide whether to enable input locality or not. Therefore, I prefer to let Flink decide it automatically for users, like the proposed change above.
;;;","23/Feb/23 07:19;huwh;[~zhuzh] That sounds good to me.;;;","27/Feb/23 08:33;zhuzh;Sorry I made a mistake! [~jto]
What I meant is to change the check to {{consumedPartitionGroup.getConsumerVertexGroup().size() > MAX_DISTINCT_CONSUMERS_TO_CONSIDER}}, like [this|https://github.com/zhuzhurk/flink/commit/860ec8855de5f36ee2758336a5eba7abac3f215a].
;;;","28/Feb/23 13:50;jto;Hey there!

I finally found some time to test the fix proposed by [~zhuzh] and it does solve the problem :);;;","03/Mar/23 06:16;JunRuiLi;Hi, [~zhuzh]. I'd like to fix this issue, could you help to assign this ticket to me? ;;;","03/Mar/23 06:23;zhuzh;Thanks for volunteering! [~JunRuiLi]
I have assigned you the ticket.;;;","03/Mar/23 09:21;jto;Awesome! Thanks for volunteering [~JunRuiLi] 

I'm happy to help if you need something. Just let me know :);;;","10/Mar/23 09:11;zhuzh;master:
6d79f6fc52bcd338d0528d19cd1f7289d8e339da

release-1.17:
5b90c075bf9b987de2fa4a02e0cf63602152eba1;;;","13/Mar/23 13:23;martijnvisser;[~zhuzh] Not 100% sure but I think this PR has resulted in a drastic performance improvement in startScheduling.STREAMING, see http://codespeed.dak8s.net:8000/timeline/#/?exe=8&ben=startScheduling.STREAMING&extr=on&quarts=on&equid=off&env=2&revs=200;;;","13/Mar/23 13:48;huwh;[~martijnvisser], yes, this change will improve the performance both of Streaming and Batch jobs with high parallelism. The benchmark result is same with my local.;;;","14/Mar/23 05:41;zhuzh;IIRC, the parallelism of that benchmark job (startScheduling.STREAMING) is 4000, which is relatively large. So the scheduling time can be significantly reduced with this improvement.;;;","24/Mar/23 09:28;jto;Hey [~martijnvisser]  this is great! Thanks for sharing :) 
Spotify is in a good position to identify inefficiencies and possible optimizations on high parallelism jobs.
I'm hopeful we can contribute to improve he performances further in the future!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid request: offset doesn't match when restarting from a savepoint,FLINK-31143,13525450,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,20/Feb/23 16:43,23/Feb/23 07:21,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"I tried to run the following case:
{code:java}
public static void main(String[] args) throws Exception {
        final String createTableQuery =
                ""CREATE TABLE left_table (a int, c varchar) ""
                        + ""WITH (""
                        + ""     'connector' = 'datagen', ""
                        + ""     'rows-per-second' = '1', ""
                        + ""     'fields.a.kind' = 'sequence', ""
                        + ""     'fields.a.start' = '0', ""
                        + ""     'fields.a.end' = '100000'""
                        + "");"";
        final String selectQuery = ""SELECT * FROM left_table;"";

        final Configuration initialConfig = new Configuration();
        initialConfig.set(CoreOptions.DEFAULT_PARALLELISM, 1);

        final EnvironmentSettings initialSettings =
                EnvironmentSettings.newInstance()
                        .inStreamingMode()
                        .withConfiguration(initialConfig)
                        .build();
        final TableEnvironment initialTableEnv = TableEnvironment.create(initialSettings);

        // create job and consume two results
        initialTableEnv.executeSql(createTableQuery);
        final TableResult tableResult = initialTableEnv.sqlQuery(selectQuery).execute();
        tableResult.await();
        System.out.println(tableResultIterator.next()); 
        System.out.println(tableResultIterator.next());          

        // stop job with savepoint
        final String savepointPath;
        try (CloseableIterator<Row> tableResultIterator = tableResult.collect()) {
            final JobClient jobClient =
                    tableResult.getJobClient().orElseThrow(IllegalStateException::new);

            final File savepointDirectory = Files.createTempDir();
            savepointPath =
                    jobClient
                            .stopWithSavepoint(
                                    true,
                                    savepointDirectory.getAbsolutePath(),
                                    SavepointFormatType.CANONICAL)
                            .get();
        }

        // restart the very same job from the savepoint
        final SavepointRestoreSettings savepointRestoreSettings =
                SavepointRestoreSettings.forPath(savepointPath, true);
        final Configuration restartConfig = new Configuration(initialConfig);
        SavepointRestoreSettings.toConfiguration(savepointRestoreSettings, restartConfig);

        final EnvironmentSettings restartSettings =
                EnvironmentSettings.newInstance()
                        .inStreamingMode()
                        .withConfiguration(restartConfig)
                        .build();
        final TableEnvironment restartTableEnv = TableEnvironment.create(restartSettings);

        restartTableEnv.executeSql(createTableQuery);
        restartTableEnv.sqlQuery(selectQuery).execute().print();
    }
{code}
h3. Expected behavior

The job continues omitting the inital two records and starts printing results from 2 onwards.
h3. Observed behavior

No results are printed. The logs show that an invalid request was handled:
{code:java}
org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Invalid request. Received version = b497a74f-e85c-404b-8df3-1b4b1a0c2de1, offset = 0, while expected version = b497a74f-e85c-404b-8df3-1b4b1a0c2de1, offset = 1
{code}
It looks like the right offset is not picked up from the savepoint (see [CollectSinkFunction:411|https://github.com/apache/flink/blob/5ae8cb0503449b07f76d0ab621c3e81734496b26/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunction.java#L411]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31066,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 03:04:59 UTC 2023,,,,,,,,,,"0|z1g254:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 16:46;mapohl;I didn't run this example in older Flink versions but had initially tested a more generic example manually in 1.15.3 and 1.16.1. I saw similar behavior with no results being printed. Therefore, I added the corresponding versions to this Jira issue.;;;","20/Feb/23 16:46;mapohl;[~TsReaper] can you give some insights into that behavior? Am I missing some configuration or is it a bug?;;;","21/Feb/23 17:08;Weijie Guo;I'm a little suspicious that the current logic can't handle when `JobClient` is also restarted instead of simple task failover. There are some reasons from my side. For example, the `sinkRestarted` method [here|https://github.com/apache/flink/blob/5cda70d873c9630c898d765633ec7a6cfe53e3c6/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/AbstractCollectResultBuffer.java#L87] will not be called because our version(0) is equal to `INIT_VERSION`. I'm not sure why the restart of JobClient can not be taken into account, one possible reason is that transferring data from `CollectSink` to `JobClient` is a bit like writing data to an external system, It is not easy to ensure end-to-end EXACTLY_ONCE semantic.
;;;","22/Feb/23 15:21;mapohl;Thanks for looking into it, [~Weijie Guo]. I'm not sure I fully understand your comment. Where is the {{JobClient}} restarted? Or are you suggesting that my example only works iff the {{JobClient}} is restarted? And if that's the case, how would I do that? The {{JobClient}} is only used when creating the savepoint? Any internally used jobClient, I'd assume would be in some way ""restarted"" because I'm using a new {{TableEnvironment}}.;;;","22/Feb/23 16:31;Weijie Guo;[~mapohl] Sorry, what I said before is not very clear. We resubmit a SQL job, which will create a new `JobClient`. Maybe it is not appropriate to call it the restart of `JobClient`. The key point is that this will create a new `CollectResultFetcher`. Since `CollectResultFetcher` is located on the client side, the interaction between `CollectDynamicSink` and it is somewhat similar to between external systems. A stronger mechanism is needed to ensure end-to-end exactly-once. In my opinion, it cannot guarantee fault tolerance at least from the current implementation. ;;;","22/Feb/23 16:33;mapohl;I see, can you help me find someone who can verify it? It's blocking the release-testing task FLINK-31066 right now.;;;","22/Feb/23 16:37;Weijie Guo;Sure, I will contact [~TsReaper] offline to confirm this.;;;","23/Feb/23 03:04;TsReaper;Hi [~mapohl] [~Weijie Guo].

When implementing {{collect}} I only considered job restarts and didn't consider client restarts, so {{collect}} + savepoint is currently not supported.

It is actually not easy to implement this. The biggest problem is that collect client is a stateful client and it stores some records not consumed by the user yet. If we want to support {{collect}} + savepoint, we will first need to find out where to store these unconsumed records. That is, we might need to introduce something like client state backend.

A less user-friendly but more feasible solution is that we require the user to consume all records in the {{collect}} iterator after a savepoint, before closing the iterator and starting a new job. If the user does not obey this requirement some records might be lost.

In any aspect, supporting {{collect}} + savepoint will be a new feature. As we are very close to releasing I think it is proper to introduce it (if needed) in future versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some queries lead to abrupt sql client close,FLINK-31142,13525444,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,20/Feb/23 14:46,26/Feb/23 14:00,04/Jun/24 20:41,26/Feb/23 14:00,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Client,,,,0,pull-request-available,,,,"Although the behavior has been changed in 1.17.0, I'm not sure whether it is a blocker or not, since in both cases it is invalid query.
I put it to blocker just because of regression.

The difference in the behavior is that before 1.17.0
a query like 
{code:sql}
select /* multiline comment;
{code}
fails to execute and sql client prompts to submit another query.

In 1.17.0 it  shuts down the session failing with 
{noformat}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not read from command line.
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:205)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:168)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:113)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:169)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:118)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179)
Caused by: org.apache.flink.sql.parser.impl.TokenMgrError: Lexical error at line 1, column 29.  Encountered: <EOF> after : """"
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImplTokenManager.getNextToken(FlinkSqlParserImplTokenManager.java:26752)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl$TokenIterator.scan(SqlCommandParserImpl.java:89)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl$TokenIterator.next(SqlCommandParserImpl.java:81)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.checkIncompleteStatement(SqlCommandParserImpl.java:141)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.getCommand(SqlCommandParserImpl.java:111)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.parseStatement(SqlCommandParserImpl.java:52)
	at org.apache.flink.table.client.cli.parser.SqlMultiLineParser.parse(SqlMultiLineParser.java:82)
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964)
	at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778)
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:183)
	... 6 more

Shutting down the session...
done.

{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 22:51:05 UTC 2023,,,,,,,,,,"0|z1g23s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 10:50;mapohl;[~fsk119] can you guide us on that one? It sounds like a regression.;;;","23/Feb/23 12:54;Sergey Nuyanzin;It seems that before this change (https://issues.apache.org/jira/browse/FLINK-29945) all the parser exceptions are handled via {{org.apache.calcite.sql.parser.SqlAbstractParserImpl#normalizeException}} which under the hood catches {{{}TokenMgrError{}}}. Now it works directly with {{TokenIterator}} that's why extra catch is required.;;;","24/Feb/23 22:51;Sergey Nuyanzin;Merged to master: 464ded1c2a0497255b70f711167c3b7ae52ea0f7
Merged to 1.17     : 05f207cab637f46709feb9986b137456ecbe5b7a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CreateTableAsITCase.testCreateTableAs fails,FLINK-31141,13525443,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,fanrui,fanrui,20/Feb/23 14:40,29/Nov/23 01:54,04/Jun/24 20:41,29/Nov/23 01:54,1.17.0,1.18.0,,,,,,,,,,1.17.3,1.18.1,1.19.0,,Tests,,,,0,stale-assigned,test-stability,,,"CreateTableAsITCase.testCreateTableAs fails in [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46323&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14772]

 
{code:java}
Feb 20 13:50:12 [ERROR] Failures: 
Feb 20 13:50:12 [ERROR] CreateTableAsITCase.testCreateTableAs
Feb 20 13:50:12 [ERROR]   Run 1: Did not get expected results before timeout, actual result: [{""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":1},""op"":""c""}, {""before"":null,""after"":{""user_name"":""Alice"",""order_cnt"":1},""op"":""c""}, {""before"":{""user_name"":""Bob"",""order_cnt"":1},""after"":null,""op"":""d""}, {""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":2},""op"":""c""}]. ==> expected: <true> but was: <false>
Feb 20 13:50:12 [INFO]   Run 2: PASS
Feb 20 13:50:12 [INFO] 
Feb 20 13:50:12 [INFO] 
Feb 20 13:50:12 [ERROR] Tests run: 15, Failures: 1, Errors: 0, Skipped: 0 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 01:54:34 UTC 2023,,,,,,,,,,"0|z1g23k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 14:43;fanrui;Hi [~lsy] , I see you created the test in FLINK-29219, and I'm not familiar with the test. Would you mind take a look this issue in your free time? thanks:);;;","22/Feb/23 01:41;lsy;I will take a look. Seems like it's just an occasional failure now?;;;","22/Feb/23 02:13;fanrui;Yep, I just found it failing once.;;;","27/Feb/23 13:26;mapohl; [~lsy] any updates on that one?;;;","30/May/23 09:23;renqs;Downgraded to major as this issue only happened once;;;","24/Jul/23 23:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51607&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=16482;;;","24/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","28/Nov/23 06:49;leonard;Fixed in master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52;;;","28/Nov/23 09:03;mapohl;Sorry for reopening it. But can we also create backports? The test instability seem to have been appearing in older releases (according to the ""Affected Versions"") as well.;;;","29/Nov/23 01:54;leonard;Fixed in

master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52

release-1.18：b3b7240cc34e552273b26d8090d45e492474c9ea

release-1.17: 0053db03772a70c70de0516cc46f7ab363dc74f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load additional dependencies in operator classpath ,FLINK-31140,13525431,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tamirsagi,tamirsagi,20/Feb/23 12:36,02/Mar/23 15:09,04/Jun/24 20:41,02/Mar/23 15:09,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"To date is is not possible to add additional jars to operator classpath. 

In our case, We have a Kafka appender with custom layout that works with IAM authentication and SSL along with AWS MSK.

log4j.properties file

 
{code:java}
appender.kafka.type = Kafka
appender.kafka.name = Kafka
appender.kafka.bootstrap.servers = <brokers>
appender.kafka.topic = <topic>
appender.kafka.security.protocol = SASL_SSL
appender.kafka.sasl.mechanism = AWS_MSK_IAM
appender.kafka.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required;
appender.kafka.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
appender.kafka.layout.type = CustomJsonLayout
appender.kafka.layout.class = our.package.layouts.CustomJsonLayout 
appender.kafka.layout.service_name = <service-name>{code}
if CustomLayout is not present in classpath the logger fails to start.

*main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.KafkaAppender for element Kafka: java.lang.NullPointerException java.lang.NullPointerException*

furthermore, all the essential AWS dependencies must be added to path.

 

To support additional jars I needed to add them and then replace the following lines

[https://github.com/apache/flink-kubernetes-operator/blob/main/docker-entrypoint.sh#L32]

[https://github.com/apache/flink-kubernetes-operator/blob/main/docker-entrypoint.sh#L37]

using 'sed' command.

LOGGER_JAR in that case is an uber jar contains all the necessary jars.
{code:java}
ENV OPERATOR_LIB=/opt/flink/operator-lib
RUN mkdir -p $OPERATOR_LIB
COPY target/$LOGGER_JAR $OPERATOR_LIB
USER root

RUN sed -i 's/java -cp /java -cp $USER_DEPENDENCIES_DIR/*:/' /docker-entrypoint.sh

USER flink{code}
It works great but not ideal and IMO should not be handled that way.

 

The idea of that ticket is to allow users to add additional jars into specific location which is added to classpath while Kubernetes operator starts.

I'd like to add the 'OPERATOR_LIB' ENV and create that folder (/opt/flink/operator-lib) as well in root Dockerfile.

along with a minor modification in 'docker-entrypoint.sh' (Add  $OPERATOR_LIB to 'cp' command)

 

Once it's supported , the only thing users have to do is copy all dependencies to that folder ($OPERATOR_LIB) while building custom image

I tested it locally and it seems to be working.",,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,FLINK-30115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 15:09:13 UTC 2023,,,,,,,,,,"0|z1g20w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 21:49;asardaes;If you don't mind me asking a question here (and please correct me if I'm wrong), in Flink itself, logging jars must be placed in {{lib}} because logging libraries are loaded with the main class loader, so putting them in {{usrlib}} wouldn't work because that's loaded by the user class loader.

I imagine such a mechanism doesn't make much sense for the Kubernetes operator (except maybe for plugins), so this additional directory for dependencies would also be loaded with the ""main"" class loader, right?;;;","28/Feb/23 07:31;tamirsagi;I think you are confusing with Flink application.

Oppose to Flink Application where the user jar is placed in usrlib and gets loaded via different class loader, Flink Operator runs its own code only, hence we would like to load additional dependencies along with the Operator's classpath.

In my case, a custom layout is used which requires AWS dependencies along with in-house artifacts, they all must be loaded into the Operator's classpath as well as the main artifacts.

 

does that make sense to you?

 ;;;","28/Feb/23 08:10;asardaes;It does, I just wanted to corroborate, thanks.;;;","02/Mar/23 15:09;gyfora;merged to main ff5c00c695cdf844aca0c68242279da3afad124c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
not upload empty state changelog file,FLINK-31139,13525430,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,20/Feb/23 12:18,15/Aug/23 07:53,04/Jun/24 20:41,15/Aug/23 07:53,1.15.3,1.16.1,,,,,,,,,,1.16.3,1.17.2,1.18.0,,Runtime / State Backends,,,,0,pull-request-available,stale-assigned,,,"h1. Problem

*_BatchingStateChangeUploadScheduler_* will upload many empty changelog files (file size == 1  and only contains compressed flag).

!image-2023-02-20-19-51-34-397.png|width=1062,height=188!

These files are not referenced by any checkpoints, are not cleaned up, and become more numerous as the job runs. Taking our big job as an example, 2292 such files were generated within 7 hours. It only takes about 4 months and the number of files in the changelog directory will exceed a million.
h1. Problem causes

This problem is caused by *_BatchingStateChangeUploadScheduler#drainAndSave_* not checking whether the task collection is empty. The data in the scheduled queue may have been uploaded when the _*BatchingStateChangeUploadScheduler#drainAndSave*_ method is executed.

 

So we should check whether the task collection is empty in *_BatchingStateChangeUploadScheduler#drainAndSave_* . WDYT [~roman] , [~Yanfei Lei] ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/23 11:51;Feifan Wang;image-2023-02-20-19-51-34-397.png;https://issues.apache.org/jira/secure/attachment/13055633/image-2023-02-20-19-51-34-397.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 07:52:38 UTC 2023,,,,,,,,,,"0|z1g20o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 19:45;roman;Merged into master as fb37e6a87d92912897c6a8c4b182048e13686dee.;;;","22/Mar/23 02:07;Feifan Wang;Hi [~roman] , thanks for your reply, and should we merge this fix to release-1.16 & release-1.17 ?;;;","22/Mar/23 20:33;roman;Yes [~Feifan Wang] , I think so.

I've pushed to backport-branches to make sure the change doesn't break anything:

1.16: [https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1681&view=results] (PASS)

1.17: [https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1680&view=results] (FAIL)

I'm going to re-run failed stages for 1.17 and merge if it passes.;;;","27/Apr/23 06:24;Feifan Wang;Thanks [~roman] , remember to re-run failed stages for 1.17.;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","13/Aug/23 11:37;Feifan Wang;Hi [~roman] , is there any method to re-run  failed stages for 1.17 ?;;;","14/Aug/23 14:48;roman;[~Feifan Wang] thanks for the reminder, I have rebased the and force-pushed the changes:
- https://github.com/rkhachatryan/flink/tree/f31139-1.16
- https://github.com/rkhachatryan/flink/tree/f31139-1.17

(please feel free to open backport PRs if necessary).
;;;","15/Aug/23 07:52;roman;Merged into release-1.16 as 5157ac5921d406f577c83a4fe57b373d8ae0bf79, into release-1.17 as 8798fde04e4a28af8bee7f2641b4aa89f2e0995f.
Merged into master as fb37e6a87d92912897c6a8c4b182048e13686dee earlier
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Either StreamCheckpointingITCase/StreamFaultToleranceTestBase or EventTimeWindowCheckpointingITCase are timinng out,FLINK-31138,13525417,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,mapohl,mapohl,20/Feb/23 10:57,06/Mar/23 12:48,04/Jun/24 20:41,06/Mar/23 12:48,1.15.3,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798

This build timed out. The stacktraces revealed multiple tests that might have caused this:
* {{StreamCheckpointingITCase}} through {{StreamFaultToleranceTestBase.runCheckpointedProgram}}
{code}
[...]
2023-02-18T07:37:47.6861582Z    - locked <0x0000000083c85250> (a java.lang.Object)
2023-02-18T07:37:47.6862179Z    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2023-02-18T07:37:47.6862981Z    at org.apache.flink.test.checkpointing.StreamCheckpointingITCase$StringGeneratingSourceFunction.run(StreamCheckpointingITCase.java:169)
2023-02-18T07:37:47.6863762Z    - locked <0x0000000083c85250> (a java.lang.Object)
[...]
2023-02-18T07:37:47.7904307Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fca5000b800 nid=0x56636 waiting on condition [0x00007fca57c58000]
2023-02-18T07:37:47.7904803Z    java.lang.Thread.State: WAITING (parking)
2023-02-18T07:37:47.7905160Z    at sun.misc.Unsafe.park(Native Method)
2023-02-18T07:37:47.7905932Z    - parking to wait for  <0x0000000083c9df48> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-18T07:37:47.7906498Z    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2023-02-18T07:37:47.7907074Z    at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2023-02-18T07:37:47.7907764Z    at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2023-02-18T07:37:47.7908457Z    at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2023-02-18T07:37:47.7909019Z    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2023-02-18T07:37:47.7909605Z    at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:93)
2023-02-18T07:37:47.7910413Z    at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:134)
[...]
{code}
or {{LocalRecoveryITCase}}/{{EventTimeWindowCheckpointingITCase}}:
{code}
[...]
2023-02-18T07:37:51.6744983Z ""main"" #1 prio=5 os_prio=0 tid=0x00007efc4000b800 nid=0x5645a waiting on condition [0x00007efc49b4e000]
2023-02-18T07:37:51.6745471Z    java.lang.Thread.State: WAITING (parking)
2023-02-18T07:37:51.6745823Z    at sun.misc.Unsafe.park(Native Method)
2023-02-18T07:37:51.6746482Z    - parking to wait for  <0x000000008718cce8> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-18T07:37:51.6747147Z    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2023-02-18T07:37:51.6747725Z    at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2023-02-18T07:37:51.6748313Z    at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2023-02-18T07:37:51.6748892Z    at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2023-02-18T07:37:51.6749457Z    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2023-02-18T07:37:51.6750118Z    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1989)
2023-02-18T07:37:51.6750881Z    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1969)
2023-02-18T07:37:51.6751694Z    at org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.testSlidingTimeWindow(EventTimeWindowCheckpointingITCase.java:524)
2023-02-18T07:37:51.6752476Z    at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:84)
2023-02-18T07:37:51.6753157Z    at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:66)
2023-02-18T07:37:51.6753727Z    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31036,,,FLINK-28076,,,,,,,,"21/Feb/23 13:04;fanrui;logs-cron_azure-test_cron_azure_finegrained_resource_management-1676692614.zip;https://issues.apache.org/jira/secure/attachment/13055688/logs-cron_azure-test_cron_azure_finegrained_resource_management-1676692614.zip",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 12:48:38 UTC 2023,,,,,,,,,,"0|z1g1xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 10:58;mapohl;[~roman] may you have a look to ensure that it's not also an issue in newer versions of Flink?;;;","20/Feb/23 15:11;fanrui;Hi [~mapohl] , I checked the StreamCheckpointingITCase, it's normal. It starts too late, so we see the its stack. And I checked the log, it successfully run finally.

 
{code:java}
07:38:20,322 [                main] INFO  org.apache.flink.test.checkpointing.StreamCheckpointingITCase [] -
--------------------------------------------------------------------------------
Test runCheckpointedProgram[FailoverStrategy: RestartAllFailoverStrategy](org.apache.flink.test.checkpointing.StreamCheckpointingITCase) successfully run. {code}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12947;;;","21/Feb/23 06:16;fanrui;Too many tests run slowly, I'm not sure whether the container has some issues. And 2 tests run slowly and throw `IO error: No space left on device`:

 

 
{code:java}
Time elapsed: 10,161.144 s <<< FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12690

Time elapsed: 2,612.2 s <<< FAILURE! - in org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12874{code};;;","21/Feb/23 12:11;mapohl;{quote}
Hi Matthias Pohl , I checked the StreamCheckpointingITCase, it's normal. It starts too late, so we see the its stack. And I checked the log, it successfully run finally.
{quote}

Thanks for looking into it, [~fanrui]. But where do you see that {{StreamCheckpointingITCase}} actually finishes successfully? I struggle to find the {{[...] StreamCheckpointingITCase) successfully run.}} line in the logs.;;;","21/Feb/23 13:04;fanrui;{quote}where do you see that {{StreamCheckpointingITCase}} actually finishes successfully?
{quote}
From the uploaded log[1], I downloaded it from the artifacts[2], but  I don't know why it cannot be find  now. So I upload it now.

[1] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=f0c99625-3655-5ede-d540-0124145e2e74&l=9

[2] [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=artifacts&pathAsName=false&type=publishedArtifacts]

 

 ;;;","21/Feb/23 17:45;roman;Thanks [~mapohl] and [~fanrui] , 

In the artifacts (logs-cron_azure-test_cron_azure_finegrained_resource_management-1676692614/mvn-1.log),

I've found the same problem with PartiallyFinishedSourcesITCase at 04:46 as in FLINK-31133: checkpoint failure -> running for too long -> no space left on device.

So I'd close this ticket as a duplicate.

 

However, there are some strange exceptions before that:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=5584] 
{code:java}
Feb 18 03:58:47 [INFO] Running org.apache.flink.core.memory.OffHeapUnsafeMemorySegmentTest
Exception in thread ""Thread-13"" java.lang.IllegalStateException: MemorySegment can be freed only once!
    at org.apache.flink.core.memory.MemorySegment.free(MemorySegment.java:244)
    at java.lang.Thread.run(Thread.java:748)
Exception in thread ""Thread-15"" java.lang.IllegalStateException: MemorySegment can be freed only once!
    at org.apache.flink.core.memory.MemorySegment.free(MemorySegment.java:244)
    at java.lang.Thread.run(Thread.java:748)
Exception in thread ""Thread-17"" java.lang.IllegalStateException: MemorySegment can be freed only once!
    at org.apache.flink.core.memory.MemorySegment.free(MemorySegment.java:244)
    at java.lang.Thread.run(Thread.java:748){code}
 

Starting at 04:08:
{code:java}
04:32:18,352 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job f1ec611893996fc2fc1830697195194b reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.flink.runtime.taskmanager.RuntimeEnvironment.getInputGate(RuntimeEnvironment.java:274)
        at org.apache.flink.runtime.jobmanager.Tasks.consumeInputs(Tasks.java:111)
        at org.apache.flink.runtime.jobmanager.Tasks.access$000(Tasks.java:30)
        at org.apache.flink.runtime.jobmanager.Tasks$AgnosticTertiaryReceiver.invoke(Tasks.java:102)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
 
{code}
 

 
{code:java}
05:45:35,489 [CHAIN DataSource (at testIncorrectSerializer2(CustomSerializationITCase.java:104) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) -> Map (Map at testIncorrectSerializer  2(CustomSerializationITCase.java:105)) (5/5)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for CHAIN DataSource (at testIncorrectSerializer2(C  ustomSerializationITCase.java:104) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) -> Map (Map at testIncorrectSerializer2(CustomSerializationITCase.java:105)) (5/5)#0 (2288df9d2da462bd61d2000e88d726ab).
05:45:35,490 [   Partition (4/5)#0] ERROR org.apache.flink.runtime.operators.BatchTask                 [] - Error in task code:  Partition (4/5)
java.io.IOException: Serializer consumed more bytes than the record had. This indicates broken serialization. If you are using custom serialization types (Value or Writable), check their seriali  zation methods. If you are using a Kryo-serialized type, check the corresponding Kryo serializer.
        at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readInto(NonSpanningWrapper.java:339) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNonSpanningRecord(SpillingAdaptiveSpanningRecordDeserializer.java:128) ~[flink-run  time-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:103) ~[flink-runtime-1.  15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:93) ~[flink-runtime-1.15  -SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:118) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:48) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:73) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.operators.NoOpDriver.run(NoOpDriver.java:100) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:514) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:357) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.lang.IndexOutOfBoundsException: pos: 140625469934036, length: 32941, index: 4, offset: 0
        at org.apache.flink.core.memory.MemorySegment.get(MemorySegment.java:453) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readFully(NonSpanningWrapper.java:101) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readFully(NonSpanningWrapper.java:92) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.test.misc.CustomSerializationITCase$ConsumesTooMuchSpanning.read(CustomSerializationITCase.java:214) ~[test-classes/:?]
        at org.apache.flink.api.java.typeutils.runtime.ValueSerializer.deserialize(ValueSerializer.java:123) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.api.java.typeutils.runtime.ValueSerializer.deserialize(ValueSerializer.java:118) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.api.java.typeutils.runtime.ValueSerializer.deserialize(ValueSerializer.java:46) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:53) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readInto(NonSpanningWrapper.java:337) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        ... 14 more{code}
 

cc: [~pnowojski] ;;;","22/Feb/23 09:31;pnowojski;First about the lack of space on the device. Hasn't something changed on the release-1.15 branch quite recently? Are those issues only visible on the release-1.15 branch? Maybe some backported change doesn't work correctly on that branch? (FLINK-30461?).

The other errors spotted by [~roman] also look strange...;;;","27/Feb/23 12:59;mapohl;About the MemorySegment issue: I created FLINK-31244 to cover it. I guess, removing the error output sounds reasonable. How do we proceed with the other issue?;;;","02/Mar/23 23:54;roman;I'd close this ticket if there will be no failures after FLINK-31133 and FLINK-27169 are resolved, WDYT?;;;","06/Mar/23 12:48;mapohl;Fair point. Thanks [~roman] I'm closing this issue due to its unclearity. We're gonna create a new issue if something like that pops up again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client doesn't print results for SHOW CREATE TABLE/DESC in hive dialect,FLINK-31137,13525415,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,20/Feb/23 10:23,21/Feb/23 12:03,04/Jun/24 20:41,21/Feb/23 12:03,1.17.0,,,,,,,,,,,1.17.0,1.18.0,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 07:06:40 UTC 2023,,,,,,,,,,"0|z1g1xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 07:06;fsk119;Merged into release-1.17: 3ae7b1f3d85db71f0950f1166d60be76720b49f5
Merged into master: 0508d82ae9377bf5674c409c74ffe36f1887cdab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client Gateway mode should not read read execution config,FLINK-31136,13525414,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,20/Feb/23 10:21,23/Feb/23 03:24,04/Jun/24 20:41,23/Feb/23 03:24,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Client,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 02:47:40 UTC 2023,,,,,,,,,,"0|z1g1x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 02:47;fsk119;Merged into release-1.17: db15e1e2014f02cbeb58d8a4fee1befdbd5a3ac8
Merged into master: dec3ba078decbdc212a6ea16ad8728aa7409d9c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigMap DataSize went > 1 MB and cluster stopped working,FLINK-31135,13525412,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,sriramgr,sriramgr,20/Feb/23 10:12,04/May/23 13:39,04/Jun/24 20:41,04/May/23 13:39,kubernetes-operator-1.2.0,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"I am Flink Operator to manage clusters. Flink version: 1.15.2. Flink jobs failed with the below error. It seems the config map size went beyond 1 MB (default size). 

Since it is managed by the operator and config maps are not updated with any manual intervention, I suspect it could be an operator issue. 

 
{code:java}
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://<IP>/api/v1/namespaces/<NS>/configmaps/<job>-config-map. Message: ConfigMap ""<job>-config-map"" is invalid: []: Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=[], message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong, additionalProperties={})], group=null, kind=ConfigMap, name=<job>-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=ConfigMap ""<job>-config-map"" is invalid: []: Too long: must have at most 1048576 bytes, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).
at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:673) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:612) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleUpdate(OperationSupport.java:347) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleUpdate(OperationSupport.java:327) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleUpdate(BaseOperation.java:781) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.lambda$replace$1(HasMetadataOperation.java:183) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:188) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:130) ~[flink-dist-1.15.2.jar:1.15.2]
at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:41) ~[flink-dist-1.15.2.jar:1.15.2]
at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$attemptCheckAndUpdateConfigMap$11(Fabric8FlinkKubeClient.java:325) ~[flink-dist-1.15.2.jar:1.15.2]
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
... 3 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 03:57;zhihaochen;dump_cm.yaml;https://issues.apache.org/jira/secure/attachment/13057308/dump_cm.yaml","03/May/23 04:02;zhihaochen;flink--kubernetes-application-0-parked-logs-ingestion-644b80-b4bc58747-lc865.log.zip;https://issues.apache.org/jira/secure/attachment/13057794/flink--kubernetes-application-0-parked-logs-ingestion-644b80-b4bc58747-lc865.log.zip","18/Apr/23 23:48;zhihaochen;image-2023-04-19-09-48-19-089.png;https://issues.apache.org/jira/secure/attachment/13057377/image-2023-04-19-09-48-19-089.png","03/May/23 03:47;zhihaochen;image-2023-05-03-13-47-51-440.png;https://issues.apache.org/jira/secure/attachment/13057791/image-2023-05-03-13-47-51-440.png","03/May/23 03:51;zhihaochen;image-2023-05-03-13-50-54-783.png;https://issues.apache.org/jira/secure/attachment/13057792/image-2023-05-03-13-50-54-783.png","03/May/23 03:51;zhihaochen;image-2023-05-03-13-51-21-685.png;https://issues.apache.org/jira/secure/attachment/13057793/image-2023-05-03-13-51-21-685.png","27/Apr/23 06:52;zhihaochen;jobmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13057628/jobmanager_log.txt","03/May/23 03:46;zhihaochen;parked-logs-ingestion-644b80-3494e4c01b82eb7a75a76080974b41cd-config-map.yaml;https://issues.apache.org/jira/secure/attachment/13057790/parked-logs-ingestion-644b80-3494e4c01b82eb7a75a76080974b41cd-config-map.yaml",,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 13:39:13 UTC 2023,,,,,,,,,,"0|z1g1wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 10:15;sriramgr;[~gyfora] - Please add your thoughts.;;;","09/Mar/23 11:17;Swathi Chandrashekar;[~sriramgr] , which is the configmap which failed in this scenario ?

If it is a pod-template configmap, then it indirectly depends on the user applied CR as the user defines entries of this file ( most of them ).

If it is a job specific config map ( which has the meta info of all the checkpoints pointers ) , if the retained checkpoints are very high, then I believe, we can hit this issue. ;;;","09/Mar/23 11:52;ram_krish;So are we adding all the checkpoints data back to the CR? ;;;","09/Mar/23 13:10;Swathi Chandrashekar;For job specific config map, its not a flink operator issue and we do not pass the checkpoints data to CR. 

Whenever we create JM HA in flink in kubernetes, the flink creates certain config maps ( dispatcher config map, RM leader config map, etc ).

Similarly whenever we create a job, a job config map ( job master config map ) is created per job by the flink which has keeps track of the pointers to the actual checkpoint data.

So, when the retained.checkpoints are configured to a higher value, many entries will be added in this case in this configMap , which could contribute to the configMap size. 

But, to this specific issue which is mentioned, we are not sure which configMap cause the issue which would help us to investigate further;;;","10/Mar/23 12:22;sriramgr;[~Swathi Chandrashekar] - Yes you're right. It seems it is job specific config map. But let me try validate once again.;;;","17/Mar/23 12:07;mxm;This has been addressed in FLINK-31345.;;;","17/Mar/23 12:08;mxm;Closing because this has been resolved already on the main branch.;;;","17/Mar/23 12:09;mxm;Oh, just realized this is unrelated to FLINK-31345 but a separate config map issue. Reopening :);;;","17/Apr/23 01:43;zhihaochen;I have encountered the same issue. Actually, it's an ongoing issue for us. I believe it has nothing to do with the Flink-Kubernetes-operator as it happened with both Flink Standalone Kubernetes deployment and Flink-kubernetes-operator deployment.

 

I have checked our configuration but didn't find anything interesting.;;;","17/Apr/23 02:49;Swathi Chandrashekar;Thanks [~zhihaochen] , can you please share the configmap which hit the issue ?;;;","17/Apr/23 03:59;zhihaochen;Hi [~Swathi Chandrashekar] , please see the attached configmap file:

[^dump_cm.yaml]

 

^The error shown in Flink dashboard is as:^

^*Checkpoint Detail:*^
*Path:* - *Discarded:* - *Checkpoint Type:* aligned checkpoint *Failure Message:* io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.32.228.1/api/v1/namespaces/parked-logs-ingestion-16805773-a96408/configmaps/parked-logs-ingestion-16805773-a96408-110331249bb495a4d23b4d69849c8224-config-map. Message: ConfigMap ""parked-logs-ingestion-16805773-a96408-110331249bb495a4d23b4d69849c8224-config-map"" is invalid: []: Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=[], message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong, additionalProperties={})], group=null, kind=ConfigMap, name=parked-logs-ingestion-16805773-a96408-110331249bb495a4d23b4d69849c8224-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=ConfigMap ""parked-logs-ingestion-16805773-a96408-110331249bb495a4d23b4d69849c8224-config-map"" is invalid: []: Too long: must have at most 1048576 bytes, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).;;;","18/Apr/23 13:04;Swathi Chandrashekar;Thanks [~zhihaochen] . The maximum configmap supported by kubernetes is 1MB.

The configmap which you shared was a job specific config map which is used to retain all the checkpoint pointers per job. Since you have configured the retained checkpoints ( state.checkpoints.num-retained ) to a very high value, hence this issue has been hit.

Please try to reduce the  state.checkpoints.num-retained configuration and you should hit the issue again.;;;","18/Apr/23 13:06;Swathi Chandrashekar;[~mxm] , [~sriramgr] , [~zhihaochen] let us know if we can mark this issue as resolved or let me know if any further investigation pending;;;","18/Apr/23 14:27;sriramgr;[~Swathi Chandrashekar] - In my case, state.checkpoints.num-retained is the default which is 1. I tried to reproduce this issue. I couldn't. ;;;","18/Apr/23 23:46;zhihaochen;Hi [~Swathi Chandrashekar] , in my case, the state.checkpoints.num-retained for our flink jobs is always set as 5, but looks like that's not respected tho. Please see the code snippet from the flinkdeployment via flink-kubernetes-operator. 

 

 
{code:java}
// code placeholder

apiVersion: v1
items:
- apiVersion: flink.apache.org/v1beta1
  kind: FlinkDeployment
  metadata:
    creationTimestamp: ""2023-04-04T03:02:25Z""
    finalizers:
    - flinkdeployments.flink.apache.org/finalizer
    generation: 2
    labels:
      instanceId: parked-logs-ingestion-16805773-a96408
      jobName: parked-logs-ingestion-16805773
    name: parked-logs-ingestion-16805773-a96408
    namespace: parked-logs-ingestion-16805773-a96408
    resourceVersion: ""533476748""
    uid: 182b9c7e-74cc-490b-8045-9fddaa7b8aa9
  spec:
    flinkConfiguration:
      execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION
      execution.checkpointing.interval: ""60000""
      execution.checkpointing.max-concurrent-checkpoints: ""1""
      execution.checkpointing.min-pause: 5s
      execution.checkpointing.mode: EXACTLY_ONCE
      execution.checkpointing.prefer-checkpoint-for-recovery: ""true""
      execution.checkpointing.timeout: 60min
      high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
      high-availability.storageDir: s3://eureka-flink-data-prod/parked-logs-ingestion-16805773-a96408/ha
      jobmanager.memory.process.size: 1024m
      metrics.reporter.stsd.factory.class: org.apache.flink.metrics.statsd.StatsDReporterFactory
      metrics.reporter.stsd.host: localhost
      metrics.reporter.stsd.interval: 30 SECONDS
      metrics.reporter.stsd.port: ""8125""
      metrics.reporters: stsd
      metrics.scope.jm: jobmanager
      metrics.scope.jm.job: jobmanager.<job_name>
      metrics.scope.operator: taskmanager.<job_name>.<operator_name>
      metrics.scope.task: taskmanager.<job_name>.<task_name>
      metrics.scope.tm: taskmanager
      metrics.scope.tm.job: taskmanager.<job_name>
      metrics.system-resource: ""true""
      metrics.system-resource-probing-interval: ""30000""
      restart-strategy: fixed-delay
      restart-strategy.fixed-delay.attempts: ""2147483647""
      state.backend: hashmap
      state.checkpoint-storage: filesystem
      state.checkpoints.dir: s3://eureka-flink-data-prod/parked-logs-ingestion-16805773-a96408/checkpoints
      state.checkpoints.num-retained: ""5""
      state.savepoints.dir: s3://eureka-flink-data-prod/parked-logs-ingestion-16805773-a96408/savepoints
      taskmanager.memory.managed.size: ""0""
      taskmanager.memory.network.fraction: ""0.1""
      taskmanager.memory.network.max: 1000m
      taskmanager.memory.network.min: 64m
      taskmanager.memory.process.size: 2048m
      taskmanager.numberOfTaskSlots: ""10""
      web.cancel.enable: ""false""
    flinkVersion: v1_15

 {code}
in UI:

!image-2023-04-19-09-48-19-089.png|width=590,height=386!

I got the same issue before we switched to the flink-kubenertes-operator. At that time we were using flink standalone deployment on Kubernetes. We set state.checkpoints.num-retained as 5, but hit the same issue.

 ;;;","26/Apr/23 00:57;zhihaochen;Hi [~Swathi Chandrashekar], can I ask do we have any update on this?;;;","26/Apr/23 17:58;Swathi Chandrashekar;I missed your previous comment. The configuration your using to retain the checkpoints seemscorrect. Can you please check the JM logs once if there's error while cleaning the checkpoints ?

[https://github.com/apache/flink/blob/release-1.15/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointsCleaner.java#L85] . 

In 1.15, irrespective of whether the cleanup was successful or not, the no. of checkpoints to clean is always decremented.

The JM logs might help to understand why the cleanup failed. Or if your using custom clean up logic, might need to check that once.;;;","27/Apr/23 06:53;zhihaochen;[~Swathi Chandrashekar], please see the attached log from JM with this issue. I didn't find the error message of discard completed checkpoint tho.

[^jobmanager_log.txt];;;","02/May/23 18:12;Swathi Chandrashekar;[~zhihaochen] , looks like for

jobId: 07bfdfef145a87c2071965081aaff548 , it tried to recover the job according to JM logs and tried to recover the checkpoints 1012 - 1015. The pointers were present in configMap :

parked-logs-ingestion-16818796-0c2923-07bfdfef145a87c2071965081aaff548-config-map"".

The configMap of this was already 1Mb and any checkpoint which is triggered failed in this case with the following error.

{""@timestamp"":""2023-04-26T23:46:54.190Z"",""ecs.version"":""1.2.0"",""log.level"":""WARN"",""message"":""An error occurred while writing checkpoint 11211 to the underlying metadata store. Flink was not able to determine whether the metadata was successfully persisted. The corresponding state located at 's3://eureka-flink-data-prod/parked-logs-ingestion-16818796-0c2923/checkpoints/07bfdfef145a87c2071965081aaff548/{*}chk-11211' won't be discarded and needs to be cleaned up manually.{*}"",""process.thread.name"":""jobmanager-io-thread-1"",""log.logger"":""org.apache.flink.runtime.checkpoint.CheckpointCoordinator""} {""@timestamp"":""2023-04-26T23:46:54.248Z"",""ecs.version"":""1.2.0"",""log.level"":""WARN"",""message"":""{*}Error while processing AcknowledgeCheckpoint{*} message"",""process.thread.name"":""jobmanager-io-thread-1"",""log.logger"":""org.apache.flink.runtime.jobmaster.JobMaster"",""error.type"":""org.apache.flink.runtime.checkpoint.CheckpointException"",""error.message"":""Could not complete the pending checkpoint 11211. Failure reason: Failure to finalize checkpoint."",""error.stack_trace"":""org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete the pending checkpoint 11211. Failure reason: Failure to finalize checkpoint.\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.addCompletedCheckpointToStoreAndSubsumeOldest(CheckpointCoordinator.java:1404)\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1249)\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1134)\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89)\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.flink.runtime.persistence.PossibleInconsistentStateException: io.fabric8.kubernetes.client.KubernetesClientException: *Failure executing: PUT at: https://10.32.228.1/api/v1/namespaces/parked-logs-ingestion-16818796-0c2923/configmaps/parked-logs-ingestion-16818796-0c2923-07bfdfef145a87c2071965081aaff548-config-map. Message: ConfigMap \""parked-logs-ingestion-16818796-0c2923-07bfdfef145a87c2071965081aaff548-config-map\"" is invalid: []: Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=[], message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong*

These were the only logs present from the JM and from the log attached, couldnt find the logs when the previous checkpoints were taken.  As in this new JM logs, the JobId was recovered, so possible that there was a JM restart and JobId was also restarted.

 ;;;","03/May/23 03:53;zhihaochen;hey [~Swathi Chandrashekar], thank you for looking into it.
{quote}This error was populated for all the checkpoints due to state inconsistency which resulted in storing lot of checkpoints in S3, which eventually caused the size of the configMap > 1MB ]
{quote}
I don't think that's the case. Instead, none of the checkpoint records in the CM was ever cleaned up. The error ""Flink was not able to determine whether the metadata was successfully persisted"" starts to happen when the CM reaches the 1MB size limitation.

I have another flink job running here as an example.

Configmap: [^parked-logs-ingestion-644b80-3494e4c01b82eb7a75a76080974b41cd-config-map.yaml]
The checkpoint ids are as ""checkpointID-0000000000000000001"", ""checkpointID-0000000000000000002"", ... ""checkpointID-0000000000000001040"". in a consecutive way. Worth reminding the IDs are from ""1"" to ""1040"". The configmap has reached the 1MB size limitation.
 
The ""Flink was not able to determine whether the metadata was successfully persisted."" actually happens when the CM attached the record ""1040"". Please see the logs below. The bottom one is first error log, which complains about the record ""1041"". I think that makes sense as it's not recorded in the CM, hence Flink can't determine if the metadata was successfully persisted.
!image-2023-05-03-13-47-51-440.png|width=1465,height=799!
 
The flink dashboard log also reflects the assumption.
!image-2023-05-03-13-51-21-685.png|width=1473,height=783!
 JM log:[^flink--kubernetes-application-0-parked-logs-ingestion-644b80-b4bc58747-lc865.log.zip]
 
My guess is that Flink never cleaned any of the records in CM at all for our cases.
 ;;;","03/May/23 07:41;Swathi Chandrashekar;[~zhihaochen] , thanks for the attachment of the JM logs where the job is first submitted. Earlier JM logs did not help much to root cause, as it had recovered a job which already had multiple checkpoints and further checkpoints trigger post that failed due to >1MB issue and needs to be manually cleaned up, if succeeded.

The new JM attachment helped to deduce the issue as it starts with job submission.

After the initial 5 checkpoints, when the cleanup was happening, flink threw the following error :
{code:java}
{""@timestamp"":""2023-05-02T07:21:34.047Z"",""ecs.version"":""1.2.0"",""log.level"":""WARN"",""message"":""Fail to subsume the old checkpoint."",""process.thread.name"":""jobmanager-io-thread-1"",""log.logger"":""org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper"",""error.type"":""java.util.concurrent.ExecutionException"",""error.message"":""java.io.IOException: /parked-logs-ingestion-644b80/ha/parked-logs-ingestion-644b80/completedCheckpointf2bebc94bd32 could not be deleted for unknown reasons."",""error.stack_trace"":""java.util.concurrent.ExecutionException: java.io.IOException: /parked-logs-ingestion-644b80/ha/parked-logs-ingestion-644b80/completedCheckpointf2bebc94bd32 could not be deleted for unknown reasons.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)\n\tat java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)\n\tat org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.releaseAndTryRemove(KubernetesStateHandleStore.java:526)\n\tat org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.tryRemove(DefaultCompletedCheckpointStore.java:242)\n\tat org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.tryRemoveCompletedCheckpoint(DefaultCompletedCheckpointStore.java:227)\n\tat org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.lambda$addCheckpointAndSubsumeOldestOne$0(DefaultCompletedCheckpointStore.java:145)\n\tat org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper.subsume(CheckpointSubsumeHelper.java:70)\n\tat org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.addCheckpointAndSubsumeOldestOne(DefaultCompletedCheckpointStore.java:141)\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.addCompletedCheckpointToStoreAndSubsumeOldest(CheckpointCoordinator.java:1382)\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1249)\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1134)\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89)\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.io.IOException: /parked-logs-ingestion-644b80/ha/parked-logs-ingestion-644b80/completedCheckpointf2bebc94bd32 could not be deleted for unknown reasons.\n\tat org.apache.flink.fs.s3presto.FlinkS3PrestoFileSystem.deleteObject(FlinkS3PrestoFileSystem.java:135)\n\tat org.apache.flink.fs.s3presto.FlinkS3PrestoFileSystem.delete(FlinkS3PrestoFileSystem.java:66)\n\tat org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.delete(PluginFileSystemFactory.java:155)\n\tat org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:89)\n\tat org.apache.flink.runtime.state.RetrievableStreamStateHandle.discardState(RetrievableStreamStateHandle.java:76)\n\tat org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.lambda$releaseAndTryRemove$12(KubernetesStateHandleStore.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(Unknown Source)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)\n\tat org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperation$1(FutureUtils.java:201)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)\n\tat java.base/java.util.concurrent.CompletableFuture$Completion.run(Unknown Source)\n\t... 3 more\n""} {code}
Flink is encountering a IO Exception when Flink is trying to delete a file in S3. It says file .
{code:java}
java.io.IOException: /parked-logs-ingestion-644b80/ha/parked-logs-ingestion-644b80/completedCheckpointf2bebc94bd32 could not be deleted for unknown reasons.{code}
{code:java}
java.io.IOException: /parked-logs-ingestion-644b80/ha/parked-logs-ingestion-644b80/completedCheckpointf2bebc94bd32 could not be deleted for unknown reasons.\n\tat org.apache.flink.fs.s3presto.FlinkS3PrestoFileSystem.deleteObject(FlinkS3PrestoFileSystem.java:135)\n\tat {code}
Possible reasons:

1) Permission issues: Flink might not have permission to delete the objects from S3 where the checkpoints are stored.

2) Compatibility issues between the flink version and the S3 file system implemention you are using.

3) S3 related some config issue.;;;","04/May/23 00:57;zhihaochen;[~Swathi Chandrashekar] thank you for pointing it out! I believe there are some S3 permission issues from our side. I've missed the error information. I'll fix it from our side and let you know if it's all good. Please feel free to close this ticket.;;;","04/May/23 04:45;zhihaochen;It's working as expected now after we fixed our S3 deletion issue. Thanks for your help!;;;","04/May/23 05:02;Swathi Chandrashekar;That's great [~zhihaochen] :)

[~zhihaochen] , [~sriramgr] , [~mxm] I don't have the permission to close the issue. Can you please close the issue if no other action needed. ;;;","04/May/23 13:38;sriramgr;In my case, it is not a permission issue. I couldn't repro the issue. My hunch is there could be a network issue during that time. 

Thanks, [~Swathi Chandrashekar] [~zhihaochen]. I am closing it.;;;","04/May/23 13:39;sriramgr;It was an intermittent issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Out of memory error in KafkaSourceE2ECase>SourceTestSuiteBase.testScaleDown,FLINK-31134,13525411,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,20/Feb/23 10:10,16/Oct/23 12:15,04/Jun/24 20:41,16/Oct/23 12:15,1.15.3,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=17378

{code}
2023-02-20T02:35:33.6571979Z Feb 20 02:35:33 [ERROR] org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.testScaleDown(TestEnvironment, DataStreamSourceExternalContext, CheckpointingMode)[1]  Time elapsed: 3.864 s  <<< FAILURE!
2023-02-20T02:35:33.6601326Z Feb 20 02:35:33 java.lang.AssertionError: 
2023-02-20T02:35:33.6604200Z Feb 20 02:35:33 
2023-02-20T02:35:33.6609074Z Feb 20 02:35:33 Expecting
2023-02-20T02:35:33.6609502Z Feb 20 02:35:33   <CompletableFuture[Failed with the following stack trace:
2023-02-20T02:35:33.6612012Z Feb 20 02:35:33 java.lang.RuntimeException: Failed to fetch next result
2023-02-20T02:35:33.6619282Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2023-02-20T02:35:33.6620147Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2023-02-20T02:35:33.6623837Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.compareWithExactlyOnceSemantic(CollectIteratorAssert.java:116)
2023-02-20T02:35:33.6624650Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.matchesRecordsFromSource(CollectIteratorAssert.java:71)
2023-02-20T02:35:33.7035280Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.lambda$checkResultWithSemantic$3(SourceTestSuiteBase.java:739)
2023-02-20T02:35:33.7041843Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2023-02-20T02:35:33.7048557Z Feb 20 02:35:33 	at java.lang.Thread.run(Thread.java:750)
2023-02-20T02:35:33.7052749Z Feb 20 02:35:33 Caused by: java.io.IOException: Failed to fetch job execution result
2023-02-20T02:35:33.7058304Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2023-02-20T02:35:33.7059262Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2023-02-20T02:35:33.7060076Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2023-02-20T02:35:33.7060620Z Feb 20 02:35:33 	... 6 more
2023-02-20T02:35:33.7061345Z Feb 20 02:35:33 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: deb052f89a69f00f2c8df55054c62136)
2023-02-20T02:35:33.7062049Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2023-02-20T02:35:33.7066005Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-20T02:35:33.7072200Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2023-02-20T02:35:33.7077028Z Feb 20 02:35:33 	... 8 more
2023-02-20T02:35:33.7081588Z Feb 20 02:35:33 Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: deb052f89a69f00f2c8df55054c62136)
2023-02-20T02:35:33.7088309Z Feb 20 02:35:33 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
2023-02-20T02:35:33.7093780Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2023-02-20T02:35:33.7099183Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2023-02-20T02:35:33.7099867Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.7103374Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2023-02-20T02:35:33.7165354Z Feb 20 02:35:33 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
2023-02-20T02:35:33.7171212Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2023-02-20T02:35:33.7175410Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2023-02-20T02:35:33.7180022Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.7184148Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2023-02-20T02:35:33.7185495Z Feb 20 02:35:33 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$26(RestClusterClient.java:708)
2023-02-20T02:35:33.7192096Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2023-02-20T02:35:33.7197730Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2023-02-20T02:35:33.7203549Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.7207460Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2023-02-20T02:35:33.7211377Z Feb 20 02:35:33 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
2023-02-20T02:35:33.7216442Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2023-02-20T02:35:33.7217215Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2023-02-20T02:35:33.7221961Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.7226071Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2023-02-20T02:35:33.7231583Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2023-02-20T02:35:33.7242894Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2023-02-20T02:35:33.7248811Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-02-20T02:35:33.7253454Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-02-20T02:35:33.7254177Z Feb 20 02:35:33 	... 1 more
2023-02-20T02:35:33.7258240Z Feb 20 02:35:33 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2023-02-20T02:35:33.7262591Z Feb 20 02:35:33 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2023-02-20T02:35:33.7270403Z Feb 20 02:35:33 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128)
2023-02-20T02:35:33.7276023Z Feb 20 02:35:33 	... 24 more
2023-02-20T02:35:33.7279492Z Feb 20 02:35:33 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2023-02-20T02:35:33.7280484Z Feb 20 02:35:33 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2023-02-20T02:35:33.7284357Z Feb 20 02:35:33 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2023-02-20T02:35:33.7289462Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
2023-02-20T02:35:33.7292875Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
2023-02-20T02:35:33.7299034Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
2023-02-20T02:35:33.7305815Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
2023-02-20T02:35:33.7315490Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2023-02-20T02:35:33.7316250Z Feb 20 02:35:33 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2023-02-20T02:35:33.7322640Z Feb 20 02:35:33 	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
2023-02-20T02:35:33.7324728Z Feb 20 02:35:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-02-20T02:35:33.7330531Z Feb 20 02:35:33 	at java.lang.reflect.Method.invoke(Method.java:498)
2023-02-20T02:35:33.7336350Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2023-02-20T02:35:33.7341911Z Feb 20 02:35:33 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2023-02-20T02:35:33.7357167Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2023-02-20T02:35:33.7359230Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2023-02-20T02:35:33.7363705Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2023-02-20T02:35:33.7370712Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2023-02-20T02:35:33.7375882Z Feb 20 02:35:33 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2023-02-20T02:35:33.7379972Z Feb 20 02:35:33 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2023-02-20T02:35:33.7384918Z Feb 20 02:35:33 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2023-02-20T02:35:33.7385694Z Feb 20 02:35:33 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2023-02-20T02:35:33.7388858Z Feb 20 02:35:33 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2023-02-20T02:35:33.7393884Z Feb 20 02:35:33 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2023-02-20T02:35:33.7398005Z Feb 20 02:35:33 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2023-02-20T02:35:33.7403562Z Feb 20 02:35:33 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2023-02-20T02:35:33.7409240Z Feb 20 02:35:33 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2023-02-20T02:35:33.7413825Z Feb 20 02:35:33 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2023-02-20T02:35:33.7415974Z Feb 20 02:35:33 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2023-02-20T02:35:33.7418540Z Feb 20 02:35:33 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2023-02-20T02:35:33.7427043Z Feb 20 02:35:33 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2023-02-20T02:35:33.7432180Z Feb 20 02:35:33 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2023-02-20T02:35:33.7438071Z Feb 20 02:35:33 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2023-02-20T02:35:33.7438998Z Feb 20 02:35:33 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2023-02-20T02:35:33.7444284Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2023-02-20T02:35:33.7449118Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2023-02-20T02:35:33.7467810Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2023-02-20T02:35:33.7474847Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2023-02-20T02:35:33.7481114Z Feb 20 02:35:33 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2023-02-20T02:35:33.7485476Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
2023-02-20T02:35:33.7486278Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
2023-02-20T02:35:33.7496774Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
2023-02-20T02:35:33.7502745Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
2023-02-20T02:35:33.7518007Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
2023-02-20T02:35:33.7563887Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2023-02-20T02:35:33.7567884Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
2023-02-20T02:35:33.7571624Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2023-02-20T02:35:33.7572539Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807)
2023-02-20T02:35:33.7576541Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756)
2023-02-20T02:35:33.7580076Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
2023-02-20T02:35:33.7585975Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
2023-02-20T02:35:33.7590344Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
2023-02-20T02:35:33.7594197Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
2023-02-20T02:35:33.7598104Z Feb 20 02:35:33 	at java.lang.Thread.run(Thread.java:750)
2023-02-20T02:35:33.7598653Z Feb 20 02:35:33 Caused by: java.lang.OutOfMemoryError: Java heap space
2023-02-20T02:35:33.7605912Z Feb 20 02:35:33 	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
2023-02-20T02:35:33.7610463Z Feb 20 02:35:33 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
2023-02-20T02:35:33.7640707Z Feb 20 02:35:33 	at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30)
2023-02-20T02:35:33.7645540Z Feb 20 02:35:33 	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:113)
2023-02-20T02:35:33.7721460Z Feb 20 02:35:33 	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
2023-02-20T02:35:33.7722379Z Feb 20 02:35:33 	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
2023-02-20T02:35:33.7761361Z Feb 20 02:35:33 	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
2023-02-20T02:35:33.7785499Z Feb 20 02:35:33 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
2023-02-20T02:35:33.7789413Z Feb 20 02:35:33 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
2023-02-20T02:35:33.7793849Z Feb 20 02:35:33 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
2023-02-20T02:35:33.7798951Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
2023-02-20T02:35:33.7799705Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
2023-02-20T02:35:33.7804180Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
2023-02-20T02:35:33.7808929Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
2023-02-20T02:35:33.7812802Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
2023-02-20T02:35:33.7817983Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.lambda$removeEmptySplits$4(KafkaPartitionSplitReader.java:338)
2023-02-20T02:35:33.7823184Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader$$Lambda$1426/1494823239.get(Unknown Source)
2023-02-20T02:35:33.7827117Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.retryOnWakeup(KafkaPartitionSplitReader.java:439)
2023-02-20T02:35:33.7831473Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.removeEmptySplits(KafkaPartitionSplitReader.java:337)
2023-02-20T02:35:33.7887570Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(KafkaPartitionSplitReader.java:215)
2023-02-20T02:35:33.7918922Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
2023-02-20T02:35:33.7931450Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
2023-02-20T02:35:33.7934964Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
2023-02-20T02:35:33.7938991Z Feb 20 02:35:33 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2023-02-20T02:35:33.7942734Z Feb 20 02:35:33 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2023-02-20T02:35:33.7947863Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-02-20T02:35:33.7948538Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-02-20T02:35:33.7953567Z Feb 20 02:35:33 	... 1 more
2023-02-20T02:35:33.7960358Z Feb 20 02:35:33 ]>
2023-02-20T02:35:33.7966095Z Feb 20 02:35:33 to be completed within 2M.
2023-02-20T02:35:33.7970270Z Feb 20 02:35:33 
2023-02-20T02:35:33.7970867Z Feb 20 02:35:33 exception caught while trying to get the future result: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Failed to fetch next result
2023-02-20T02:35:33.7977783Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2023-02-20T02:35:33.7982225Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-20T02:35:33.7989134Z Feb 20 02:35:33 	at org.assertj.core.internal.Futures.assertSucceededWithin(Futures.java:109)
2023-02-20T02:35:33.7993863Z Feb 20 02:35:33 	at org.assertj.core.api.AbstractCompletableFutureAssert.internalSucceedsWithin(AbstractCompletableFutureAssert.java:400)
2023-02-20T02:35:33.8005460Z Feb 20 02:35:33 	at org.assertj.core.api.AbstractCompletableFutureAssert.succeedsWithin(AbstractCompletableFutureAssert.java:396)
2023-02-20T02:35:33.8021207Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.checkResultWithSemantic(SourceTestSuiteBase.java:741)
2023-02-20T02:35:33.8022121Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:329)
2023-02-20T02:35:33.8039855Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testScaleDown(SourceTestSuiteBase.java:279)
2023-02-20T02:35:33.8047148Z Feb 20 02:35:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-02-20T02:35:33.8053834Z Feb 20 02:35:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-02-20T02:35:33.8057756Z Feb 20 02:35:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-02-20T02:35:33.8062979Z Feb 20 02:35:33 	at java.lang.reflect.Method.invoke(Method.java:498)
2023-02-20T02:35:33.8067430Z Feb 20 02:35:33 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2023-02-20T02:35:33.8068205Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-02-20T02:35:33.8084092Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-02-20T02:35:33.8086430Z Feb 20 02:35:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2023-02-20T02:35:33.8091529Z Feb 20 02:35:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2023-02-20T02:35:33.8095711Z Feb 20 02:35:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2023-02-20T02:35:33.8100200Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2023-02-20T02:35:33.8105626Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2023-02-20T02:35:33.8106481Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-02-20T02:35:33.8110824Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-02-20T02:35:33.8115401Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-02-20T02:35:33.8119477Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-02-20T02:35:33.8123232Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2023-02-20T02:35:33.8126522Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2023-02-20T02:35:33.8131150Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2023-02-20T02:35:33.8134609Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8135399Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2023-02-20T02:35:33.8139846Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2023-02-20T02:35:33.8144099Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2023-02-20T02:35:33.8148759Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-02-20T02:35:33.8153437Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8258078Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8287534Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8288428Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8306752Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8311332Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8321055Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8325096Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2023-02-20T02:35:33.8330535Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2023-02-20T02:35:33.8335300Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2023-02-20T02:35:33.8340477Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2023-02-20T02:35:33.8341206Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2023-02-20T02:35:33.8341860Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-20T02:35:33.8342446Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-20T02:35:33.8344739Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2023-02-20T02:35:33.8345312Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-20T02:35:33.8350125Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-20T02:35:33.8350695Z Feb 20 02:35:33 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2023-02-20T02:35:33.8353059Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2023-02-20T02:35:33.8353637Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2023-02-20T02:35:33.8357250Z Feb 20 02:35:33 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2023-02-20T02:35:33.8362239Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-20T02:35:33.8366794Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-20T02:35:33.8372939Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-20T02:35:33.8373550Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-20T02:35:33.8380470Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-20T02:35:33.8384170Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-20T02:35:33.8384746Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2023-02-20T02:35:33.8388492Z Feb 20 02:35:33 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2023-02-20T02:35:33.8394487Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-20T02:35:33.8396502Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-20T02:35:33.8398692Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-20T02:35:33.8402044Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-20T02:35:33.8404194Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-20T02:35:33.8404748Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-20T02:35:33.8407167Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2023-02-20T02:35:33.8407900Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2023-02-20T02:35:33.8410326Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-02-20T02:35:33.8411062Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8414460Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8415147Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8415775Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8416489Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8417195Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8421929Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8422630Z Feb 20 02:35:33 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-02-20T02:35:33.8425526Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2023-02-20T02:35:33.8426375Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-02-20T02:35:33.8427049Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8430288Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8430959Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8431613Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8432330Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8433030Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8436589Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8437309Z Feb 20 02:35:33 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-02-20T02:35:33.8437966Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2023-02-20T02:35:33.8438791Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-02-20T02:35:33.8439643Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8453519Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8454213Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8454878Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8455596Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8456243Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8456912Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8463365Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2023-02-20T02:35:33.8464211Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2023-02-20T02:35:33.8464958Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
2023-02-20T02:35:33.8465684Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2023-02-20T02:35:33.8480449Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2023-02-20T02:35:33.8481350Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2023-02-20T02:35:33.8482105Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2023-02-20T02:35:33.8483031Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2023-02-20T02:35:33.8483717Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2023-02-20T02:35:33.8488892Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2023-02-20T02:35:33.8489606Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2023-02-20T02:35:33.8490348Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2023-02-20T02:35:33.8491103Z Feb 20 02:35:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2023-02-20T02:35:33.8491796Z Feb 20 02:35:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2023-02-20T02:35:33.8495235Z Feb 20 02:35:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2023-02-20T02:35:33.8495922Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2023-02-20T02:35:33.8496555Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2023-02-20T02:35:33.8497159Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2023-02-20T02:35:33.8499964Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2023-02-20T02:35:33.8500551Z Feb 20 02:35:33 Caused by: java.lang.RuntimeException: Failed to fetch next result
2023-02-20T02:35:33.8501305Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2023-02-20T02:35:33.8502083Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2023-02-20T02:35:33.8502933Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.compareWithExactlyOnceSemantic(CollectIteratorAssert.java:116)
2023-02-20T02:35:33.8506223Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.matchesRecordsFromSource(CollectIteratorAssert.java:71)
2023-02-20T02:35:33.8507029Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.lambda$checkResultWithSemantic$3(SourceTestSuiteBase.java:739)
2023-02-20T02:35:33.8507751Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2023-02-20T02:35:33.8508295Z Feb 20 02:35:33 	at java.lang.Thread.run(Thread.java:750)
2023-02-20T02:35:33.8508741Z Feb 20 02:35:33 Caused by: java.io.IOException: Failed to fetch job execution result
2023-02-20T02:35:33.8511646Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2023-02-20T02:35:33.8512406Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2023-02-20T02:35:33.8515917Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2023-02-20T02:35:33.8516485Z Feb 20 02:35:33 	... 6 more
2023-02-20T02:35:33.8517057Z Feb 20 02:35:33 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: deb052f89a69f00f2c8df55054c62136)
2023-02-20T02:35:33.8517782Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2023-02-20T02:35:33.8520857Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-20T02:35:33.8521702Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2023-02-20T02:35:33.8522272Z Feb 20 02:35:33 	... 8 more
2023-02-20T02:35:33.8525696Z Feb 20 02:35:33 Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: deb052f89a69f00f2c8df55054c62136)
2023-02-20T02:35:33.8526439Z Feb 20 02:35:33 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
2023-02-20T02:35:33.8527127Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2023-02-20T02:35:33.8533130Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2023-02-20T02:35:33.8533796Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.8534374Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2023-02-20T02:35:33.8535047Z Feb 20 02:35:33 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
2023-02-20T02:35:33.8540462Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2023-02-20T02:35:33.8541129Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2023-02-20T02:35:33.8541772Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.8546133Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2023-02-20T02:35:33.8546842Z Feb 20 02:35:33 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$26(RestClusterClient.java:708)
2023-02-20T02:35:33.8547629Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2023-02-20T02:35:33.8548277Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2023-02-20T02:35:33.8548914Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.8553248Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2023-02-20T02:35:33.8553937Z Feb 20 02:35:33 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
2023-02-20T02:35:33.8554607Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2023-02-20T02:35:33.8558492Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2023-02-20T02:35:33.8559185Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2023-02-20T02:35:33.8559811Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2023-02-20T02:35:33.8560682Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2023-02-20T02:35:33.8561396Z Feb 20 02:35:33 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2023-02-20T02:35:33.8565571Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-02-20T02:35:33.8566217Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-02-20T02:35:33.8568485Z Feb 20 02:35:33 	... 1 more
2023-02-20T02:35:33.8568955Z Feb 20 02:35:33 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2023-02-20T02:35:33.8569570Z Feb 20 02:35:33 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2023-02-20T02:35:33.8573199Z Feb 20 02:35:33 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128)
2023-02-20T02:35:33.8573935Z Feb 20 02:35:33 	... 24 more
2023-02-20T02:35:33.8577229Z Feb 20 02:35:33 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2023-02-20T02:35:33.8578006Z Feb 20 02:35:33 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2023-02-20T02:35:33.8578798Z Feb 20 02:35:33 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2023-02-20T02:35:33.8579566Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
2023-02-20T02:35:33.8582693Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
2023-02-20T02:35:33.8583446Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
2023-02-20T02:35:33.8584170Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
2023-02-20T02:35:33.8584852Z Feb 20 02:35:33 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2023-02-20T02:35:33.8587756Z Feb 20 02:35:33 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2023-02-20T02:35:33.8588341Z Feb 20 02:35:33 	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
2023-02-20T02:35:33.8588915Z Feb 20 02:35:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-02-20T02:35:33.8589496Z Feb 20 02:35:33 	at java.lang.reflect.Method.invoke(Method.java:498)
2023-02-20T02:35:33.8590099Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2023-02-20T02:35:33.8594219Z Feb 20 02:35:33 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2023-02-20T02:35:33.8594946Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2023-02-20T02:35:33.8595606Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2023-02-20T02:35:33.8596238Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2023-02-20T02:35:33.8596905Z Feb 20 02:35:33 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2023-02-20T02:35:33.8599678Z Feb 20 02:35:33 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2023-02-20T02:35:33.8600384Z Feb 20 02:35:33 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2023-02-20T02:35:33.8600947Z Feb 20 02:35:33 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2023-02-20T02:35:33.8601498Z Feb 20 02:35:33 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2023-02-20T02:35:33.8602012Z Feb 20 02:35:33 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2023-02-20T02:35:33.8605347Z Feb 20 02:35:33 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2023-02-20T02:35:33.8605918Z Feb 20 02:35:33 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2023-02-20T02:35:33.8606487Z Feb 20 02:35:33 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2023-02-20T02:35:33.8607025Z Feb 20 02:35:33 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2023-02-20T02:35:33.8609612Z Feb 20 02:35:33 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2023-02-20T02:35:33.8610126Z Feb 20 02:35:33 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2023-02-20T02:35:33.8610686Z Feb 20 02:35:33 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2023-02-20T02:35:33.8611205Z Feb 20 02:35:33 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2023-02-20T02:35:33.8611913Z Feb 20 02:35:33 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2023-02-20T02:35:33.8615550Z Feb 20 02:35:33 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2023-02-20T02:35:33.8616048Z Feb 20 02:35:33 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2023-02-20T02:35:33.8616534Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2023-02-20T02:35:33.8617120Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2023-02-20T02:35:33.8617704Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2023-02-20T02:35:33.8621123Z Feb 20 02:35:33 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2023-02-20T02:35:33.8621710Z Feb 20 02:35:33 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2023-02-20T02:35:33.8622378Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
2023-02-20T02:35:33.8623118Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
2023-02-20T02:35:33.8623783Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
2023-02-20T02:35:33.8628484Z Feb 20 02:35:33 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
2023-02-20T02:35:33.8629161Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
2023-02-20T02:35:33.8629869Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2023-02-20T02:35:33.8630557Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
2023-02-20T02:35:33.8633587Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2023-02-20T02:35:33.8634401Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807)
2023-02-20T02:35:33.8634994Z Feb 20 02:35:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756)
2023-02-20T02:35:33.8635625Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
2023-02-20T02:35:33.8636244Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
2023-02-20T02:35:33.8640770Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
2023-02-20T02:35:33.8641348Z Feb 20 02:35:33 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
2023-02-20T02:35:33.8641850Z Feb 20 02:35:33 	at java.lang.Thread.run(Thread.java:750)
2023-02-20T02:35:33.8642276Z Feb 20 02:35:33 Caused by: java.lang.OutOfMemoryError: Java heap space
2023-02-20T02:35:33.8681059Z Feb 20 02:35:33 	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
2023-02-20T02:35:33.8681610Z Feb 20 02:35:33 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
2023-02-20T02:35:33.8682175Z Feb 20 02:35:33 	at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30)
2023-02-20T02:35:33.8685445Z Feb 20 02:35:33 	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:113)
2023-02-20T02:35:33.8686093Z Feb 20 02:35:33 	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
2023-02-20T02:35:33.8686662Z Feb 20 02:35:33 	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
2023-02-20T02:35:33.8687259Z Feb 20 02:35:33 	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
2023-02-20T02:35:33.8687866Z Feb 20 02:35:33 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
2023-02-20T02:35:33.8691297Z Feb 20 02:35:33 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
2023-02-20T02:35:33.8692048Z Feb 20 02:35:33 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
2023-02-20T02:35:33.8692690Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
2023-02-20T02:35:33.8693406Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
2023-02-20T02:35:33.8694070Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
2023-02-20T02:35:33.8697040Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
2023-02-20T02:35:33.8697676Z Feb 20 02:35:33 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
2023-02-20T02:35:33.8698417Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.lambda$removeEmptySplits$4(KafkaPartitionSplitReader.java:338)
2023-02-20T02:35:33.8699179Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader$$Lambda$1426/1494823239.get(Unknown Source)
2023-02-20T02:35:33.8704983Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.retryOnWakeup(KafkaPartitionSplitReader.java:439)
2023-02-20T02:35:33.8707504Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.removeEmptySplits(KafkaPartitionSplitReader.java:337)
2023-02-20T02:35:33.8708303Z Feb 20 02:35:33 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(KafkaPartitionSplitReader.java:215)
2023-02-20T02:35:33.8712760Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
2023-02-20T02:35:33.8713580Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
2023-02-20T02:35:33.8716986Z Feb 20 02:35:33 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
2023-02-20T02:35:33.8719588Z Feb 20 02:35:33 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2023-02-20T02:35:33.8722985Z Feb 20 02:35:33 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2023-02-20T02:35:33.8723587Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-02-20T02:35:33.8724163Z Feb 20 02:35:33 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-02-20T02:35:33.8727430Z Feb 20 02:35:33 	... 1 more
2023-02-20T02:35:33.8727747Z Feb 20 02:35:33 
2023-02-20T02:35:33.8728294Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.checkResultWithSemantic(SourceTestSuiteBase.java:741)
2023-02-20T02:35:33.8729099Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:329)
2023-02-20T02:35:33.8732079Z Feb 20 02:35:33 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testScaleDown(SourceTestSuiteBase.java:279)
2023-02-20T02:35:33.8732680Z Feb 20 02:35:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-02-20T02:35:33.8733247Z Feb 20 02:35:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-02-20T02:35:33.8733957Z Feb 20 02:35:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-02-20T02:35:33.8734617Z Feb 20 02:35:33 	at java.lang.reflect.Method.invoke(Method.java:498)
2023-02-20T02:35:33.8738501Z Feb 20 02:35:33 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2023-02-20T02:35:33.8739162Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-02-20T02:35:33.8739904Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-02-20T02:35:33.8740745Z Feb 20 02:35:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2023-02-20T02:35:33.8741428Z Feb 20 02:35:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2023-02-20T02:35:33.8744395Z Feb 20 02:35:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2023-02-20T02:35:33.8745162Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2023-02-20T02:35:33.8745922Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2023-02-20T02:35:33.8746676Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-02-20T02:35:33.8749636Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-02-20T02:35:33.8750397Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-02-20T02:35:33.8751085Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-02-20T02:35:33.8753946Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2023-02-20T02:35:33.8754600Z Feb 20 02:35:33 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2023-02-20T02:35:33.8755317Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2023-02-20T02:35:33.8756148Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8759894Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2023-02-20T02:35:33.8760846Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2023-02-20T02:35:33.8761514Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2023-02-20T02:35:33.8762229Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-02-20T02:35:33.8762946Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8766342Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8767027Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8767698Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8768396Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8769052Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8769698Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8773694Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2023-02-20T02:35:33.8774527Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2023-02-20T02:35:33.8775414Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2023-02-20T02:35:33.8776162Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2023-02-20T02:35:33.8779002Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2023-02-20T02:35:33.8779702Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-20T02:35:33.8780254Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-20T02:35:33.8780856Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2023-02-20T02:35:33.8781459Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-20T02:35:33.8785484Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-20T02:35:33.8786089Z Feb 20 02:35:33 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2023-02-20T02:35:33.8786751Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2023-02-20T02:35:33.8787311Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2023-02-20T02:35:33.8787915Z Feb 20 02:35:33 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2023-02-20T02:35:33.8790592Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-20T02:35:33.8791199Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-20T02:35:33.8791895Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-20T02:35:33.8792520Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-20T02:35:33.8797120Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-20T02:35:33.8797677Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-20T02:35:33.8798269Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2023-02-20T02:35:33.8798876Z Feb 20 02:35:33 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2023-02-20T02:35:33.8799476Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-20T02:35:33.8811653Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-20T02:35:33.8812316Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-20T02:35:33.8812912Z Feb 20 02:35:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-20T02:35:33.8813523Z Feb 20 02:35:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-20T02:35:33.8814115Z Feb 20 02:35:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-20T02:35:33.8819568Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2023-02-20T02:35:33.8820298Z Feb 20 02:35:33 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2023-02-20T02:35:33.8821033Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-02-20T02:35:33.8821753Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8822438Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8825202Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8825874Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8826582Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8827268Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8827949Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8828521Z Feb 20 02:35:33 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-02-20T02:35:33.8829189Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2023-02-20T02:35:33.8832902Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-02-20T02:35:33.8833624Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8834336Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8835001Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8837220Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8837950Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8838717Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8839353Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8843500Z Feb 20 02:35:33 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-02-20T02:35:33.8844217Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2023-02-20T02:35:33.8849137Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-02-20T02:35:33.8849858Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8850652Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-02-20T02:35:33.8851320Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-20T02:35:33.8851946Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-02-20T02:35:33.8854746Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-20T02:35:33.8855444Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-02-20T02:35:33.8856119Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-02-20T02:35:33.8856894Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2023-02-20T02:35:33.8860669Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2023-02-20T02:35:33.8861538Z Feb 20 02:35:33 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
2023-02-20T02:35:33.8862262Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2023-02-20T02:35:33.8862931Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2023-02-20T02:35:33.8863672Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2023-02-20T02:35:33.8864433Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2023-02-20T02:35:33.8868534Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2023-02-20T02:35:33.8869224Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2023-02-20T02:35:33.8871773Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2023-02-20T02:35:33.8872481Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2023-02-20T02:35:33.8873163Z Feb 20 02:35:33 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2023-02-20T02:35:33.8876179Z Feb 20 02:35:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2023-02-20T02:35:33.8876898Z Feb 20 02:35:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2023-02-20T02:35:33.8877701Z Feb 20 02:35:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2023-02-20T02:35:33.8878383Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2023-02-20T02:35:33.8894343Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2023-02-20T02:35:33.8895004Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2023-02-20T02:35:33.8895569Z Feb 20 02:35:33 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:12 UTC 2023,,,,,,,,,,"0|z1g1wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 17:10;mapohl;[~gsomogyi] do you have an idea on that one? Unfortunately, we don't have any heap dump provided for this failure.;;;","21/Feb/23 13:14;gaborgsomogyi;Not much idea, mostly standing in Akka.;;;","06/Mar/23 08:53;mapohl;This OOM reappeared in 1.15 this time causing {{KafkaSourceE2ECase.testMultipleSplits}} to fail:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46765&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=16001

{code}
Mar 03 04:14:10 [ERROR] Tests run: 16, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 249.597 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.KafkaSourceE2ECase
Mar 03 04:14:10 [ERROR] org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.testMultipleSplits(TestEnvironment, DataStreamSourceExternalContext, CheckpointingMode)[1]  Time elapsed: 3.742 s  <<< ERROR!
Mar 03 04:14:10 java.lang.RuntimeException: Failed to fetch next result
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Mar 03 04:14:10 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.compareWithExactlyOnceSemantic(CollectIteratorAssert.java:116)
Mar 03 04:14:10 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.matchesRecordsFromSource(CollectIteratorAssert.java:71)
Mar 03 04:14:10 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.checkResultWithSemantic(SourceTestSuiteBase.java:744)
Mar 03 04:14:10 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testMultipleSplits(SourceTestSuiteBase.java:212)
Mar 03 04:14:10 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 03 04:14:10 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 03 04:14:10 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 03 04:14:10 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 03 04:14:10 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Mar 03 04:14:10 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
Mar 03 04:14:10 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
Mar 03 04:14:10 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
Mar 03 04:14:10 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
Mar 03 04:14:10 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 03 04:14:10 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Mar 03 04:14:10 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
Mar 03 04:14:10 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
Mar 03 04:14:10 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 03 04:14:10 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Mar 03 04:14:10 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Mar 03 04:14:10 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Mar 03 04:14:10 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
Mar 03 04:14:10 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
Mar 03 04:14:10 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 03 04:14:10 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Mar 03 04:14:10 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Mar 03 04:14:10 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Mar 03 04:14:10 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Mar 03 04:14:10 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
Mar 03 04:14:10 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Mar 03 04:14:10 	at java.util.ArrayList.forEach(ArrayList.java:1259)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Mar 03 04:14:10 	at java.util.ArrayList.forEach(ArrayList.java:1259)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
Mar 03 04:14:10 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Mar 03 04:14:10 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Mar 03 04:14:10 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Mar 03 04:14:10 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Mar 03 04:14:10 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
Mar 03 04:14:10 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Mar 03 04:14:10 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Mar 03 04:14:10 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Mar 03 04:14:10 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Mar 03 04:14:10 Caused by: java.io.IOException: Failed to fetch job execution result
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Mar 03 04:14:10 	... 113 more
Mar 03 04:14:10 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 99114f4e3eff6350942bc140c234f1e8)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
Mar 03 04:14:10 	... 115 more
Mar 03 04:14:10 Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 99114f4e3eff6350942bc140c234f1e8)
Mar 03 04:14:10 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Mar 03 04:14:10 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Mar 03 04:14:10 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$26(RestClusterClient.java:708)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Mar 03 04:14:10 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
Mar 03 04:14:10 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Mar 03 04:14:10 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Mar 03 04:14:10 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Mar 03 04:14:10 	at java.lang.Thread.run(Thread.java:750)
Mar 03 04:14:10 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Mar 03 04:14:10 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
Mar 03 04:14:10 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128)
Mar 03 04:14:10 	... 24 more
Mar 03 04:14:10 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Mar 03 04:14:10 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
Mar 03 04:14:10 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
Mar 03 04:14:10 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
Mar 03 04:14:10 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
Mar 03 04:14:10 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
Mar 03 04:14:10 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
Mar 03 04:14:10 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
Mar 03 04:14:10 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
Mar 03 04:14:10 	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
Mar 03 04:14:10 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 03 04:14:10 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 03 04:14:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
Mar 03 04:14:10 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
Mar 03 04:14:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
Mar 03 04:14:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
Mar 03 04:14:10 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
Mar 03 04:14:10 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
Mar 03 04:14:10 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
Mar 03 04:14:10 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
Mar 03 04:14:10 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
Mar 03 04:14:10 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
Mar 03 04:14:10 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
Mar 03 04:14:10 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Mar 03 04:14:10 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Mar 03 04:14:10 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Mar 03 04:14:10 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
Mar 03 04:14:10 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
Mar 03 04:14:10 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
Mar 03 04:14:10 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
Mar 03 04:14:10 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
Mar 03 04:14:10 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
Mar 03 04:14:10 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
Mar 03 04:14:10 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
Mar 03 04:14:10 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 03 04:14:10 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 03 04:14:10 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 03 04:14:10 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 03 04:14:10 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
Mar 03 04:14:10 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
Mar 03 04:14:10 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
Mar 03 04:14:10 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
Mar 03 04:14:10 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
Mar 03 04:14:10 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
Mar 03 04:14:10 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
Mar 03 04:14:10 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
Mar 03 04:14:10 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
Mar 03 04:14:10 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807)
Mar 03 04:14:10 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756)
Mar 03 04:14:10 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
Mar 03 04:14:10 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
Mar 03 04:14:10 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
Mar 03 04:14:10 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
Mar 03 04:14:10 	at java.lang.Thread.run(Thread.java:750)
Mar 03 04:14:10 Caused by: java.lang.OutOfMemoryError: Java heap space
Mar 03 04:14:10 	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
Mar 03 04:14:10 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
Mar 03 04:14:10 	at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30)
Mar 03 04:14:10 	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:113)
Mar 03 04:14:10 	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
Mar 03 04:14:10 	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
Mar 03 04:14:10 	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
Mar 03 04:14:10 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
Mar 03 04:14:10 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
Mar 03 04:14:10 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:164)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.Fetcher.fetchOffsetsByTimes(Fetcher.java:557)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.Fetcher.beginningOrEndOffset(Fetcher.java:581)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.internals.Fetcher.endOffsets(Fetcher.java:569)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.KafkaConsumer.endOffsets(KafkaConsumer.java:2215)
Mar 03 04:14:10 	at org.apache.kafka.clients.consumer.KafkaConsumer.endOffsets(KafkaConsumer.java:2187)
Mar 03 04:14:10 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.acquireAndSetStoppingOffsets(KafkaPartitionSplitReader.java:314)
Mar 03 04:14:10 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(KafkaPartitionSplitReader.java:212)
Mar 03 04:14:10 	at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
Mar 03 04:14:10 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
Mar 03 04:14:10 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
Mar 03 04:14:10 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Mar 03 04:14:10 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Mar 03 04:14:10 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Mar 03 04:14:10 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Mar 03 04:14:10 	... 1 more
{code};;;","06/Mar/23 08:56;mapohl;Maybe, it's a Kafka version issue: 1.15 uses 2.8.1 (see [code|https://github.com/apache/flink/blob/release-1.15/flink-connectors/flink-connector-kafka/pom.xml#L39]) whereas 1.16+ uses 3.2.3 (see [1.16 code|https://github.com/apache/flink/blob/release-1.16/flink-connectors/flink-connector-kafka/pom.xml#L38]).;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase hangs if a checkpoint fails,FLINK-31133,13525410,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,20/Feb/23 10:08,08/Mar/23 17:55,04/Jun/24 20:41,03/Mar/23 20:13,1.15.3,1.16.1,1.17.1,1.18.0,,,,,,,,1.15.4,1.16.2,1.17.1,1.18.0,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b

This build ran into a timeout. Based on the stacktraces reported, it was either caused by [SnapshotMigrationTestBase.restoreAndExecute|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=13475]:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f23d800b800 nid=0x60cdd waiting on condition [0x00007f23e1c0d000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase.restoreAndExecute(SnapshotMigrationTestBase.java:382)
	at org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.testSnapshot(TypeSerializerSnapshotMigrationITCase.java:172)
	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
[...]
{code}

or [PartiallyFinishedSourcesITCase.test|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10401]:
{code}
2023-02-20T07:13:05.6084711Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd35c00b800 nid=0x8c8a waiting on condition [0x00007fd363d0f000]
2023-02-20T07:13:05.6085149Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2023-02-20T07:13:05.6085487Z 	at java.lang.Thread.sleep(Native Method)
2023-02-20T07:13:05.6085925Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2023-02-20T07:13:05.6086512Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
2023-02-20T07:13:05.6087103Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
2023-02-20T07:13:05.6087730Z 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
2023-02-20T07:13:05.6088410Z 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
2023-02-20T07:13:05.6088957Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

Still, it sounds odd: Based on a code analysis it's quite unlikely that those two caused the issue. The former one has a 5 min timeout (see related code in [SnapshotMigrationTestBase:382|https://github.com/apache/flink/blob/release-1.15/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L382]). For the other one, we found it being not responsible in the past when some other concurrent test caused the issue (see FLINK-30261).

An investigation on where we lose the time for the timeout revealed that {{AdaptiveSchedulerITCase}} took 2980s to finish (see [build logs|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5265]).
{code}
2023-02-20T03:43:55.4546050Z Feb 20 03:43:55 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-02-20T03:43:58.0448506Z Feb 20 03:43:58 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
2023-02-20T04:33:38.6824634Z Feb 20 04:33:38 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,980.445 s - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 20:13:48 UTC 2023,,,,,,,,,,"0|z1g1w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 10:14;mapohl;[~chesnay] could you have a look at that one if you find time? I'm worried that there's an issue that's also present in newer versions. But I guess, it's hard to investigate if we don't have logs.;;;","20/Feb/23 12:54;mapohl;[~chesnay] discovered multiple checkpoint failure due to exceeding the tolerable failure threshold for creating a checkpoint (which seems to be 10min):
{code:java}
04:24:18,587 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 4 for job af8411cc8af5e2485beb4466ffd452a3. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2143) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
04:24:18,588 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Restarting job.
org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
        at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:206) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleJobLevelCheckpointException(CheckpointFailureManager.java:169) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:122) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2082) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2061) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java:98) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2143) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
[...]
{code}
[~roman]  can you help with that one?;;;","21/Feb/23 16:25;roman;I think the issue is actually PartiallyFinishedSourcesITCase.

It starts at 3:40
{code:java}
03:40:55,702 [                main] INFO  org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase [] -
================================================================================
Test test[complex graph ALL_SUBTASKS, failover: true, strategy: full](org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase) is running.
{code}
then a checkpoint fails because of a timeout:
{code:java}
03:41:10,775 [ChangelogRetryScheduler-1] INFO  org.apache.flink.changelog.fs.RetryingExecutor               [] - failed with 3 attempts: Attempt 3 timed out after 1000ms
03:41:10,777 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - transform-2-keyed (4/4)#0 - asynchronous part of checkpoint 2 could not be completed.
java.util.concurrent.CompletionException: java.io.IOException: java.util.concurrent.TimeoutException: Attempt 3 timed out after 1000ms
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
        at org.apache.flink.changelog.fs.FsStateChangelogWriter$UploadCompletionListener.onFailure(FsStateChangelogWriter.java:383) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.changelog.fs.FsStateChangelogWriter.lambda$null$0(FsStateChangelogWriter.java:223) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.ArrayList.removeIf(ArrayList.java:1415) ~[?:1.8.0_292]
        at org.apache.flink.changelog.fs.FsStateChangelogWriter.lambda$handleUploadFailure$4(FsStateChangelogWriter.java:222) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.IOException: java.util.concurrent.TimeoutException: Attempt 3 timed out after 1000ms
        at org.apache.flink.changelog.fs.FsStateChangelogWriter$UploadCompletionListener.onFailure(FsStateChangelogWriter.java:377) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        ... 15 more
Caused by: java.util.concurrent.TimeoutException: Attempt 3 timed out after 1000ms
        at org.apache.flink.changelog.fs.RetryingExecutor$RetriableActionAttempt.fmtError(RetryingExecutor.java:285) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.changelog.fs.RetryingExecutor$RetriableActionAttempt.lambda$scheduleTimeout$1(RetryingExecutor.java:280) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
{code}
 

After which it runs normally, ~1.4K checkpoints succeed.

At 6:56, it finally reches no space left on device:
{code:java}
06:56:53,713 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1396 for job 1e2e4e86643f8249324da01fe5f8a04a (34948548049 bytes, checkpointDuration=6869 ms, finalizationTime=12 ms).
06:56:53,715 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1397 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1676876213715 for job 1e2e4e86643f8249324da01fe5f8a04a.
...
06:57:02,402 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - MultipleInputOperator (1/4)#1 - asynchronous part of checkpoint 1397 could not be completed.
java.util.concurrent.ExecutionException: java.io.IOException: No space left on device
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_292]
        at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:66) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.IOException: No space left on device
        at java.io.FileOutputStream.writeBytes(Native Method) ~[?:1.8.0_292]
        at java.io.FileOutputStream.write(FileOutputStream.java:326) ~[?:1.8.0_292]
        at org.apache.flink.core.fs.local.LocalDataOutputStream.write(LocalDataOutputStream.java:68) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.core.fs.FSDataOutputStreamWrapper.write(FSDataOutputStreamWrapper.java:65) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:296) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAP  SHOT]
        at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_292]
        at java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_292]
        at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.getBytes(NetworkBuffer.java:397) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractUnpooledSlicedByteBuf.getBytes(AbstractUnpooledSlicedByteBuf.java:392) ~[flink-shaded-netty-4.1.70.Final-15.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:26) ~[flink-shaded-netty-4.1.70.Final-15.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.buffer.ReadOnlyByteBuf.getBytes(ReadOnlyByteBuf.java:264) ~[flink-shaded-netty-4.1.70.Final-15.0.jar:?]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializerImpl.writeData(ChannelStateSerializer.java:164) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.lambda$write$2(ChannelStateCheckpointWriter.java:171) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.runWithChecks(ChannelStateCheckpointWriter.java:295) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.write(ChannelStateCheckpointWriter.java:165) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.writeInput(ChannelStateCheckpointWriter.java:138) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$write$0(ChannelStateWriteRequest.java:63) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildWriteRequest$2(ChannelStateWriteRequest.java:92) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.execute(ChannelStateWriteRequest.java:211) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:85) ~[flink-runtime-1.15-SNAPSHOT.jar:1  .15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAP  SHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        ... 1 more
{code}
 

Some other tests failed as a result with serialization failures or ""no such file""

 

I'm going to change the priority to Major because it's a pure test issue.;;;","25/Feb/23 22:34;roman;There are two issues in case of a checkpoint failure:
 # FAIL command might be dispatched to the source task that's already finished execution
 # waiting for the failover times out, but it then waits indefinitely to obtain job status result

The issue affects only 1.15 because in later versions, state upload timeout and nr. of attempts were increased in FLINK-27169.

I've created a [PR|https://github.com/apache/flink/pull/22022] to address (1) and (2) and reopened FLINK-27169 to backport increased timeouts/attempts to 1.15.;;;","02/Mar/23 06:58;liyu;The 1.15.4 version is about to release with [RC under vote|https://lists.apache.org/thread/4463cypc257l7j9rj2pycofbsdbbjx59]. Please check and confirm whether this issue could still make into 1.15.4 and move it out if not. Thanks.;;;","02/Mar/23 07:44;roman;Thanks [~liyu] , you are right, it will likely not make it into 1.15.4. I'll updated the version.;;;","02/Mar/23 08:23;liyu;Thanks for the quick action [~roman];;;","03/Mar/23 20:13;roman;Fix merged as 

1.15 51660f840cfc505b9b9cb72530fde7f9f8a4dee2
1.16 cf04b2c08fa04091845bd310990497129c3bcbe8
1.17 6e7703738cdefed17277ea86d2c9dc25393eceac
master 4aacff572a9e3996c5dee9273638831e4040c767;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compact without setting parallelism does not follow the configured sink parallelism for HiveTableSink,FLINK-31132,13525408,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,fsk119,fsk119,20/Feb/23 10:03,22/Feb/23 14:45,04/Jun/24 20:41,22/Feb/23 14:45,1.17.0,,,,,,,,,,,1.17.0,,,,Connectors / Hive,Table SQL / Planner,,,0,pull-request-available,,,,"If the parallelism of compact operator was not set, it should use the sink parallelism and disable parallelism inference when using adaptive batch scheduler to avoid take much time to finish compaction.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30951,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 10:18:26 UTC 2023,,,,,,,,,,"0|z1g1vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 11:37;Weijie Guo;This may be related to the change of batch default scheduler.;;;","22/Feb/23 10:18;Weijie Guo;master(1.18) via 2e91543836d667a0b367688bb5ce290c3164479c.
release-1.17 via 2f86dcb98e065503e12ba121813d643ba976d041.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The INITIALIZING of ExecutionState is missed in the state_machine doc,FLINK-31131,13525404,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,fanrui,fanrui,20/Feb/23 09:40,24/Apr/23 02:07,04/Jun/24 20:41,24/Apr/23 02:07,1.16.0,1.17.0,,,,,,,,,,1.16.2,1.17.1,1.18.0,,Documentation,,,,0,pull-request-available,,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/internals/job_scheduling/#jobmanager-data-structures]

 

The INITIALIZING of ExecutionState is missed in the state_machine doc, it should be between DEPLOYING and RUNNING.

 

!image-2023-02-20-17-39-22-557.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/23 09:39;fanrui;image-2023-02-20-17-39-22-557.png;https://issues.apache.org/jira/secure/attachment/13055627/image-2023-02-20-17-39-22-557.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 23 07:45:03 UTC 2023,,,,,,,,,,"0|z1g1uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 08:36;Wencong Liu;[~fanrui] Are you working on this? If you are busy on other issues, I could take it. WDYT?;;;","20/Apr/23 10:12;fanrui;Hi [~Wencong Liu] , thanks for your reply.

I see the svg is drawn by Inkscape[1], I'm not sure whether the flink community has a specification for making svg, so I prefer use Inkscape. I tried to use Inkscape last weekend, however, I didn't finish it due to I'm very busy this week.

If you are interested in this issue and Inkscape, I can assign it to you. WDYT?

 

[1] https://github.com/apache/flink/blob/d2e4b74e4a291f36e0771b2c6a7ded76d393f235/docs/static/fig/state_machine.svg?short_path=8d0f570#L21;;;","20/Apr/23 10:15;Wencong Liu;[~fanrui]  Sure, I'll be happy to finish it.;;;","23/Apr/23 07:25;fanrui;Hi [~Wencong Liu] , thanks for your contribution. Would you mind backport it to 1.16 and 1.17?

 

Merged master commit: 5c4db55d306a4c65175f7cb4250d02729542901c

Merged 1.17-release commit: ca27fb9f05ebc250148c405003d4c70ca1d7a5e4

Merged 1.16-release commit: 226ec59837db39d1142934d79df9c59c8c3c8163;;;","23/Apr/23 07:45;Wencong Liu;OK. [~fanrui]. Another two pull requests have been opened.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve version info shown in the doc of SQL Gateway,FLINK-31130,13525384,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,20/Feb/23 06:57,20/Feb/23 09:31,04/Jun/24 20:41,20/Feb/23 09:24,,,,,,,,,,,,1.18.0,,,,Documentation,Table SQL / Gateway,,,0,pull-request-available,,,,"In the sql gateway doc, In the part of [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql-gateway/overview/#starting-the-sql-gateway,|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql-gateway/overview/#starting-the-sql-gateway]

 
{code:java}
$ curl http://localhost:8083/v1/info
{""productName"":""Apache Flink"",""version"":""1.16-SNAPSHOT""} {code}
the version will be always 1.16-SNAPSHOT whatever the Flink version is . It may cause use feel confused. 

 

For user, when it's for flink 1.17 doc, it should show ""version"":""1.17""

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 09:24:07 UTC 2023,,,,,,,,,,"0|z1g1qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 09:24;Weijie Guo;master(1.18) via 804072aef4f512f61f26d1c0c9950250da333f79.

release-1.17 via 0b90d3c38430a8a53e11cc69f679df3584db2fe4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkEmbeddedHiveRunner for junit5 in table store,FLINK-31129,13525368,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,20/Feb/23 05:41,29/Mar/23 01:59,04/Jun/24 20:41,29/Mar/23 01:59,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,,,,,Introduce FlinkEmbeddedHiveRunner for junit5 in table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-20 05:41:50.0,,,,,,,,,,"0|z1g1mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Create Table As for flink table store,FLINK-31128,13525335,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,19/Feb/23 14:49,01/Mar/23 09:35,04/Jun/24 20:41,01/Mar/23 09:35,table-store-0.3.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Add Create Table As for flink table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 09:35:00 UTC 2023,,,,,,,,,,"0|z1g1fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:35;lzljs3620320;master: 4adf18df258bc3c55026053d94cd3e4a6c281896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add public API classes for FLIP-289,FLINK-31127,13525317,13519878,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,19/Feb/23 09:25,01/Mar/23 09:58,04/Jun/24 20:41,01/Mar/23 09:58,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 09:58:01 UTC 2023,,,,,,,,,,"0|z1g1bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:58;lindong;Merged to flink-ml master branch f0c1ce7440ff3b5cb4010f7699956e556c66e66e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move classes not depending on Flink runtime from flink-ml-core to flink-ml-servable-core,FLINK-31126,13525316,13519878,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,19/Feb/23 08:51,01/Mar/23 09:59,04/Jun/24 20:41,01/Mar/23 09:59,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"We would like to be able to deploy servable classes on edge devices (e.g. phone) with minimal library dependencies and memory footprint. Thus flink-ml-servable-lib module, which contains the servable classes such as KMeansServable, should not depend on Flink runtime modules (e.g. flink-table-runtime).

We need to do the following to achieve this goal:
- Keep the classes (e.g. AlgoOperator) whose API depends on Flink runtime classes (e.g. Table, DataStream) in flink-ml-core. Move the remaining classes from flink-ml-core to flink-ml-servable-core.
- flink-ml-core and all modules that currently depend on flink-ml-core should now depend on flink-ml-servable-core.
- flink-ml-servable-lib should depend on flink-ml-servable-core but not flink-ml-core.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 09:58:59 UTC 2023,,,,,,,,,,"0|z1g1bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:58;lindong;Merged to flink-ml master branch b032ebdb122f32ba04bd21ad03a2f971b471e5bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML benchmark framework should minimize the source operator overhead,FLINK-31125,13525214,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,17/Feb/23 14:16,01/Mar/23 09:58,04/Jun/24 20:41,01/Mar/23 09:58,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Flink ML benchmark framework estimates the throughput by having a source operator generate a given number (e.g. 10^7) of input records with random values, let the given AlgoOperator process these input records, and divide the number of records by the total execution time. 

The overhead of generating random values for all input records has observable impact on the estimated throughput. We would like to minimize the overhead of the source operator so that the benchmark result can focus on the throughput of the AlgoOperator as much as possible.

Note that [spark-sql-perf|https://github.com/databricks/spark-sql-perf] generates all input records in advance into memory before running the benchmark. This allows Spark ML benchmark to read records from memory instead of generating values for those records during the benchmark.

We can generate value once and re-use it for all input records. This approach minimizes the source operator head and allows us to compare Flink ML benchmark result with Spark ML benchmark result (from spark-sql-perf) fairly.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 09:58:30 UTC 2023,,,,,,,,,,"0|z1g0ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:58;lindong;Merged to flink-ml master branch 5bcbd01169a3bfce0c92e0aae56dc55cf2489d37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add it case for HiveTableSink speculative execution,FLINK-31124,13525207,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,csq,SleePy,SleePy,17/Feb/23 13:06,23/Feb/23 06:02,04/Jun/24 20:41,23/Feb/23 06:02,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,,The part of HiveTableSink has supported speculative execution in https://issues.apache.org/jira/browse/FLINK-30823. We would like to add some integration test cases for this feature.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 06:02:51 UTC 2023,,,,,,,,,,"0|z1g0nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 18:17;csq;Hi [~zhuzh] I would like to help finish to issue, could you please assign it to me?;;;","18/Feb/23 03:27;zhuzh;[~csq] Thanks for volunteering!
I have assigned you the ticket.;;;","18/Feb/23 13:37;csq;Thank you [~zhuzh], I have created the PR and will be grateful if you could help review it.  BTW, the username of the assignee is not the valid one of mine, my current JIRA id is `csq`.;;;","18/Feb/23 15:27;zhuzh;Thanks for creating the PR! I will take a look.
The ticket is now re-assigned to `csq`. Sorry for the inconvenience.;;;","22/Feb/23 11:45;csq;Never mind, it doesn't block the contribution.;;;","23/Feb/23 06:02;zhuzh;master:
db6e01e028ec69d7702d3371d6b3db253aafbbf8

release-1.17:
c94a0799016ae7f8d6e348930124936bc71a61aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add it case for FileSink speculative execution,FLINK-31123,13525204,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,freeke,SleePy,SleePy,17/Feb/23 12:59,23/Feb/23 10:15,04/Jun/24 20:41,23/Feb/23 10:15,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,,The FileSink has supported speculative execution in https://issues.apache.org/jira/browse/FLINK-30823. We would like to add some integration test cases for this feature.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 10:15:57 UTC 2023,,,,,,,,,,"0|z1g0mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/23 11:53;freeke;Hi [~zhuzh] , I'd like to do this testing work, could you please assign it to me?;;;","18/Feb/23 15:29;zhuzh;Thanks for volunteering to take this task. [~freeke]
I have assigned you the ticket.;;;","23/Feb/23 10:15;zhuzh;master:
f244945aa7cf9d3fb5d2fd2fe49446dd759f9aef

release-1.17:
e0773803fa8b1b6a62ba02ffc778bbdbdd188b19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose MetricGroup to Committer of new Unified Sink v2,FLINK-31122,13525199,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,TheoD,TheoD,17/Feb/23 12:11,17/Feb/23 12:11,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,0,,,,,"When writing my own committer, I want to include metrics on the operations performed as for any other custom function in the pipeline.

The goal of this story is to provide MetricGroup to Committer in some way so that in sink2.Committer, within the commit method, I somehow have access to the MetricGroup.

The usecase for me with regards to this story: I want to notify Impala about newly added partitions written via FileSink and track via metrics the number of performed Impala queries and their duration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-17 12:11:08.0,,,,,,,,,,"0|z1g0lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink should be able to catch and ignore exp via config on/off,FLINK-31121,13525183,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zjureel,jingge,jingge,17/Feb/23 10:33,05/Sep/23 23:56,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,Connectors / Kafka,,,,1,pull-request-available,stale-assigned,,,"It is a common requirement for users to catch and ignore exp while sinking the event to to downstream system like Kafka. It will be convenient for some use cases, if Flink Sink can provide built-in functionality and config to turn it on and off, especially for cases that data consistency is not very important or the stream contains dirty events. [1][2]

First of all, consider doing it for KafkaSink. Long term, a common solution that can be used by any connector would be even better.

 

[1][https://lists.apache.org/thread/wy31s8wb9qnskq29wn03kp608z4vrwv8]

[2]https://stackoverflow.com/questions/52308911/how-to-handle-exceptions-in-kafka-sink

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 05 23:56:27 UTC 2023,,,,,,,,,,"0|z1g0i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 10:52;zjureel;I'd like to fix it, can you assign to me? THX [~jingge];;;","17/Feb/23 12:46;jingge;[~zjureel] it's your, thanks!;;;","03/Apr/23 21:36;mason6345;[~zjureel] are you still pursuing implementing the fix? I also would like to use this feature;;;","13/Apr/23 16:07;mason6345;[~zjureel] gentle ping^ let me know if you are still into it!;;;","17/Apr/23 07:54;martijnvisser;[~mason6345] Couldn't we solve this directly in a generic way, over solving this per connector? ;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","05/Sep/23 23:56;mason6345;[~martijnvisser] [~zjureel] Hi! I'm going to take a look at this–making error handling more generic;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException occurred in StringFunctionsITCase.test,FLINK-31120,13525178,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,csq,mapohl,mapohl,17/Feb/23 10:09,23/Feb/23 07:24,04/Jun/24 20:41,23/Feb/23 02:43,1.17.0,,,,,,,,,,,1.16.2,1.17.0,1.18.0,,Table SQL / Runtime,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46255&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12334

{code}
Feb 17 04:51:25 [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.725 s <<< FAILURE! - in org.apache.flink.table.planner.functions.StringFunctionsITCase
Feb 17 04:51:25 [ERROR] org.apache.flink.table.planner.functions.StringFunctionsITCase.test(TestCase)[4] Time elapsed: 4.367 s <<< ERROR!
Feb 17 04:51:25 org.apache.flink.table.api.TableException: Failed to execute sql
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:974)
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:354)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:320)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:113)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:93)
Feb 17 04:51:25 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 07:24:30 UTC 2023,,,,,,,,,,"0|z1g0gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 12:28;Weijie Guo;Sorry, I accidentally touched it.;;;","17/Feb/23 18:14;csq;Maybe it needs concurrent control for access to the static field `collectIterators` in `StreamExecutionEnvironment`. There are four test cases executing when running StringFunctionsITCase in a concurrent execution mode, that has chance  for a thread to add a collectorIterator through `registerCollectIterator` while there is a foreach loop in executeAsync() in another thread.;;;","18/Feb/23 15:19;leonard;[~TsReaper] Would you like to review this PR?;;;","20/Feb/23 09:10;aitozi;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46301&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0c940707-2659-5648-cbe6-a1ad63045f0a;;;","21/Feb/23 07:22;samrat007;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46347&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a;;;","22/Feb/23 03:24;TsReaper;I've left my review in the github PR. My main concern is that why do we need a static variable. I see [~chesnay] is the author of the related code. Would you please take a look?;;;","22/Feb/23 09:41;mapohl;{{ConstructedAccessFunctionsITCase.testTableApiFlattenStructuredType}} instability due to a {{ConcurrentModificationException}} with a similar stacktrace:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46387&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11938
{code}
Feb 22 04:26:07 [ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.108 s <<< FAILURE! - in org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase
Feb 22 04:26:07 [ERROR] org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.testTableApiFlattenStructuredType  Time elapsed: 5.107 s  <<< ERROR!
Feb 22 04:26:07 org.apache.flink.table.api.TableException: Failed to execute sql
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:974)
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
Feb 22 04:26:07 	at org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.testTableApiFlattenStructuredType(ConstructedAccessFunctionsITCase.java:192)
Feb 22 04:26:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 04:26:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 04:26:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 04:26:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 04:26:07 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Feb 22 04:26:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Feb 22 04:26:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Feb 22 04:26:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Feb 22 04:26:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Feb 22 04:26:07 Caused by: java.util.ConcurrentModificationException
Feb 22 04:26:07 	at java.util.ArrayList.forEach(ArrayList.java:1262)
Feb 22 04:26:07 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2202)
Feb 22 04:26:07 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:955)
Feb 22 04:26:07 	... 40 more
{code};;;","22/Feb/23 09:46;csq;I agree with [~TsReaper] that the variable does not need to be static since it is accessed only by current environment. Or is there any other consideration?;;;","23/Feb/23 02:44;TsReaper;master: fa6a2aed6136ae59ed14cd01819e8f94867840b7
release-1.17: 7201d0aa1c69ae0a52f07cea38a0545203f67c17
release-1.16: 4ba657ea60452f16d3f6175031f8471b3b7f042f;;;","23/Feb/23 07:24;mapohl;This build failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46433&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobRecoveryITCase.testTaskFailureRecovery failed due to the job not finishing successfully,FLINK-31119,13525177,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,mapohl,mapohl,17/Feb/23 10:03,20/Feb/23 14:08,04/Jun/24 20:41,20/Feb/23 14:08,1.17.0,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46247&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8523

{code}
Feb 17 02:24:35 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 24.074 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobRecoveryITCase
Feb 17 02:24:35 [ERROR] org.apache.flink.runtime.jobmaster.JobRecoveryITCase.testTaskFailureRecovery  Time elapsed: 20.981 s  <<< FAILURE!
Feb 17 02:24:35 java.lang.AssertionError: 
Feb 17 02:24:35 
Feb 17 02:24:35 Expected: is <true>
Feb 17 02:24:35      but: was <false>
Feb 17 02:24:35 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Feb 17 02:24:35 	at org.junit.Assert.assertThat(Assert.java:964)
Feb 17 02:24:35 	at org.junit.Assert.assertThat(Assert.java:930)
Feb 17 02:24:35 	at org.apache.flink.runtime.jobmaster.JobRecoveryITCase.runTaskFailureRecoveryTest(JobRecoveryITCase.java:79)
Feb 17 02:24:35 	at org.apache.flink.runtime.jobmaster.JobRecoveryITCase.testTaskFailureRecovery(JobRecoveryITCase.java:63)
Feb 17 02:24:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

The actual cause is that unexpected data was received:
{code}
02:24:35,301 [    Receiver (5/5)#1] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Receiver (5/5)#1 (d88e16a5e3c6f2c08cf3924d93ea18e2_28065fbb1d26fe99e018d3b846860dd3_4_1) switched from RUNNING to FAILED with failure cause:
java.lang.Exception: Wrong data received.
        at org.apache.flink.runtime.jobmaster.TestingAbstractInvokables$Receiver.invoke(TestingAbstractInvokables.java:83) ~[test-classes/:?]
        at org.apache.flink.runtime.jobmaster.JobRecoveryITCase$FailingOnceReceiver.invoke(JobRecoveryITCase.java:126) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931) [classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30895,,,,,,,,,,,"17/Feb/23 10:52;mapohl;FLINK-31119.20230217.1.log;https://issues.apache.org/jira/secure/attachment/13055584/FLINK-31119.20230217.1.log","17/Feb/23 10:52;mapohl;FLINK-31119.20230217.4.log;https://issues.apache.org/jira/secure/attachment/13055585/FLINK-31119.20230217.4.log",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 14:08:56 UTC 2023,,,,,,,,,,"0|z1g0go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 10:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46250&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8521

{code}
01:07:57,099 [    Receiver (1/6)#1] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Receiver (1/6)#1 (e701d0caf3247ea7554acfb5dd8df541_cb0a5d4bcd60528ae7c4e8c99900a321_0_1) switched from RUNNING to FAILED with failure cause:
java.lang.NullPointerException: null
        at org.apache.flink.runtime.jobmaster.TestingAbstractInvokables$Receiver.invoke(TestingAbstractInvokables.java:82) ~[test-classes/:?]
        at org.apache.flink.runtime.jobmaster.JobRecoveryITCase$FailingOnceReceiver.invoke(JobRecoveryITCase.java:126) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931) [classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}

This one fails with a {{NullPointerException}} in the same method [TestingAbstractInvokables.Receiver#invoke:71ff|https://github.com/apache/flink/blob/026675a5cb8a3704c51802fb549d6b0bc4759835/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/TestingAbstractInvokables.java#L71]. Essentially, the data that has been received seems to be corrupted

Update:
There was a Wrong data exception also thrown in this case. It appeared while cancelling the tasks which was caused by the expected {{FlinkRuntimeException}}. It didn't have an impact because the job was already transitioning into CANCELLING, I guess.;;;","17/Feb/23 10:52;mapohl;The logs for both test runs are attached to this issue.;;;","17/Feb/23 11:11;mapohl;Running the test locally 8500 times didn't reveal anything.

[~chesnay] I'm curious about your opinion. The tests ran at around the same time (2:24:14 and 1:07:36) on two different Alibaba machines. I don't like to blame it on a hick-up. Browsing through git logs doesn't reveal anything either.

One other thing I could think of is that an external process send packages to the port that is used for the communication.;;;","20/Feb/23 09:07;mapohl;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46282&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8691;;;","20/Feb/23 09:08;mapohl;release-1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46285&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8445;;;","20/Feb/23 09:16;mapohl;release-1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46292&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8525;;;","20/Feb/23 09:17;mapohl;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46290&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8521;;;","20/Feb/23 09:28;mapohl;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46298&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8522;;;","20/Feb/23 09:29;mapohl;release-1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46300&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8448;;;","20/Feb/23 13:57;mapohl;lowering priority since it's a test code issue.;;;","20/Feb/23 14:08;chesnay;master: d3902e70dfdcd8bb92bc7ca9f8c2eac1fce4745a
1.17: 2046d431ec012c2322bcd5262cc4e6b3ac8ad6ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_UNION supported in SQL & Table API,FLINK-31118,13525172,13076759,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jackylau,jackylau,jackylau,17/Feb/23 09:40,16/May/23 09:32,04/Jun/24 20:41,16/May/23 09:32,1.18.0,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Remove all elements that equal to element from array.

Syntax:
array_union(array)

Arguments:
array: An ARRAY to be handled.

Returns:

An ARRAY. If value is NULL, the result is NULL. 
Examples:
{code:sql}
> SELECT array_union(array(1, 2, 3), array(1, 3, 5));
 [1,2,3,5] {code}
See also
spark [https://spark.apache.org/docs/latest/api/sql/index.html#array_union]

presto [https://prestodb.io/docs/current/functions/array.html]",,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 09:32:16 UTC 2023,,,,,,,,,,"0|z1g0fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 09:32;dwysakowicz;Implemented in 1957ef5fabed0622b1d3e4b542f1df0ee070fc33;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split flink connector to each module of each version,FLINK-31117,13525159,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,17/Feb/23 08:22,24/Feb/23 02:50,04/Jun/24 20:41,24/Feb/23 02:50,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,This will make compilation and testing much easier.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 02:50:20 UTC 2023,,,,,,,,,,"0|z1g0co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 02:50;lzljs3620320;master: 37f75a85091697b75c6968293edfb060595adae3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support taskmanager related parameters in session mode Support job granularity setting,FLINK-31116,13525156,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,17/Feb/23 07:47,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,1.20.0,,,,Runtime / Task,,,,0,,,,,"In session mode, taskmanager related parameters are supported and job granularity settings are supported.
If the yarn session is submitted, taskmanager.numberOfTaskSlots is set
=2, most jobs can be configured according to this. But occasionally when submitting job2, I want taskmanager to be set to taskmanager.numberOfTaskSlots=1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 08:46:34 UTC 2023,,,,,,,,,,"0|z1g0c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 08:16;Weijie Guo;For coarse-grained resource manage, how many slots on a TM are determined when the TM starts. How can you configure it at job granularity?;;;","17/Feb/23 10:06;huwh;[~waywtdcc]

Flink's resource management is hierarchical.

JobMaster is not aware of TaskManager's resources, it only needs to declare the required resources (per slot) to the ResourceManager. ResourceManager will allocate resources (slots) from TaskManager to JobMaster. 

I don't think it is a good idea to let the job know how many slots TaskManager should have.

Could you describe the production scenario in detail? Does [Fine-Grained Resource Management|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/finegrained_resource/] satisfy it?

 ;;;","20/Feb/23 03:01;waywtdcc;Fine-Grained Resource Management is not enough. Fine-Grained Resource Management is task level and api, mine is job level and sql.;;;","20/Feb/23 03:02;waywtdcc;It is used for real-time synchronization, using yarn session mode. Each table synchronization is a job. The memory required by each job is quite different, but I can't set job-level resources.;;;","23/Feb/23 08:46;xtsong;[~waywtdcc],

TaskManagers belong to the whole session cluster, not individual jobs. It is possible that a TM has multiple slots that are used by different jobs. In such cases, which job's configuration should be used?

In order to achieve what you described, we would need another resource management mode for session clusters, that each TM can only be used by 1 job (or a predefined group of jobs). This would be a quite large and fundamental change and would require more thorough discussions and a FLIP process.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support a task to specify multiple slots,FLINK-31115,13525152,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,waywtdcc,waywtdcc,17/Feb/23 06:52,16/Mar/23 08:28,04/Jun/24 20:41,16/Mar/23 08:28,1.16.1,,,,,,,,,,,,,,,Runtime / Task,,,,0,,,,,"Supports specifying multiple slots for one task.
Different tasks require different slot cpu cores and memory. Like the spark.task.cpus parameter of the spark engine.",,,,,,,,,,,,,,,,,,,,,,FLINK-31267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 08:55:39 UTC 2023,,,,,,,,,,"0|z1g0b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 07:03;wanglijie; Do you mean to specifiy different resources for different type of operators? Maybe the [fine-grained resource management|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/finegrained_resource] can satisfy your requirement.;;;","20/Feb/23 03:03;waywtdcc;Fine-Grained Resource Management is not enough. Fine-Grained Resource Management is task level and api, mine is job level and sql.;;;","02/Mar/23 08:25;Weijie Guo;Fine-Grained resource is defined in slot sharing group granularity, I thinks it can meet your requirement. But it's not support sql / table api now. So I think the reasonable requirement is to enable fine-grained support for SQL API.;;;","02/Mar/23 08:55;waywtdcc;[~Weijie Guo]  Yes, I think so too, raised a new issue;;;","02/Mar/23 08:55;waywtdcc;[~Weijie Guo]

FLINK-31267;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch job fails with IllegalStateException when using adaptive batch scheduler,FLINK-31114,13525139,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wanglijie,wanglijie,wanglijie,17/Feb/23 05:44,10/May/23 02:25,04/Jun/24 20:41,24/Feb/23 11:39,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"This is caused by FLINK-30942. Currently, if two job vertices have the same input and the same parallelism(even the parallelism is -1), they will share partitions. However after FLINK-30942, the scheduler may change the job vertices' parallelism before scheduling, resulting in two job vertices having the same parallelism in  compilation phase (in which case will share partitions), but different parallelism in the scheduling phase, and then cause the following exception:

{code:java}
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Consumers must have the same max parallelism.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:975)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
        ... 37 more
Caused by: java.lang.IllegalStateException: Consumers must have the same max parallelism.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
        at org.apache.flink.runtime.executiongraph.IntermediateResult.getConsumersMaxParallelism(IntermediateResult.java:219)
        at org.apache.flink.runtime.executiongraph.Execution.getPartitionMaxParallelism(Execution.java:501)
        at org.apache.flink.runtime.executiongraph.Execution.registerProducedPartitions(Execution.java:472)
        at org.apache.flink.runtime.executiongraph.Execution.registerProducedPartitions(Execution.java:431)
        at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$registerProducedPartitions$5(DefaultExecutionDeployer.java:277)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
        ... 38 more
{code}

Putting the following test into {{AdaptiveBatchSchedulerITCase}} can reproduce the problem：

{code:java}
    @Test
    void testDifferentConsumerParallelism() throws Exception {
        final Configuration configuration = createConfiguration();
        final StreamExecutionEnvironment env =
                StreamExecutionEnvironment.createLocalEnvironment(configuration);
        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
        env.setParallelism(8);

        final DataStream<Long> source1 =
                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                        .setParallelism(8)
                        .name(""source1"")
                        .slotSharingGroup(""group1"");

        final DataStream<Long> source2 =
                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                        .setParallelism(8)
                        .name(""source2"")
                        .slotSharingGroup(""group2"");

        source1.forward()
                .union(source2)
                .map(new NumberCounter())
                .name(""map1"")
                .slotSharingGroup(""group3"");

        source2.map(new NumberCounter()).name(""map2"").slotSharingGroup(""group4"");

        env.execute();
    }
{code}

",,,,,,,,,,,,,,,,,,,,,,,FLINK-30942,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 11:36:56 UTC 2023,,,,,,,,,,"0|z1g088:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 11:36;wanglijie;Fixed via 

master: b987ae18d0bc353c631bc54871b0c16be39dbad2

release-1.17: c66ef2540c3cb53f4cf3218ff07f5b440511ad84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support AND filter for flink-orc ,FLINK-31113,13525128,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,17/Feb/23 04:27,21/Feb/23 12:20,04/Jun/24 20:41,21/Feb/23 12:20,1.17.0,,,,,,,,,,,1.18.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,Support AND filter in flink-orc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 12:20:15 UTC 2023,,,,,,,,,,"0|z1g05s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 12:20;libenchao;Fixed via 5cda70d873c9630c898d765633ec7a6cfe53e3c6 (1.18.0)

 

[~zjureel] Thanks for supporting this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to Change the default mode of the volume attached to flink-config-volume in a pod on Flink native Kubernetes,FLINK-31112,13525127,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hob007,hob007,17/Feb/23 04:05,20/Feb/23 06:42,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"The default mode of the volume attached to flink-config-volume in the pod created by Flink native Kubernetes is 420. How Do I Change the default mode of the volume attached to flink-config-volume in a pod on Flink native Kubernetes?

 

Pod volume like this:

!image-2023-02-17-12-01-14-770.png!

 

It seems the configuration is hard code in the source code. !image-2023-02-17-12-02-28-386.png!",Flink version 1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/23 04:01;hob007;image-2023-02-17-12-01-14-770.png;https://issues.apache.org/jira/secure/attachment/13055538/image-2023-02-17-12-01-14-770.png","17/Feb/23 04:02;hob007;image-2023-02-17-12-02-28-386.png;https://issues.apache.org/jira/secure/attachment/13055537/image-2023-02-17-12-02-28-386.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 06:42:06 UTC 2023,,,,,,,,,,"0|z1g05k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 05:47;huwh;Can i ask why do you need change the mode?;;;","17/Feb/23 07:06;hob007;Hi Hu, Because security requires minimum permissions. 600 could be more secure.;;;","20/Feb/23 06:42;huwh;The Flink pod will only run TM/JM processes internally, no other processes outside of Flink will read the flink-conf file. Do you have any other cases?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce CatalogTestBase,FLINK-31111,13525126,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,17/Feb/23 03:37,29/Mar/23 03:10,04/Jun/24 20:41,29/Mar/23 03:10,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Currently, only tests for FlinkCatalog or ITCase, we should add cases for catalogs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 03:50:31 UTC 2023,,,,,,,,,,"0|z1g05c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 03:50;zhangjun;I think we should add a test class for parameterized test for catalog(hive,file),format (parquet,orc), mode(stream.batch);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Web UI shows ""User Configuration"" preserving lines and whitespaces",FLINK-31110,13525100,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,16/Feb/23 22:53,04/May/23 03:40,04/Jun/24 20:41,04/May/23 03:40,1.16.1,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"Currently one can use \{{env.getConfig().setGlobalJobParameters(...)}} for setting user configurations. It will also show up in the Web UI > Running Jobs > Job Configuration > User Configuration section. This is nice so users can confirm the user configuration (key/value pair) gets populated.

However, it does not preserves whitespaces and line breaks in HTML page. For example, we have some prettified JSON configuration and sometimes formatted SQL statements in those configurations, and it's showing in a compacted HTML format - not human readable original formatted string.

I propose we keep the whitespaces and lines for this ""User Configuration"" section in the Web UI. The implementation can be as simple as adding {{style=""white-space: pre-wrap;""}} to the rows in that section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/23 09:24;liuml07;Screenshot 2023-02-18 at 1.24.20 AM.png;https://issues.apache.org/jira/secure/attachment/13055597/Screenshot+2023-02-18+at+1.24.20+AM.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 03:40:30 UTC 2023,,,,,,,,,,"0|z1fzzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 22:59;liuml07;CC: [~markcho];;;","04/May/23 03:40;junhan;master: 246c17b8aeb75e66057d3d663d0c1a127108bc2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fails with proxy user not supported even when security.kerberos.fetch.delegation-token is set to false,FLINK-31109,13525087,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,16/Feb/23 21:23,27/May/24 05:35,04/Jun/24 20:41,27/Feb/23 13:54,1.17.0,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"With
{code:java}
security.kerberos.fetch.delegation-token: false
{code}
and delegation tokens obtained through our internal service which sets both HADOOP_TOKEN_FILE_LOCATION to pick up the DTs and also sets the HADOOP_PROXY_USER which fails with the below error
{code:java}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/export/home/vsowrira/flink-1.18-SNAPSHOT/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/export/apps/hadoop/hadoop-bin_2100503/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
org.apache.flink.runtime.security.modules.SecurityModule$SecurityInstallException: Unable to set the Hadoop login user
	at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:106)
	at org.apache.flink.runtime.security.SecurityUtils.installModules(SecurityUtils.java:76)
	at org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:57)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1188)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.UnsupportedOperationException: Proxy user is not supported
	at org.apache.flink.runtime.security.token.hadoop.KerberosLoginProvider.throwProxyUserNotSupported(KerberosLoginProvider.java:137)
	at org.apache.flink.runtime.security.token.hadoop.KerberosLoginProvider.isLoginPossible(KerberosLoginProvider.java:81)
	at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:73)
	... 4 more
{code}

This seems to have gotten changed after [480e6edf|https://github.com/apache/flink/commit/480e6edf9732f8334ef7576080fdbfc98051cb28] ([FLINK-28330][runtime][security] Remove old delegation token framework code)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 26 20:09:21 UTC 2023,,,,,,,,,,"0|z1fzwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 00:35;vsowrirajan;[~gaborgsomogyi] I will take a stab on fixing the issue.;;;","17/Feb/23 07:59;gaborgsomogyi;[~vsowrirajan] thanks for reporting the issue. I agree that proxy support is not allowed only in case of DT but in normal usage it should work. The mentioned check must be turned off/avoided all cases where no DT is involved (HadoopModule for sure, YARN area maybe affected). Ping me on the PR and I can review it...;;;","17/Feb/23 08:02;gaborgsomogyi;cc [~martijnvisser];;;","17/Feb/23 08:26;martijnvisser;Which version has been used for testing? Please update the affected version for this. Especially if it's marked as a blocker;;;","17/Feb/23 08:34;gaborgsomogyi;This exists on the latest master.;;;","17/Feb/23 08:49;martijnvisser;OK, then this is a release blocker for 1.17. We should get this fixed asap;;;","17/Feb/23 09:40;gaborgsomogyi;Waiting on [~vsowrirajan] since they have the test env where it can be validated. What I can help is to review when PR is available.;;;","17/Feb/23 09:46;martijnvisser;Perfect, thanks [~gaborgsomogyi] and [~vsowrirajan];;;","20/Feb/23 19:47;vsowrirajan;[~gaborgsomogyi] Thanks for the confirmation. Yeah I am still working on the fix. Will post it soon after testing it internally.;;;","21/Feb/23 08:08;martijnvisser;[~vsowrirajan] Is there any eta on this? Since its a blocker for 1.17, we would like to understand how long this is expecting to take before being resolved.;;;","21/Feb/23 18:25;vsowrirajan;[~martijnvisser] I'm still looking into it. I would probably need till the end of this week for the fix and all the internal testing that needs to be done before raising the PR. Do you have any timelines in mind?;;;","21/Feb/23 22:08;martijnvisser;[~vsowrirajan] Its planned to have all release testing and blockers resolved at the end of this week, so we can create a release candidate on Monday next week. ;;;","24/Feb/23 02:09;vsowrirajan;[~martijnvisser] Sure, I have posted a PR with the fix after discussing with [~gaborgsomogyi] . Please take a look whenever you get a chance. Thanks.;;;","26/Feb/23 20:09;martijnvisser;Fixed in

Master: 78136133fbec4ca145dec66d4bc0c324c8e16d82
Release-1.17: 29f4181e1076712dcaeaadeaad0c1bcb2ef25b70;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use StreamARN for API calls in Kinesis Connector,FLINK-31108,13525064,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,16/Feb/23 17:05,02/May/23 10:07,04/Jun/24 20:41,02/May/23 10:07,1.15.3,aws-connector-4.1.0,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / Kinesis,,,,0,pull-request-available,,,,"Currently, the FlinkKinesisConsumer (Polling + EFO) + FlinkKinesisProducer uses the stream name during API calls

We want to change this to the StreamARN. There are two reasons for this:
 - This allows lower latency calls to the Kinesis endpoint for GetRecords API
 - Paves the way for allowing user target cross-account streams without assume role (i.e. IAM role in account A but target stream in account B)

 

The APIs that are currently called:
 * ListShards
 * GetShardIterator
 * GetRecords
 * DescribeStream
 * DescribeStreamSummary
 * DescribeStreamConsumer (already uses StreamARN)
 * RegisterStreamConsumer (already uses StreamARN)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 02 10:07:46 UTC 2023,,,,,,,,,,"0|z1fzrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/23 10:07;dannycranmer;Merged commit [{{a62e3d8}}|https://github.com/apache/flink-connector-aws/commit/a62e3d866338f71345a92ac9d7e056eba603a763] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't subscribe the non-persistent topics by using regex in Pulsar 2.11.0,FLINK-31107,13525060,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,syhily,syhily,16/Feb/23 16:29,24/Jul/23 06:55,04/Jun/24 20:41,,pulsar-4.0.0,,,,,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,"This is a ticket for tracking [the known issue|https://github.com/apache/pulsar/issues/19316] in Pulsar. Pulsar changes its internal logic for topic pattern subscribe and didn't return any {{non-persistent}} topics. We will close this issue after Pulsar fixes this bug in the future.

Another issue for tracking this BUG: https://github.com/apache/pulsar/issues/19493",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 06:27:11 UTC 2023,,,,,,,,,,"0|z1fzqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 06:27;syhily;According to the [explain|https://github.com/apache/pulsar/issues/19493#issuecomment-1638226172] from the Pulsar develop team. The non-partitioned topic subscription should be supported with a little change on the polling time. We shall try to resolved it on the Connector if this works.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip history server archiving for suspended jobs on JsonResponseHistoryServerArchivist,FLINK-31106,13525013,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xuzifu,xuzifu,16/Feb/23 12:12,22/Feb/23 01:55,04/Jun/24 20:41,16/Feb/23 13:58,1.14.5,1.15.1,1.16.1,,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"JsonResponseHistoryServerArchivist would archivist data on FileSystem when TerminalState is not GLOBALLY. cause FileAlreadyExistsException like https://issues.apache.org/jira/browse/FLINK-24232

exception as：

INFO org.apache.flink.runtime.dispatcher.MiniDispatcher [] - Could not archive completed job ctdb_dw_push_ivideo_send_link_monitor_hi_prd(70f90a6c7bb2490d203f6c0d1818708d) to the history server. java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.hadoop.fs.FileAlreadyExistsException: /flink/completed-jobs/70f90a6c7bb2490d203f6c0d1818708d for client xxx.xxx.xxx.xxx already exists at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2967) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2856) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2741) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:620) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:115) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:412) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2281) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2277) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2275) at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_192] at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) [?:1.8.0_192] at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1629) [?:1.8.0_192] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_192] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_192] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_192] Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.FileAlreadyExistsException: /flink/completed-jobs/70f90a6c7bb2490d203f6c0d1818708d for client 10.194.100.17 already exists at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2967) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2856) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2741) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:620) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:115) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:412) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2281) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2277) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2275) at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316) ~[flink-dist_2.11-1.13.2.vivo-SNAPSHOT.jar:1.13.2.vivo-SNAPSHOT] at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:51) ~[flink-dist_2.11-1.13.2.vivo-SNAPSHOT.jar:1.13.2.vivo-SNAPSHOT] at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_192] ... 3 more Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: /flink/completed-jobs/70f90a6c7bb2490d203f6c0d1818708d for client xxx.xxx.xxx.xxx already exists at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2967) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2856) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2741) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:620) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:115) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:412) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2281) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2277) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2275)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 14:16:32 UTC 2023,,,,,,,,,,"0|z1fzg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 13:58;chesnay;This has been fixed in FLINK-24232.;;;","16/Feb/23 14:16;xuzifu;ok，i got it. in title i had notice FLINK-24232

and now comfirm it，thanks [~chesnay] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SqlClient: Make HELP, EXIT and QUIT also work without a semicolon",FLINK-31105,13525008,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,mapohl,mapohl,16/Feb/23 11:12,21/Aug/23 10:35,04/Jun/24 20:41,21/Aug/23 10:35,1.16.1,1.17.0,,,,,,,,,,,,,,Table SQL / Client,,,,0,stale-minor,starter,,,"The special commands {{HELP}}, {{EXIT}}, and {{QUIT}} are not really part of the SQL syntax but rather tool-specific commands. Having the requirement to have the semicolon follow-up on these commands might not be necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:30 UTC 2023,,,,,,,,,,"0|z1fzf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 11:14;mapohl;While writing the Jira issue I noticed that I should always put the semicolon after the command. This solves the issue.;;;","16/Feb/23 11:19;Sergey Nuyanzin;may be as an improvement it makes sense to add a semicolon to autocompletion for {{{}EXIT{}}}, {{HELP}} and {{QUIT}} since anyway there is nothing expected after having these commands in written but submitting a command;;;","16/Feb/23 12:22;mapohl;Fair enough, I updated the issue's type, title and description accordingly and reopened the task.;;;","16/Feb/23 12:32;jingge;Afaik, semicolon is quite convenience since we can write multiple lines command and finish it with one semicolon. Some coding guidelines even make it explicit that semicolon is mandatory and the semicolon must be written in the separate line to improve the readability.;;;","16/Feb/23 12:43;mapohl;This Jira issue is not about removing the semicolon but making it optional for the tool-specific commands. It took me a bit to realize that I just missed the semicolon for the commands to work. Of course, one can blame my beginner like attitude with the SQL interface. But maybe, that's just an opportunity to remove frustration for newcomers who just want to try SQL out. WDYT?;;;","16/Feb/23 17:16;jingge;I would recommend to stick to SQL standard syntax and behaviour, i.e. using semicolon to explicitly close a command/query.;;;","16/Feb/23 17:32;Sergey Nuyanzin;There is a slightly different idea: changing autocompletion.
Right now if a user types first letters and presses tab then it will autocompleted to the whole command except semicolon. For instance {{EX}} to {{EXIT}}, {{Q}} to {{QUIT}} and so on. 
In this case it is required to explicitly type semicolon before submitting a command. 
The idea is to make autocompletion with semicolon like {{EX}} to {{EXIT;}}, {{Q}} to {{QUIT;}}, {{HE}} to {{HELP;}} only for these commands since there are not requiring any args .
In this case SQL standard and behavior are satisfied ;;;","17/Feb/23 07:52;mapohl;[~jingge] out of curiosity, where do you see the flaws in the approach allowing to omit the semicolon for the non-standard methods QUIT, HELP, EXIT? It wouldn't change anything in terms of using these methods with a semicolon. I'm not passionate to push that change forward. I'm just curious where you see the disadvantage of this being implemented.

[~Sergey Nuyanzin] has a point with the auto-completion. I agree to that proposal as well.;;;","17/Feb/23 08:21;Weijie Guo;Improving automatic completion seems to be a good way from my side. +1 for [~Sergey Nuyanzin] 's proposal.;;;","17/Feb/23 09:20;jingge;[~mapohl] It is more about the user behaviour consistency. Just like we use semicolon in Java, Scala and Python don't. But all of them have their own consistency (Scala case is not very clean, that's why there are lots of discussions because users are confused). Back to SQL, even those commands are not standard, they still belong to Flink (new) SQL. They should stick to the standard, i.e. semicolon is required.

[~Sergey Nuyanzin] generally speaking, I am with you, it is a good idea to help users with autocompletion. But not including semicolon because again we should care about the behaviour consistency. If only some cases support the semicolon autocompletion, most of others are not, users will be confused and start asking more autocompletion for all other cases, like e.g. with (...), after "")"" and press TAB, the semicolon autocompletion will also be expected because of the behaviour consistency. There will be more and more cases rising. 

From design and implementation's perspective, when more corner case requirements are rising, it will quickly become complicated if we don't want to use hard coding if...else and offer a general solution. A general solution might make wrong decision and add semicolon at the place where semicolon is not required or not required yet. Afaik, that is why most IDEs e.g. Intellij Idea offer excellent autocompletion but don't take care of corner case of semicolon autocompletion.;;;","20/Feb/23 14:42;mapohl;I see your points, [~jingge]. But there's a clear separation between EXIT/QUIT, HELP and the FlinkSQL commands:
EXIT/QUIT and HELP are exclusive commands of the sql client and not part of the FlinkSQL standard which tries to be as close as possible to the SQL standard, AFAIU. This doesn't apply to the aforementioned commands. They are sql client-specific and, therefore, can be handled differently without opening the box of the pandora for other FlinkSQL commands.

Any, I'm also not that emotionally committed to that feature. I'm fine with closing the issue if there are objections against it. ¯\_(ツ)_/¯;;;","19/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;mapohl;Closing this issue since we didn't reach consensus and it's also just a minor issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS test timed out in query 36,FLINK-31104,13524996,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,Weijie Guo,mapohl,mapohl,16/Feb/23 09:55,13/Aug/23 10:35,04/Jun/24 20:41,,1.16.1,1.17.0,,,,,,,,,,,,,,Table SQL / Runtime,Tests,,,0,stale-assigned,test-stability,,,"There has a timeout happened in [apache-flink:flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/query/query36.sql|https://github.com/apache/flink/blob/20c983c26262057c4d59bd591aed89969a8ff525/flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/query/query36.sql] of the TPC-DS test suite:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46202&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=880

{code}
[...]
Feb 16 04:58:23 [INFO]Run TPC-DS query 36 ...
Feb 16 04:58:23 Job has been submitted with JobID 4d0c1e6cbde9f0b6ae8b9f9afd159c06
{code}

Unfortunately, no further logs are provided.",,,,,,,,,,,,,,,,,,FLINK-31288,FLINK-31293,,,,FLINK-26762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 10:35:01 UTC 2023,,,,,,,,,,"0|z1fzcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 10:08;mapohl;With the limited artifacts that were collected by CI, it's hard to tell what's going on here. The only poiter we have as far as I can see is the query itself.

[~snuyanzin] can you get anything out of it based on the query to rule out that it's related to the calcite updates?
[~fsk119] is there anything related to recent changes that might be related here?;;;","16/Feb/23 14:00;Sergey Nuyanzin;Currently it's not obvious to me ...
At the same time there is an improvement for TPC-DS within 1.17.0 https://issues.apache.org/jira/browse/FLINK-27583
May be [~337361684@qq.com] is aware of it;;;","20/Feb/23 02:08;fsk119;[~zhengyunhong97] and [~lsy]  Could you share some thoughts about this?;;;","27/Feb/23 13:32;mapohl;[~fsk119] [~zhengyunhong97] [~lsy] any updates on that one?;;;","28/Feb/23 08:29;lsy;cc [~guoweijie] , can you also help take a look?;;;","28/Feb/23 08:36;lsy;I also occur the query timeout when running tpcds in the yarn cluster, the thread stack as follows:
{code:java}
""HashJoin[3174] [Source: store_sales[3210], Source: household_demographics[3185], Source: store_sales[3144]] -> Calc[3175] -> HashAggregate[3176] -> Calc[3177] (940/1500)#0"" Id=7959 WAITING on java.util.concurrent.CompletableFuture$Signaller@4a270956
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.CompletableFuture$Signaller@4a270956
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
    at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
    at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
    at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:384)
    at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:350)
    at org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.requestNetworkBuffers(SortMergeResultPartition.java:339)
    at org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.createNewDataBuffer(SortMergeResultPartition.java:307)
    at org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.getBroadcastDataBuffer(SortMergeResultPartition.java:302)
    at org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.emit(SortMergeResultPartition.java:256)
    at org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.broadcast(SortMergeResultPartition.java:248)
    at org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.broadcastRecord(SortMergeResultPartition.java:223)
    at org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriter.broadcastEmit(BroadcastRecordWriter.java:48)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:121)
    at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:43)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:604)
    at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:57)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.emitWatermark(ChainingOutput.java:107)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:604)
    at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:57)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.emitWatermark(ChainingOutput.java:107)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:604)
    at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:57)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.emitWatermark(ChainingOutput.java:107)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:604)
    at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:57)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:609)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark2(AbstractStreamOperator.java:618)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitWatermark(StreamTwoInputProcessorFactory.java:268)
    at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:199)
    at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:114)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:148)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:547)
    at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$653/803691740.runDefaultAction(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:834)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:783)
    at org.apache.flink.runtime.taskmanager.Task$$Lambda$1273/1500345048.run(Unknown Source)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:750)""System Time Trigger for HashJoin[3174] [Source: store_sales[3210], Source: household_demographics[3185], Source: store_sales[3144]] -> Calc[3175] -> HashAggregate[3176] -> Calc[3177] (707/1500)#0"" Id=7955 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@5358647
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@5358647
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750) {code};;;","28/Feb/23 09:47;Weijie Guo;The failure case in CI may not be easy to troubleshoot. I will investigate this in the reproducable environment that [~lsy] provided.;;;","02/Mar/23 09:31;Weijie Guo;Hi all,

To be honest, the root cause of this ticket is difficult to investigate because there is no enough context information provided here like thread dump or heap dump. But I have reproduced the problem of TPC-DS timeout on our internal cluster. I guess that is caused by the same reason as this ticket.

After some investigations, [~fanrui] and I found that we do have a bug in {{{}LocalBufferPool{}}}, but this should be due to FLINK-26762, which has been merged into the master since 1.16.

To fix this bug, I created FLINK-31293. At the same time, I found that in the batch scenario, we do not need the overdraft buffer actually. I will disable it in FLINK-31288, which should make our TPC-DS test stable.

Since the problem was not introduced in 1.17 and can be temporarily fixed through FLINK-31288, I suggest lowering the priority to unblock 1.17 release process. [~renqs] , [~mapohl] WDYT?;;;","02/Mar/23 10:05;renqs;Thanks for the investigation and feedback [~Weijie Guo] ! LGTM to downgrade the issue as this already exists in 1.16. ;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Public Table Reader API for table store,FLINK-31103,13524991,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Feb/23 09:47,17/Feb/23 07:02,04/Jun/24 20:41,17/Feb/23 07:02,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 07:02:38 UTC 2023,,,,,,,,,,"0|z1fzbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 07:02;lzljs3620320;master: 726af803d233e2752cc5532b59ed977eecc8c5aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_REMOVE supported in SQL & Table API,FLINK-31102,13524990,13076759,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,16/Feb/23 09:44,30/Mar/23 05:05,04/Jun/24 20:41,26/Mar/23 21:36,1.18.0,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Remove all elements that equal to element from array.

Syntax:
array_remove(array, needle)

Arguments:
array: An ARRAY to be handled.

Returns:

An ARRAY. If array is NULL, the result is NULL. 
Examples:
{code:sql}
SELECT array_remove(array[1, 2, 3, null, 3], 3); 
-- [1,2,null]
{code}
See also
spark [https://spark.apache.org/docs/latest/api/sql/index.html#array_remove]

presto [https://prestodb.io/docs/current/functions/array.html]

postgresql https://www.postgresql.org/docs/12/functions-array.html#ARRAY-FUNCTIONS-TABLE",,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 26 21:36:08 UTC 2023,,,,,,,,,,"0|z1fzb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 12:30;Sergey Nuyanzin;{quote}

Syntax:
array_remove(array)

{quote}

I guess it should be corrected since it takes 2 args

{quote}

spark [[https://spark.apache.org/docs/latest/api/sql/index.html#array_size]|https://spark.apache.org/docs/latest/api/sql/index.html#array_remove]

{quote}

looks like a link to another function.

{quote}

An ARRAY. If value is NULL, the result is NULL. 

{quote}

i guess it should return an array without nulls if first arg is an array and second is a null, correct?

 ;;;","10/Mar/23 14:47;jackylau;[~Sergey Nuyanzin] yeap, it is a misspelling and i have fixed description;;;","10/Mar/23 16:02;Sergey Nuyanzin;Thanks, it already looks better
however there is still room for improvements
1. spark link still contains {{array_size}} in its name which is not related to this issue
2. {quote}An ARRAY. If value is NULL, the result is NULL. {quote}
   what is {{value}} here?
3. just a nit comment
{quote}postgresql https://w3resource.com/PostgreSQL/postgresql_array_remove-function.php {quote}
it's better to refer to official documentation like https://www.postgresql.org/docs/12/functions-array.html#ARRAY-FUNCTIONS-TABLE
rather than 3-rd party

same is applied to github's PR description;;;","10/Mar/23 23:01;jackylau;[~Sergey Nuyanzin] ,thanks for your suggestion and have fixed. and I will be aware of these problem when supporting other array functions later.;;;","26/Mar/23 21:36;Sergey Nuyanzin;Merged to master as [f3c653ed2e4264315ed83a5b4b2494a7dcc41474|https://github.com/apache/flink/commit/f3c653ed2e4264315ed83a5b4b2494a7dcc41474];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace all the JUnit 5 assertions to AssertJ,FLINK-31101,13524977,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,16/Feb/23 09:00,16/Feb/23 14:19,04/Jun/24 20:41,16/Feb/23 14:19,pulsar-4.0.0,,,,,,,,,,,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,"The [common rules|https://flink.apache.org/contributing/code-style-and-quality-common.html#tooling] in Flink contribution guide deprecates the use of JUnit assertions. We shall move all the test assertion to AssertJ.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 14:19:27 UTC 2023,,,,,,,,,,"0|z1fz88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 14:19;Weijie Guo;main(4.0) via 2c43292cc4af2bfedcfc801d0d53576cdbed92a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the short topic pattern subscriber on default tenant and namespace,FLINK-31100,13524971,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,16/Feb/23 08:02,17/Feb/23 05:16,04/Jun/24 20:41,17/Feb/23 05:16,pulsar-4.0.0,,,,,,,,,,,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,The connector can subscribe the topics by a regular expression. But currently we may require the regular expression has the tenant and namespace. We shall add the default tenant and namespace support for a simple topic regular expression.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 05:16:56 UTC 2023,,,,,,,,,,"0|z1fz6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 05:16;Weijie Guo;main(4.0) via 47b588f62bd8b9630303d62d0f3b36469f81d5fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chained WindowOperator throws NPE in PyFlink ThreadMode,FLINK-31099,13524963,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,16/Feb/23 07:22,16/Feb/23 09:31,04/Jun/24 20:41,16/Feb/23 09:31,1.16.1,1.17.0,,,,,,,,,,1.16.2,1.17.0,,,API / Python,,,,0,pull-request-available,,,,"Test case
{code:python}
config = Configuration()
config.set_string(""python.execution-mode"", ""process"")
env = StreamExecutionEnvironment.get_execution_environment(config)

class MyTimestampAssigner(TimestampAssigner, ABC):
    def extract_timestamp(self, value: tuple, record_timestamp: int) -> int:
        return value[0]

ds = env.from_collection(
    [(1676461680000, ""a1"", ""b1"", 1), (1676461680000, ""a1"", ""b1"", 1),
     (1676461680000, ""a2"", ""b2"", 1), (1676461680000, ""a1"", ""b2"", 1),
     (1676461740000, ""a1"", ""b1"", 1), (1676461740000, ""a2"", ""b2"", 1)]
).assign_timestamps_and_watermarks(
    WatermarkStrategy.for_monotonous_timestamps().with_timestamp_assigner(MyTimestampAssigner())
)
ds.key_by(
    lambda x: (x[0], x[1], x[2])
).window(
    TumblingEventTimeWindows.of(Time.minutes(1))
).reduce(
    lambda x, y: (x[0], x[1], x[2], x[3] + y[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING(), Types.INT()])
# ).filter(
#     lambda x: x[1] == ""a1""
).map(
    lambda x: (x[0], x[1], x[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.INT()])
).print()
env.execute()
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 09:31:57 UTC 2023,,,,,,,,,,"0|z1fz54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 09:31;hxbks2ks;Merged into master via ca770b3d905936d8a93071210bd6542b6733221d
Merged into release-1.17 via c7c035a2413c04cd75948d8364e0770b97499901
Merged into release-1.16 via e3c0060e7fca53e0e01cb91e00607c8146b85604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_SIZE supported in SQL & Table API,FLINK-31098,13524942,13076759,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,16/Feb/23 04:07,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,,"Returns an unordered array containing the keys of the map.

Syntax:
array_size(array)

Arguments:
array: An ARRAY to be handled.

Returns:

An ARRAY. If value is NULL, the result is NULL. 
Examples:
{code:sql}
SELECT array_size(ARRAY[1, 2, 3, 2, 1]);
-- 5
SELECT array_size(ARRAY[1, NULL, 1]);
-- 3
{code}
See also
spark [https://spark.apache.org/docs/latest/api/sql/index.html#array_size]

snowflake [https://docs.snowflake.com/en/sql-reference/functions/array_size]
h4.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 10:35:02 UTC 2023,,,,,,,,,,"0|z1fz0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 10:21;Sergey Nuyanzin;Do we really need {{ARRAY_SIZE}}?
I'm asking because there is already existing {{CARDINALITY}} which also gives size of array or map and returns {{NULL}} for null input.

I would rather suggest something like {{ARRAY_LENGTH}} from postgres which could return the size of the specified array dimension
{noformat}
array_length ( anyarray, integer ) → integer

Returns the length of the requested array dimension. (Produces NULL instead of 0 for empty or missing array dimensions.)

array_length(array[1,2,3], 1) → 3

array_length(array[]::int[], 1) → NULL

array_length(array['text'], 2) → NULL
{noformat}
https://www.postgresql.org/docs/current/functions-array.html#ARRAY-FUNCTIONS-TABLE;;;","16/Feb/23 10:34;Sergey Nuyanzin;I converted it into a subtask of a more generic issue for built-in functions;;;","16/Feb/23 11:23;jackylau;but i think CARDINALITY is not a standard usage, and  we can support array_length too [~Sergey Nuyanzin] ;;;","16/Feb/23 11:59;Sergey Nuyanzin;Could you please clarify what do you mean by `standard`?

There is a SQL Standard 2011 where cardinality is mentioned many times.
In context of arrays there is such quote from there
{quote}An array is a collection A in which each element is associated with exactly one ordinal position in A. If n is
the cardinality of A, then the ordinal position p of an element is an integer in the range 1 (one) ≤ p ≤ n. If EDT
is the element type of A, then A can thus be considered as a function of the integers in the range 1 (one) to n
into EDT.
{quote}

P.S. I looked at draft standards from here http://www.wiscorp.com/SQLStandards.html;;;","17/Feb/23 02:16;jackylau;[~Sergey Nuyanzin] thanks for your detailedly explain. but supports array_size like spark/snowflake is ok, what do you think?

If you don't think it's necessary, then I'll close the issue and pr

 ;;;","20/Feb/23 15:12;martijnvisser;I agree with [~Sergey Nuyanzin] that since we already have CARDINALITY which is part of the SQL standard, I see little value in adding the same function under a different name. ;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mysql catalog datatype mapping error tinyint mapping boolean ,FLINK-31097,13524938,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hiscat,hiscat,16/Feb/23 03:16,16/Feb/23 03:23,04/Jun/24 20:41,16/Feb/23 03:23,1.16.0,1.16.1,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"mysql ddl 
`auto_borrow` tinyint(1) DEFAULT '0',
 
flink sql client
mysql catalog query 
  `auto_borrow` BOOLEAN,
 
so weird.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 03:23:58 UTC 2023,,,,,,,,,,"0|z1fyzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 03:23;hiscat;mysql driver bug;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify versions in pom of table store,FLINK-31096,13524936,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,16/Feb/23 02:53,16/Feb/23 05:27,04/Jun/24 20:41,16/Feb/23 05:27,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 05:27:10 UTC 2023,,,,,,,,,,"0|z1fyz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 05:27;lzljs3620320;master: d767860c566b85e561801b3a32f3fe5f305a94e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink doesn't work with s3a on EKS,FLINK-31095,13524932,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,linqyd218,linqyd218,16/Feb/23 02:08,23/May/23 18:49,04/Jun/24 20:41,23/May/23 18:49,1.16.1,,,,,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"FileSink gives below exception on AWS EKS cluster:
{code:java}
Caused by: java.lang.UnsupportedOperationException: This s3 file system implementation does not support recoverable writers.
	at org.apache.flink.fs.s3.common.FlinkS3FileSystem.createRecoverableWriter(FlinkS3FileSystem.java:136) ~[?:?]
	at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.createRecoverableWriter(PluginFileSystemFactory.java:134) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder.createBucketWriter(FileSink.java:475) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder.getCommittableSerializer(FileSink.java:466) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.sink.FileSink.getCommittableSerializer(FileSink.java:175) ~[flink-connector-files-1.16.1.jar:1.16.1]{code}
[https://github.com/apache/flink/blob/278dc7b793303d228f7816585054629708983af6/flink-filesystems/flink-s3-fs-base/src/main/java/org/apache/flink/fs/s3/common/FlinkS3FileSystem.java#LL136C16-L136C16]

And this may be related to https://issues.apache.org/jira/browse/FLINK-23487?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 18:48:36 UTC 2023,,,,,,,,,,"0|z1fyy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 03:55;sap1ens;It works for me (specifically on EKS). Did you enable *flink-s3-fs-hadoop* plugin? It's the only plugin that supports S3A. ;;;","16/Feb/23 07:54;martijnvisser;That error is not related to EKS, but you're trying to use the Presto plugin for reading FileSystem. You should use the Hadoop one for reading from FileSystem, Presto for checkpointing as outlined on https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/s3/;;;","16/Feb/23 19:13;linqyd218;Thank you both for the reply!

I've enabled the *flink-s3-fs-hadoop* plugin, after working with [~sap1ens] offline, we changed the S3 path prefix from `s3` to `s3a` and above error got fixed.

But with above *UnsupportedOperationException* fixed, we have a new permission error for S3 access denied:
{code:java}
Caused by: java.nio.file.AccessDeniedException: java.nio.file.AccessDeniedException: translated/***: initiate MultiPartUpload on translated/***: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: ***; S3 Extended Request ID: ***; Proxy: null), S3 Extended Request ID: ***=:AccessDenied
{code}
And we pull the S3 request log it shows as below, the request IAM role is the node group IAM role, but not the IAM role we bind to the service account.
{code:java}
User Agent: Hadoop 3.3.2, aws-sdk-java/1.11.951 Linux/5.4.209-116.367.amzn2.x86_64 OpenJDK_64-Bit_Server_VM/25.362-b09 java/1.8.0_362 vendor/Temurin

Requester: arn:aws:iam::***:role/second_group-eks-node-group-***

Response Code: 403

Error Code: AccessDenied {code}
 

And confirmed with [~sap1ens] on slack, he needs to rely on the node group iam role, which is not the way how IRSA suppose to work.;;;","17/Feb/23 02:03;tzulitai;Hi [~linqyd218],

I think your latest findings are expected - based on FLINK-23487, unfortunately it looks like IRSA isn't yet supported for flink-s3-fs-hadoop.

Pulling in [~airblader] to provide some context. Do you have some insight on why we were only able to fix IRSA for flink-s3-fs-presto in FLINK-23487?;;;","17/Feb/23 14:44;martijnvisser;I did a quick check on the PR belonging to FLINK-23487 which includes this comment from [~airblader] in https://github.com/apache/flink/pull/16592/files#r679835626

| For Hadoop you need special configuration because Hadoop ships its own credentials provider chain mechanism.

I am by no means an S3 or a Hadoop expert, but shouldn't it then be possible to follow https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/assumed_roles.html to setup IRSA?;;;","21/Feb/23 22:08;linqyd218;Thanks for reply! [~martijnvisser] [~tzulitai] 

Based on the hadoop doc you shared above, I checked here, seems we do have to configure something: [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/s3/#hadooppresto-s3-file-systems-plugins] 
But i didn't find the related s3 hadoop Flink config list here: [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/] 

Any other place I can take a look? Or it follows certain convention? ;;;","21/Feb/23 22:43;martijnvisser;[~linqyd218] I think it's highlighted in this part on the Hadoop/Presto S3 File Systems plugins section

| For example, Hadoop has a fs.s3a.connection.maximum configuration key. If you want to change it, you need to put s3.connection.maximum: xyz to the flink-conf.yaml. Flink will internally translate this back to fs.s3a.connection.maximum. There is no need to pass configuration parameters using Hadoop’s XML configuration files.

So let's say you need to set key {{fs.s3a.aws.credentials.provider}} to value {{org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider}}, you would need to edit the flink-conf.yaml and put {{s3.aws.credentials.provider: org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider}} there.;;;","13/Mar/23 05:33;linqyd218;Hey [~martijnvisser] , Thanks for reply!

I tried above configs against Flink 1.16.1, but now it's giving error:
{code:java}
Caused by: java.nio.file.AccessDeniedException: : org.apache.hadoop.fs.s3a.auth.NoAwsCredentialsException: SimpleAWSCredentialsProvider: No AWS credentials in the Hadoop configuration
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:784) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:698) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:631) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:877) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534) ~[?:?]
	at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:127) ~[?:?]
	at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder.createBucketWriter(FileSink.java:475) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder.getCommittableSerializer(FileSink.java:466) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.sink.FileSink.getCommittableSerializer(FileSink.java:175) ~[flink-connector-files-1.16.1.jar:1.16.1] {code}

Below is my Flink config:
{code:java}
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""2""
    ...
    s3.aws.credentials.provider: org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider
    s3.assumed.role.arn: arn:aws:iam::[aws-accnt-num]:role/my-iam-role {code}

And hadoop plugin had been added.


can you point me to `s3` related config keys?






 ;;;","13/Mar/23 08:00;martijnvisser;[~linqyd218] Do you know have both plugins (Hadoop and Presto) running or just one? If you have loaded both have them, then please try s3a for the Hadoop config options and presto.s3 for the Presto config options (instead of s3);;;","13/Mar/23 13:14;martijnvisser;[~linqyd218] From the original PR that introduced IRSA, it's mentioned that for Hadoop the following config should be used:

{code}
fs.s3a.aws.credentials.provider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider
{code};;;","23/May/23 07:39;martijnvisser;[~linqyd218] Did you end up getting this working? ;;;","23/May/23 18:48;linqyd218;Yeah, it's working! Thanks! I can close the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Add table name to Writer,FLINK-31094,13524930,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,DaiXinyu,DaiXinyu,DaiXinyu,16/Feb/23 01:37,16/Feb/23 04:34,04/Jun/24 20:41,16/Feb/23 04:33,table-store-0.4.0,,,,,,,,,,,table-store-0.3.1,table-store-0.4.0,,,Table Store,,,,0,pull-request-available,,,," Add table name to write operator

 before 

  !image-2023-02-16-09-36-15-721.png!

 

after 

 

!image-2023-02-16-09-37-37-980.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/23 01:35;DaiXinyu;image-2023-02-16-09-35-15-916.png;https://issues.apache.org/jira/secure/attachment/13055490/image-2023-02-16-09-35-15-916.png","16/Feb/23 01:36;DaiXinyu;image-2023-02-16-09-36-15-721.png;https://issues.apache.org/jira/secure/attachment/13055489/image-2023-02-16-09-36-15-721.png","16/Feb/23 01:37;DaiXinyu;image-2023-02-16-09-37-37-980.png;https://issues.apache.org/jira/secure/attachment/13055488/image-2023-02-16-09-37-37-980.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 04:33:51 UTC 2023,,,,,,,,,,"0|z1fyxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 04:33;lzljs3620320;master: 9edee3afec998c186726bb7d30d486f34d47c461
release-0.3: cee8b4ac3bd8e6081883afba50a8669c7a72a509;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullpointerException when restoring a FlinkSQL job from a savepoint,FLINK-31093,13524876,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Not A Problem,,mapohl,mapohl,15/Feb/23 16:04,16/Feb/23 10:35,04/Jun/24 20:41,16/Feb/23 10:15,1.17.0,,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"I tried to restore a FlinkSQL job from a savepoint and ran into a {{NullPointerException}}:
{code}
2023-02-15 16:38:24,835 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'collect' (0263d02536654102f2aa903f843cacd1).
2023-02-15 16:38:24,858 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 0263d02536654102f2aa903f843cacd1 reached terminal state FAILED.
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.flink.api.common.ExecutionConfig.getNumberOfExecutionRetries(ExecutionConfig.java:486)
        at org.apache.flink.api.common.ExecutionConfig.getRestartStrategy(ExecutionConfig.java:459)
        at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:99)
        at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:348)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
        ... 3 more
{code}

The SQL job was submitted through the SQL client:
{code}
$ -- table created in Flink 1.16.1
$ CREATE TABLE MyTable (
>    a bigint,
>    b int not null,
>    c varchar,
>    d timestamp(3)
> ) with ('connector' = 'datagen', 'rows-per-second' = '1', 'fields.a.kind' = 'sequence', 'fields.a.start' = '0', 'fields.a.end' = '1000000');
$ -- SELECT statement ran in Flink 1.16.1 session cluster
$ SELECT a FROM MyTable WHERE a = 1 or a = 2 or a IS NOT NULL;
{code}
The job was stopped with a savepoint from the command line:
{code}
$ ./bin/flink stop --type native --savepointPath ../1.16.1-savepoint 6029e8e5632a9852c630b1b0e4b62477
{code}
A new 1.17-SNAPSHOT (commit: {{21158c06}}) session cluster was started and the following SQL code was executed from within the SQL client:
{code}
$ SET 'execution.savepoint.path' = '/home/mapohl/research/FLINK-31066/1.16.1-savepoint/savepoint-6029e8-ef1e50f0dd2e';
$ SELECT a FROM MyTable WHERE a = 1 or a = 2 or a IS NOT NULL;
[ERROR] Could not execute SQL statement. Reason:
java.util.concurrent.CompletionException: java.lang.NullPointerException
{code}

This caused the {{NullPointerException}} with the aforementioned stacktrace.

The error is caused by [ExecutionConfig:486|https://github.com/apache/flink/blob/143464d82814e342aa845f3ac976ae2854fc892f/flink-core/src/main/java/org/apache/flink/api/common/ExecutionConfig.java#L486]. The line can only cause a {{NullPointerException}} if the corresponding configuration is not set. This only happens if the {{ExecutionConfig}} is deserialized but the {{configuration}} field is not deserialized which leaves the field to be {{null}} initialized.

This field is not set to {{null}} in any other way.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,FLINK-31066,,,,,"15/Feb/23 16:17;mapohl;flink-conf.yaml;https://issues.apache.org/jira/secure/attachment/13055480/flink-conf.yaml","15/Feb/23 16:17;mapohl;flink-mapohl-standalonesession-0-aiven-mapohl.log;https://issues.apache.org/jira/secure/attachment/13055479/flink-mapohl-standalonesession-0-aiven-mapohl.log",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 10:15:20 UTC 2023,,,,,,,,,,"0|z1fyls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 16:12;mapohl;This doesn't appear to happen in 1.16.1. [~shengkai] can you have a look at it? I'm increasing the priority to blocker until we have a better understanding on why this is happening.;;;","15/Feb/23 16:17;mapohl;I added the JobManager logs with the {{NullPointerException}} stacktrace and the used Flink configuration.;;;","16/Feb/23 08:31;mapohl;I see that FLINK-30683 touched the {{ExecutionConfig}} in [JobGraphGenerator:268|https://github.com/apache/flink/blob/143464d82814e342aa845f3ac976ae2854fc892f/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java#L268]. [~zhuzh] [~JunRuiLi] is that something that might have caused this behavior?;;;","16/Feb/23 10:07;JunRuiLi;[~mapohl] According to FLINK-29379, [ExecutionConfig:486|https://github.com/apache/flink/blob/143464d82814e342aa845f3ac976ae2854fc892f/flink-core/src/main/java/org/apache/flink/api/common/ExecutionConfig.java#L486] was introduced in 1.17. Could you confirm whether the version of sql client and cluster are both 1.17?;;;","16/Feb/23 10:15;mapohl;args, you got me on that one - I did run the wrong sql client. I used the 1.16.1 sql client for the 1.17-SNAPSHOT test run as well. I verified that the problem described in this Jira issue only appears with the 1.16.1 sql client on a 1.17-SNAPSHOT Flink cluster. It doesn't appear with the 1.17-SNAPSHOT sql client on a 1.17-SNAPSHOT Flink cluster.

Sorry for the noise. That was my mistake.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive ITCases fail with OutOfMemoryError,FLINK-31092,13524864,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,mapohl,mapohl,15/Feb/23 14:55,06/Mar/23 11:04,04/Jun/24 20:41,06/Mar/23 11:04,1.16.1,1.17.0,1.18.0,,,,,,,,,1.16.2,1.17.0,1.18.0,,Connectors / Hive,,,,0,pull-request-available,test-stability,,,"We're experiencing an OutOfMemoryError where the heap space reaches the upper limit:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46161&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23142

{code}
Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
Feb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap space
Feb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...
Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.maven.surefire.booter.ForkedBooter.cancelPingScheduler(ForkedBooter.java:209)
	at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:419)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:186)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30556,FLINK-20945,,,,,,,,,,"01/Mar/23 07:40;fsk119;-__w-2-s-flink-connectors-flink-connector-hive-target-surefire-reports-2023-02-15T05-01-18_982-jvmRun4.dump;https://issues.apache.org/jira/secure/attachment/13055926/-__w-2-s-flink-connectors-flink-connector-hive-target-surefire-reports-2023-02-15T05-01-18_982-jvmRun4.dump","15/Feb/23 15:03;mapohl;VisualVM-FLINK-31092.png;https://issues.apache.org/jira/secure/attachment/13055473/VisualVM-FLINK-31092.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 07:41:08 UTC 2023,,,,,,,,,,"0|z1fyj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 14:58;mapohl;I'm linking FLINK-30556 and FLINK-20945 here because they also deal with OOM in the Hive connector. That's the reason why I'm keeping this one at {{Critical}} instead of {{Blocker}}.;;;","15/Feb/23 15:03;mapohl;There's a heap dump created that reveals that the majority of the data (44.2%=~850MB) comes from {{char[]}} instances (e.g. SQL queries). There are also various {{ServiceLoaderUtil.LoadResult}} instances stored in the heap (24%=~460MB)  that reference an {{IllegalStateException}}.
 !VisualVM-FLINK-31092.png! ;;;","15/Feb/23 15:04;mapohl;[~Wencong Liu] may you have a look at it since you worked on FLINK-30556?;;;","15/Feb/23 15:18;Wencong Liu;Hi [~mapohl], is the heap dump generated before OOM error occurs?;;;","15/Feb/23 16:35;mapohl;I'm not sure - based on the logs, it sounds like it was created as soon as the OOM error happened:
{code}
Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
Feb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap space
Feb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...
Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]
java.lang.OutOfMemoryError: Java heap space
[...]
{code}
We're also setting {{+HeapDumpOnOutOfMemoryError}} in [tools/ci/test_controller.sh:67|https://github.com/apache/flink/blob/7e37d59f834bca805f5fbee99db87eb909d1814f/tools/ci/test_controller.sh#L67]. AFAIU, this makes the JVM generate the heap dump as soon as the OOM occurs.;;;","21/Feb/23 08:15;martijnvisser;[~luoyuxia] Can you take a look?;;;","22/Feb/23 02:20;luoyuxia;Sure, I'll have a look. ;;;","27/Feb/23 04:28;luoyuxia;Try to analyze the heap dump, I found most of object will be `ServiceLoaderUtil#LoadResult(IllegalStateException)`, and the exception message is `

Trying to access closed classloader.xxxx`. I think that's the cause of OOM. From the code:

 
{code:java}
static <T> List<LoadResult<T>> load(Class<T> clazz, ClassLoader classLoader) {
    List<LoadResult<T>> loadResults = new ArrayList<>();

    Iterator<T> serviceLoaderIterator = ServiceLoader.load(clazz, classLoader).iterator();

    while (true) {
        try {
            T next = serviceLoaderIterator.next();
            loadResults.add(new LoadResult<>(next));
        } catch (NoSuchElementException e) {
            break;
        } catch (Throwable t) {
            loadResults.add(new LoadResult<>(t));
        }
    }

    return loadResults;
} {code}
Seems it'll then loop indefinitely when `serviceLoaderIterator.next()` throw exception other than NoSuchElementException. And it'll add more and more `LoadResult`  utill OOM.

 

And the stack where OOM happens is as follows:

 
{code:java}
 at java.lang.OutOfMemoryError.<init>(OutOfMemoryError.java:48)
    at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:179)
    at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResources(FlinkUserCodeClassLoaders.java:213)
    at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:348)
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:364)
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
    at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42)
    at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:805)
    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:524)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:45)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.createStreamTableEnvironment(OperationExecutor.java:375)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.getTableEnvironment(OperationExecutor.java:332)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:190)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl$$Lambda$1007.apply(<unknown string>)
    at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:110)
    at org.apache.flink.table.gateway.service.operation.OperationManager$$Lambda$1008.call(<unknown string>)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:242)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation$$Lambda$1010.run(<unknown string>)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748){code}
 

 

[~fsk119] Could you please have a look? Is it possible it'll access a closed classloader? 

 ;;;","27/Feb/23 10:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46558&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22559;;;","27/Feb/23 11:11;mapohl;I'm increasing the priority of this issue to blocker since there was actual work done on the Hive code in 1.17 and so far we're only seeing the issue in 1.17 (i.e. {{master}} or {{{}release-1.17{}}});;;","28/Feb/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46609&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22520;;;","01/Mar/23 12:12;fsk119;Hi, all. I think [~luoyuxia] is right. `ServiceLoaderUtil#LoadResult(IllegalStateException)` keeps loading the exception when the classloader is closed by the SessionManager. The case is possible to happen when the session is closed but the operation is running. But it's difficult for Gateway to cancel the task that is submitted to the `ExecutorService` by force if the task doesn't respect the interrupted flag.

[~slinkydeveloper], [~twalthr] could you share some thoughts about this? Can we limit the type of exception here to prevent endless loading?
{code:java}
static <T> List<LoadResult<T>> load(Class<T> clazz, ClassLoader classLoader) {
    List<LoadResult<T>> loadResults = new ArrayList<>();

    Iterator<T> serviceLoaderIterator = ServiceLoader.load(clazz, classLoader).iterator();

    while (true) {
        try {
            T next = serviceLoaderIterator.next();
            loadResults.add(new LoadResult<>(next));
        } catch (NoSuchElementException e) {
            break;
        } catch (Throwable t) {
            loadResults.add(new LoadResult<>(t));
        }
    }

    return loadResults;
} 

{code};;;","01/Mar/23 15:34;mapohl;From an outsider's perspective: What is preventing us from providing cancellation functionality in the Operator interface? It feels like the session should stop any running operation when it is shut down, shouldn't it? 🤔 As a quickfix it sounds reasonable to replace the {{IllegalStateException}} in [FlinkUserCodeClassLoaders:179|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-core/src/main/java/org/apache/flink/util/FlinkUserCodeClassLoaders.java#L179] by a {{ClosedClassloaderException}} which extends {{FlinkRuntimeException}} and make this being caught in the code you already mentioned ([ServiceLoaderUtil:44|https://github.com/apache/flink/blob/dd446c9d56be5f33c683611102ec7026cf95e395/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/ServiceLoaderUtil.java#L44]). But again, it sounds we're missing this cancel feature if I understand you correctly.;;;","02/Mar/23 08:05;fsk119;Yes, Gateway will try to cancel the execution of the running operation by explicitly interrupting the thread in [Operation:377|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/service/operation/OperationManager.java#L377] and wait all task finish. However, the Gateway seems close the resource before all task finishes. This confuses me a lot. BTW, I think we can propose a quick fix like this to prevent endless loading here. WDYT [~mapohl] ,[~Leonard] ?
{code:java}
static <T> List<LoadResult<T>> load(Class<T> clazz, ClassLoader classLoader) {
        List<LoadResult<T>> loadResults = new ArrayList<>();

        Iterator<T> serviceLoaderIterator = ServiceLoader.load(clazz, classLoader).iterator();

        while (true) {
            try {
                T next = serviceLoaderIterator.next();
                loadResults.add(new LoadResult<>(next));
            } catch (NoSuchElementException e) {
                break;
            } catch (Throwable t) {
                // check whether the throwable is as expected
                if (!clazz.isInstance(t)) {
                    throw t;
                }
                loadResults.add(new LoadResult<>(t));
            }
        }

        return loadResults;
    }
{code};;;","02/Mar/23 08:43;mapohl;I guess, your proposal wouldn't work: It looks like {{LoadResult(Throwable)}} is used to mark failed load operations. {{clazz}} in contrast is used for actually loading a specifc class. The proposed if condition seems to mix up these two separate code paths.;;;","02/Mar/23 08:53;mapohl;{quote}
Yes, Gateway will try to cancel the execution of the running operation by explicitly interrupting the thread in Operation:377 and wait all task finish. However, the Gateway seems close the resource before all task finishes. 
{quote}
Do we understand why exactly the classloaders are closed before cancelling the operation. IIUC, this should happen the other way around.;;;","02/Mar/23 09:29;fsk119;After discussing with [~lsy] , I think we get why the behavior is not expected on the Gateway side. It's because of the wrong usage of the `FutureTask` in line [Operation:377|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/service/operation/OperationManager.java#L377]. {{{}FutureTask#cancel{}}} doesn't promise the cancel the execution of the running thread by force[1]. {{FutureTask}} only interrupts the worker thread and then invokes the {{FutureTask#done}}. The {{FutureTask#done}} notifies the Gateway that all tasks are finished and ready to release used resources in the failed test. I will open a fix about this to truly kill the thread. 

[1] https://stackoverflow.com/questions/11158454/future-task-of-executorservice-not-truly-cancelling;;;","02/Mar/23 09:44;fsk119;> I guess, your proposal wouldn't work: It looks like LoadResult(Throwable) is used to mark failed load operations. clazz in contrast is used for actually loading a specifc class. The proposed if condition seems to mix up these two separate code paths.

I think the {{loadResults}} only should put the Object is an instance of the specified type rather than any objects. It's a little weird we put something unrelated into the list. What's more, we check the type explicitly in this [line|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FactoryUtil.java#L528].  ;;;","02/Mar/23 10:30;mapohl;{quote}
[...] What's more, we check the type explicitly in this line.
{quote}
Good pointer: I guess, the error handling could be refactored. Right now it's separated into two places ({{FactoryUtil.discoverFactories}} handles the {{NoClassDefFoundError}} and translates any other exception that was handled in {{ServiceLoaderUtil.load}} into a {{TableException}}) which shouldn't be the case. We shouldn't make the service loading loop forever because of an unexpected error but immediately throw a {{TableException}} in that case.;;;","02/Mar/23 11:36;fsk119;+1 to refactor this.;;;","03/Mar/23 07:41;fsk119;Merged into release-1.17: 
594010624f8084efd99d6d405b5310ab24013aeb
86e12eb3fcec54d234154e59f8cb0557fc616494

Merged into master:
d2296422933c0e3b8895e3b7e0a0a95dacdd3257
f47b3704867b1d5aa754b1f7325f54e1830014cd

Merged into release-1.16:
2dcd6cc6af0109fd1a411320cab400b393d07fe9
93cd23c55436f3d9c38051e31cfd34f2fd9c4aff
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL interval related queries stop working via SQL client,FLINK-31091,13524862,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,15/Feb/23 14:37,22/Feb/23 15:23,04/Jun/24 20:41,22/Feb/23 14:48,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Client,,,,0,pull-request-available,,,,"I put blocker since it works in 1.16.x and stopped working in 1.17 after a certain commit

Any interval related query run via SQL Client is failing with 

{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.runtime.rest.util.RestClientException: [Internal server error. Could not map response to JSON.]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:536)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:516)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

{noformat}

example of query
{code:sql}
SELECT INTERVAL '2' DAY;
SELECT 1, INTERVAL '2' YEAR;
{code}

based on tests it stopped working after this commit 
https://issues.apache.org/jira/browse/FLINK-29945

More traces from logs
{noformat}
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: Unable to serialize logical type 'INTERVAL MONTH NOT NULL'. Please check the documentation for supported types. (through reference chain: java.util.Coll
ections$UnmodifiableRandomAccessList[1]->org.apache.flink.table.gateway.rest.serde.ColumnInfo[""logicalType""])
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:392) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:351) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:316) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:782) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:79) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:18) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider.defaultSerializeField(SerializerProvider.java:1166) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.ResultInfoSerializer.serialize(ResultInfoSerializer.java:82) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.ResultInfoSerializer.serialize(ResultInfoSerializer.java:47) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider.defaultSerializeField(SerializerProvider.java:1166) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.FetchResultsResponseBodySerializer.serialize(FetchResultsResponseBodySerializer.java:60) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.FetchResultsResponseBodySerializer.serialize(FetchResultsResponseBodySerializer.java:31) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:480) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:319) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4568) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3804) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:92) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.lambda$respondToRequest$1(AbstractSqlGatewayRestHandler.java:93) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_362]
        at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_362]
        at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_362]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:91) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]

        at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_362]
        at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:336) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:308) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]
Caused by: java.lang.UnsupportedOperationException: Unable to serialize logical type 'INTERVAL MONTH NOT NULL'. Please check the documentation for supported types.
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serializeInternal(LogicalTypeJsonSerializer.java:174) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serialize(LogicalTypeJsonSerializer.java:100) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serialize(LogicalTypeJsonSerializer.java:51) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:728) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        ... 67 more

{noformat}
//cc [~fsk119]",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 15:23:07 UTC 2023,,,,,,,,,,"0|z1fyio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 13:59;mapohl;[~fsk119] [~lic] [~qingyue] (I selected you based on a proposal from [~Leonard]) do you have any insights here?;;;","21/Feb/23 03:29;qingyue;Thanks, [~Sergey Nuyanzin] and [~mapohl] for reporting this.

The reason is that org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer does not cope with `INTERVAL_YEAR_MONTH` and `INTERVAL_DAY_TIME`. cc [~fsk119] ;;;","21/Feb/23 05:26;Sergey Nuyanzin;Hi [~qingyue] thanks for volunteering, however there is already existing fix for that in ""links to"" section.
At the same time you are welcome to participate in review process
 ;;;","22/Feb/23 02:31;fsk119;Mregd into master: 14adc1679fb3d025a2808af91f23f14e7c6f6e24;;;","22/Feb/23 14:46;Sergey Nuyanzin;Merged into release-1.17 branch: 8535c26198d0e3372465aa283055ce39f4577f86;;;","22/Feb/23 15:09;mapohl;Thanks for resolving this issue. FYI in terms of fixVersion: Right now, we're at a stage where a fix (if added to {{master}} and {{release-1.17}}) is actually targeting 1.17.0. If the fix ends up on {{master}} only, the fixVersion would be 1.18.0.

The fixVersion is relevant for the creating the release notes, i.e. (assuming that this Jira issue would have release notes) you have to ask yourself in which release notes the change is meant to be included: 1.17.0 vs 1.18.0.;;;","22/Feb/23 15:20;Sergey Nuyanzin;Thanks for explanation, I think I got your point.

To double check, am i right that in case there was an issue in a released version then there could be several fix versions: one in current and others in minors in case they are still maintaining?;;;","22/Feb/23 15:23;mapohl;Yes, if you have to create a backport for release-1.16, you would have to add 1.16.2 as an additional fixVersion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL fail to select INTERVAL ,FLINK-31090,13524842,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,15/Feb/23 12:31,24/Jan/24 04:01,04/Jun/24 20:41,24/Jan/24 04:01,,,,,,,,,,,,1.18.0,,,,Table SQL / Planner,Table SQL / Runtime,,,0,,,,,"Can be reproduce in CalcITCase  with the following code:

 
{code:java}
@Test def testSelectInterval(): Unit = { checkResult(""SELECT INTERVAL 2 DAY"", data3) }
{code}
 

 

It'll throw the exception:

org.apache.flink.table.planner.codegen.CodeGenException: Interval expression type expected.

    at org.apache.flink.table.planner.codegen.CodeGenUtils$.requireTimeInterval(CodeGenUtils.scala:419)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:549)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:490)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:57)
    at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:143)
    at org.apache.flink.table.planner.codegen.ExpressionReducer.$anonfun$reduce$2(ExpressionReducer.scala:81)
    at scala.collection.immutable.List.map(List.scala:282)
    at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:81)

 ",,,,,,,,,,,,,,,,,,FLINK-31279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 10:11:57 UTC 2024,,,,,,,,,,"0|z1fye8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 12:37;luoyuxia;in  ExprCodeGenerator#generateCallExpression,
{code:java}
case MULTIPLY if isTimeInterval(resultType) =>
  val left = operands.head
  val right = operands(1)
  requireTimeInterval(left)
  requireNumeric(right)
  generateBinaryArithmeticOperator(ctx, ""*"", resultType, left, right) {code}
will require the left is a interval type, but the plan for the above sql will be:
{code:java}
LogicalSink(table=[*anonymous_collect$1*], fields=[EXPR$0])
  LogicalProject(EXPR$0=[*(1, 86400000:INTERVAL DAY)])
    LogicalValues(tuples=[[{ 0 }]]) {code}
the left is not interval type, but the right is. 

To me, I don't think it should have the constraint that the left should be interval type and the right should be numberic.;;;","15/Feb/23 12:54;Sergey Nuyanzin;shouldn't it be quoted like that
{code:sql}
SELECT INTERVAL '2' DAY;
{code}
?
at least this syntax works in Calcite;;;","15/Feb/23 14:39;Sergey Nuyanzin;Accidentally at the same time i faced this https://issues.apache.org/jira/browse/FLINK-31091 ;;;","16/Feb/23 02:00;luoyuxia;[~Sergey Nuyanzin] Thanks. Yes, the quoted 'select interval '2' day is fine.

The reason is they share different plan, for 'SELECT INTERVAL 2 DAY':

 
{code:java}
LogicalSink(table=[*anonymous_collect$1*], fields=[EXPR$0])
  LogicalProject(EXPR$0=[*(1, 86400000:INTERVAL DAY)])
    LogicalValues(tuples=[[{ 0 }]]) {code}
 

 

for 'select interval '2' day, the plan is like:

 
{code:java}
LogicalSink(table=[*anonymous_collect$1*], fields=[EXPR$0])
  LogicalProject(EXPR$0=[86400000:INTERVAL DAY])
    LogicalValues(tuples=[[{ 0 }]]) {code}
.

The exception happens in code gen stage for the expression  ' *(1, 86400000:INTERVAL DAY)'. It requires the left operand is interval type.  But i think it should be fine/reasonable that the right operand is interval type and the left is a numeric type.

 

Although quoting is fine, I still think we also should support it with no quoting for it looks like just a bug.

 ;;;","16/Jan/24 10:11;ChunJi;this bug has been fixed in FLINK-31279;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pin L0 index in memory can lead to slow memory grow finally lead to memory beyond limit,FLINK-31089,13524841,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,15/Feb/23 12:23,27/Feb/23 10:11,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,"with the setPinL0FilterAndIndexBlocksInCache true, we can see the pinned memory kept growing(in the pc blow from 48G-> 50G in about 5 hours). But if we switch it to false, we can see the pinned memory stay realtive static. In our environment, a lot of tasks restart due to memory over limit killed by k8s

!image-2023-02-15-20-26-58-604.png|width=899,height=447!

 

!image-2023-02-15-20-32-17-993.png|width=853,height=464!

the two graphs are recorded in yesterday and today, which means the data stream number per second will not differ alot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 12:27;zhoujira86;image-2023-02-15-20-26-58-604.png;https://issues.apache.org/jira/secure/attachment/13055468/image-2023-02-15-20-26-58-604.png","15/Feb/23 12:32;zhoujira86;image-2023-02-15-20-32-17-993.png;https://issues.apache.org/jira/secure/attachment/13055469/image-2023-02-15-20-32-17-993.png","17/Feb/23 08:49;zhoujira86;image-2023-02-17-16-48-59-535.png;https://issues.apache.org/jira/secure/attachment/13055564/image-2023-02-17-16-48-59-535.png","20/Feb/23 03:43;zhoujira86;l0pin_open.png;https://issues.apache.org/jira/secure/attachment/13055617/l0pin_open.png",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:11:17 UTC 2023,,,,,,,,,,"0|z1fye0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 12:38;zhoujira86;[~yunta] master, please kindly review. I have also tested the performance.  

disable PinTopLevelIndexAndFilter can significantly decrease performance

disable PinL0FilterAndIndexBlocksInCache does not harm performance a lot.

 

[https://github.com/facebook/rocksdb/issues/4112#issuecomment-405859006] also mentioned, cache top level is enough. as rocksdb memory growing issue has a lot complain in rocksdb issues;;;","15/Feb/23 14:00;yunta;[~zhoujira86] Thanks for reporting this.
First of all, I think you have set {{state.backend.rocksdb.memory.partitioned-index-filters}} as true, right? Did you get similar results with this value as false?
Could you also use jemalloc to help heap profiling the native memory usage (FLINK-19125 , https://github.com/jemalloc/jemalloc/wiki/Use-Case%3A-Heap-Profiling )? I think that could be the tool to figure out this problem.;;;","16/Feb/23 02:35;zhoujira86;[~yunta] looks like we are already using the jemalloc

$ /usr/bin/pmap -x 1 | grep malloc
00007f434e9aa000     204     204       0 r-x-- libjemalloc.so.1
00007f434e9aa000       0       0       0 r-x-- libjemalloc.so.1
00007f434e9dd000    2044       0       0 ----- libjemalloc.so.1
00007f434e9dd000       0       0       0 ----- libjemalloc.so.1
00007f434ebdc000       8       8       8 r---- libjemalloc.so.1
00007f434ebdc000       0       0       0 r---- libjemalloc.so.1
00007f434ebde000       4       4       4 rw--- libjemalloc.so.1
00007f434ebde000       0       0       0 rw--- libjemalloc.so.1

 

and 'state.backend.rocksdb.memory.partitioned-index-filters' yes, we configured it as true. without the two_level_index_cache. the rocksdb performance is really low. 

 

And flink_taskmanager_job_task_operator_.*rocksdb_block_cache_pinned_usage can growing quickly if left PinL0FilterAndIndexBlocksInCache true;;;","16/Feb/23 03:12;yunta;[~zhoujira86] Maybe you did not get what I mean. I am sure that you use jemalloc as the default memory allocator, I hope you can use jemalloc to profile the memory usage and share the dump results to know what occupied the memory.;;;","16/Feb/23 03:28;zhoujira86;[~yunta] 

Master, Do you aware 

<jemalloc>: Invalid conf pair: prof:true
<jemalloc>: Invalid conf pair: lg_prof_interval:29
<jemalloc>: Invalid conf pair: lg_prof_sample:17
<jemalloc>: Invalid conf pair: prof_prefix:/opt/flink/jeprof.out

 

what this stand for

looks like i need to get a jemalloc with  --enable-prof

BTW, can you please share me your dingding;;;","17/Feb/23 08:50;zhoujira86;[~yunta] 

Master , I rebuilt a jemalloc from source and with the config below. the Invalid conf pair warning disappeared. But I can't find the prof files in the location I set. Can you please help suggest how to collect the evidence?

!image-2023-02-17-16-48-59-535.png|width=625,height=101!;;;","17/Feb/23 09:01;yunta;[~zhoujira86] You can set the prof_prefix as ""/tmp/jeprof"" to ensure 100% write permission.;;;","17/Feb/23 10:30;zhoujira86;[~yunta] thx

 

some background info 

jemalloc version: 

I updated the jemalloc version from  3.6.0-11 to 5.0.1

 

the first set of data I collected is setPinL0FilterAndIndexBlocksInCache false, and set the flink kafka offset to 2days ago. I saw 

Total: 2632053083 B
1693036063  64.3%  64.3% 1693036063  64.3% rocksdb::UncompressBlockContentsForCompressionType
684855869  26.0%  90.3% 684855869  26.0% os::malloc@90ca90
122444085   4.7%  95.0% 122444085   4.7% os::malloc@90cc30
50331648   1.9%  96.9% 50331648   1.9% init
41957115   1.6%  98.5% 41957115   1.6% rocksdb::Arena::AllocateNewBlock
15729360   0.6%  99.1% 18924496   0.7% rocksdb::LRUCacheShard::Insert
 8388928   0.3%  99.4% 1701424991  64.6% rocksdb::BlockBasedTable::ReadFilter
 4194432   0.2%  99.6%  4194432   0.2% std::string::_Rep::_S_create
 3704419   0.1%  99.7%  3704419   0.1% readCEN
 3195135   0.1%  99.8%  3195135   0.1% rocksdb::LRUHandleTable::Resize
 2098816   0.1%  99.9%  2098816   0.1% std::vector::vector
 1065045   0.0% 100.0%  1065045   0.0% updatewindow
 1052164   0.0% 100.0%  1052164   0.0% inflateInit2_
       0   0.0% 100.0% 87035806   3.3% 0x00007fa8f4f8b366
       0   0.0% 100.0%  1053704   0.0% 0x00007fa8f4f97f59
       0   0.0% 100.0%  1053704   0.0% 0x00007fa8f4f97f67

 

is the major part of memory consumer;;;","17/Feb/23 10:44;yunta;It seems only less than 1.6GB of memory is occupied by RocksDB, is this also larger than your configured managed memory?
BTW, could you share the visual profiling result with jeprof?;;;","17/Feb/23 10:49;zhoujira86;yes, this is not large when start up, but it keeps growing, so no matter how large the tm memory is, it will finally oom.

 

now I started up another task with setPinL0FilterAndIndexBlocksInCache true, which will have faster growing speed. I will collect another visual profile at weekend, will post it here.

 

 ;;;","20/Feb/23 03:45;zhoujira86;[~yunta] 

got some update with l0 pin open, see attache l0pin_open.

 

 

 ;;;","20/Feb/23 10:06;zhoujira86;[~yunta] 

 

I found in rocksdb log,

 

2023/02/20-17:55:33.357582 7f4092f42700        Options.compaction_filter: None
2023/02/20-17:55:33.357583 7f4092f42700        Options.compaction_filter_factory: None

 

could this lead to the index 'oom' issue?;;;","20/Feb/23 10:49;yunta;Thanks for sharing the profiling results, I will take a look recently.
BTW, compaction filter is used for TTL state clean, and I think you did not enable TTL for this job.;;;","20/Feb/23 12:35;zhoujira86;I create another task with ttl open, And will keep monitor the memory growth;;;","21/Feb/23 02:25;zhoujira86;[~yunta] Master, After turn on compaction filter, the pinned block cache size stop growing.

 

Sould we add some warning for situation if 'partitioned-index-filters' is on and no ttl configured?;;;","21/Feb/23 14:09;yunta;[~zhoujira86] Thanks for sharing the result, what will happen if no 'partitioned-index-filters' and no TTL is configured? Will the memory still keep growing?

BTW, I have sent to your emails to ask for the Dingtalk account.;;;","23/Feb/23 07:52;zhoujira86;[~yunta] if I turn off the PinTopLevelIndexAndFilter, the task can not run correctly as it takes a lot of time load cache. I also found some rank operator does not has compaction filter in LOG file;;;","23/Feb/23 08:28;zhoujira86;some more info:

1, the task with ttl on has been running for long without pinned block cache grow

2,we have many task running with 1.13, which means they are without the fix 

https://issues.apache.org/jira/browse/FLINK-22957

 

these task also with the partitioned-index-filters on. They also has oom occasionally;;;","24/Feb/23 06:47;Yanfei Lei;Let me try to summarize this issue:
 # Enable PinL0FilterAndIndexBlocksInCache or PinTopLevelIndexAndFilter, disable TTL, will result in OOM

 ## PinTopLevelIndexAndFilter can significantly affect the performance.
 ## PinL0FilterAndIndexBlocksInCache will NOT affect the performance.
 # Enable PinL0FilterAndIndexBlocksInCache or PinTopLevelIndexAndFilter, enable TTL, the memory wouldn't keep growing.  
 ## Due to https://issues.apache.org/jira/browse/FLINK-22957 , the TTL can't take effect for the Rank operator in Flink 1.13.

Is the TTL set by ""table.exec.state.ttl""? If the job is a DataStream job, maybe you can set TTL for the rank operator via StateTtlConfig.;;;","26/Feb/23 04:03;zhoujira86;[~Yanfei Lei] yes, your summary is pretty accurate. except pin l0 can improve the performance, but disable it will not influence too much. But this is not the main topic.

 

My job is a datastream job, my point is to prompt some warning as developer may forget to set the stateTtlConfig whereas they turn on the PinTopLevelIndexAndFilter. this can 100% lead to some oom issue. 

 

yet I have a new issue can also lead to oom

https://issues.apache.org/jira/browse/FLINK-31225;;;","27/Feb/23 10:11;yunta;[~zhoujira86] If a user forgets to set the state TTL config, I think he will also face disk usage problems and performance regression.
I think it would be good to add such documentation.

BTW, does FLINK-31225 has relationship with this one?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink free for flink-table-store-docs,FLINK-31088,13524825,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Feb/23 11:29,16/Feb/23 01:20,04/Jun/24 20:41,16/Feb/23 01:20,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 01:20:09 UTC 2023,,,,,,,,,,"0|z1fyag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 01:20;lzljs3620320;master: 61ddbde6ec113cf82bc8d34843bb25c1e80fd165;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce MergeIntoAction.,FLINK-31087,13524823,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,15/Feb/23 11:24,03/Mar/23 05:58,04/Jun/24 20:41,03/Mar/23 05:58,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,This action simulates the 'MERGE INTO' syntax.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 05:58:59 UTC 2023,,,,,,,,,,"0|z1fya0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 05:58;lzljs3620320;master: 7080cfae9c9cce9db54b75e607a3d43cefc19ca7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOC]update connector lable for blackhole and kafka,FLINK-31086,13524808,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,15/Feb/23 09:44,15/Feb/23 14:16,04/Jun/24 20:41,15/Feb/23 14:16,,,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,,"pdate connector label for kafka and blackhole.

Blackhole: sink:bounded unbounded

Kafka: source bounded

!https://user-images.githubusercontent.com/18002496/216600374-5c9d16db-66ac-42a4-8b21-16245a71f9ef.png|width=468,height=168!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 14:16:19 UTC 2023,,,,,,,,,,"0|z1fy6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 14:16;tison;master via e292a105eb9f7ddccfba2b0d53430dcdb06c0280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add schema option to confluent registry avro formats,FLINK-31085,13524807,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,15/Feb/23 09:35,16/Mar/23 08:22,04/Jun/24 20:41,15/Mar/23 10:42,,,,,,,,,,,,1.18.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"When using {{avro-confluent}} and {{debezium-avro-confluent}} formats with schemas already defined in the Confluent Schema Registry, serialization fails, because Flink uses a default name `record` when converting row types to avro schema. So if the predefined schema has a different name, the serialization schema will be incompatible with the registered schema due to name mismatch. Check [this|https://lists.apache.org/thread/5xppmnqjqwfzxqo4gvd3lzz8wzs566zp] thread about reproducing the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 08:22:54 UTC 2023,,,,,,,,,,"0|z1fy6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 10:42;mbalassi;e818c11 in master;;;","16/Mar/23 08:22;martijnvisser;Note: this change also includes a change in the Kafka Connector test, which (normally, haven't looked at any details) needs to be moved to flink-connector-kafka since we'll move the Kafka connector out of the Flink repo in 1.18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataGen sequence generator requires the definition of start/end values,FLINK-31084,13524806,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xzw0223,mapohl,mapohl,15/Feb/23 09:30,13/Aug/23 10:35,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Connectors / Common,,,,0,pull-request-available,stale-assigned,starter,,"The [DataGen connector's parameters|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/connectors/table/datagen/#connector-options] are not precisely documented: The start/end parameters are labeled as optional for the sequence generator even though they are (see [SequenceGeneratorVisitor:85ff|https://github.com/apache/flink/blob/ef9ce854a3169014001f39e0d5908c703453f2b8/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/connector/datagen/table/SequenceGeneratorVisitor.java#L85]).

But instead of updating the documentation, we should just come up with reasonable default values. As a user, I would expect the positive integer values to be returned starting from 0 if I don't specify anything here.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31192,,,,,,FLINK-31066,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 10:35:02 UTC 2023,,,,,,,,,,"0|z1fy68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:52;Weijie Guo;This document is really a bit confusing. `fields.#.start` is marked as optional, but it is required in sequence mode. I can't infer this requirement from the document at all. Perhaps the reason for this is that this param is not required in random mode.

IIUC, you mean that intStart and intEnd should use `0` and `Integer.MAX_VALUE` respectively as the default value?;;;","15/Feb/23 12:15;mapohl;yes, that's what I had in mind.;;;","16/Feb/23 07:24;xzw0223;I think this may be more in line with the optional statement, I think I can do this, can you allocate a ticket to me?;;;","17/Feb/23 13:01;xzw0223;[~mapohl]  Hi, I have encountered a problem, and I would like to ask your opinion. If kind is set as' sequence ', end is set as Integer.MAX_VALUE and parallelism is set as 1,https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/datagen/SequenceGenerator.java#L117 will have a problem here.

This is an existing problem. Do I need to raise an issue again and record this problem? Or can I solve it all together?

 ;;;","17/Feb/23 14:27;xzw0223;There is another problem, because we need to write the data generation into the deque in advance, the processing time at this stage is very time-consuming, and this problem will also exist when triggering ck and restoring the state, because their processing methods are similar, both need for loop.

See also https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/datagen/SequenceGenerator.java#L91

I run into this problem a lot when testing with dataGen.

 ;;;","17/Feb/23 14:31;xzw0223;My idea is that we don't have to pre-load all the data, we generate the data when we call the next method, then we record the value of the current generated data and save it as state, and when ck and restore we just need to recalculate the new value of the location we have sent to and send it downstream.

This avoids the time consuming process of preloading all the data.

[~mapohl] I would like to hear your views,looking forward for your reply, thank you.;;;","20/Feb/23 14:23;mapohl;{quote}
Matthias Pohl  Hi, I have encountered a problem, and I would like to ask your opinion. If kind is set as' sequence ', end is set as Integer.MAX_VALUE and parallelism is set as 1,https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/datagen/SequenceGenerator.java#L117 will have a problem here.
{quote}
It looks like {{saveDivide}} is misused here. That is a separate issue and should be handled in a separate Jira issue.;;;","20/Feb/23 14:28;mapohl;{quote}
My idea is that we don't have to pre-load all the data, we generate the data when we call the next method, then we record the value of the current generated data and save it as state, and when ck and restore we just need to recalculate the new value of the location we have sent to and send it downstream.

This avoids the time consuming process of preloading all the data.

Matthias Pohl I would like to hear your views,looking forward for your reply, thank you.
{quote}
That sounds reasonable. Feel free to open a PR and we can discuss the details there.;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python ProcessFunction with OutputTag cannot be reused,FLINK-31083,13524805,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,15/Feb/23 09:30,15/Feb/23 11:28,04/Jun/24 20:41,15/Feb/23 11:28,1.16.1,,,,,,,,,,,1.16.2,1.17.0,,,API / Python,,,,0,pull-request-available,,,,"{code:java}
output_tag = OutputTag(""side"", Types.STRING())

def udf(i):
    yield output_tag, i

ds1.map(udf).get_side_output(output_tag)
ds2.map(udf){code}
raises TypeError: cannot pickle '_thread.RLock' object",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 11:28:24 UTC 2023,,,,,,,,,,"0|z1fy60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 11:28;dianfu;Fixed in:
- master: 29b1be61fefd11bf1ef4c9bed5543c4c6f056f4e
- release-1.17: 712c9eeb7335c91fcecde70042881916b2045088
- release-1.16: 99582b5a5b4cd5355935450812ae3f75a26c865c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting maven property 'flink.resueForks' to false in table planner module ,FLINK-31082,13524793,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,15/Feb/23 08:36,01/Mar/23 12:26,04/Jun/24 20:41,01/Mar/23 01:29,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"This issue is created to alleviate the OOM problem mentioned in issue: https://issues.apache.org/jira/browse/FLINK-18356

Setting maven property 'flink.resueForks' to false in table planner module can only reduce the frequency of oom, but can't solve this problem. To completely solve this problem, we need to identify the specific reasons, but this is a time-consuming work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 12:26:06 UTC 2023,,,,,,,,,,"0|z1fy3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 13:36;martijnvisser;Fixed in:

master: 3b8951e91a961c150758043c3fc87de90c63fa2a
release-1.17: 12908421b7852d7716338a2edc89ac1ccd2e9e4c;;;","28/Feb/23 08:12;martijnvisser;Re-opened to create backport to release-1.16;;;","01/Mar/23 01:29;godfrey;Fixed in 1.16.2: 247a099cc358e0006aa0e387a55cf6d547814f98;;;","01/Mar/23 12:26;martijnvisser;[~godfrey] Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Jira links in How To Contribute guide,FLINK-31081,13524792,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ericbrzezenski,mapohl,mapohl,15/Feb/23 08:13,27/Mar/23 10:51,04/Jun/24 20:41,27/Mar/23 10:51,1.17.0,,,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,starter,,,"FLINK-30007 added a description in [the community docs|https://flink.apache.org/community.html#issue-tracker] on how to get access to the Flink Jira. But several other locations mentioned and link Jira as well (especially in the how to contribute sections). Newcomers might be miss the paragraph in community and wonder how they could get a working Jira account.

This issue is about replacing all the Jira links (where it's useful) by a reference to the issue track section in the community docs (https://flink.apache.org/community.html#issue-tracker). That way, they are redirected to the information on how they can gain access to the Flink jira board.

Locations that needs to be updated are (not exclusively!):
* https://flink.apache.org/contributing/contribute-code.html
* https://flink.apache.org/gettinghelp.html
*  https://flink.apache.org/contributing/how-to-contribute.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30007,,,,FLINK-31627,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 27 10:51:12 UTC 2023,,,,,,,,,,"0|z1fy34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 04:23;ericbrzezenski;Do you mind assigning this to me? ill update these locations and any others that I may find. ;;;","20/Mar/23 09:12;mapohl;Thanks for offering your help, [~ericbrzezenski];;;","27/Mar/23 10:51;mapohl;asf-site: 8478a252a1238da60465bcf22847b40c554bb5ff (rebuild: 3fd8d6b48d7b47eab772743edea026b8e64879ae);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idle slots are not released due to a mismatch in time between DeclarativeSlotPoolService and SlotSharingSlotAllocator,FLINK-31080,13524786,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,prabhujoseph,prabhujoseph,15/Feb/23 07:24,13/Aug/23 10:35,04/Jun/24 20:41,,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,"Due to a timing mismatch between {{DeclarativeSlotPoolService}} and {{{}SlotSharingSlotAllocator{}}}, idle slots are not released.

{{DeclarativeSlotPoolService}} uses {{{}SystemClock#relativeTimeMillis{}}}, i.e., {{{}System.nanoTime{}}}() / 1_000_000, while offering a slot, whereas {{SlotSharingSlotAllocator}} uses {{{}System.currentTimeMillis{}}}() while freeing the reserved slot. 

The idle timeout check fails wrongly as ""{{{}System.currentTimeMillis(){}}}"" will have a very high value compared to ""{{{}SystemClock#relativeTimeMillis{}}}"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 10:35:03 UTC 2023,,,,,,,,,,"0|z1fy1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:01;Weijie Guo;Ahh, this may indeed be a problem. Thanks [~prabhujoseph] for reporting this. ;;;","27/Apr/23 17:20;samrat007;[~Weijie Guo] we have worked internally to fix this bug. 
If you are fine , can i share the patch in flink-runtime ? 

cc: [~prabhujoseph] ;;;","28/Apr/23 02:55;huwh;IIUC, currently declarativeSlotPool is used by two components (SlotPoolService, Scheduler). Slots are offered (with timestamp) by SlotPoolService. Used, released in Scheduler (update timestamp). This requires all components using declarativeSlotPool to have aligned timestamp semantics, but we don't have a mechanism to ensure this.
So I think we need to put timestamp maintenance inside the declarativeSlotPool to maintain uniform timestamp semantics.

WDYT, [~Weijie Guo] [~prabhujoseph] 

 ;;;","28/Apr/23 03:55;samrat007;[~huwh] i have raised pr with changes . can you please review the changes if this is the changes you mean ?;;;","28/Apr/23 04:08;Weijie Guo;Thanks for pick-up this, I didn't fix this anytime soon because I found out that this code path (releasing the idle slot under the adaptive scheduler) should never have been triggered before, but after FLINK-31399 it should be, so let's fix it now.

> I think we need to put timestamp maintenance inside the declarativeSlotPool to maintain uniform timestamp semantics.

Yes, they should share the same clock Ideally, and we can also pass the clock to the {{Scheduler}}.

>If you are fine , can i share the patch in flink-runtime ? 

Sure, but I'm leaning toward unifying the clocks between {{SlotPoolService}} and {{Scheduler}}, like Weihua said. What do you think?;;;","28/Apr/23 06:25;samrat007;>  Sure, but I'm leaning toward unifying the clocks between {{SlotPoolService}} and {{{}Scheduler{}}}, like Weihua said. What do you think?

Yes ! completely agree with [~huwh] . Allow me sometime to come up with the patch changes unifying the clock between {{SlotPoolService}} and {{Scheduler. }}

 

On high level , I am thinking to introduce `ClockService` class that will be shared by both SlotPoolService and Scheduler. 
Please enlighten if you see any risk or component breaking  [~Weijie Guo]  [~huwh] ;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-29663 Further improvements of adaptive batch scheduler,FLINK-31079,13524782,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,miamiaoxyz,wanglijie,wanglijie,15/Feb/23 07:05,22/Feb/23 07:14,04/Jun/24 20:41,22/Feb/23 07:14,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,,,,,"This task aims to verify FLINK-29663 which improves the adaptive batch scheduler.

Before the change of FLINK-29663, adaptive batch scheduler will distribute subpartitoins according to the number of subpartitions, make different downstream subtasks consume roughly the same number of subpartitions. This will lead to imbalance loads of different downstream tasks when the subpartitions contain different amounts of data.

To solve this problem, in FLINK-29663, we let the adaptive batch scheduler distribute subpartitoins according to the amount of data, so that different downstream subtasks consume roughly the same amount of data. Note that currently it only takes effect for All-To-All edges.

The documentation of adaptive scheduler can be found [here|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/#adaptive-batch-scheduler]

One can verify it by creating intended data skew on All-To-All edges.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/23 06:00;lsy;image-2023-02-22-14-00-13-646.png;https://issues.apache.org/jira/secure/attachment/13055716/image-2023-02-22-14-00-13-646.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 07:14:01 UTC 2023,,,,,,,,,,"0|z1fy0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:11;337361684@qq.com;I hope to get this ticket, [~wanglijie] ;;;","15/Feb/23 08:18;wanglijie;Thanks [~337361684@qq.com] :D;;;","22/Feb/23 06:02;lsy;I have finished the verification work, it works well. Looks good to me. The verification step is as follows:

1. create a source table by datagen source in SqlClient, it produces 100 rows, 50  of which have the same key, creating a data skew case.
{code:java}
CREATE TABLE source (
    f0 INT,
    f1 BIGINT,
    f2 DOUBLE,
    f3 VARCHAR(30),
    f4 INT,
    f5 BIGINT,
    f6 FLOAT, 
    f7 DOUBLE,
    f8 DECIMAL(10, 5),
    f9 DECIMAL(38, 18),
    f10 DATE,
    f11 TIMESTAMP)
WITH (
    'connector' = 'datagen',
     'number-of-rows' = '100',
     'rows-per-second' = '100',
     'duplicated-field-num' = '1',
     'sample-rate' = '0.5') {code}
 

2. set option `execution.batch.adaptive.auto-parallelism.avg-data-volume-per-task: 2k` in flink-conf.yaml

 

3. run a agg query, the hash key is field `f0', 
{code:java}
create table sink with('connector' = 'print') as select f0, sum(f4), sum(f6) from source group by f0; {code}
4. Watch the Flink UI, we can see task 1 only consumes skewed subpartition which contains  50 rows, other subpartition is processed by other tasks, the 

!image-2023-02-22-14-00-13-646.png!;;;","22/Feb/23 06:07;lsy;But I have a confusion, does the option `[{{execution.batch.adaptive.auto-parallelism.avg-data-volume-per-task}}|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#execution-batch-adaptive-auto-parallelism-avg-data-volume-per-ta]` is job level or cluster level? I found it only works when I configure it in flink-conf.yaml;;;","22/Feb/23 07:14;wanglijie;Thanks [~lsy]. Currently, the [{{execution.batch.adaptive.auto-parallelism.avg-data-volume-per-task}}|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#execution-batch-adaptive-auto-parallelism-avg-data-volume-per-ta] is cluster level, but I personally think it makes sense to make it job level, especially when using session cluster, I will evaluate it in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-table-planner free for flink-table-store-core,FLINK-31078,13524776,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Feb/23 06:41,15/Feb/23 11:42,04/Jun/24 20:41,15/Feb/23 11:42,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 11:42:57 UTC 2023,,,,,,,,,,"0|z1fxzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 11:42;lzljs3620320;master: de4395fb192b21216f29460abea48cbbe72f5d47;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trigger checkpoint failed but it were shown as COMPLETED by rest API,FLINK-31077,13524769,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,15/Feb/23 06:09,27/Feb/23 07:09,04/Jun/24 20:41,27/Feb/23 07:09,1.15.3,1.16.1,1.17.0,,,,,,,,,1.16.2,1.17.0,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"Currently, we can trigger a checkpoint and poll the status of the checkpoint until it is finished by rest according to FLINK-27101. However, even if the checkpoint status returned by rest is completed, it does not mean that the checkpoint is really completed. If an exception occurs after marking the pendingCheckpoint completed([here|https://github.com/apache/flink/blob/bf0ad52cbcb052961c54c94c7013f5ac0110ef8a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1309]), the checkpoint is not written to the HA service and we can not failover from this checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 07:09:45 UTC 2023,,,,,,,,,,"0|z1fxy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:09;JunRuiLi;cc [~gaoyunhaii] ;;;","15/Feb/23 06:40;zhuzh;Thanks for reporting this issue! [~JunRuiLi]
-I think it is indeed a problem. Considering the case of stop-with-savepoint, it's possible that the final savepoint is lost if the savepoint is considered to be done and the job gets terminated, before it is recorded to HA.-
Do you want to fix it?

Correction: The problem does not affect savepoints which do not rely on CompletedCheckpointStore. So the actual problem will be that the query result of a [manually triggered checkpoint|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-checkpoints] is returned as ""COMPLETED"", while on the web UI it is ""FAILED"", which may confuse users. 
Therefore the problem is not that critical. I will lower its priority.;;;","15/Feb/23 06:43;JunRuiLi;[~zhuzh] Sure, I'll fix it.;;;","27/Feb/23 07:09;zhuzh;master:
eb17ec3f05d4bd512bc70ee79296d0b884894eaf

release-1.17:
dca819556fb9b675852df99ada45e0f22262cb28

release-1.16:
4c8159140028cd0654a93dcb7c25fe074ad1f059;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports filter predicate in Parquet format of table store,FLINK-31076,13524768,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,15/Feb/23 05:55,29/Mar/23 02:08,04/Jun/24 20:41,29/Mar/23 02:08,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Parquet format is the main file format of table store, which doesn't support filter predicate. Filter predicate should also support in Parquet format, not only the ORC format.",,,,,,,,,,FLINK-30723,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-15 05:55:18.0,,,,,,,,,,"0|z1fxxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade test units in table store to junit5,FLINK-31075,13524765,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Feb/23 05:37,22/Feb/23 01:38,04/Jun/24 20:41,22/Feb/23 01:38,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Current there are junit4 and junit5 test case in table store, we should upgrade all junit4 test cases to junit5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 01:38:16 UTC 2023,,,,,,,,,,"0|z1fxx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 01:38;lzljs3620320;master: 4e921c8e00a7ceb98680dc777c61b6a03da2f2da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce value filter for table store,FLINK-31074,13524764,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,15/Feb/23 05:34,29/Mar/23 02:02,04/Jun/24 20:41,29/Mar/23 02:02,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,,,,,"Currently table store supports filter key/partition to get files from partition. Besides key stats, table store also has value stats which can be used for filter too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 06:12:41 UTC 2023,,,,,,,,,,"0|z1fxww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:12;lzljs3620320;Currently we can only perform filter push down on keys
consider this case:
- data file 1: insert key = a, value = 1
- data file 2: update key = a, value = 2
- filter: value = 1
if we perform filter push down on values, data file 1 will be chosen, but data file 2 will be ignored, and the final result will be key = a, value = 1 while the correct result is an empty set.

If we want a value filter, we need to think a solution for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyflink testing library can't be used out of the box,FLINK-31073,13524763,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,engnatha,engnatha,15/Feb/23 05:29,15/Feb/23 05:29,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"The pyflink distribution comes with a `pyflink.testing.test_case_utils.py` that makes it appear like unit testing pyflink tooling is supported. It actually takes some non-trivial effort to figure out which packages are needed in order to run a simple no-op test case that makes it through the class setup in that module.

The user has to add the following jar files to their system in order to get through the set up steps.
{code:bash}
flink-runtime-1.16.1-tests.jar
flink-test-utils-1.16.1.jar
hamcrest-core-1.3.jar
junit-4.13.2.jar
{code}
The first is needed because the gathering of `MiniClusterResourceConfiguration` fails to be retrieved. The second is needed because it provides `MiniClusterWithClientResource`. The junit jars are needed because they are a dependency of `MiniClusterWithClientResource` and the user is met with a `ClassNotFoundError` for `org.junit.rules.ExternalResource` when trying to set up the mini cluster resource.

Further, these jars have to be put in a place where `pyflink_gateway_server.py:construct_test_classpath` is set up to look. It has some patterns that are expected under the source root of the installation. For pyflink, this is typically inside a virtual environment folder that a user should not be modifying. The only alternative to not putting the files inside the virtual environment directories is to override that function with a custom function that looks for jar files to add somewhere else.

The documentation available has no mention of python unit testing examples. Most of the motivation for this fix came from https://github.com/dianfu/pyflink-faq/tree/main/testing .","Ubuntu 20.04
Python 3.8.10
Pyflink 1.16.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-15 05:29:57.0,,,,,,,,,,"0|z1fxwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce streaming-read-atomic to ensure UB and UA cannot be split,FLINK-31072,13524757,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Feb/23 03:47,16/Feb/23 01:21,04/Jun/24 20:41,16/Feb/23 01:21,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently, streaming source will be checkpoint in any time, this means UPDATE_BEFORE and UPDATE_AFTER can be split into two checkpoint.
Downstream can see intermediate state. This is weird in some cases.
So in this ticket, add streaming-read-atomic:
The option to enable return per iterator instead of per record in streaming read. This can ensure that there will be no checkpoint segmentation in iterator consumption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 01:21:30 UTC 2023,,,,,,,,,,"0|z1fxvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 01:21;lzljs3620320;master: 13270ee11d28d9a1b246f0f8725293220536c6c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-283 Use adaptive batch scheduler as default scheduler for batch jobs,FLINK-31071,13524753,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,xiasun,wanglijie,wanglijie,15/Feb/23 03:06,21/Feb/23 02:23,04/Jun/24 20:41,21/Feb/23 02:23,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,release-testing,,,,"This task aims to verify FLIP-283 Use adaptive batch scheduler as default scheduler for batch jobs.
The documentation of adaptive batch scheduler can be found [here|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/#adaptive-batch-scheduler] .

Things to verify:


1. Verify the adaptive batch scheduler is the default scheduler of batch jobs. By default, Flink will automatically decide parallelism for operators of batch jobs if the scheduler is not specified.

2. Verify the configuration options({{{}execution.batch.adaptive.auto-parallelism.xxx{}}}) take effect. Besides, it is also necessary to verify that the default parallelism set via {{parallelism.default}} or {{StreamExecutionEnvironment#setParallelism()}} will be used as upper bound of allowed parallelism if the execution.batch.adaptive.auto-parallelism.max-parallelism is not configured.

3. Verify the final data produced are correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 02:22:45 UTC 2023,,,,,,,,,,"0|z1fxuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:00;xiasun;hi [~wanglijie] ,  I'd like to do this testing work. Could you assign it to me?;;;","15/Feb/23 08:08;wanglijie;Thanks [~xiasun]! I have assigned to you.;;;","20/Feb/23 08:17;xiasun;Hi  [~wanglijie] ,
    I have done the testing work according to the description and all test results are as expected.
The verification steps are as follows:
 # I create a simple batch job with a source and a sink, and rebalance them in two different slotSharingGroups. The sink parallelism is automatically decided when the config option `jobmanager.scheduler` is not set.
 # All of configuration options with `execution.batch.adaptive.auto-parallelism` prefix are verified to take effect. And the sink parallelism is adjusted automatically by changing the source data volume or the config option `execution.batch.adaptive.auto-parallelism.avg-data-volume-per-task`, and also make it reach the upper or lower bound configured.
    The default parallelism set via parallelism.default and StreamExecutionEnvironment#setParallelism() are also verified when the `execution.batch.adaptive.auto-parallelism.max-parallelism` is not configured. And I also found that if user configure the two configs together, the method of StreamExecutionEnvironment#setParallelism() have higher priority. That doesn't seem to be a problem.
 # The data correctness has been verified in miniCluster mode, and all data produced matches expectations.;;;","20/Feb/23 10:18;wanglijie;Thanks [~xiasun] !

-> And I also found that if user configure the two configs together, the method of StreamExecutionEnvironment#setParallelism() have higher priority


I think this is by design. Maybe [~JunRuiLi] can double confirm.;;;","20/Feb/23 11:30;JunRuiLi;Thanks [~xiasun] and [~wanglijie] .

This situation is by design, there are four ways for setting parallelism ([here|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/parallel/]). Except for the operator level, the others are to set the global default parallelism, and the priority is that system < cli < environment < operator.;;;","21/Feb/23 01:59;xiasun;Thanks [~wanglijie] and [~JunRuiLi] , it looks good to me.;;;","21/Feb/23 02:22;wanglijie;Thanks [~xiasun] [~JunRuiLi] :). I think we can close this issue now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update jline to 3.22.0,FLINK-31070,13524730,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,14/Feb/23 22:08,04/Sep/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Table SQL / Client,,,,0,auto-deprioritized-major,pull-request-available,,,"Among changes https://github.com/jline/jline3/commit/1315fc0bde9325baff8bc4035dbf29184b0b79f7 which could simplify parse of comments in cli
and infinite loop fix
https://github.com/jline/jline3/commit/4dac9b0ce78a0ac37f580e708267d95553a999eb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 22:35:13 UTC 2023,,,,,,,,,,"0|z1fxpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyflink 1.16.1 has unclosed resources at the end of unit tests,FLINK-31069,13524714,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,engnatha,engnatha,14/Feb/23 20:13,15/Feb/23 05:31,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,"A simple pyflink unit test has unclosed resources at the end of the testing. A minimally reproducable example of this can be seen with the following in 1.16.1

{code:python}
from pyflink.testing import test_case_utils


class InputBroadcastProcessFunctionTests(
        test_case_utils.PyFlinkStreamingTestCase):
    def test_nothing(self):
        pass
{code}

When `pytest.ini`is instructed to have `filterwarnings = errors`, the user is met with errors like the following

{code:bash}
$ pytest example_test.py 
================================================================================================================================================ test session starts ================================================================================================================================================
platform linux -- Python 3.8.10, pytest-7.2.1, pluggy-1.0.0
rootdir: /home/my_repo, configfile: pytest.ini
plugins: forked-1.6.0, anyio-3.6.2, timeout-2.1.0, typeguard-2.13.3, xdist-2.5.0, rabbitmq-2.2.1, cov-4.0.0
timeout: 60.0s
timeout method: signal
timeout func_only: False
collected 1 item                                                                                                                                                                                                                                                                                                    

example_test.py E                                                                                                                                                                                                                             [100%]

====================================================================================================================================================== ERRORS =======================================================================================================================================================
_________________________________________________________________________________________________________________________ ERROR at setup of InputBroadcastProcessFunctionTests.test_nothing _________________________________________________________________________________________________________________________

cls = <class '_pytest.runner.CallInfo'>, func = <function call_runtest_hook.<locals>.<lambda> at 0x7f42530f0af0>, when = 'setup', reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: ""Callable[[], TResult]"",
        when: ""Literal['collect', 'setup', 'call', 'teardown']"",
        reraise: Optional[
            Union[Type[BaseException], Tuple[Type[BaseException], ...]]
        ] = None,
    ) -> ""CallInfo[TResult]"":
        """"""Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """"""
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: Optional[TResult] = func()

dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/_pytest/runner.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/_pytest/runner.py:260: in <lambda>
    lambda: ihook(item=item, **kwds), when=when, reraise=reraise
dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/pluggy/_hooks.py:265: in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/pluggy/_manager.py:80: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/_pytest/unraisableexception.py:83: in pytest_runtest_setup
    yield from unraisable_exception_runtest_hook()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def unraisable_exception_runtest_hook() -> Generator[None, None, None]:
        with catch_unraisable_exception() as cm:
            yield
            if cm.unraisable:
                if cm.unraisable.err_msg is not None:
                    err_msg = cm.unraisable.err_msg
                else:
                    err_msg = ""Exception ignored in""
                msg = f""{err_msg}: {cm.unraisable.object!r}\n\n""
                msg += """".join(
                    traceback.format_exception(
                        cm.unraisable.exc_type,
                        cm.unraisable.exc_value,
                        cm.unraisable.exc_traceback,
                    )
                )
>               warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))
E               pytest.PytestUnraisableExceptionWarning: Exception ignored in: <function Popen.__del__ at 0x7f427ce715e0>
E               
E               Traceback (most recent call last):
E                 File ""/usr/lib/python3.8/subprocess.py"", line 946, in __del__
E                   _warn(""subprocess %s is still running"" % self.pid,
E               ResourceWarning: subprocess 1906862 is still running

dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/_pytest/unraisableexception.py:78: PytestUnraisableExceptionWarning
----------------------------------------------------------------------------------------------------------------------------------------------- Captured stderr setup -----------------------------------------------------------------------------------------------------------------------------------------------
13:11:22.743 I test_case_utils.py@91: Using /home/my_repo/dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/pyflink as FLINK_HOME...
------------------------------------------------------------------------------------------------------------------------------------------------ Captured log setup -------------------------------------------------------------------------------------------------------------------------------------------------
INFO     root:test_case_utils.py:91 Using /home/my_repo/dist/export/python/virtualenvs/python-default/3.8.10/lib/python3.8/site-packages/pyflink as FLINK_HOME...
============================================================================================================================================== short test summary info ==============================================================================================================================================
ERROR example_test.py::InputBroadcastProcessFunctionTests::test_nothing - pytest.PytestUnraisableExceptionWarning: Exception ignored in: <function Popen.__del__ at 0x7f427ce715e0>
================================================================================================================================================= 1 error in 1.22s ==================================================================================================================================================
{code}","Ubuntu 20.04
Python 3.8.10
Pyflink 1.16.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 05:31:14 UTC 2023,,,,,,,,,,"0|z1fxm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 05:31;engnatha;Once you get around/ignore this, a new class of issues has to be solved to get the base tooling to work. I've opened FLINK-31073 to track my findings there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document how to use Opensearch connector with OpenSearch 1.x / 2.x / 3.x (upcoming) clusters,FLINK-31068,13524704,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,reta,reta,reta,14/Feb/23 18:16,23/Feb/23 15:53,04/Jun/24 20:41,23/Feb/23 15:53,opensearch-1.0.0,,,,,,,,,,,opensearch-1.0.1,,,,Connectors / Opensearch,,,,0,pull-request-available,,,,"By default, Opensearch connector uses the OpenSearch client from 1.3.x release line (due to Apache Flink's JDK-8 baseline requirement). However, Flink Opensearch Connector is fully compatible with 1.x / 2.x / 3.x (upcoming) release lines. The documentation does not describe the way how to switch off the Opensearch client libraries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 15:53:17 UTC 2023,,,,,,,,,,"0|z1fxjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 15:53;dannycranmer;Merged commit [{{b272fc8}}|https://github.com/apache/flink-connector-opensearch/commit/b272fc8c127da2c3c5ed296f8d43f5314ea3ba1b] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add pull request template for flink-connector-pulsar,FLINK-31067,13524680,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,14/Feb/23 14:52,15/Feb/23 06:20,04/Jun/24 20:41,15/Feb/23 06:20,pulsar-4.0.0,,,,,,,,,,,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 06:20:04 UTC 2023,,,,,,,,,,"0|z1fxeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:20;Weijie Guo;main(4.0) via a39770d36153d7cee31dee4b69e8a33aa35e1639.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-29932 Upgrade Calcite to 1.29.0,FLINK-31066,13524676,13523226,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,14/Feb/23 13:57,27/Feb/23 11:04,04/Jun/24 20:41,27/Feb/23 09:59,,,,,,,,,,,,,,,,Tests,,,,0,release-testing,,,,"In fact this is a task to check all 3 Calcite upgrade related issues (1.27.0, 1.28.0 and 1.29.0)

Since there were added optimization for Sarg in Calcite 1.27.0 it would make sense to check that different queries with Sarg operator are working ok.
Also would make sense to check that SQL jobs with Sarg related queries could be restored from previous Flink version.

An example of SQL 
{code:sql}
SELECT a FROM MyTable WHERE a = 1 or a = 2 or a IS NOT NULL;{code}
{code:sql}
SELECT a FROM MyTable WHERE a = 1 or a = 2 or a IS NULL;

{code}
where MyTable is for instance
{code:sql}
CREATE TABLE MyTable (
   a bigint,
   b int not null,
   c varchar,
   d timestamp(3)
) with (...) 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29932,FLINK-21239,FLINK-20873,,,,,FLINK-31084,FLINK-31093,FLINK-31143,,"24/Feb/23 14:16;mapohl;SavepointReleaseTesting.java;https://issues.apache.org/jira/secure/attachment/13055813/SavepointReleaseTesting.java",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 09:58:47 UTC 2023,,,,,,,,,,"0|z1fxdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 15:24;mapohl;TODO: The configuration page of the JobManager in the UI doesn't seem to work;;;","24/Feb/23 14:14;mapohl;I attached the job to this Jira issue that I could use to generate different execution plans for 1.16.1 and 1.17-SNAPSHOT.
1.16.1:
{code}
 [7]:Join(joinType=[InnerJoin], where=[(SEARCH(a, Sarg[100]) OR ((c = 200) AND (b = 400)))], select=[a, c, b], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
+- StreamingFileWriter
   +- Sink: end
{code}
1.17-SNAPSHOT:
{code}
 [7]:Join(joinType=[InnerJoin], where=[((a = 100) OR ((c = 200) AND (b = 400)))], select=[a, c, b], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
+- StreamingFileWriter
   +- Sink: end
{code}

Restoring the job from a savepoint that was generated with 1.16.1 resulted in the following error:
{code}
./flink-1.17-21158c06-SNAPSHOT/bin/flink run -s /home/mapohl/research/FLINK-31066/run-1.16.1/savepoints/savepoint-ce617a-f674354c199c flink-examples-table_2.12-1.17-SNAPSHOT-SavepointReleaseTesting.jar $(pwd)/run-1.17-SNAPSHOT                                                                                                                        

------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Failed to execute sql
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
        at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: org.apache.flink.table.api.TableException: Failed to execute sql
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:938)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:883)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:989)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:765)
        at org.apache.flink.table.examples.java.basics.SavepointReleaseTesting.main(SavepointReleaseTesting.java:116)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        ... 9 more
Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'insert-into_default_catalog.default_database.sink_table'.
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2212)
        at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:189)
        at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:921)
        ... 18 more
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:75)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
        at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint Checkpoint Metadata. Max parallelism mismatch between checkpoint/savepoint state and new program. Cannot map operator 8b481b930a189b6b1762a9d95a61ada1 with max parallelism 128 to new program with max parallelism 1. This indicates that the program has been changed in a non-compatible way after the checkpoint/savepoint.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
        ... 3 more
Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint Checkpoint Metadata. Max parallelism mismatch between checkpoint/savepoint state and new program. Cannot map operator 8b481b930a189b6b1762a9d95a61ada1 with max parallelism 128 to new program with max parallelism 1. This indicates that the program has been changed in a non-compatible way after the checkpoint/savepoint.
        at org.apache.flink.runtime.checkpoint.Checkpoints.loadAndValidateCheckpoint(Checkpoints.java:190)
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1842)
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:223)
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:198)
        at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365)
        at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:210)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:136)
        at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
        at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:348)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
        ... 3 more
{code};;;","24/Feb/23 14:19;mapohl;I ran the sample code (once compiled with Flink release-1.16 and once compiled with release-1.17) on a Flink cluster based on the binaries of 1.16.1 and 1.17-SNAPSHOT (commit 21158c06)

The only config changed I added was the following one:
{code}
state.backend: hashmap
state.backend.type: filesystem
{code};;;","24/Feb/23 17:05;mapohl;I should have remembered that the backwards compatibility between minor Flink version with changed execution plan is not given [[1]|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/overview/#stateful-upgrades-and-evolution] [[2]|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/#table-api--sql]. 🤦‍♂️ So, I guess, there's no point of checking this. [~Sergey Nuyanzin] do you consider it good enough to prove that the calcite update could be visualized through the Flink UI?;;;","27/Feb/23 09:58;Sergey Nuyanzin;[~mapohl] thank you very much for your efforts here.

Yes I think you are right.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support more split assigner strategy for batch job,FLINK-31065,13524673,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,14/Feb/23 13:35,19/Jan/24 10:45,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"Currently flink use LocatableInputSplitAssigner as the default split assigner. 

Which splits the task will consume are dynamic in the runtime.

It is not a good strategy in the batch mode.

For example, we have 100 splits and the job has 100 tasks. 

When the job start, we don't have enough resource to start the 100 tasks, only 10 tasks started in the first time.( it is a common case in batch mode)

These 10 tasks will consume all splits, and other 90 tasks will reach finish state.

This is obviously not a good idea in batch mode. 

In extreme cases, 99 tasks finished, only one task running, and if this running task failed,

it will take much time to rerun this task( Because it need to consume it's 10 split again).

Spark will bind splits to it's task, I think it is a better way in the batch mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 04:08:00 UTC 2023,,,,,,,,,,"0|z1fxcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 11:36;luoyuxia;[~yunfanfighting@foxmail.com] Thanks for raising it up.  I also noticed it before and then tried a static split assign in our internal  TPC-DS benmark. Unfortunately, I haven't found any performance improvement, so I give up. But I do think it's a good point considering failover. 

Apart from introducing static split assign strategy, will you plan also to make some sources  like Hive use it? ;;;","16/Feb/23 04:08;yunfanfighting@foxmail.com;[~luoyuxia] Yes, hive is the main usage of this strategy. And I don't find any other source that need this strategy.

The main purpose is reduce the cost of failover. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error while retrieving the leader gateway,FLINK-31064,13524652,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tshofer,tshofer,14/Feb/23 10:54,31/Aug/23 06:17,04/Jun/24 20:41,,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"JobManager enters corrupt state after restart (e.g. increasing restartNonce).
{code:java}
WARN  | flink-akka.actor.default-dispatcher-5 | RpcGatewayRetriever              | Error while retrieving the leader gateway. Retrying to connect to akka.tcp://flink@my-session.flink:6123/user/rpc/resourcemanager_*.
org.apache.flink.util.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.
    at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:293)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(Unknown Source)
    at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1275)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
    ...
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.exceptions.RpcConnectionException: Could not connect to rpc endpoint under address akka.tcp://flink@my-session.flink:6123/user/rpc/resourcemanager_*.
    at org.apache.flink.runtime.rpc.akka.AkkaRpcService.lambda$resolveActorAddress$11(AkkaRpcService.java:604)
    at scala.concurrent.java8.FuturesConvertersImpl$CF$$anon$1.accept(FutureConvertersImpl.scala:59)
    ... 5 more
Caused by: org.apache.flink.runtime.rpc.exceptions.RpcConnectionException: Could not connect to rpc endpoint under address akka.tcp://flink@my-session.flink:6123/user/rpc/resourcemanager_*.
    ... 7 more
Caused by: akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka://flink/), Path(/user/rpc/resourcemanager_*)]
    at akka.actor.ActorSelection.$anonfun$resolveOne$1(ActorSelection.scala:74)
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
    at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
    at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:81)
    at akka.dispatch.internal.SameThreadExecutionContext$$anon$1.unbatchedExecute(SameThreadExecutionContext.scala:21)
    at akka.dispatch.BatchingExecutor.execute(BatchingExecutor.scala:130)
    at akka.dispatch.BatchingExecutor.execute$(BatchingExecutor.scala:124)
    at akka.dispatch.internal.SameThreadExecutionContext$$anon$1.execute(SameThreadExecutionContext.scala:20)
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:312)
    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:303)
    at akka.actor.ActorSelection.resolveOne(ActorSelection.scala:72)
    at akka.actor.ActorSelection.resolveOne(ActorSelection.scala:89)
    at akka.actor.ActorSelection.resolveOne(ActorSelection.scala:130)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcService.resolveActorAddress(AkkaRpcService.java:598)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcService.connectInternal(AkkaRpcService.java:549)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcService.connect(AkkaRpcService.java:232)
    at org.apache.flink.runtime.webmonitor.retriever.impl.RpcGatewayRetriever.lambda$null$0(RpcGatewayRetriever.java:66)
    at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.thenCompose(Unknown Source)
    at org.apache.flink.runtime.webmonitor.retriever.impl.RpcGatewayRetriever.lambda$createGateway$1(RpcGatewayRetriever.java:64)
    at org.apache.flink.util.concurrent.FutureUtils.retryOperationWithDelay(FutureUtils.java:259)
    at org.apache.flink.util.concurrent.FutureUtils.lambda$null$4(FutureUtils.java:279)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
{code}
I run Flink v1.16 on GKE v1.24. The hostname ‘my-session.flink’ can get resolved successfully in k8s."," 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:54;tshofer;jobmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13055428/jobmanager_log.txt","15/Feb/23 17:54;tshofer;my-job.yaml;https://issues.apache.org/jira/secure/attachment/13055482/my-job.yaml","15/Feb/23 17:42;tshofer;my-session.yaml;https://issues.apache.org/jira/secure/attachment/13055481/my-session.yaml",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 06:17:56 UTC 2023,,,,,,,,,,"0|z1fx88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:56;tshofer;I first tried to get help on the user mailing list. Without success so far.

https://lists.apache.org/thread/bb4vnw8c7l4725tlokz9xp2bjjdkgmo1.;;;","14/Feb/23 10:59;tshofer;I found the following entry in the meantime: [https://www.mail-archive.com/user@flink.apache.org/msg37424.html|https://www.mail-archive.com/user@flink.apache.org/msg37424.html.]

Suggesting to do
{code:java}
strategy:
  type: RollingUpdate
  rollingUpdate: maxSurge: 0
  maxUnavailable: 1 {code}
But operator CRD do not allow me to change the update strategy (or at least I don't know how).;;;","15/Feb/23 16:46;gyfora;Can you please include the CR Yaml that you are submitting?;;;","15/Feb/23 17:54;tshofer;Hi [~gyfora], in the meantime I'm not sure anymore that the exception in the description is really representing the problem, because I was aware of the very same exception in the logs of a properly running session.

The only other log entry that my be a hint can be found in the operator log.

{color:#174ea6}ERROR | ReconcilerExecutor-flinksessionjobcontroller-31 | org.apache.flink.kubernetes.operator.observer.sessionjob.FlinkSessionJobObserver - Missing Session Job{color}

But next output says: {color:#174ea6}nothing to do...{color} even so no job is running.

Find attached my session and job config.;;;","31/Aug/23 06:17;gyfora;[~tshofer] is still still an issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent duplicate reading when restoring from a checkpoint.,FLINK-31063,13524649,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,14/Feb/23 10:48,20/Feb/23 10:22,04/Jun/24 20:41,20/Feb/23 10:22,mongodb-1.0.0,,,,,,,,,,,mongodb-1.0.0,,,,Connectors / MongoDB,,,,0,pull-request-available,,,,"Exact-once semantics may not be guaranteed at present on partial reads.
We use a number fetchSize to limit the records count for every fetch loop but we didn't record the offset into the split state. When resuming the split reader from a partially completed split, we may re-read some data.

We should record the current reading offset into split state.
Skip this offset when restoring to prevent duplicate reading.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 10:22:49 UTC 2023,,,,,,,,,,"0|z1fx7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:49;jiabao.sun;Hi [~chesnay], please assign this ticket to me.;;;","20/Feb/23 10:22;chesnay;main: f67176de5c46ae11e8c791cbd986dab5826646b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-30365 New dynamic partition pruning strategy to support more dpp patterns,FLINK-31062,13524626,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,337361684@qq.com,337361684@qq.com,14/Feb/23 08:45,21/Feb/23 06:55,04/Jun/24 20:41,21/Feb/23 06:55,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,,,,,"This issue aims to verify  the FLINK-31062: New dynamic partition pruning strategy to support more dpp patterns . This is an extension work of FLIP-248 : [https://cwiki.apache.org/confluence/display/FLINK/FLIP-248%3A+Introduce+dynamic+partition+pruning] to enable more query patterns support dynamic partition pruning, like subQuery, Union etc.

This work can be verified in SQL client after we build the flink-disk package.
 # create a partition table and a non-partition table (only hive connector is supported now), and then insert some data.
 # you can verify this work by checking the query plan by executing 'explain'.  Most basic dpp patterns have been verified in Flink-1.16 [1]. So we only need to focus on verifying the newly supported patterns:

 *  Test the case that the dim table and the fact table are not adjacent in the join order: disable join reorder by setting config: 'table.optimizer.join-reorder-enabled = false',  and write a query like the test 'DynamicPartitionPruningProgramTest.testDppWithoutJoinReorder()'. Dpp will success even if the fact table and dim table are not adjacent.
 * Test the case that the dim side have one dim table but the dim side is an subQuery:  we can write the query like the test 'DynamicPartitionPruningProgramTest.testDppWithSubQuery()'. Dpp will success even if the dim side is a sub query (Notes: the dim side only can contain one same table with filter conddiition).
 * Test the case that  there are 'Union' in fact side: we can write the query like the test 'DynamicPartitionPruningProgramTest.testDppWithUnionInFactSide()'. Dpp will success.
 * Test the case that there are 'Agg' in fact side: we can write query as the tests in 'DynamicPartitionPruningProgramTest'. For this case, in order to ensure the semantic accuracy, we must require that the partition key cannot be in the group key of 'agg' operator. This case will not success.

      3. execute the above plan and verify the execution result. (the execution result should be same with the execution plan which disable dynamic filtering via setting 'table.optimizer.dynamic-filtering.enabled = false'

 

[1] [https://issues.apache.org/jira/browse/FLINK-28940|http://example.com/]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/23 06:34;Weijie Guo;dpp-test-result;https://issues.apache.org/jira/secure/attachment/13055621/dpp-test-result",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 06:34:22 UTC 2023,,,,,,,,,,"0|z1fx2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 06:34;Weijie Guo;Thanks [~337361684@qq.com] for your patient guidance. I have tested all cases in the descriptor and all results are in line with expectations. I have uploaded the test SQL and the results of plans, we can see that `DPP` has worked.

BTW, I also find that for the case of `Agg` in fact side, we must disable `SortAgg` operator. Otherwise, dpp optimization will not take effect. This is the reason why I set `table.exec.disabled-operators = SortAgg` for the last test case.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-30376 Introduce a new flink bushy join reorder rule which based on greedy algorithm,FLINK-31061,13524625,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,337361684@qq.com,337361684@qq.com,14/Feb/23 08:42,22/Feb/23 06:53,04/Jun/24 20:41,22/Feb/23 06:53,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,,,,,"This issue aims to verify FLINK-30376: [Introduce a new flink bushy join reorder rule which based on greedy algorithm|https://issues.apache.org/jira/browse/FLINK-30376].

 In Flink-1.17, bushy join reorder strategy is the default join reorder strategy, and this strategy can be disable by setting factor '

table.optimizer.bushy-join-reorder-threshold' smaller that the table number need to be reordered. If disabled, the Lopt join reorder strategy, which is default join reorder strategy in Flink-1.16, will be choosen. 

We can verify it in SQL client after we build the flink-dist package.
 # Firstly, we need to create several tables (The best case is that these tables have table and column statistics).
 # Secondly, we need to set 'table.optimizer.join-reorder-enabled = true' to open join reorder.
 # Verify bushy join reorder (The default bushy join reorder threshold is 12, so if the number of table smaller than 12, the join reorder strategy is bushy join reorder).
 # Compare the results of bushy join reorder and Lopt join reorder strategy. Need to be same.
 # If we want to create a bushy join tree after join reorder, we need to add statistics. Like：'JoinReorderITCaseBase.testBushyTreeJoinReorder'. 

If you meet any problems, it's welcome to ping me directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 06:53:25 UTC 2023,,,,,,,,,,"0|z1fx28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:05;JunRuiLi;Thanks [~337361684@qq.com] for creating this issue, I'd like to do this testing work.;;;","20/Feb/23 04:26;JunRuiLi;I have tested it and it looks good to me.
I used sql client to do the test by connecting it to Hive and set factor 'table.optimizer.bushy-join-reorder-threshold' = 1 to disable bushy join reorder and set 'table.optimizer.bushy-join-reorder-threshold' = 1000 to enable bushy join reorder. By running testing jobs, I can see that bushy join reorder is taking effect: the join operator plan is different and the job result is also as expected.;;;","22/Feb/23 06:53;337361684@qq.com;Thanks, [~JunRuiLi]  for your careful testing. no problems were found during the test, so I will close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-30542 Support adaptive local hash aggregate in runtime,FLINK-31060,13524623,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,337361684@qq.com,337361684@qq.com,14/Feb/23 08:38,21/Feb/23 04:36,04/Jun/24 20:41,21/Feb/23 04:36,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / Runtime,,,,0,,,,,"This issue aims to verify FLINK-30542: Support adaptive local hash aggregate in runtime.

Adaptive local hash aggregation is an optimization of local hash aggregation, which can adaptively determine whether to continue to do local hash aggregation according to the distinct value rate of sampling data. If distinct value rate bigger than defined threshold (see parameter: 'table.exec.local-hash-agg.adaptive.distinct-value-rate-threshold'), we will stop aggregating and just send the input data to the downstream after a simple projection. Otherwise, we will continue to do aggregation.

We can verify it in SQL client after we build the flink-dist package.
 # Create a source table firstly. (Note: the source table need have different degree of aggregation, means the distinct count can be controlled by source connector, we recommend to modify dataGen table source to produce different data with different distinct row number).
 # Verify the result with different distinct value rate. (See: table.exec.local-hash-agg.adaptive.distinct-value-rate-threshold)
 # Check the log in 'TM' to see whether the adaptive local hash aggregate works.

If you meet any problems, it's welcome to ping me directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 04:36:06 UTC 2023,,,,,,,,,,"0|z1fx1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:07;lsy;[~337361684@qq.com] I  can take this ticket.;;;","21/Feb/23 04:12;lsy;I verified this feature according to the description, and everything seems to meet expectations.;;;","21/Feb/23 04:36;lsy;Verify step, I create a source table using datagen:

 
{code:java}
CREATE TABLE source (
    f0 INT,
    f1 BIGINT,
    f2 DOUBLE,
    f3 VARCHAR(30),
    f4 INT,
    f5 BIGINT,
    f6 FLOAT,
    f7 DOUBLE,
    f8 DECIMAL(10, 5),
    f9 DECIMAL(38, 18),
    f10 DATE,
    f11 TIMESTAMP,
    f12 ARRAY<BIGINT>)
WITH (
    'connector' = 'datagen',
     'number-of-rows' = '1000',
     'rows-per-second' = '100000'); {code}
 

 

I have verified the following case:
 # test the lower aggregation degree of group by key `f0` by the query `select f0, sum(f4), sum(f6) from source group by f0`, I saw the local hash agg is skipped via TM log, it meets the expectation
 # test the higher aggregation degree of group by key 'f0' by the query  `select f0, sum(f4), sum(f6) from source group by f0`, I saw all the rows are aggregated in local hash agg phase via TM log, it meets  the expectation
 # If the query contains a function that doesn't support adaptive local hash agg such as `select f0, sum(f4), sum(f6), STDDEV_POP(f7) from source group by f0`, the adaptive local hash agg is skipped by the planner in compile phase, it meets the expectation. But the generated code by the planner contains the variable `localAggSuppressed`, this shouldn't be caused when the adaptive local hash agg is disabled by planner. It would be better if we could improve the code generate logic.
 # Set the option `table.exec.local-hash-agg.adaptive.enabled = false`, adaptive local hash agg is disabled by the planner, it meets the expectation;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-29717 Supports hive udaf such as sum/count by native implementation,FLINK-31059,13524620,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,miamiaoxyz,lsy,lsy,14/Feb/23 08:31,23/Feb/23 08:12,04/Jun/24 20:41,21/Feb/23 09:13,1.17.0,,,,,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,,,,,"This task aims to verify [FLINK-29717|https://issues.apache.org/jira/browse/FLINK-29717] which improves the hive udaf performance.

As the document [PR|https://github.com/apache/flink/pull/21789] description, please veriy:
1. Enabling the option `table.exec.hive.native-agg-function.enabled`, use the sum/count/avg/min/max functions separately in the query to verify if the hash-agg strategy is chosen via plan, and verify if the data results are the same as when the option `table.exec.hive.native-agg-function.enabled` is disabled.
2. Enabling the option `table.exec.hive.native-agg-function.enabled`, combine sum/count/avg/min/max functions in query, verify if the hash-agg strategy is chosen via plan, and verify if the data results are the same as when option `table.exec.hive.native-agg-function.enabled` is disabled.
3. Enabling the option `table.exec.hive.native-agg-function.enabled`, count or max array&struct and other complex types in query, verify whether the sort-agg strategy is chosen via plan, verify whether the data result is the same as when option `table.exec.hive.native-agg-function.enabled` is disabled.
4. Enabling the option `table.exec.hive.native-agg-function.enabled`, use the sum/count and first_value/last_value functions in the query simultaneously, verify that the sort-agg strategy is chosen via plan, verify that the data is the same as when option `table.exec.hive.native-agg-function.enabled` is disabled.
5. Enabling the option `table.exec.hive.native-agg-function.enabled`, use the sum/count/avg/min/max functions in the query and open sort-agg strategy forcibly, verify that the data results are the same as when option `table.exec.hive.native-agg-function.enabled` is disabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 07:45;miamiaoxyz;image-2023-02-21-15-45-48-226.png;https://issues.apache.org/jira/secure/attachment/13055656/image-2023-02-21-15-45-48-226.png","21/Feb/23 07:46;miamiaoxyz;image-2023-02-21-15-46-13-966.png;https://issues.apache.org/jira/secure/attachment/13055657/image-2023-02-21-15-46-13-966.png","21/Feb/23 07:47;miamiaoxyz;image-2023-02-21-15-47-54-043.png;https://issues.apache.org/jira/secure/attachment/13055658/image-2023-02-21-15-47-54-043.png","21/Feb/23 07:50;miamiaoxyz;image-2023-02-21-15-49-58-854.png;https://issues.apache.org/jira/secure/attachment/13055659/image-2023-02-21-15-49-58-854.png","21/Feb/23 07:59;miamiaoxyz;image-2023-02-21-15-59-44-470.png;https://issues.apache.org/jira/secure/attachment/13055660/image-2023-02-21-15-59-44-470.png","21/Feb/23 08:28;miamiaoxyz;image-2023-02-21-16-28-22-038.png;https://issues.apache.org/jira/secure/attachment/13055661/image-2023-02-21-16-28-22-038.png","21/Feb/23 08:29;miamiaoxyz;image-2023-02-21-16-29-42-983.png;https://issues.apache.org/jira/secure/attachment/13055662/image-2023-02-21-16-29-42-983.png","21/Feb/23 08:32;miamiaoxyz;image-2023-02-21-16-31-58-361.png;https://issues.apache.org/jira/secure/attachment/13055663/image-2023-02-21-16-31-58-361.png","21/Feb/23 08:35;miamiaoxyz;image-2023-02-21-16-35-46-294.png;https://issues.apache.org/jira/secure/attachment/13055664/image-2023-02-21-16-35-46-294.png",,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 08:12:58 UTC 2023,,,,,,,,,,"0|z1fx14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:42;337361684@qq.com;I hope to get this ticket, [~lsy] ;;;","14/Feb/23 12:12;lsy;[~337361684@qq.com] Very happy you can take this ticket, I've listed the test cases.;;;","21/Feb/23 08:53;miamiaoxyz;I first use sql client to test the feature, but the `exec.hive.native-agg-function.enabled`  do not work. 


I then use ITCase to verify.
 #    I use IT case to turn on and off `exec.hive.native-agg-function.enabled` to verify the two results are the same, testSql function test whether the same sql get same result with on and off `exec.hive.native-agg-function.enabled`.

I verfied that the plan use Hashagg when turn on the `exec.hive.native-agg-function.enabled`, and the plan use SortAgg when turn off by IT case.
!image-2023-02-21-15-45-48-226.png|width=549,height=234!
It pass all the IT Case below. !image-2023-02-21-15-46-13-966.png|width=501,height=371!
2. I verified that data results are the same when combine sum/count/avg/min/max functions in query using `exec.hive.native-agg-function.enabled` on and off using the IT case below.

I verfied that the plan use Hashagg when turn on the `exec.hive.native-agg-function.enabled`, and the plan use SortAgg when turn off by IT case.

!image-2023-02-21-15-49-58-854.png|width=536,height=219!

3. For  `array` and `struct` do not support the max function.  For count function, it does not store `array` or `struct` in agg, so they use bigint instead, and hash-agg is chosen  .

!image-2023-02-21-15-59-44-470.png|width=1016,height=189!

4. For `first_value` and `last_value` are not implemented in hive,  [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inAggregateFunctions(UDAF)] I use `collect_set` to test  instead. All the plan use SortAgg, and get same result, which meet the expectations.

```

---  turn on `table.exec.hive.native-agg-function.enabled`

== Abstract Syntax Tree ==
LogicalProject(x=[$0], _o__c1=[$1])
+- LogicalAggregate(group=[\{0}], agg#0=[collect_set($1)])
   +- LogicalProject($f0=[$0], $f1=[$1])
      +- LogicalTableScan(table=[[test-catalog, default, foo]])

== Optimized Physical Plan ==
SortAggregate(isMerge=[true], groupBy=[x], select=[x, Final_collect_set($f1) AS $f1])
+- Sort(orderBy=[x ASC])
   +- Exchange(distribution=[hash[x]])
      +- LocalSortAggregate(groupBy=[x], select=[x, Partial_collect_set(y) AS $f1])
         +- Sort(orderBy=[x ASC])
            +- TableSourceScan(table=[[test-catalog, default, foo]], fields=[x, y])

== Optimized Execution Plan ==
SortAggregate(isMerge=[true], groupBy=[x], select=[x, Final_collect_set($f1) AS $f1])
+- Exchange(distribution=[forward])
   +- Sort(orderBy=[x ASC])
      +- Exchange(distribution=[hash[x]])
         +- LocalSortAggregate(groupBy=[x], select=[x, Partial_collect_set(y) AS $f1])
            +- Exchange(distribution=[forward])
               +- Sort(orderBy=[x ASC])
                  +- TableSourceScan(table=[[test-catalog, default, foo]], fields=[x, y])

---  turn off `table.exec.hive.native-agg-function.enabled`

== Abstract Syntax Tree ==
LogicalProject(x=[$0], _o__c1=[$1])
+- LogicalAggregate(group=[\{0}], agg#0=[collect_set($1)])
   +- LogicalProject($f0=[$0], $f1=[$1])
      +- LogicalTableScan(table=[[test-catalog, default, foo]])

== Optimized Physical Plan ==
SortAggregate(isMerge=[true], groupBy=[x], select=[x, Final_collect_set($f1) AS $f1])
+- Sort(orderBy=[x ASC])
   +- Exchange(distribution=[hash[x]])
      +- LocalSortAggregate(groupBy=[x], select=[x, Partial_collect_set(y) AS $f1])
         +- Sort(orderBy=[x ASC])
            +- TableSourceScan(table=[[test-catalog, default, foo]], fields=[x, y])

== Optimized Execution Plan ==
SortAggregate(isMerge=[true], groupBy=[x], select=[x, Final_collect_set($f1) AS $f1])
+- Exchange(distribution=[forward])
   +- Sort(orderBy=[x ASC])
      +- Exchange(distribution=[hash[x]])
         +- LocalSortAggregate(groupBy=[x], select=[x, Partial_collect_set(y) AS $f1])
            +- Exchange(distribution=[forward])
               +- Sort(orderBy=[x ASC])
                  +- TableSourceScan(table=[[test-catalog, default, foo]], fields=[x, y])

```

 

 

!image-2023-02-21-16-31-58-361.png|width=620,height=261!

 

5. I disable the hashagg to force use sortagg to process all of the test above, which  can see that the result of forcing to close hashagg is the same as the result of turn on and off`exec.hive.native-agg-function.enabled`, which meets the expectations

!image-2023-02-21-16-35-46-294.png|width=632,height=392!

 

Problems:

a.  The `exec.hive.native-agg-function.enabled`  do not work on sql client. the hashagg is not chosen on sql client.

!https://intranetproxy.alipay.com/skylark/lark/0/2023/png/83756403/1676952029939-182fa078-3a07-4e45-bdbb-832f7f74c838.png|width=703,height=383,id=u4fc84338!

b. Enable and disable `table.exec.hive.native-agg-function.enabled` get different result.

!image-2023-02-21-16-28-22-038.png|width=618,height=283!

!image-2023-02-21-16-29-42-983.png|width=713,height=129!
 ;;;","21/Feb/23 09:12;lsy;[~miamiaoxyz] Thanks for your verify, for the problem, I will see it.;;;","23/Feb/23 08:12;lsy;[~miamiaoxyz] For point a, I've tested it on my machine, yes, it doesn't work in SqlClient if we load the HiveModule first and then enable the option. But If we enable the option first and load the HiveModule, this option works well. After deep dive into the related code, I think there exists some problem in the code implementation on SqlClient side. If we want this option can work at the job level, we need to modify the public API `Module`. Therefore, I have created an [issue|https://issues.apache.org/jira/browse/FLINK-31193]to track it, we will fix it in Flink 1.18. Currently, if we want to use this option in SqlClient, here's what we should:
{code:java}
// enable option first
SET 'table.exec.hive.native-agg-function.enabled' = 'true';

// then laod hive module
LOAD MODULE hive WITH ('hive-version' = '...') {code}
 In addition, I will add some notes to the document to remind users.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade usage of aws sdk to v2 for flink-connector-aws,FLINK-31058,13524615,13523890,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,14/Feb/23 08:10,14/Feb/23 08:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-14 08:10:27.0,,,,,,,,,,"0|z1fx00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade usage of aws sdk to v2 for flink-json-glue-schema-registry,FLINK-31057,13524614,13523890,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,14/Feb/23 08:09,14/Feb/23 08:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-14 08:09:45.0,,,,,,,,,,"0|z1fwzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade usage of aws sdk to v2 for flink-avro-glue-schema-registry ,FLINK-31056,13524613,13523890,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,14/Feb/23 08:09,14/Feb/23 08:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-14 08:09:00.0,,,,,,,,,,"0|z1fwzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The dynamic flag of stream graph does not take effect when translating the transformations,FLINK-31055,13524605,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,14/Feb/23 07:12,16/Feb/23 15:16,04/Jun/24 20:41,16/Feb/23 15:16,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"Currently, the dynamic flag of stream graph is not set when [translate transformations|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java#L324]. However, the dynamic flag will be used ([here|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java#L696]) when translating, we should set the dynamic flag before the translating.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 15:16:08 UTC 2023,,,,,,,,,,"0|z1fwxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 07:15;wanglijie;cc [~JunRuiLi] [~zhuzh] ;;;","14/Feb/23 08:14;JunRuiLi;[~wanglijie] Thanks for creating this issue. As you said, I think it is a bug. Could you fix it？Thanks!;;;","14/Feb/23 09:31;wanglijie;[~JunRuiLi] I 'll fix it.;;;","16/Feb/23 15:16;wanglijie;Fixed via
master: 1c0870ae08730688706540b999c04b2f4c4498ee
release-1.17: 629bc9a3d35f5141324ef4cfa9b255dfcc069d20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink free to common codegen core shade,FLINK-31054,13524596,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,14/Feb/23 06:26,14/Feb/23 11:44,04/Jun/24 20:41,14/Feb/23 11:44,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 11:44:32 UTC 2023,,,,,,,,,,"0|z1fwvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 11:44;lzljs3620320;master: 6a5e4084d95563f7094deec62c36aa1e921f9ec6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Example Repair the log output format of the CheckpointCoordinator,FLINK-31053,13524591,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xzw0223,xzw0223,14/Feb/23 05:39,23/Aug/23 02:40,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"!image-2023-02-14-13-38-32-967.png|width=708,height=146!

The log output format is incorrect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/23 05:38;xzw0223;image-2023-02-14-13-38-32-967.png;https://issues.apache.org/jira/secure/attachment/13055416/image-2023-02-14-13-38-32-967.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 02:40:37 UTC 2023,,,,,,,,,,"0|z1fwuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 02:40;masteryhx;Hi, [~xzw0223] 

I saw you closed your pr, are you still working on this ?

I think it's better to use failure.getMessage() to print. WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-30707 Improve slow task detection,FLINK-31052,13524574,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,JunRuiLi,zhuzh,zhuzh,14/Feb/23 02:59,21/Feb/23 07:57,04/Jun/24 20:41,21/Feb/23 07:57,,,,,,,,,,,,1.17.0,,,,Runtime / Coordination,,,,0,,,,,"This task aims to verify [FLINK-30707|https://issues.apache.org/jira/browse/FLINK-30707] which improves the slow task detection. 

The slow task detection now takes the input data volume of tasks into account. Tasks which has a longer execution time but consumes more data may not be considered as slow. This improvement helps to eliminate the negative impacts of data skew on slow task detecting.

The documentation of speculative execution can be found [here|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/speculative_execution/#speculative-execution] .

One can verify it by creating intended data skew.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 07:57:59 UTC 2023,,,,,,,,,,"0|z1fwqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:09;JunRuiLi;[~zhuzh] Thanks for creating this issue, I'd like to do this testing work.;;;","15/Feb/23 08:11;wanglijie;Thanks [~JunRuiLi]. I have assigned it to you.;;;","20/Feb/23 09:53;JunRuiLi;I have tested it and it looks good to me.
I created two simple batch jobs with only source and sink operator connected by hash edges. Job 1 has data skew, while Job 2 has no data skew but has a taskManager that processes data very slowly. And I set the `execution.batch.speculative.enabled` = `true` and `slow-task-detector.execution-time.baseline-lower-bound` = `0s` and `slow-task-detector.execution-time.baseline-ratio` = `0.1` to enable speculative scheduler and slow task detection more qucikly.
As expected job 1 did not detect slow tasks, while job 2 did.;;;","21/Feb/23 07:57;zhuzh;Thanks for helping with the testing! [~JunRuiLi];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce `delete` action to support deleting from table,FLINK-31051,13524572,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,14/Feb/23 02:03,15/Feb/23 01:24,04/Jun/24 20:41,15/Feb/23 01:24,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 01:24:11 UTC 2023,,,,,,,,,,"0|z1fwqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 01:24;lzljs3620320;master: d5a72f12a6fb4f25a8034481134ca03bfde340c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add cases for IN and NOT IN predicate in ORC format,FLINK-31050,13524570,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,14/Feb/23 01:41,15/Feb/23 08:14,04/Jun/24 20:41,15/Feb/23 08:14,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 08:14:02 UTC 2023,,,,,,,,,,"0|z1fwq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:14;lzljs3620320;master: bbd87fea9f9aadefbc26f6739477337ac25f749d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Kafka record headers to KafkaSink,FLINK-31049,13524536,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Axeman,Axeman,Axeman,13/Feb/23 21:05,26/Jan/24 12:56,04/Jun/24 20:41,18/Dec/23 09:30,,,,,,,,,,,,kafka-3.1.0,,,,Connectors / Kafka,,,,0,KafkaSink,pull-request-available,stale-assigned,,"The default org.apache.flink.connector.kafka.sink.KafkaSink does not support adding Kafka record headers. In some implementations, downstream consumers might rely on Kafka record headers being set.
 

A way to add Headers would be to create a custom KafkaRecordSerializationSchema and inject that into the KafkaSink.

However, I'm assuming the KafkaRecordSerializationSchemaBuilder was added for convenience and allows a more usable approach of creating a KafkaSink without having to deal with details like the RecordProducer directly. This builder does not support adding record headers.

This is where I think it should be added.

The code responsible for creating the Kafka record involves  org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaWrapper where the RecordProducer is created. 
It is relatively simple to add support for record headers by adding a ""HeaderProducer"" to the KafkaRecordSerializationSchemaBuilder next to the key and value serializers and using the appropriate RecordProducer constructor.
 
The issue was discussed [here|https://lists.apache.org/thread/shlbbcqho0q9w5shjwdlscnsywjvbfro].
 ",,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 09:30:17 UTC 2023,,,,,,,,,,"0|z1fwig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 21:58;sap1ens;Hi Alex,

Would you mind correcting the language a little bit? KafkaSink does allow setting headers by accepting a custom {{KafkaRecordSerializationSchema}} via {{setRecordSerializer}} method. You don't need to use {{{}KafkaRecordSerializationSchemaWrapper{}}}. 

Would you add HeaderProducer as a top-level entity for the KafkaSink?;;;","14/Feb/23 14:20;Axeman;[~sap1ens] ;;;","14/Feb/23 17:16;sap1ens;Thanks for the update! It makes sense 👍;;;","14/Feb/23 18:14;Axeman;Thanks [~sap1ens] . Can anyone assign this ticket to me?;;;","21/Feb/23 10:19;martijnvisser;Given that we're in the process of externalizing the Kafka connector, can we hold off merging this to Flink since it should go to the flink-connector-kafka repository?;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","18/Dec/23 09:30;martijnvisser;Merged by [~tzulitai] in apache/flink-connector-kafka:main a7785630e714af303b224c38d9a6caa89a551265;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java doc of PulsarSourceBuilder was not updated in time,FLINK-31048,13524506,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,13/Feb/23 17:00,14/Feb/23 03:04,04/Jun/24 20:41,14/Feb/23 03:03,pulsar-4.0.0,,,,,,,,,,,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,"The java doc of `PulsarSourceBuilder` was not updated in time.

IIUC, 
{code:java}
setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(new SimpleStringSchema())) {code}
should be replaced by
{code:java}
setDeserializationSchema(new SimpleStringSchema()) {code}
BTW, I also checked `pulsar.md` and luckily it was correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 03:03:36 UTC 2023,,,,,,,,,,"0|z1fwbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 03:03;Weijie Guo;main(4.0) via : fb98096d9a67d26b1965b8ef181125317a59a5c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support topology spread constrains,FLINK-31047,13524504,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tamirsagi,tamirsagi,tamirsagi,13/Feb/23 16:51,19/Feb/23 17:44,04/Jun/24 20:41,19/Feb/23 17:44,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,flink-kubernetes-operator,,,,"In case HA is enabled, it should be possible to define topology spread constrains to make sure the replicas are deployed on different nodes/zones

for instance:

topologySpreadConstraints:
 - maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector :
  matchLabels:
     key:  value

Reference:
[https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:41;tamirsagi;image-2023-02-15-08-38-49-353.png;https://issues.apache.org/jira/secure/attachment/13055445/image-2023-02-15-08-38-49-353.png","15/Feb/23 06:44;tamirsagi;image-2023-02-15-08-42-04-150.png;https://issues.apache.org/jira/secure/attachment/13055444/image-2023-02-15-08-42-04-150.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 19 17:44:16 UTC 2023,,,,,,,,,,"0|z1fwbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 13:15;tamirsagi;I added the PR for that ticket along with FLINK-31046

[https://github.com/apache/flink-kubernetes-operator/pull/531]

Tested it locally

3 replicas
!image-2023-02-15-08-38-49-353.png!

The pods have been scheduled on different nodes
!image-2023-02-15-08-42-04-150.png!;;;","19/Feb/23 17:44;gyfora;merged to main 8e1f5cff8d0c5784391245b8818517ec9b8554e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide extra environment variables from field ref,FLINK-31046,13524503,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tamirsagi,tamirsagi,tamirsagi,13/Feb/23 16:41,19/Feb/23 17:44,04/Jun/24 20:41,19/Feb/23 17:43,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,0,flink-kubernetes-operator,pull-request-available,,,"The operator should provide several environment variables that are available only upon deployment.

 

hostIP which is taken from fieldRef status.hostIP

podIP which is taken from fieldRef status.podIP

podName which is taken from fieldRef metadata.name

 

Example
Namespace is provided

[https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/templates/flink-operator.yaml#L81-L84]


!image-2023-02-16-16-49-25-688.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/23 14:51;tamirsagi;image-2023-02-16-16-49-25-688.png;https://issues.apache.org/jira/secure/attachment/13055516/image-2023-02-16-16-49-25-688.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 19 17:43:58 UTC 2023,,,,,,,,,,"0|z1fwb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/23 17:43;gyfora;merged to main 8e1f5cff8d0c5784391245b8818517ec9b8554e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In IntelliJ: flink-clients cannot find symbol symbol: class TestUserClassLoaderJobLib,FLINK-31045,13524502,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,pnowojski,pnowojski,13/Feb/23 16:27,22/Feb/23 09:32,04/Jun/24 20:41,22/Feb/23 09:32,1.18.0,,,,,,,,,,,1.18.0,,,,Build System,Client / Job Submission,,,0,pull-request-available,,,,"When trying to build/run some tests in the IDE, IntelliJ is reporting the following compilation failure:

{noformat}
/XXX/flink/flink-clients/src/test/java/org/apache/flink/client/testjar/TestUserClassLoaderJob.java:33:38
java: cannot find symbol
  symbol:   class TestUserClassLoaderJobLib
  location: class org.apache.flink.client.testjar.TestUserClassLoaderJob
{noformat}

A workaround seems to be to:
# right click on {{flink-clients}}
# Rebuild module (flink-clients)

The issue is probably related to the comment from the flink-clients/pom.xml file:
{noformat}
			<!--Remove the external jar test code from the test-classes directory since it mustn't be in the
			classpath when running the tests to actually test whether the user code class loader
			is properly used.-->
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 09:32:58 UTC 2023,,,,,,,,,,"0|z1fwaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 02:23;taoran;+1 got same problem many times.;;;","21/Feb/23 06:49;Weijie Guo;also got same problem many times;;;","22/Feb/23 09:32;chesnay;master: 3fc3527dc804ed6e58246e545bc272d8d8c4308e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected behavior using greedy on looping pattern with notFollowedBy at end,FLINK-31044,13524482,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Juntao Hu,Juntao Hu,13/Feb/23 13:48,13/Feb/23 13:50,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Library / CEP,,,,0,,,,,"Pattern without greedy: begin(""A"" , SKIP_PAST_LAST).oneOrMore().consecutive().notFollowedBy(""B"").within(Time.milliseconds(10))

Pattern with greedy: begin(""A"", SKIP_PAST_LAST).oneOrMore().consecutive().greedy().notFollowedBy(""B"").within(Time.milliseconds(10))

For sequence: <a1, 1L> <a2, 2L> <a3, 3L>, after FLINK-31040 and FLINK-31042 fixed, both of the patterns should produce [a1, a2, a3] , when currently the pattern with greedy option makes no output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-13 13:48:56.0,,,,,,,,,,"0|z1fw6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyError exception is thrown in CachedMapState,FLINK-31043,13524463,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,dianfu,dianfu,13/Feb/23 11:50,13/Feb/23 12:13,04/Jun/24 20:41,13/Feb/23 12:12,1.15.0,,,,,,,,,,,1.15.4,1.16.2,1.17.0,,API / Python,,,,0,,,,,"Have seen the following exception in a PyFlink job which runs in Flink 1.15. It happens occasionally and may indicate a bug of the state cache of MapState:
{code:java}
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 131: Traceback (most recent call last):
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
    response = task()
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
    element.data)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 158, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 170, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/table/operations.py"", line 417, in finish_bundle
    return self.group_agg_function.finish_bundle()
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 597, in pyflink.fn_execution.table.aggregate_fast.GroupTableAggFunction.finish_bundle
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 652, in pyflink.fn_execution.table.aggregate_fast.GroupTableAggFunction.finish_bundle
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 389, in pyflink.fn_execution.table.aggregate_fast.SimpleTableAggsHandleFunction.emit_value
  File ""/tmp/pyflink/17360444-8c0b-46a5-90a4-689c376ea4ed/0e2967b5-181c-4663-bd7a-267d47509cf5/whms_dws_stock_python_sps_1_output.py"", line 29, in emit_value
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/table/state_data_view.py"", line 147, in get
    return self._map_state.get(key)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 915, in get
    return self.get_internal_state().get(key)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 773, in get
    self._state_key, map_key, self._map_key_encoder, self._map_value_decoder)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 418, in blocking_get
    cached_map_state.put(map_key, (exists, value))
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 319, in put
    super(CachedMapState, self).put(key, exists_and_value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 68, in put
    self._on_evict(name, value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 305, in on_evict
    self._cached_keys.remove(key)
KeyError: 'SPAREPARTS_M11F010L4L1_01'
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 12:12:52 UTC 2023,,,,,,,,,,"0|z1fw28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 12:12;dianfu;This issue has already been fixed in:
- master via 838b79f5b9cc1a4cf253b2c17009f337bf569ecc
- release-1.16 via f91ca7d2cd4e466c0aa6d4dc56ee982abc5ca5e1
- release-1.15 via fa77fd4b1333b23a2769c77994644ba8a8d7d4ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AfterMatchSkipStrategy not working on notFollowedBy ended pattern,FLINK-31042,13524460,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,13/Feb/23 11:17,17/Feb/23 08:28,04/Jun/24 20:41,17/Feb/23 08:28,1.16.0,,,,,,,,,,,1.16.2,1.17.0,,,Library / CEP,,,,0,,,,,"Pattern: begin(""A"", SkipToNext()).oneOrMore().allowCombinations().followedBy(""C"").notFollowedBy(""B"").within(Time.milliseconds(10L))

Sequence: <a1, 1L> <a2, 2L> <a3, 3L> <c1, 4L> will produce

[a1, a2, a3, c1]

[a1, a2, c1]

[a1, c1]

[a2, a3, c1]

[a2, c1]

[a3, c1]

Using SkipPastLastEvent() also produce the same result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 08:28:48 UTC 2023,,,,,,,,,,"0|z1fw1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 08:28;dianfu;Merged to:
- master: 919d46ef91488627a282d1474208490b6dc7a820
- 1.17: 257be14bb47f7481927ea17bc82b4b6ff63e8b30
- 1.16: d61e4a1884e265e0a33f0f4dbad50898df14677f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build up of pending global failures causes JM instability,FLINK-31041,13524459,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,huwh,dannycranmer,dannycranmer,13/Feb/23 11:09,23/Feb/23 08:48,04/Jun/24 20:41,23/Feb/23 08:43,1.15.3,1.16.1,,,,,,,,,,1.15.4,1.16.2,1.17.0,,Runtime / Coordination,,,,0,pull-request-available,,,,"h4. Context

When a job creates multiple sources that use the {{SourceCoordinator}} (FLIP-27), there is a failure race condition that result in a ""leak"" of ExecutionVertextVersion due to a ""queue"" of pending global failures. 

This results in the Job Manager becoming unresponsive.
h4. !flink-31041-heap-dump.png!
h4. Reproduction Steps

This can be reproduced by a job that creates multiple sources that fail in the {{{}SplitEnumerator{}}}. We observed this with multiple {{KafkaSource's}} trying to load a non-existent cert from the file system and throwing FNFE. Thus, here is a simple job to reproduce (BE WARNED: running this locally will lock up your IDE):
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
env.setRestartStrategy(new RestartStrategies.FailureRateRestartStrategyConfiguration(10000, Time.of(10, TimeUnit.SECONDS), Time.of(10, TimeUnit.SECONDS)));

KafkaSource<String> source = KafkaSource.<String>builder()
        .setProperty(""security.protocol"", ""SASL_SSL"")
        // SSL configurations
        // Configure the path of truststore (CA) provided by the server
        .setProperty(""ssl.truststore.location"", ""/path/to/kafka.client.truststore.jks"")
        .setProperty(""ssl.truststore.password"", ""test1234"")
        // Configure the path of keystore (private key) if client authentication is required
        .setProperty(""ssl.keystore.location"", ""/path/to/kafka.client.keystore.jks"")
        .setProperty(""ssl.keystore.password"", ""test1234"")
        // SASL configurations
        // Set SASL mechanism as SCRAM-SHA-256
        .setProperty(""sasl.mechanism"", ""SCRAM-SHA-256"")
        // Set JAAS configurations
        .setProperty(""sasl.jaas.config"", ""org.apache.kafka.common.security.scram.ScramLoginModule required username=\""username\"" password=\""password\"";"")
        .setBootstrapServers(""http://localhost:3456"")
        .setTopics(""input-topic"")
        .setGroupId(""my-group"")
        .setStartingOffsets(OffsetsInitializer.earliest())
        .setValueOnlyDeserializer(new SimpleStringSchema())
        .build();

List<SingleOutputStreamOperator<String>> sources = IntStream.range(0, 32)
        .mapToObj(i -> env
                .fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source "" + i).uid(""source-"" + i)
                .keyBy(s -> s.charAt(0))
                .map(s -> s))
        .collect(Collectors.toList());

env.fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source"").uid(""source"")
        .keyBy(s -> s.charAt(0))
        .union(sources.toArray(new SingleOutputStreamOperator[] {}))
        .print();

env.execute(""test job""); {code}
h4. Root Cause

We can see that the {{OperatorCoordinatorHolder}} already has a [debounce mechanism|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/OperatorCoordinatorHolder.java#L609], however the {{DefaultScheduler}} does not. We need a debounce mechanism in the {{DefaultScheduler}} since it handles many {{{}OperatorCoordinatorHolder{}}}.
h4. Fix

I have managed to fix this, I will open a PR, but would need feedback from people who understand this code better than me!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:43;zhuzh;failovers.log;https://issues.apache.org/jira/secure/attachment/13055458/failovers.log","13/Feb/23 11:11;dannycranmer;flink-31041-heap-dump.png;https://issues.apache.org/jira/secure/attachment/13055400/flink-31041-heap-dump.png","15/Feb/23 10:42;zhuzh;test-restart-strategy.log;https://issues.apache.org/jira/secure/attachment/13055457/test-restart-strategy.log",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 07:55:32 UTC 2023,,,,,,,,,,"0|z1fw1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:41;zhuzh;Thanks for reporting this issue! [~dannycranmer]

I have tried to re-produce the problem locally, but not able yet to fully re-produce it.
Here is what I see by running the example in the JIRA description:
1. The restart strategy works. by changing the strategy to {{RestartStrategies.fixedDelayRestart(100, Time.of(10, TimeUnit.SECONDS)}}, the job failed after encountering 100 failures.(see attached test-restart-strategy.log) I guess the reason it is not working as expected in the above example is that it tolerates 10000 failures in 10 seconds.
2. The debounce mechanism of DefaultScheduler works. The execution vertex versioning is introduced for this purpose.
I tested it by reducing the source number to 2(see attached failovers.log) for easier diagnostics. I can see 2 global failures happened every 10 seconds, and only one job restarts in triggered by them.

Regarding the ""leak of ExecutionVertexVersion"", I'm not sure but guess it is caused by that there are many pending global failures in JM's main thread. It's not leaking, although not ideal. Yet this can be avoided if failures are not triggered too frequently.

Regarding the specific case(or similar ones) in the JIRA description, looks to me it can be resolved by setting the tolerable failure rate to a proper value. 
WDYT?

;;;","15/Feb/23 11:02;dannycranmer;[~zhuzh] thanks for looking in to this.

 

If I understand correctly, you are saying the ""leak"" is due to a ""queue"" of pending global failures. They would eventually get debounced, but there is some intermediate holding area.

 

We are running with jobmanager.execution.failover-strategy = full. In this mode I would not expect multiple errors to trigger a global job failure. Possibly we can debounce the failures earlier (as per my naive PR) and prevent this build up? For additional context, the actual job that was causing the problem for us originally, simply had 2x Kafka sources, and the same FileNotFoundException was triggering the failover. 

 

> Regarding the specific case(or similar ones) in the JIRA description, looks to me it can be resolved by setting the tolerable failure rate to a proper value.

Unfortunately this is not possible for us. We do not want the job to transition to \{{FAILED}}. We require the job to retry indefinitely. The job could be failing due to transient errors, like permission issues that can be fixed without resubmitting the job. We use a fixed delay of 10 seconds.;;;","16/Feb/23 09:45;huwh;[~zhuzh] [~dannycranmer]
I successfully reproduced this problem locally. I found that:
 # OperatorCoordinators are re-created and started unexpectedly. DefaultScheduler will manage multiple OperatorCoordinators. Each coordinator could call DefaultScheduler#handleGlobalFailure in their thread, and DefaultScheduler would handle them one by one. Executions will not be restarted multiple times Since the vertex version provides the debounce mechanism. But some other functions will be called multiple times, which may cause unexpected impact. Such as SchedulerBase#restoreState, it will recreate-start new operator coordinators. If there are two SourceCoordinators (A, B), and both of them will throw exception in start(). The fail of A will recreate A1,B1 and restart all executions, The fail of B will recreate A2, B2 but not restart executions. Then A1/B1/A2/B2 will fail in start(), triggering more DefaultScheduler#handleGlobalFailure.
 # The retrigger of DefaultScheduler#handleGlobalFailure will make RestartStrategy  not working as expected. For example, if we set RestartStrategies.fixedDelayRestart(1, Time.of(10, TimeUnit.SECONDS). The job should restart twices, but when this job has 2 source coordinators, the job will fail immediately.

IMO, DefaultScheduler should also have debounce mechanism in handleGlobalFailure. 
Please correct me if I am wrong.;;;","16/Feb/23 09:49;zhuzh;Thanks for the inputs! [~dannycranmer]

I took another look through the failure handling process. I think that why the problem is disturbing is that outdated global failure recovery is still heavy, compared to outdated regional failure recovery. An outdated regional failure recovery almost does nothing because it operates on an empty task set (tasks to restart are filtered out due to outdated). For a global failure recovery which is superseded by another, however, it still conducts {{checkpointCoordinator#restoreLatestCheckpointedStateToAll(...)}}, which I think is a heavy invocation.

So I think maybe we can just skip that {{restoreLatestCheckpointedStateToAll(...)}} invocation if {{jobVerticesToRestore}} is empty, like [this change|https://github.com/zhuzhurk/flink/commit/a522edeb4e23ab1ce3f5a34eb3269116723e77a5]. I tested it locally, the test case runs much more smoothly. Would you also verify if it works for you?

I think we cannot totally ignore a global failure if another global failover is in progress. Because these global failures can be different ones (with different reasons), which should be recorded and exposed in the exception history, even though we do not want the later one to trigger one more job restart.
;;;","16/Feb/23 10:20;zhuzh;[~huwh] I just see you comments. I think it's mostly answered in my above comment, except for the restart strategy.

Currently, Flink notifies the {{RestartBackoffStrategy}} each time a failure happens. So it is more about how many failures are tolerable, instead of how many restarts are tolerable. This means superseded failures are still counted. This is not aligned with the [description of the restart strategies|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/task_failure_recovery/#task-failure-recovery]. Maybe we can improve it in later versions. However, compatibility issues should be considered for existing jobs.;;;","16/Feb/23 11:54;huwh;[~zhuzh]  Thanks for the explanation, I have one more small question, could we just skip the restoreState when verticesToRestart is empty?;;;","17/Feb/23 04:06;zhuzh;I think yes, or even a wider scope in {{DefaultScheduler#restartTasks()}}. ;;;","17/Feb/23 10:16;huwh;[~dannycranmer] Does it solve your problem?  I can take this ticket (add some ut and do more tests) if you have no PR yet.  [~zhuzh] ;;;","17/Feb/23 12:58;dannycranmer;Hey [~zhuzh] / [~huwh] . Thank-you for the deep dive and explanation. Yes [this change|https://github.com/zhuzhurk/flink/commit/a522edeb4e23ab1ce3f5a34eb3269116723e77a5] solved the problem for me. Thanks [~huwh] for offering to pick up the fix, that is most appreciated. ;;;","18/Feb/23 03:29;zhuzh;I have assigned you the ticket. [~huwh]
Feel free to open a pr for it.;;;","22/Feb/23 07:55;dannycranmer;Merged commit [{{b3e1492}}|https://github.com/apache/flink/commit/b3e14928e815dd6dbdffbe3c5616733d4c7c8825] into apache:master

Merged commit [{{50021a2}}|https://github.com/apache/flink/commit/50021a220261123caa6b31fe1a6938aa03479f17] into apache:release-1.17

Merged commit [{{df3ac1e}}|https://github.com/apache/flink/commit/df3ac1ea706080a0637d6c54574e5f1862d3b80b] into apache:release-1.16 

Merged commit [{{613650f}}|https://github.com/apache/flink/commit/613650fea229816d2a6390c09aa8ea46c74c6555] into apache:release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Looping pattern notFollowedBy at end missing an element,FLINK-31040,13524458,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Juntao Hu,Juntao Hu,13/Feb/23 11:06,06/Mar/23 03:09,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,Library / CEP,,,,0,,,,,"Pattern: begin(""A"", SKIP_TO_NEXT).oneOrMore().consecutive().notFollowedBy(""B"").within(Time.milliseconds(3))

Sequence: <a1, 1L> <a2, 2L> <a3, 3L> <a4, 4L> <c1, 10L> will produce results [a1, a2], [a2, a3], [a3], which obviously should be [a1, a2, a3], [a2, a3, a4], [a3, a4], [a4].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 03:09:28 UTC 2023,,,,,,,,,,"0|z1fw14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 10:21;martijnvisser;[~nicholasjiang] WDYT?;;;","24/Feb/23 19:21;nicholasjiang;[~Juntao Hu], [~martijnvisser], thanks for the reporter. Looping pattern notFollowedBy at end indeed misses an element. Could you like to fix the missing?;;;","06/Mar/23 03:09;Juntao Hu;[~nicholasjiang], [~martijnvisser] I could take this, but since it needs mofication on generated NFA, I can't guarantee to fix this in latest minor version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogWithKeyFileStoreTableITCase in table store is not stable,FLINK-31039,13524456,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,13/Feb/23 10:52,29/Mar/23 02:03,04/Jun/24 20:41,29/Mar/23 02:03,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,0,,,,,"FAILURE! - in org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase
Error:  testFullCompactionChangelogProducerStreamingRandom  Time elapsed: 600.077 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 600000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:244)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.checkFullCompactionTestResult(ChangelogWithKeyFileStoreTableITCase.java:395)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerRandom(ChangelogWithKeyFileStoreTableITCase.java:343)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerStreamingRandom(ChangelogWithKeyFileStoreTableITCase.java:300)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)

[INFO] 
[INFO] Results:
[INFO] 
Error:  Errors: 
Error:    ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerStreamingRandom:300->testFullCompactionChangelogProducerRandom:343->checkFullCompactionTestResult:395 » TestTimedOut

https://github.com/apache/flink-table-store/actions/runs/4161755735/jobs/7200106408",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-13 10:52:45.0,,,,,,,,,,"0|z1fw0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid accessing non-TableStore tables in HiveCatalog.listTables,FLINK-31038,13524447,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Feb/23 10:23,13/Feb/23 12:17,04/Jun/24 20:41,13/Feb/23 12:17,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"In HiveCatalog.listTables, in the current implementation, getTable will be called for each TableName. However, the environment here may not be able to access non-TableStore tables.
We can avoid access non-TableStore tables by judging whether it is a TableStore table in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 12:17:00 UTC 2023,,,,,,,,,,"0|z1fvyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 12:17;lzljs3620320;master: bd6036cc1f9cc853e04f45dac7b65b66ffdb669f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store supports streaming reading a whole snapshot in one checkpoint,FLINK-31037,13524442,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lzljs3620320,lzljs3620320,13/Feb/23 10:12,13/Feb/23 10:12,04/Jun/24 20:41,,,,,,,,,,,,,table-store-0.4.0,,,,,,,,0,,,,,"At present, in the streaming reading of tablestore, the checkpoint may be performed when a snapshot is not read completely, and then a single snapshot may be cut into multiple slices, and the downstream will see the intermediate state.

In some scenarios, this intermediate state is not allowed. We need to support a mode to prohibit this situation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-13 10:12:00.0,,,,,,,,,,"0|z1fvxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateCheckpointedITCase timed out,FLINK-31036,13524431,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,mapohl,mapohl,13/Feb/23 09:22,27/Feb/23 10:33,04/Jun/24 20:41,21/Feb/23 10:03,1.17.0,,,,,,,,,,,1.17.0,1.18.0,,,Runtime / Checkpointing,Tests,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608

{code}
""Legacy Source Thread - Source: Custom Source -> Filter (6/12)#69980"" #13718026 prio=5 os_prio=0 tid=0x00007f05f44f0800 nid=0x128157 waiting on condition [0x00007f059feef000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f0a974e8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:384)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:356)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:414)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:390)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForRecordContinuation(BufferWritingResultPartition.java:328)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:161)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:105)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:91)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39)
	at org.apache.flink.streaming.runtime.io.RecordProcessorUtils$$Lambda$1311/1256184070.accept(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	- locked <0x00000000d55035c0> (a java.lang.Object)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.test.checkpointing.StateCheckpointedITCase$StringGeneratingSourceFunction.run(StateCheckpointedITCase.java:178)
	- locked <0x00000000d55035c0> (a java.lang.Object)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-26803,,,,,FLINK-31138,,,,,,,,"16/Feb/23 12:29;fanrui;image-2023-02-16-20-29-52-050.png;https://issues.apache.org/jira/secure/attachment/13055511/image-2023-02-16-20-29-52-050.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 15:16:08 UTC 2023,,,,,,,,,,"0|z1fvv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 09:25;mapohl;[~roman] [~ym] can you help identifying the cause? I struggle to pin-point the change that might have caused the issue.;;;","13/Feb/23 09:56;roman;Thanks [~mapohl] ,

I briefly looked at the issue and it'ss related to State Backends, but rather to Unaligned Checkpoints (channel state writer).

 

Also not sure if this is a deadlock:
 * Legacy Source Thread locked 0x00000000a29b0ae8 (""checkpoint lock"") 
 * multiple instances of ChannelStateWriteRequestExecutorFactory are trying to acquire it (as part of task initialization)

The latter was introduced in FLINK-26803.

[~fanrui] I beleive you have more context, could you please take a look?

cc: [~pnowojski] ;;;","13/Feb/23 10:19;fanrui;Thanks [~mapohl]  reports this issue, and thanks [~roman] help analyze it, I will take a look asap~;;;","14/Feb/23 10:44;fanrui;Hi [~mapohl] [~roman] , I'm sorry, after analysis, I didn't find the root cause, I just suspect it's not caused by FLINK-26803.

From jstack we can know that StateCheckpointedITCase has timed out. The logic of this test is: OnceFailingAggregator fails after running for a while, and then tests whether the checkpoint meets expectations. 

I suspect that the test has encountered some problems and is restarting frequently, so the tasks are frequently initialized. FLINK-26803 added a lock in TM level during task is initializing, so we see that many tasks are waiting for locks.

I want to check the log of the test to analyze what these tasks are doing, however I didn't found it here[1]. Whether the log is too big due to it runs for 4 hours? And I checked the upload log[2], it can't be shown.

 

Please correct me if I'm wrong. BTW, I run this test locally and it always succeeds. Does this test fail only once?

 

[1] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=artifacts&pathAsName=false&type=publishedArtifacts

[2]https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=8856ec62-ca1b-53d4-5e1a-5db3770f6c8a;;;","14/Feb/23 10:55;mapohl;{quote}
BTW, I run this test locally and it always succeeds. Does this test fail only once?
{quote}

so far, I'm only aware of this one test failure.;;;","15/Feb/23 02:14;fanrui;{quote} I'm only aware of this one test failure.
{quote}
Thanks for your feedback, I can continue the analysis  after it happens again and hope the log can be viewed next time.

Or if other masters have some ideas, welcome to give feedback here.;;;","15/Feb/23 15:07;mapohl;Why would you assume to get more from the logs than what is already offered in the most-recent test failure? Did you add more log messages? I'm just wondering because there's no PR opened for that issue, yet.;;;","16/Feb/23 03:08;fanrui;{quote}Why would you assume to get more from the logs than what is already offered in the most-recent test failure?
{quote}
I suspect that the test has encountered some problems and is restarting frequently, so the tasks are frequently initialized. FLINK-26803 added a lock at TM level during task is initializing, so we see that many tasks are waiting for locks. 

I want to look at the logs of the failed tests to analyze what those tasks are doing, and what's wrong with this test?

 
{quote}Did you add more log messages?
{quote}
 

I didn't add any log, the current log is enough. We can use the log to confirm whether the test job has been restarting and the reason for restarting.

 

I have run it dozens of times locally, but I can't reproduce it. If it can be reproduced and has a log, it should be helpful to find the root cause.

 ;;;","16/Feb/23 07:33;mapohl;{quote}
FLINK-26803 added a lock at TM level during task is initializing, so we see that many tasks are waiting for locks. I want to look at the logs of the failed tests to analyze what those tasks are doing, and what's wrong with this test?
{quote}
I see. But I still miss to understand why the current stacktrace isn't enough to do so when we don't add more logs and are fine with what the current logs are revealing?
The reason I am asking is because we might want to come up with a strategy if the issue cannot be resolved before the rc creation is started (because it might not appear that frequent). Reverting FLINK-26803 by then sounds reasonable considering that it's ""only"" an improvement. WDYT?;;;","16/Feb/23 07:48;fanrui;Hi, [~pnowojski] , do you have time help take a look this JIRA?

I have analyzed the stack, but I didn't find why `StateCheckpointedITCase` failed, thanks a lot.:);;;","16/Feb/23 08:25;fanrui;{quote}Reverting FLINK-26803 by then sounds reasonable considering that it's ""only"" an improvement. WDYT?
{quote}
Hi [~mapohl] , after my analysis, any bugs that cause jobs to be restarted or checkpoints not to be recovered can cause this stacktrace. This stack runs on FLINK-26803 code, but may not be caused by FLINK-26803.

Has flink-1.17 or master branch fixed some bugs that caused jobs to be restarted or checkpoints not to be restored?

 

BTW, StateCheckpointedITCase also enabled the ChangeLogStateBackend [1],  I see FLINK-28440 and FLINK-30561 is merged recently. It is possible that the root cause has been resolved, and StateCheckpointedITCase will always succeeds in the future.

So after reverting FLINK-26803, if StateCheckpointedITCase always succeeds, it does not prove that FLINK-26803 caused the problem. WDYT?

cc [~pnowojski] [~roman] 

Please correct me if I'm wrong.

[1] https://github.com/apache/flink/blob/b64739a5ef976e003bf87250b41ae1142e541497/flink-tests/src/test/java/org/apache/flink/test/checkpointing/StreamFaultToleranceTestBase.java#L91

 ;;;","16/Feb/23 08:34;fanrui;I see that StateCheckpointedITCase fails when this commit[1] is merged.

I have an idea: Based on this commit, use a script to run StateCheckpointedITCase(with LOG enabled) repeatedly to see if it can be reproduced. I'll update here when I come to any conclusions.

[1]https://github.com/flink-ci/flink-mirror/commit/331abad6dd2d6ec79ecde7116f3b48f2a249b9ef;;;","16/Feb/23 08:55;mapohl;Is this a copy&paste error? The commit [331abad6d|https://github.com/flink-ci/flink-mirror/commit/331abad6dd2d6ec79ecde7116f3b48f2a249b9ef] points to a docs change in {{flink-ci/flink-mirror}};;;","16/Feb/23 09:05;fanrui;{quote}Is this a copy&paste error? The commit [331abad6d|https://github.com/flink-ci/flink-mirror/commit/331abad6dd2d6ec79ecde7116f3b48f2a249b9ef] points to a docs change in {{flink-ci/flink-mirror}}
{quote}
No, I copied the commit from the failed CI [1], I'm not saying this commit is the root cause, I mean: root cause should be before this commit, so I run the StateCheckpointedITCase based on this commit to troubleshoot.

 

[1] [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608];;;","16/Feb/23 09:43;mapohl;Looks like it's not really a one-time thing:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608;;;","16/Feb/23 10:09;fanrui;{quote}Looks like it's not really a one-time thing:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608]
{quote}
It’s your first link, right?;;;","16/Feb/23 10:26;mapohl;Yes, sorry for the confusion. I must have ran into a copy&paste error when comparing the builds. Here's the actual one:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46199&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=11651
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007fce4000b800 nid=0x28152 waiting on condition [0x00007fce478dc000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000a468c860> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:99)
	at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:136)
[...]
{code}

It doesn't state the actual ITCase in the stacktrace. {{StateCheckpointedITCase}} [is started|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46199&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10020] but never finished based on the Maven output.;;;","16/Feb/23 11:12;pnowojski;I will try to take a quick look.

When trying to reproduce it locally, keep in mind that we have some parameters/configuraiton randomisation implemented AFAIR based on the git commit. Most likely one of the parameters that gets randomised is unaligned checkpoints turned on/off, so if the failures are happening only in one of those modes, make sure that locally you are using the same setting. If it's randomised based on the git commit hash, then it's probably best to just loop the test on the same commit that has failed in the CI. 

Sometimes it also helps to stress the local machine much more, like loop the same test 4 or 8 times running in parallel. ;;;","16/Feb/23 12:34;fanrui;{quote}Here's the actual one:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46199&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=11651]
{quote}
Thanks a lot for the link, it's very very useful, It has log. The exception is `java.lang.RuntimeException: Test failed due to unexpected recovered state size 0`. I will continue to analyze why the restored state is wrong. 

!image-2023-02-16-20-29-52-050.png!;;;","16/Feb/23 16:26;pnowojski;In this recent log I see two issues

# {{checkpoint 20}} is failing due to ""Size of the state is larger than the maximum permitted memory-backed state. Size=5621456, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage""
# recovery from {{checkpoint 19}} is failing because ""java.lang.RuntimeException: Test failed due to unexpected recovered state size 0""



# Is probably caused by FLINK-26803, probably a benign configuration issue
# is just a minor bug/unsupported case in this test, since shortly before {{checkpoint 19}}, some tasks have finished. {{StateCheckpointedITCase.StringRichFilterFunction#restoreState}} simply doesn't support that. This test was created before FLIP-147 and doesn't expect the second failover caused by the 1.;;;","17/Feb/23 03:51;fanrui;Hi [~pnowojski] , thanks a lot for the analysis, sounds makes sense to me.
{quote}{{checkpoint 20}} is failing due to ""Size of the state is larger than the maximum permitted memory-backed state. Size=5621456, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage""

Is probably caused by FLINK-26803, probably a benign configuration issue
{quote}
The maxSize of memory checkpoint stream cannot be changed, and I see some comments in the `JobManagerCheckpointStorage`, it means we cannot increase the maxSize due to the limitation in the PRC side.
{code:java}
<p><b>WARNING:</b> Increasing the size of this value beyond the default value ({@value
* #DEFAULT_MAX_STATE_SIZE}) should be done with care. The checkpointed state needs to be send
* to the JobManager via limited size RPC messages, and there and the JobManager needs to be
* able to hold all aggregated state in its memory.{code}
Also, I think all tests may have this problem, especially some tests with high parallelism. It is not introduced by FLINK-26803. If the test of the high parallelism job does not merge the channel state file, this problem may also exist.

So should we use the `FileSystemCheckpointStorage` instead of `JobManagerCheckpointStorage`?

 
{quote}Recovery from {{checkpoint 19}} is failing because ""java.lang.RuntimeException: Test failed due to unexpected recovered state size 0""

Is just a minor bug/unsupported case in this test, since shortly before {{{}checkpoint 19{}}}, some tasks have finished. {{StateCheckpointedITCase.StringRichFilterFunction#restoreState}} simply doesn't support that. This test was created before FLIP-147 and doesn't expect the second failover caused by the 1.
{quote}
As I understand, when problem 1 is solved, this problem will also be solved incidentally.

However, I'm not sure if these tests are robust after FLIP-147. When OnceFailingAggregator throw exception after some source tasks are finished, StateCheckpointedITCase will also fail(This is more likely to happen when the dataset is small.). Therefore, I guess there may be many old checkpoint-related tests that have similar risks after FLIP-147.;;;","17/Feb/23 07:56;pnowojski;{quote}
So should we use the `FileSystemCheckpointStorage` instead of `JobManagerCheckpointStorage`?
{quote}
Indeed that seems like the only feasible solution. Do you mean setting it for this test, or for all of the unit tests? ITCases? What would be the scope of such change?

Re the 2nd problem, I think indeed the problem should go away. Nevertheless it would be nice if there is an easy way to fix this in  {{StateCheckpointedITCase}}, but that's I think optional.;;;","20/Feb/23 14:37;mapohl;Is FLINK-31138 a duplicate of this Jira issue?;;;","20/Feb/23 14:47;fanrui;merged commit ae53a8de47b31e248fef216428a83c8bf3f07b13 into apache:release-1.17

merged commit 6fae8b58a49216d50e422d5e4e7be3d7ea3b4462 into apache:master;;;","20/Feb/23 15:16;fanrui;{quote}Is FLINK-31138 a duplicate of this Jira issue?
{quote}
Hi [~mapohl] , I did a quick checked, it isn't similar to this Jira, I have commented there. But I don't find the root cause now, I can continue analyze it tomorrow.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add warn info to user when NoNodeException happend,FLINK-31035,13524430,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuzifu,xuzifu,xuzifu,13/Feb/23 09:18,12/Jul/23 15:26,04/Jun/24 20:41,12/Jul/23 15:26,1.8.4,,,,,,,,,,,1.18.0,,,,Runtime / Queryable State,,,,0,pull-request-available,,,,"when KeeperException.NoNodeException happens in 
ZooKeeperStateHandleStore::getAllHandles，we need logs to retrace",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 15:26:13 UTC 2023,,,,,,,,,,"0|z1fvuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 15:26;roman;Merged into master as f89be70c2b00522ccde0c47f45c2236acdd4ad6d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy Maven Snapshot failed,FLINK-31034,13524429,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,mapohl,mapohl,13/Feb/23 09:13,21/Aug/23 08:54,04/Jun/24 20:41,21/Aug/23 08:54,1.15.3,1.16.1,,,,,,,,,,,,,,Build System / CI,,,,0,auto-deprioritized-major,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46022&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=851

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.8.2:deploy (default-deploy) on project flink-parent: Failed to deploy artifacts: Could not transfer artifact org.apache.flink:flink-parent:pom:1.16-20230211.011900-322 from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): Failed to transfer file: https://repository.apache.org/content/repositories/snapshots/org/apache/flink/flink-parent/1.16-SNAPSHOT/flink-parent-1.16-20230211.011900-322.pom. Return code is: 502, ReasonPhrase: Proxy Error. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
##[error]Bash exited with code '1'.
{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-30104,,,FLINK-30869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:08 UTC 2023,,,,,,,,,,"0|z1fvuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 09:14;mapohl;It shouldn't be related to FLINK-30869 but I'm mentioning it anyway because I renamed the remote branch {{cron-master-maven_compat}} last week which sounds related.;;;","13/Feb/23 14:28;mapohl;It looks like a temporary connection issue. I'm gonna keep this one open for now to track how often it appears. We can close it in the future if it appears to be a rare issue.;;;","06/Mar/23 09:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46800&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=12524;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UsingRemoteJarITCase.testUdfInRemoteJar failed with assertion,FLINK-31033,13524426,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,mapohl,mapohl,13/Feb/23 09:01,29/Nov/23 01:54,04/Jun/24 20:41,29/Nov/23 01:54,1.16.2,1.17.1,1.18.0,,,,,,,,,1.17.3,1.18.1,1.19.0,,Table SQL / API,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,"{{UsingRemoteJarITCase.testUdfInRemoteJar}} failed with assertion:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46009&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=18050

{code}
Feb 10 15:28:15 [ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 249.499 s <<< FAILURE! - in org.apache.flink.table.sql.codegen.UsingRemoteJarITCase
Feb 10 15:28:15 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar  Time elapsed: 40.786 s  <<< FAILURE!
Feb 10 15:28:15 org.opentest4j.AssertionFailedError: Did not get expected results before timeout, actual result: [{""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":1},""op"":""c""}, {""before"":null,""after"":{""user_name"":""Alice"",""order_cnt"":1},""op"":""c""}, {""before"":{""user_name"":""Bob"",""order_cnt"":1},""after"":null,""op"":""d""}, {""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":2},""op"":""c""}]. ==> expected: <true> but was: <false>
Feb 10 15:28:15 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
Feb 10 15:28:15 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40)
Feb 10 15:28:15 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:210)
Feb 10 15:28:15 	at org.apache.flink.table.sql.codegen.SqlITCaseBase.checkJsonResultFile(SqlITCaseBase.java:168)
Feb 10 15:28:15 	at org.apache.flink.table.sql.codegen.SqlITCaseBase.runAndCheckSQL(SqlITCaseBase.java:111)
Feb 10 15:28:15 	at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.testUdfInRemoteJar(UsingRemoteJarITCase.java:106)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-29007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 01:54:00 UTC 2023,,,,,,,,,,"0|z1fvts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 13:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47390&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15972;;;","15/May/23 03:56;Weijie Guo;testCreateTemporarySystemFunctionUsingRemoteJar also report the same error:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48964&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=12631;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 11:13;mapohl;I'm wondering whether it would be enough here to remove the deadline for verifying the test. It's hard to investigate anything because we don't have any other logs.;;;","28/Nov/23 06:49;leonard;Fixed in master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52;;;","28/Nov/23 09:04;mapohl;Sorry for reopening this one. But it would be helpful to backport test instability fixes as well if the error also appeared in older releases.;;;","29/Nov/23 01:54;leonard;Fixed in

master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52

release-1.18：b3b7240cc34e552273b26d8090d45e492474c9ea

release-1.17: 0053db03772a70c70de0516cc46f7ab363dc74f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports AND predicate in orc format for table store,FLINK-31032,13524424,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,13/Feb/23 08:51,14/Feb/23 06:22,04/Jun/24 20:41,14/Feb/23 06:22,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Supports `AND` predicate push down in orc format,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 06:22:56 UTC 2023,,,,,,,,,,"0|z1fvtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 06:22;lzljs3620320;master: 39e8b7744713c3219f83de73405a9f2c2d9c391c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable the output buffer of Python process to make it more convenient for interactive users ,FLINK-31031,13524409,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,13/Feb/23 06:52,13/Feb/23 08:42,04/Jun/24 20:41,13/Feb/23 08:42,,,,,,,,,,,,1.15.4,1.16.2,1.17.0,,API / Python,,,,0,,,,,See [https://apache-flink.slack.com/archives/C03G7LJTS2G/p1676270127585559?thread_ts=1676238623.588979&cid=C03G7LJTS2G] for more details.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 08:42:14 UTC 2023,,,,,,,,,,"0|z1fvq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 08:42;dianfu;Fixed in:
- master via e22f06130a8c663ab454586102c88ba403beee56
- release-1.17 via 5344b8a59afbb17740ee363f22fe79fe0d5d50b2
- release-1.16 via f0e0069a74103a303f4ce49a65c5c27eecb424d1
- release-1.15 via 2fa360ae387215455b536633808d6a82ca1e17ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support more binary classification evaluation metrics.,FLINK-31030,13524407,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hongfanxo,hongfanxo,13/Feb/23 06:34,15/Aug/23 09:22,04/Jun/24 20:41,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Current `BinaryClassificationEvaluator` only supports 'areaUnderROC', 'areaUnderPR', 'ks' and 'areaUnderLorenz'. We should support more evaluation metrics, some of which are basic ones, e.g., precision, recall, F-measure, and so on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 06:06:42 UTC 2023,,,,,,,,,,"0|z1fvpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 06:06;complone;I am interested in this, and I will start working on it after I sort out the execution process of the BinaryClassificationEvaluator#transform method;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KBinsDiscretizer gives wrong bin edges in 'quantile' strategy when input data contains only 2 distinct values,FLINK-31029,13524405,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,hongfanxo,hongfanxo,13/Feb/23 06:23,01/Apr/23 02:52,04/Jun/24 20:41,01/Apr/23 02:52,,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"When one input column contains only 2 distinct values and their counts are same, KBinsDiscretizer transforms this column to all 0s using `quantile` strategy. An example of such column is `[0, 0, 0, 1, 1, 1]`.

When the 2 distinct values have different counts, the transformed values are also all 0s, which cannot distinguish them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 01 02:52:37 UTC 2023,,,,,,,,,,"0|z1fvp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/23 02:52;lindong;Merged to apache/flink-ml master branch 5dacbd97429a525b0f7e81931f55f3d87f79de57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide different compression methods for per level,FLINK-31028,13524399,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,13/Feb/23 05:13,17/Feb/23 05:54,04/Jun/24 20:41,17/Feb/23 05:54,table-store-0.3.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,"Different compression are provided for different levels.

For level 0 ,because the amount of data in this level is not large, we do not want to use compression in exchange for better write performance . For normal levels, we use lz4 . For the last level, access is generally less and data volume is large. we hope to use gzip to reduce space size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 05:54:08 UTC 2023,,,,,,,,,,"0|z1fvns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 05:54;lzljs3620320;master: f5850b6f9ce71818e3391436697c07c4c433871b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce annotation for table store,FLINK-31027,13524397,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,13/Feb/23 05:10,14/Feb/23 03:03,04/Jun/24 20:41,14/Feb/23 03:03,table-store-0.4.0,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,Introduce annotation for table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 03:03:22 UTC 2023,,,,,,,,,,"0|z1fvnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 03:03;lzljs3620320;master: 706a92ffeea7cdf8467d7839aee86132eae50eb5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KBinsDiscretizer gives wrong bin edges when all values are same.,FLINK-31026,13524391,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,13/Feb/23 04:02,16/Feb/23 11:38,04/Jun/24 20:41,16/Feb/23 11:38,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Current implements gives bin edges of \{Double.MIN_VALUE, Double.MAX_VALUE} when all values are same.
However, this bin cannot cover negative values and 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 11:38:04 UTC 2023,,,,,,,,,,"0|z1fvm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 11:38;zhangzp;Fixed on master via 0af95f256d438a12946f9152c0a65ec916a61323;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-30650 Introduce EXPLAIN PLAN_ADVICE to provide SQL advice,FLINK-31025,13524381,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,Weijie Guo,qingyue,qingyue,13/Feb/23 03:02,20/Feb/23 02:45,04/Jun/24 20:41,20/Feb/23 02:45,1.17.0,,,,,,,,,,,1.17.0,,,,Table SQL / API,,,,0,,,,,"This ticket aims for verifying FLINK-30650: Introduce EXPLAIN PLAN_ADVICE to provide SQL advice.

More details about this feature and how to use it can be found in this [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/explain/].

The verification is divided into two parts:

Part I: Verify ""EXPLAIN PLAN_ADVICE"" can work with different queries under the streaming mode, such as a single select/insert/statement set w/ or w/o sub-plan reuse (configured by ""table.optimizer.reuse-sub-plan-enabled"").

This indicates once specifying the ExplainDetail as ""PLAN_ADVICE""
{code:sql}
EXPLAIN PLAN_ADVICE [SELECT ... FROM ...| INSERT INTO ...| EXECUTE STATEMENT SET BEGIN INSERT INTO ... END]
{code}
You should find the output should be like the following format (note that the title is changed to ""Optimized Physical Plan with Advice"")
{code:sql}
== Abstract Syntax Tree ==
...
 
== Optimized Physical Plan With Advice ==
...
 
 
== Optimized Execution Plan ==
...
{code}
The available advice is attached at the end of ""== Optimized Physical Plan With Advice =="", and in front of  ""== Optimized Execution Plan ==""

If switching to batch mode, you should find the ""EXPLAIN PLAN_ADVICE"" should throw UnsupportedOperationException as expected.

 

Part II: Verify the advice content

Write a group aggregate query, and enable/disable the local-global two-phase configuration, and test the output.

You should find once the following configurations are enabled, you will get the ""no available advice..."" output.
{code:java}
table.exec.mini-batch.enabled
table.exec.mini-batch.allow-latency
table.exec.mini-batch.size
table.optimizer.agg-phase-strategy {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 06:48;Weijie Guo;image-2023-02-13-14-48-53-758.png;https://issues.apache.org/jira/secure/attachment/13055370/image-2023-02-13-14-48-53-758.png","13/Feb/23 06:52;Weijie Guo;image-2023-02-13-14-52-41-855.png;https://issues.apache.org/jira/secure/attachment/13055371/image-2023-02-13-14-52-41-855.png","13/Feb/23 06:46;Weijie Guo;test1.png;https://issues.apache.org/jira/secure/attachment/13055369/test1.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 06:13:29 UTC 2023,,,,,,,,,,"0|z1fvjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 06:41;Weijie Guo;Thanks [~qingyue] for creating this, I'd like to do this testing work.;;;","13/Feb/23 06:54;Weijie Guo;I verified this feature according to the description, and everything seems to meet expectations.

 

Verification steps:

I just create a simple table like 
```
CREATE TABLE MyTable(
f1 INT,
f2 STRING
) WITH (
'connector' = 'filesystem',
'path' = '/data.csv',
'format' = 'csv'
);
```
1. Execute `EXPLAIN PLAN_ADVICE SELECT max(f1) FROM MyTable GROUP BY f2;`, the results are shown in the figure below:
!test1.png|width=563,height=326!

We can see the advice before `Optimized Execution Plan`.

2. Set runtime-mode to batch, re-execute the explain cmd, the results are shown in the figure below:

!image-2023-02-13-14-48-53-758.png|width=573,height=99!

3. Set parameters as follow:

```
SET 'table.exec.mini-batch.enabled' = 'true';
SET 'table.exec.mini-batch.allow-latency' = '5s';
SET 'table.exec.mini-batch.size' = '200';
SET 'table.optimizer.agg-phase-strategy' = 'AUTO';
```

re-execute the explain cmd, the result are shown in the figure blow:

!image-2023-02-13-14-52-41-855.png|width=475,height=244!;;;","13/Feb/23 07:04;Weijie Guo;BTW. I also found a typo in `explain.md`, I have created `[https://github.com/apache/flink/pull/21913]` to fix this, could you help reviewing this?;;;","15/Feb/23 06:13;qingyue;[~Weijie Guo] Thanks for testing thoroughly and finding the typo! I'll take a look right now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Copy code splitter to table store from flink table,FLINK-31024,13524349,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,12/Feb/23 07:53,13/Feb/23 12:15,04/Jun/24 20:41,13/Feb/23 12:15,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 12:15:15 UTC 2023,,,,,,,,,,"0|z1fvcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 12:15;lzljs3620320;master: a9b33ad59f08ae8b3ba19291dc166371f529719f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ConfigOption for table store,FLINK-31023,13524328,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Feb/23 15:55,14/Feb/23 06:26,04/Jun/24 20:41,14/Feb/23 06:26,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 06:26:30 UTC 2023,,,,,,,,,,"0|z1fv88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 06:26;lzljs3620320;master: 1796d3fa960169649299bb4bdaffac6d3848718e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using new Serializer for table store,FLINK-31022,13524324,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Feb/23 13:13,13/Feb/23 07:57,04/Jun/24 20:41,13/Feb/23 07:57,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 07:57:16 UTC 2023,,,,,,,,,,"0|z1fv7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 07:57;lzljs3620320;master: 5b6ff938922aa067b59a0132c62e6cd675680b63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaCodeSplitter doesn't split static method properly,FLINK-31021,13524309,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xccui,xccui,11/Feb/23 03:28,14/Feb/23 04:31,04/Jun/24 20:41,,1.14.4,1.15.3,1.16.1,,,,,,,,,,,,,,,,,1,,,,,"The exception while compiling the generated source
{code:java}
cause=org.codehaus.commons.compiler.CompileException: Line 3383, Column 90: Instance method ""default void org.apache.flink.formats.protobuf.deserialize.GeneratedProtoToRow_655d75db1cf943838f5500013edfba82.decodeImpl(foo.bar.LogData)"" cannot be invoked in static context,{code}
The original method header 
{code:java}
public static RowData decode(foo.bar.LogData message){{code}
The code after split
 
{code:java}
Line 3383: public static RowData decode(foo.bar.LogData message){ decodeImpl(message); return decodeReturnValue$0; } 
Line 3384:
Line 3385: void decodeImpl(foo.bar.LogData message) {{code}
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 04:31:52 UTC 2023,,,,,,,,,,"0|z1fv40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 06:14;KristoffSC;Hi, i have few questions.
1. Could you provide full body of original decode method? 
 2. do you have sql query that reproduces the problem?

3. You marked affect version as 1.16.1 and below. Did you in fact had this on those or on a current master? Im asking because recently there was a change in code splitter merged to master 1.17 and 1.16 release that is not included in 1.16.1 so I'm wondering if this is a regression or something new.

Let me know,
Cheers.;;;","11/Feb/23 14:09;KristoffSC;Ok so I verified and it seems that this is NOT a regression caused by my recent change to Code Splitter. 
It seems that splitting static methods was never supported. I've checked on 1.15 branch.

I think that the question we should ask here is this in fact a bug and do we need to make Code Splitter to handle Static methods? Quating [~TsReaper] from https://github.com/apache/flink/pull/21393#pullrequestreview-1273870828
{code:java}
Our code splitter is not a universal solution. It only works for Flink generated code under several restrictions.
{code}

Having said that, [~xccui] is there Flink SQL query that makes Planner to generate Java code with static methods? If so, could you provide one?

If in fact this is needed feature I can work on it since recently I've made bigger changes to code splitter and I would be fairly easy for me to add this.


[~TsReaper] What do you think?
;;;","13/Feb/23 05:05;xccui;I'm playing with [https://github.com/apache/flink/blob/c096c03df70648b60b665a09816635b956b201cc/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/deserialize/ProtoToRowConverter.java#L98] 

The generated converter source is too large sometimes. I fixed it locally by removing the static keyword for now. FYI [~libenchao] 

It's fine if the code splitter doesn't support static methods. But at least we should inform users with a proper message instead of generating incorrect code.

 ;;;","13/Feb/23 05:23;libenchao;[~xccui] Thanks for involving me.

I totally agree that the generated code should also be split because it will hit some JIT optimization limitations when the code grows too large. Do we have any issue tracking this?

I'm fine with both way:
- change flink-protobuf to do not use static methods

- change code-splitter to support static methods;;;","14/Feb/23 04:31;xccui;[~libenchao] Thanks for the comments. 

I'm a bit busy these days. Will try to replace the static methods with non-static ones for {{flink-protobuf}} if no one works on this before I get some time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read-only mode for Rest API,FLINK-31020,13524307,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,omkardeshpande8,omkardeshpande8,11/Feb/23 01:59,23/Feb/23 08:20,04/Jun/24 20:41,23/Feb/23 08:20,1.16.1,,,,,,,,,,,,,,,Runtime / REST,,,,0,,,,,"We run Flink jobs on application cluster on Kubernetes. We don't submit/cancel or modify jobs from rest API or web UI. If there was an option to enable only GET operations on the rest service, it would greatly solve the problem of configuring access control and reduce the attack surface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 03:24:16 UTC 2023,,,,,,,,,,"0|z1fv3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 14:47;Wencong Liu;Thanks [~omkardeshpande8] for the proposal! I think it is a tricky behavior to only allow GET operations. We cannot guarantee that REST APIs other than submit/cancel/modify do not use POST/PUT operations on the web UI. If you think it's unsafe, you can disable the rest server.;;;","15/Feb/23 15:03;chesnay;??We cannot guarantee that REST APIs other than submit/cancel/modify do not use POST/PUT operations on the web UI.??

This statement doesn't make any sense.

If we were to disable all mutating operations in the REST API, then any attempts from the UI to use these APIs will simply fail.


The bigger issue to me is that this is an extremely specific use-case that is only applicable in application mode where you _never_ even cancel a job with a savepoint.;;;","16/Feb/23 02:52;Wencong Liu;Sorry [~chesnay] , my statement may not be very accurate. My opinion is consistent with yours. Directly disabling mutating API may affect the normal operation of the web UI.;;;","16/Feb/23 03:24;xtsong;Speaking of attacks, the biggest concern is usually in being able to submit arbitrary jobs that may contain malicious codes. This is not possible in application mode, because the rest api for submitting jobs is already disabled.

For other non-GET apis (stoping/canceling jobs, triggering checkpoints/savepoints), I agree with [~chesnay] that they should not be disabled in general.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate FileSystemTableSink to FileSink,FLINK-31019,13524285,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,10/Feb/23 20:35,10/Feb/23 20:38,04/Jun/24 20:41,10/Feb/23 20:37,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"{{FileSystemTableSink}} currently depends on most of the capabilities from {{StreamingFileSink}}, for example https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/FileSystemTableSink.java#L223-L243

This is necessary to complete FLINK-28641",,,,,,,,,,,,,,,,,,,,,FLINK-30627,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-10 20:35:52.0,,,,,,,,,,"0|z1fuyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client -j option does not load user jars to classpath.,FLINK-31018,13524242,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Bug,,KristoffSC,KristoffSC,10/Feb/23 15:05,10/Feb/23 17:41,04/Jun/24 20:41,10/Feb/23 17:39,1.16.1,1.17.0,,,,,,,,,,,,,,Table SQL / Client,,,,0,,,,,"SQL Client '-j' option does not load custom jars to classpath as it was for example in Flink 1.15
As a result Flink 1.16 SQL Client is not able to discover classes through Flink's Factory discovery mechanism throwing an error like:

{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Could not find any factories that implement 'com.getindata.connectors.http.LookupQueryCreatorFactory' in the classpath.
{code}

The same Jar and sample job are working fine with Flink 1.15.

Flink 1.15.2
./bin/sql-client.sh -j flink-http-connector-0.9.0.jar
 !image-2023-02-10-15-53-39-330.png! 

Flink 1.16.1
./bin/sql-client.sh -j flink-http-connector-0.9.0.jar
 !image-2023-02-10-15-54-32-537.png! 

ADD JAR command does not solve "" Could not find any factories"" issue although jar seems to be added:
 !image-2023-02-10-16-05-12-407.png! 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/23 14:53;KristoffSC;image-2023-02-10-15-53-39-330.png;https://issues.apache.org/jira/secure/attachment/13055353/image-2023-02-10-15-53-39-330.png","10/Feb/23 14:54;KristoffSC;image-2023-02-10-15-54-32-537.png;https://issues.apache.org/jira/secure/attachment/13055352/image-2023-02-10-15-54-32-537.png","10/Feb/23 15:05;KristoffSC;image-2023-02-10-16-05-12-407.png;https://issues.apache.org/jira/secure/attachment/13055351/image-2023-02-10-16-05-12-407.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 17:41:03 UTC 2023,,,,,,,,,,"0|z1fup4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 15:10;martijnvisser;[~KristoffSC] Could this be related to FLINK-15635 ?;;;","10/Feb/23 17:39;KristoffSC;[~martijnvisser] yes it seemt that this is the case.

I;ve used DynamicTableFactory.Context#getClassLoader instead Thread.currentThread().getContextClassLoader() as suggested in one of the comments and it seems that problem disappeared. 

Thanks,
Thicket can be closed.;;;","10/Feb/23 17:41;martijnvisser;Great to hear that fixes it, thanks for updating it!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Early-started partial match timeout not yield completed matches,FLINK-31017,13524233,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/Feb/23 14:35,17/Feb/23 08:29,04/Jun/24 20:41,17/Feb/23 08:28,1.16.0,,,,,,,,,,,1.16.2,1.17.0,,,Library / CEP,,,,0,pull-request-available,,,,"Pattern example:
{code:java}
Pattern.begin(""A"").where(startsWith(""a"")).oneOrMore().consecutive().greedy()
    .followedBy(""B"")
    .where(count(""A"") > 2 ? startsWith(""b"") : startsWith(""c""))
    .within(Time.seconds(3));{code}
Sequence example, currently without any output:

a1 a2 a3 a4 c1

When match[a3, a4, c1] completes, partial match[a1, a2, a3, a4] is earlier, so NFA#processMatchesAccordingToSkipStrategy() won't give any result, which is the expected behavior. However, when partial match[a1, a2, a3, a4] is timed-out, completed match[a3, a4, c1] should be ""freed"" from NFAState to output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 08:28:07 UTC 2023,,,,,,,,,,"0|z1fun4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 15:11;martijnvisser;[~Juntao Hu] Are you committing to fixing this in 1.18? Else I'll remove the fixVersion for now, because that should only be set if indeed it's expected that it will make that version;;;","17/Feb/23 07:33;Juntao Hu;[~martijnvisser] I've changed the tags, thx for reminding;;;","17/Feb/23 08:28;dianfu;Merged to:
- master: 919d46ef91488627a282d1474208490b6dc7a820
- 1.17: 257be14bb47f7481927ea17bc82b4b6ff63e8b30
- 1.16: d61e4a1884e265e0a33f0f4dbad50898df14677f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Run kubernetes pyflink application test"" fails due to Rust package manager not being listed in the PATH",FLINK-31016,13524216,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,mapohl,mapohl,10/Feb/23 11:53,21/Aug/23 09:09,04/Jun/24 20:41,21/Aug/23 09:09,1.16.1,1.17.0,,,,,,,,,,,,,,Test Infrastructure,,,,0,auto-deprioritized-major,test-stability,,,"We experience a test instability in ""Run kubernetes pyflink application test"":
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45972&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=11901

{code}
Feb 09 15:31:22   Preparing metadata (pyproject.toml): finished with status 'error'
Feb 09 15:31:22   error: subprocess-exited-with-error
Feb 09 15:31:22   
Feb 09 15:31:22   × Preparing metadata (pyproject.toml) did not run successfully.
Feb 09 15:31:22   │ exit code: 1
Feb 09 15:31:22   ╰─> [6 lines of output]
Feb 09 15:31:22       
Feb 09 15:31:22       Cargo, the Rust package manager, is not installed or is not on PATH.
Feb 09 15:31:22       This package requires Rust and Cargo to compile extensions. Install it through
Feb 09 15:31:22       the system's package manager or via https://rustup.rs/
Feb 09 15:31:22       
Feb 09 15:31:22       Checking for Rust toolchain....
Feb 09 15:31:22       [end of output]
Feb 09 15:31:22   
Feb 09 15:31:22   note: This error originates from a subprocess, and is likely not a problem with pip.
Feb 09 15:31:22 error: metadata-generation-failed
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 09:09:17 UTC 2023,,,,,,,,,,"0|z1fujc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 11:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45973&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=11816;;;","10/Feb/23 12:11;mapohl;Looks like some version bump caused some incompatibility? [~Yellow] can you have a look?;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 09:09;mapohl;Closing this one because it hasn't reappeared. Additionally, both failures happened in close timely proximity to each other. There was also a Docker repo image pull error for both of them.

It looks like it was an infrastructure issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitReader doesn't extend AutoCloseable but implements close,FLINK-31015,13524208,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taoran,mapohl,mapohl,10/Feb/23 10:53,01/Mar/23 11:43,04/Jun/24 20:41,14/Feb/23 13:24,1.16.1,1.17.0,1.18.0,,,,,,,,,1.18.0,,,,Connectors / Common,,,,0,pull-request-available,,,,"The {{SplitReader}} provides a {{close}} method (see [SplitReader:71|https://github.com/apache/flink/blob/0612a997ddcc791ee54f500fbf1299ce04987679/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java#L71]) but doesn't implement {{AutoCloseable}}. This would enable us to utilize Java's try close feature and use more generic utility classes.

This issue should not only cover the {{SplitReader}} but also solve this kind of problem also in other places of the connector framework (I didn't check whether it also occurs somewhere else). ",,,,,,,,,,FLINK-31281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 13:24:38 UTC 2023,,,,,,,,,,"0|z1fuhk:",9223372036854775807,SplitReader interface now extends AutoCloseable instead of providing its own method signature.,,,,,,,,,,,,,,,,,,,"13/Feb/23 08:29;taoran;[~mapohl]  I think you are right. Can you assign this issue to me? btw. I checked fink-connecor-base and did't find other occurs.  But some other module e.g. 
PythonFunctionRunner:close() and LeaderRetrievalDriver:close() has same issue.;;;","13/Feb/23 08:57;mapohl;Thanks for picking it up. I would treat {{PythonFunctionRunner}} and {{LeaderretrievalDriver}} separately because they are non-public code. {{SplitReader}} is publicly evolving API, relevant for connector implementations and, therefore, helpful for ""external"" projects. This ticket should cover API that is related to the connector/source/sink APIs to have this issue's context clearly defined.;;;","13/Feb/23 09:21;taoran;[~mapohl] Thanks. Got it.;;;","14/Feb/23 13:24;mapohl;master: 057dbc5f62a5fce2b03de1c0a74a064ebbe9ac3e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose exceptions in connector testing framework,FLINK-31014,13524201,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,10/Feb/23 10:26,10/Feb/23 10:31,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,API / DataStream,Test Infrastructure,,,0,,,,,"I noticed for a few interfaces that we don't expose checked exceptions where it's actually useful. {{DataStreamSinkV2ExternalContext.createSink}} or {{ExternalSystemSplitDataWriter.writeRecords}}, for instance, is not exposing any Throwable except for {{UnsupportedOperationException}}. This implies that internal caught exception need to be either caught and transformed into a {{RuntimeException}} or handled internally. AFAIU, that's not what we want. The connector testing framework wants exceptions to be exposed. Therefore, we should improve the interfaces of the framework to allow exceptions to happen (probably depending on the usecase).

This makes it easier for contributors implementing test cases for their connector giving them the freedom to just expose undesired exceptions instead of handling them internally. JUnit should be the actor dealing with these exceptions.",,,,,,,,,,,,,,,,,,,,,,,FLINK-25286,,,,,,,,FLINK-30109,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 10:30:03 UTC 2023,,,,,,,,,,"0|z1fug0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 10:30;mapohl;[~renqs] Does this sound reasonable? Or were there other reasons to be that restrictive in the interfaces?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
