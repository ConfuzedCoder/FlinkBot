Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Outward issue link (Required),Outward issue link (Required),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Shade flink-connector-base info flink-sql-connector-connector,FLINK-33013,13549351,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,jiabao.sun,jiabao.sun,01/Sep/23 03:26,13/Sep/23 06:51,04/Jun/24 20:40,13/Sep/23 06:51,mongodb-1.0.2,,,,,,,,,Connectors / MongoDB,,,,,,0,pull-request-available,,,Shade flink-connector-base info flink-sql-connector-connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 13 06:50:39 UTC 2023,,,,,,,,,,"0|z1k4r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/23 04:07;jiabao.sun;PR is ready https://github.com/apache/flink-connector-mongodb/pull/15;;;","13/Sep/23 06:49;ruanhang1993;Hi,all. I think we should reject this issue and keep the provided scope.

And I will check all connectors about this part in FLINK-30400.;;;","13/Sep/23 06:50;jiabao.sun;Thanks [~ruanhang1993], it make sense to me.

As the [Externalized Connector development|https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development] says. 

Bundling of flink-connector-base
Connectors should not bundle the connector-base module from Flink and instead set it to provided, as contained classes may rely on internal Flink classes.

The flink-connector-base has been put into recent fink dist lib.
Closed this issue because we needn't shaded flink-connector-base into connectors any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderElectionTest.testHasLeadership fails on AZP due to fsync not coming back fast enough,FLINK-33012,13549260,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,31/Aug/23 09:33,31/Aug/23 11:15,04/Jun/24 20:40,,1.16.2,1.17.1,1.18.0,1.19.0,,,,,,Runtime / Coordination,,,,,,0,test-stability,,,"This build [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52871&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=7151]
fails as
{noformat}
Aug 31 01:04:17 Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
Aug 31 01:04:17 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
Aug 31 01:04:17 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:885)
Aug 31 01:04:17 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:1025)
Aug 31 01:04:17 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:986)
Aug 31 01:04:17 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:97)
Aug 31 01:04:17 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:376)
Aug 31 01:04:17 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Aug 31 01:04:17 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
Aug 31 01:04:17 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
Aug 31 01:04:17 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
Aug 31 01:04:17 	at java.base/java.lang.Thread.run(Thread.java:833)
Aug 31 01:04:17 	Suppressed: org.apache.flink.shaded.curator5.org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
Aug 31 01:04:17 		at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:1031)
Aug 31 01:04:17 		... 8 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28523,FLINK-30108,,,,,,,,,,,,,,,"31/Aug/23 10:18;mapohl;FLINK-33012.failure.log;https://issues.apache.org/jira/secure/attachment/13062626/FLINK-33012.failure.log","31/Aug/23 10:18;mapohl;FLINK-33012.zookeeper-client.log;https://issues.apache.org/jira/secure/attachment/13062627/FLINK-33012.zookeeper-client.log","31/Aug/23 10:18;mapohl;FLINK-33012.zookeeper-server.log;https://issues.apache.org/jira/secure/attachment/13062628/FLINK-33012.zookeeper-server.log",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 11:14:51 UTC 2023,,,,,,,,,,"0|z1k46w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 09:36;Sergey Nuyanzin;[~mapohl] Could it be related to FLINK-26522 ?;;;","31/Aug/23 10:01;mapohl;Thanks for reporting this issue. I looked into the logs. It's an infrastructure issue where the fsync didn't come back fast enough:
{code}
[...]
01:03:36,229 [        SyncThread:0] INFO  org.apache.zookeeper.server.persistence.FileTxnLog           [] - Creating new log file: log.1
01:04:16,985 [        SyncThread:0] WARN  org.apache.zookeeper.server.persistence.FileTxnLog           [] - fsync-ing the write ahead log in SyncThread:0 took 40754ms which will adversely effect operation latency.File size is 67108880 bytes. See the ZooKeeper troubleshooting guid
[...]
{code}
This delay of 40s caused the trouble and made the system unstable. I'm gonna attach the failure-related logs to this ticket. But we've see test instabilities due to the same cause in the past (e.g. FLINK-30108, FLINK-28523). I'm gonna check whether we can make the test more stable in this regard.;;;","31/Aug/23 10:17;mapohl;I added 1.18, 1.17 and 1.16 as affected versions and lowered the priority to {{Major}} to reflect that it's not a constant problem. We can increase the priority again if it pops up more regularly (but which I don't assume).;;;","31/Aug/23 11:14;mapohl;I would leave this one open and see whether it pops up again. But generally, I would blame the infrastructure for it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator deletes HA data unexpectedly,FLINK-33011,13549258,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,ruibin,ruibin,31/Aug/23 09:28,30/Oct/23 10:37,04/Jun/24 20:40,13/Sep/23 12:19,1.17.1,kubernetes-operator-1.6.0,,,,kubernetes-operator-1.6.1,kubernetes-operator-1.7.0,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"We encountered a problem where the operator unexpectedly deleted HA data.

The timeline is as follows:

12:08 We submitted the first spec, which suspended the job with savepoint upgrade mode.

12:08 The job was suspended, while the HA data was preserved, and the log showed the observed job deployment status was MISSING.

12:10 We submitted the second spec, which deployed the job with the last state upgrade mode.

12:10 Logs showed the operator deleted both the Flink deployment and the HA data again.

12:10 The job failed to start because the HA data was missing.

According to the log, the deletion was triggered by https://github.com/apache/flink-kubernetes-operator/blob/a728ba768e20236184e2b9e9e45163304b8b196c/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java#L168

I think this would only be triggered if the job deployment status wasn't MISSING. But the log before the deletion showed the observed job status was MISSING at that moment.

Related logs:

 
{code:java}
2023-08-30 12:08:48.190 +0000 o.a.f.k.o.s.AbstractFlinkService [INFO ][default/pipeline-pipeline-se-3] Cluster shutdown completed.
2023-08-30 12:10:27.010 +0000 o.a.f.k.o.o.d.ApplicationObserver [INFO ][default/pipeline-pipeline-se-3] Observing JobManager deployment. Previous status: MISSING
2023-08-30 12:10:27.533 +0000 o.a.f.k.o.l.AuditUtils         [INFO ][default/pipeline-pipeline-se-3] >>> Event  | Info    | SPECCHANGED     | UPGRADE change(s) detected (Diff: FlinkDeploymentSpec[image : docker-registry.randomcompany.com/octopus/pipeline-pipeline-online:0835137c-362 -> docker-registry.randomcompany.com/octopus/pipeline-pipeline-online:23db7ae8-365, podTemplate.metadata.labels.app.kubernetes.io~1version : 0835137cd803b7258695eb53a6ec520cb62a48a7 -> 23db7ae84bdab8d91fa527fe2f8f2fce292d0abc, job.state : suspended -> running, job.upgradeMode : last-state -> savepoint, restartNonce : 1545 -> 1547]), starting reconciliation.
2023-08-30 12:10:27.679 +0000 o.a.f.k.o.s.NativeFlinkService [INFO ][default/pipeline-pipeline-se-3] Deleting JobManager deployment and HA metadata.
{code}
A more complete log file is attached. Thanks.","Flink: 1.17.1

Flink Kubernetes Operator: 1.6.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 09:22;ruibin;flink_operator_logs_0831.csv;https://issues.apache.org/jira/secure/attachment/13062623/flink_operator_logs_0831.csv",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 21 07:44:24 UTC 2023,,,,,,,,,,"0|z1k46g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 12:19;gyfora;merged to main 82739f62adda33e686da7d8aa30cbd41ea13012f;;;","05/Oct/23 18:51;mason6345;[~gyfora] -could you backport this to 1.6? We are also hitting this bug in 1.6-

It has been already backported. Would be good to update the ticket fix version. Thanks for the fix!;;;","09/Oct/23 06:24;gyfora;I don't think that we are going to do an 1.6.1 release;;;","19/Oct/23 11:28;pedromazala;[~gyfora] Thank you for the fix :muscle: this operator is a life savior many times.

When is the release 1.7 happening? I'm asking because I could benefit from this fix. And I want to understand if I'll have to release a version myself to handle it.
;;;","19/Oct/23 11:39;gyfora;[~pedromazala] we will create the 1.6.1 pathc release next week, 1.7.0 is still a few weeks out. In the meantime you could also use the snapshot release built automatically that contains this fix:  [https://github.com/apache/flink-kubernetes-operator/pkgs/container/flink-kubernetes-operator/127962962?tag=3f0dc2e]

 ;;;","21/Oct/23 07:44;fanrui;Merged to 1.6.1 via: 3f0dc2ee5534084bc162e6deaded36e93bb5e384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when using GREATEST() in Flink SQL,FLINK-33010,13549257,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,hectorrios,hectorrios,31/Aug/23 09:24,12/Sep/23 06:45,04/Jun/24 20:40,10/Sep/23 21:17,1.16.1,1.16.2,,,,1.16.3,1.17.2,1.18.0,1.19.0,Table SQL / API,Table SQL / Planner,,,,,0,pull-request-available,,,"Hi,

I see NPEs in flink 1.14 and flink 1.16 when running queries with GREATEST() and timestamps. Below is an example to help in reproducing the issue.
{code:java}
CREATE TEMPORARY VIEW Positions AS
SELECT
SecurityId,
ccy1,
CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp
FROM (VALUES
(1, 'USD', '2022-01-01'),
(2, 'GBP', '2022-02-02'),
(3, 'GBX', '2022-03-03'),
(4, 'GBX', '2022-04-4'))
AS ccy(SecurityId, ccy1, publishTimestamp);

CREATE TEMPORARY VIEW Benchmarks AS
SELECT
SecurityId,
ccy1,
CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp
FROM (VALUES
(3, 'USD', '2023-01-01'),
(4, 'GBP', '2023-02-02'),
(5, 'GBX', '2023-03-03'),
(6, 'GBX', '2023-04-4'))
AS ccy(SecurityId, ccy1, publishTimestamp);

SELECT *,
GREATEST(
IFNULL(Positions.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))),
IFNULL(Benchmarks.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3)))
)
FROM Positions
FULL JOIN Benchmarks ON Positions.SecurityId = Benchmarks.SecurityId {code}
 

Using ""IF"" is a workaround at the moment instead of using ""GREATEST""

  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 10 21:16:54 UTC 2023,,,,,,,,,,"0|z1k468:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 10:19;Sergey Nuyanzin;Can you please check it with the latest 1.16.3 (not yet released) build
it's pretty likely it was fixed within FLINK-30018;;;","31/Aug/23 10:29;337361684@qq.com;Hi, [~Sergey Nuyanzin], It looks like master branch also will throw this error. The test pattern:
{code:java}
@Test
public void test1() {
    String query1 =
            ""SELECT\n""
                    + ""SecurityId,\n""
                    + ""ccy1,\n""
                    + ""CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp\n""
                    + ""FROM (VALUES\n""
                    + ""(1, 'USD', '2022-01-01'),\n""
                    + ""(2, 'GBP', '2022-02-02'),\n""
                    + ""(3, 'GBX', '2022-03-03'),\n""
                    + ""(4, 'GBX', '2022-04-4'))\n""
                    + ""AS ccy(SecurityId, ccy1, publishTimestamp)"";
    Table table = tEnv().sqlQuery(query1);
    tEnv().createTemporaryView(""Positions"", table);

    String query2 =
            ""SELECT\n""
                    + ""SecurityId,\n""
                    + ""ccy1,\n""
                    + ""CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp\n""
                    + ""FROM (VALUES\n""
                    + ""(3, 'USD', '2023-01-01'),\n""
                    + ""(4, 'GBP', '2023-02-02'),\n""
                    + ""(5, 'GBX', '2023-03-03'),\n""
                    + ""(6, 'GBX', '2023-04-4'))\n""
                    + ""AS ccy(SecurityId, ccy1, publishTimestamp)"";
    Table table2 = tEnv().sqlQuery(query2);
    tEnv().createTemporaryView(""Benchmarks"", table2);

    String sqlQuery =
            ""SELECT *,\n""
                    + ""GREATEST(\n""
                    + ""IFNULL(Positions.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))),\n""
                    + ""IFNULL(Benchmarks.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3)))\n""
                    + "")\n""
                    + ""FROM Positions\n""
                    + ""FULL JOIN Benchmarks ON Positions.SecurityId = Benchmarks.SecurityId "";
    List<String> actual =
            CollectionUtil.iteratorToList(tEnv().executeSql(sqlQuery).collect()).stream()
                    .map(Object::toString)
                    .collect(Collectors.toList());
    actual.sort(String::compareTo);
    List<String> expected = Arrays.asList("""");
    assertEquals(expected, actual);
} {code}
The error msg:
{code:java}
Caused by: java.lang.NullPointerException
    at StreamExecCalc$90.processElement_split3(Unknown Source)
    at StreamExecCalc$90.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:94)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:75)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.outputNullPadding(StreamingJoinOperator.java:349)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.processElement(StreamingJoinOperator.java:234)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.processRight(StreamingJoinOperator.java:145)
    at org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.processElement2(AbstractStreamingJoinOperator.java:141)
    at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor2$2(RecordProcessorUtils.java:116)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:254)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:179)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:143)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:89)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:613)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:1059)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:1008)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:959)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:938)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:751)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:567) {code}
The stack looks like FLINK-30018, it need deeply debug to find the problem.

 ;;;","31/Aug/23 20:54;Sergey Nuyanzin;Thanks for your response, yes confirm I was able to reproduce it on master.

Also I was able to reproduce with a simpler query
{code:sql}
SELECT GREATEST(IFNULL(1, 2), IFNULL(2, 1));
{code}
Frankly there are two types of exceptions which could be seen depending on code generation however the root cause is same.

In fact could be reproduced with {{GREATEST}}, {{LEAST}} for the result of other functions.

The reason is that during code generation it generates
{code:java}
...
/* 25 */        @Override
/* 26 */        public Object map(Object _in1) throws Exception {
/* 27 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) _in1;
/* 28 */          
/* 29 */          org.apache.flink.table.data.TimestampData externalResult$24;
/* 30 */          org.apache.flink.table.data.TimestampData result$25;
/* 31 */          boolean isNull$25;
/* 32 */          org.apache.flink.table.data.TimestampData externalResult$26;
/* 33 */          org.apache.flink.table.data.TimestampData result$27;
/* 34 */          boolean isNull$27;
/* 35 */          
/* 36 */          
/* 37 */          
/* 38 */          
/* 39 */          
/* 40 */          
/* 41 */           org.apache.flink.table.data.TimestampData tmpResult$28 = result$25;
/* 42 */           org.apache.flink.table.data.TimestampData result$28 = null;
/* 43 */           boolean nullTerm$28 = false;
/* 44 */           
/* 45 */           
...
{code}
The problem is that here we try to assign {{tmpResult$28}} to {{result$25}} however {{result$25}} is not yet initialized. This happens only when {{GREATEST}}/{{LEAST}} is applied to result of other functions.;;;","01/Sep/23 07:22;Sergey Nuyanzin;I submitted PR btw;;;","10/Sep/23 21:16;Sergey Nuyanzin;Merged as [5775fbbf02b962243d3fbe2d755be350ee98847a|https://github.com/apache/flink/commit/5775fbbf02b962243d3fbe2d755be350ee98847a]
1.16: [3597517f3301f66f3403374a7383aa462252c508|https://github.com/apache/flink/commit/3597517f3301f66f3403374a7383aa462252c508]
1.17: [8ec75899131b25d0ef7294afeb2f94ac6a37359e|https://github.com/apache/flink/commit/8ec75899131b25d0ef7294afeb2f94ac6a37359e]
1.18: [8ad22b82476aa12276db5e1762d54f0be8259a63|https://github.com/apache/flink/commit/8ad22b82476aa12276db5e1762d54f0be8259a63];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tools/release/update_japicmp_configuration.sh should only enable binary compatibility checks in the release branch,FLINK-33009,13549246,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Wencong Liu,mapohl,mapohl,31/Aug/23 08:15,27/Feb/24 08:49,04/Jun/24 20:40,,1.19.0,,,,,,,,,Release System,,,,,,0,pull-request-available,,,"According to [Flink's API compatibility constraints|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/], we only support binary compatibility between patch versions. In [apache-flink:pom.xml:2246|https://github.com/apache/flink/blob/aa8d93ea239f5be79066b7e5caad08d966c86ab2/pom.xml#L2246] we have binary compatibility enabled even in {{master}}. This doesn't comply with the rules. We should this flag disabled in {{master}}. The {{tools/release/update_japicmp_configuration.sh}} should enable this flag in the release branch as part of the release process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 17:29:48 UTC 2024,,,,,,,,,,"0|z1k43s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 08:17;mapohl;[~chesnay] just to double-check: Is the observation [which was raised in his PR comment|https://github.com/apache/flink/pull/21184#issuecomment-1700319487] by [~Wencong Liu]? To me that sounds reasonable and we should add this functionality to the {{tools/release/update_japicmp_configuration.sh}} script. WDYT?;;;","31/Aug/23 09:47;Wencong Liu;Thanks [~mapohl] for the summary. 

BTW, I found this issue because I'm trying to add a new default method to an existed *@ Public* interface. Although this behavior shouldn't introduce source(or binary) incompatibility, it cannot pass the check.
I have reported this in the japicmp community. [New static method in interface detected as METHOD_NEW_DEFAULT · Issue #289 · siom79/japicmp (github.com)|https://github.com/siom79/japicmp/issues/289].;;;","27/Dec/23 06:25;Wencong Liu;Hi [~mapohl] , I've encountered the same issue once more in FLINK-33949 when I'm making some code changes considered binary incompatible by japicmp. I'd like to take this ticket and fix it. WDYT?;;;","04/Jan/24 08:54;mapohl;Go ahead. I assigned the issue to you. Aside from changing the configuration in {{master}}, updating [tools/releasing/update_japicmp_configuration.sh|https://github.com/apache/flink/blob/c2a76f7a82cee9e089ea89731568f2e522c3ea43/tools/releasing/update_japicmp_configuration.sh#L31] might be required.;;;","11/Jan/24 03:02;Wencong Liu;I've opened a pull request and CI has passed. 😄;;;","15/Jan/24 17:29;chesnay;I would personally be in favour of keeping this check because it is useful to know whether we're introducing binary incompatible changes or not. Even if we don't guarantee compatibility, that doesn't mean we should break things willy-nilly. The plugin forces us to make a conscious decision, and the cost is _very_ small.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use KubernetesClient from JOSDK context,FLINK-33008,13549234,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,gyfora,gyfora,31/Aug/23 07:10,04/Sep/23 10:44,04/Jun/24 20:40,04/Sep/23 10:44,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,,,,"We are currently manually creating and passing around the KubernetesClient instances. 

This is now accessible directly from the JOSDK Context, so we should use it from there to simplify the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 10:44:27 UTC 2023,,,,,,,,,,"0|z1k414:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 10:10;darenwkt;Hi [~gyfora], I am interested to work on this, please assign to me if it's still available, thank you;;;","31/Aug/23 12:41;gyfora;done :) ;;;","04/Sep/23 10:44;gyfora;merged to main 3f537b839994ec726dd72411ecc3a0475a6ab9e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate autoscaler config validation into the general validator flow,FLINK-33007,13549232,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sr.praneeth@gmail.com,gyfora,gyfora,31/Aug/23 07:07,20/Nov/23 12:31,04/Jun/24 20:40,24/Oct/23 06:47,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"Currently autoscaler configs are not validated at all but cause runtime failures of the autoscaler mechanism. 

We should create a custom autoscaler config validator plugin and hook it up into the core validation flow

 

As part of this we should start validating the percentage based config ranges",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 24 06:47:57 UTC 2023,,,,,,,,,,"0|z1k40o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 07:08;gyfora;cc [~mxm] ;;;","24/Oct/23 06:47;gyfora;merged to main e905a1b84421710d9de5a886ecab10834cc24364;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add e2e test for Kubernetes Operator HA,FLINK-33006,13549231,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhenqiuHuang,gyfora,gyfora,31/Aug/23 07:05,26/Jan/24 15:51,04/Jun/24 20:40,26/Jan/24 15:51,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,There is currently no proper test coverage for operator HA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 15:51:08 UTC 2024,,,,,,,,,,"0|z1k40g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 15:51;gyfora;merged to main 6be277cd0c7421f4822c49296198f4a34b2cd721;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade JOSDK to 4.4.2 and Fabric8 to 6.8.1,FLINK-33005,13549229,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,31/Aug/23 07:04,05/Sep/23 14:28,04/Jun/24 20:40,05/Sep/23 14:28,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,,,,The JOSDK / fabric8 dept haven't been upgraded in a while. We should upgrade these critical clients to the latest stable versions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 05 14:28:33 UTC 2023,,,,,,,,,,"0|z1k400:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 14:28;gyfora;merged to main 7d01dc5a68066fef3a50a028a0e4ffe0b98ee65a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decoupling topology and network memory to support complex job topologies,FLINK-33004,13549212,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lsy,lsy,31/Aug/23 02:28,08/Mar/24 09:47,04/Jun/24 20:40,08/Mar/24 09:47,1.18.0,1.19.0,,,,,,,,Runtime / Network,,,,,,0,,,,"Currently, the default value of taskmanager.memory.network.fraction option in Flink is 0.1, and after the topology of the job is complex enough, it will run with an insufficient network buffer. We currently encountered this issue when running TPC-DS test set q9, and bypassed it by adjusting taskmanager.memory.network.fraction to 0.2. Theoretically, we should have network memory decoupled from the job topology so that arbitrarily complex jobs can be supported.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33668,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 02:36:48 UTC 2023,,,,,,,,,,"0|z1k3w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 02:29;lsy;cc [~guoweijie] ;;;","01/Sep/23 02:36;Zhanghao Chen;[~lsy] big +1 for it. Could you collaborate a bit how you would like to implement it? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML add isolationForest algorithm,FLINK-33003,13549211,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,zijun,zijun,31/Aug/23 02:22,14/Sep/23 03:19,04/Jun/24 20:40,,,,,,,,,,,Library / Machine Learning,,,,,,0,ml,pull-request-available,,"I want to use flink solve some problems related to anomaly detection, but currently flink ml lacks algorithms related to anomaly detection, so I want to add the isolation forest algorithm to library/flink ml. During the implementation process, when IterationBody is used, I try to understand the implementation of the Kmeans algorithm, and use iterative behavior to calculate the center point of the isolation forest algorithm, but in the test, I found that when the parallelism > 1, the number of iterations > 1, and there will be sometimes succeed sometimes fail (fail to find the broadcast variable). Please teachers help me to review and point out my problem. Thank you 🙏",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Aug 31 09:09:13 UTC 2023,,,,,,,,,,"0|z1k3w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 09:09;zijun;According this issue: [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=184615300,] I have solved the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump snappy-java from 1.1.4 to 1.1.10.1,FLINK-33002,13549194,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,30/Aug/23 19:57,30/Aug/23 20:23,04/Jun/24 20:40,30/Aug/23 20:23,,,,,,statefun-3.3.0,,,,Stateful Functions,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 20:23:34 UTC 2023,,,,,,,,,,"0|z1k3s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 20:23;martijnvisser;Fixed in:

apache/flink-statefun 4b1e0ff0f21d3299b0e23c2dcbf782bfb959d942;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSource in batch mode failing with exception if topic partition is empty,FLINK-33001,13549171,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,abdul,abdul,30/Aug/23 15:15,30/May/24 13:07,04/Jun/24 20:40,,1.12.7,1.14.6,1.17.1,,,,,,,Connectors / Kafka,,,,,,0,,,,"If the Kafka topic is empty in Batch mode, there is an exception while processing it. This bug was supposedly fixed but unfortunately, the exception still occurs. The original bug was reported as this https://issues.apache.org/jira/browse/FLINK-27041

We tried to backport it but it still doesn't work. 
 * The problem will occur in case of the DEBUG level of logger for class org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader
 * The same problems will occur in other versions of Flink, at least in the 1.15 release branch and tag release-1.15.4
 * The same problem also occurs in Flink 1.17.1 and 1.14

 

The minimal code to produce this is 

 
{code:java}
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		env.setRuntimeMode(RuntimeExecutionMode.BATCH);

		KafkaSource<String> kafkaSource = KafkaSource
				.<String>builder()
				.setBootstrapServers(""localhost:9092"")
				.setTopics(""test_topic"")
				.setValueOnlyDeserializer(new SimpleStringSchema())
				.setBounded(OffsetsInitializer.latest())
				.build();

		DataStream<String> stream = env.fromSource(
				kafkaSource,
				WatermarkStrategy.noWatermarks(),
				""Kafka Source""
		);
		stream.print();

		env.execute(""Flink KafkaSource test job""); {code}
This produces exception: 
{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception        at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:199)        at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:154)        at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:275)        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:67)        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:398)        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:619)        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:583)        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:758)        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:573)        at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records        at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:146)        at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:101)        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)        at java.util.concurrent.FutureTask.run(FutureTask.java:266)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)        ... 1 moreCaused by: java.lang.IllegalStateException: You can only check the position for partitions assigned to this consumer.        at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1737)        at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)        at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.maybeLogSplitChangesHandlingResult(KafkaPartitionSplitReader.java:375)        at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(KafkaPartitionSplitReader.java:232)        at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:49)        at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:138)        ... 6 more {code}
 

The only *workaround* that works fine right now is to change the DEBUG level to INFO for logging. 

 
{code:java}
logger.KafkaPartitionSplitReader.name = org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader 

logger.KafkaPartitionSplitReader.level = INFO{code}
It is strange that changing this doesn't cause the above exception. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 10:40:03 UTC 2024,,,,,,,,,,"0|z1k3n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 19:55;martijnvisser;[~abdul] Can you please verify this with the externalized version of the Kafka connector, given that that has bug fixes for some other things as well? See https://flink.apache.org/downloads/#apache-flink-kafka-connector-300;;;","30/May/24 10:40;nacisimsek;Just be able to try with the *flink-connector-kafka 3.1.0-1.18* on *Flink 1.18.1* session cluster with a batch job, and the exception is not there anymore. The job finishes gracefully, without setting any specific log level for any subclasses. I have the following log setting:
{code:java}
rootLogger.level = TRACE {code}
Therefore, the issue seems to be fixed.

However, there is another issue when setting a specific offset to start reading from, which causes an offset reset and infitinite loop of FetchTasks, so that the batch job keeps running, till there is an event received into the topic, but of course this should be a content of another ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceITCase should utilize TestExecutorExtension instead of using a ThreadFactory,FLINK-33000,13549158,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,mapohl,mapohl,30/Aug/23 13:34,05/Oct/23 04:49,04/Jun/24 20:40,05/Oct/23 04:42,1.16.2,1.17.1,1.18.0,1.19.0,,1.16.3,1.17.2,1.18.1,1.19.0,Table SQL / Gateway,Tests,,,,,0,pull-request-available,starter,,"{{SqlGatewayServiceITCase}} uses a {{ExecutorThreadFactory}} for its asynchronous operations. Instead, one should use {{TestExecutorExtension}} to ensure proper cleanup of threads.

We might also want to remove the {{AbstractTestBase}} parent class because that uses JUnit4 whereas {{SqlGatewayServiceITCase}} is already based on JUnit5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 05 04:42:51 UTC 2023,,,,,,,,,,"0|z1k3k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/23 17:39;ade_hddu;Hi [~mapohl], I would like to work on this issue as a starter. I would appreciate that if a more detailed background or guidance could be offered;;;","07/Sep/23 11:42;mapohl;Hi [~ade_hddu], thanks for offering your help. What are things that are still unclear to you?;;;","08/Sep/23 08:08;ade_hddu;[~mapohl], I have 2 questions on concrete implementation:

1. It seems that only {{ScheduledThreadPool}} or {{FixedThreadPool}} can meet the requirements. Using {{CachedThreadPool}} will result in out-of-order in cancel operation and close operation (because this thread pool is not FIFO by default). Although the test can pass at this time, it will leave a large number of exceptions. It seems that there will be at most 3-4 threads in the thread pool at the same time, so the default single thread pool is also not proper. Can the {{corePoolSize}} be hardcoded as 4? like this:
{quote}public static final TestExecutorExtension<ExecutorService> EXECUTOR_EXTENSION =
new TestExecutorExtension<>(() -> Executors.newScheduledThreadPool(4));
{quote}
I'm not really sure that if hardcoding is allowed here.

2. The {{AbstractTestBase}} class still seems to be inherited by many other Test classes, is there any risk to remove this class? The {{SQLGatewayServiceITCase}} class does not inherit any other class, I'm not quite sure which abstract class I should delete and the relation between the use of JUnit5 in this class and the removal of {{AbstractTestBase}} .;;;","08/Sep/23 08:25;mapohl;Thanks for looking into it more closely. I'm not 100% whether I fully understand the requirements you're talking about: My understanding is that the test creates threads to trigger concurrent behavior (e.g. in line [line 1073|https://github.com/apache/flink/blob/c9e87fe410c42f7e7c19c81456d4212a58564f5e/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/service/SqlGatewayServiceITCase.java#L1073]) without cleaning those threads up. The intention of this Jira was to do an explicit shutdown to clean up any threads that still linger around. Where do we have the requirement in the test code for a fixed number of threads?

{quote}
2. The AbstractTestBase class still seems to be inherited by many other Test classes, is there any risk to remove this class? The SQLGatewayServiceITCase class does not inherit any other class, I'm not quite sure which abstract class I should delete and the relation between the use of JUnit5 in this class and the removal of AbstractTestBase .
{quote}

I might have been a bit vague on removing the {{AbstractTestBase}}. I didn't mean to delete the class. You're totally correct that this class is used in other places. My concern was that {{AbstractTestBase}} provides JUnit4 functionality but {{SqlGatewayServiceITCase}} is a JUnit5-based test. My suggestion would have been to remove {{AbstractTestBase}} from {{SqlGatewayServiceITCase}} as a parent and use JUnit5 functionality instead ({{MiniClusterExtension}} for instance). Does that make sense?

and just as a FYI: I might not be responsive the next two weeks.;;;","08/Sep/23 10:29;ade_hddu;Thanks for the quick reply. In terms of the fixed number of threads, I have noticed that in [{{testSubmitOperationAndCloseOperationManagerInParallel2()}}|https://github.com/apache/flink/blob/c9e87fe410c42f7e7c19c81456d4212a58564f5e/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/service/SqlGatewayServiceITCase.java#L833] , 3 threads are opened and they wait the same {{{}CountDownLatch{}}}.  I'm not sure if these 3 threads should all start to wait for {{OperationManager}} to close, because {{startRunning}} lock is only set at 1, which is a little confusing to me. If it is ok to keep only 1 thread alive, the default {{TestExcutorExtension}} with single thread pool can be used.


As for the {{{}AbstractTextBase{}}},  I think {{SqlGatewayServiceITcase}} is not a child of AbstractTestBase now.  {{AbstractTestBase}} doesn't occur in the code and it seems that JUnit5 functionality has been already used.;;;","21/Sep/23 09:58;jiabao.sun;I think the OperationManagerTest, ResultFetcherTest should do the same change as well.;;;","27/Sep/23 08:21;fsk119;Merged into master:

de6c66ea805760f1550ae1fa348630edd9f17256
73717520cc63df8bd08fd008ac004659b210cfd1
de6c66ea805760f1550ae1fa348630edd9f17256

TODO:

Merged into release 1.18/1.17/1.16;;;","04/Oct/23 10:23;mapohl;[~fsk119] are you taking care of the backports as well?;;;","04/Oct/23 16:56;jiabao.sun;Thanks [~mapohl] and [~fsk119].
The PRs for release 1.18/1.17/1.16 is ready now.
Please help review it when you have time.;;;","05/Oct/23 04:42;mapohl;Summary of merged changes:
 * master
** [8e68b40d5cf9146a5bccadc67d9636f987003c0f|https://github.com/apache/flink/commit/8e68b40d5cf9146a5bccadc67d9636f987003c0f]
** [73717520cc63df8bd08fd008ac004659b210cfd1|https://github.com/apache/flink/commit/73717520cc63df8bd08fd008ac004659b210cfd1]
** [de6c66ea805760f1550ae1fa348630edd9f17256|https://github.com/apache/flink/commit/de6c66ea805760f1550ae1fa348630edd9f17256]
* 1.18
** [70ee020dc01224ee6e7acd63b60c43c2f5d8683f|https://github.com/apache/flink/commit/70ee020dc01224ee6e7acd63b60c43c2f5d8683f]
** [4f11dfb2a70782ae1ae652126066e55a2fe7c945|https://github.com/apache/flink/commit/4f11dfb2a70782ae1ae652126066e55a2fe7c945]
** [5fbf5d5aa192682ed14f22203f3114fe07b1f60f|https://github.com/apache/flink/commit/5fbf5d5aa192682ed14f22203f3114fe07b1f60f]
* 1.17
** [69142050e0f844421ccbe0e5173260eb12f26983|https://github.com/apache/flink/commit/69142050e0f844421ccbe0e5173260eb12f26983]
** [906216a4b541ae6ab413d8499c16f98aac84a592|https://github.com/apache/flink/commit/906216a4b541ae6ab413d8499c16f98aac84a592]
** [2ad7d97c758622bd095303c8bc5b793f20c7612e|https://github.com/apache/flink/commit/2ad7d97c758622bd095303c8bc5b793f20c7612e]
* 1.16
** [7f63abcbb14ec95ac4ea903bbe8519e0973a9bcf|https://github.com/apache/flink/commit/7f63abcbb14ec95ac4ea903bbe8519e0973a9bcf]
** [d9f8c419b89c2011320ef941fafd15526b5715aa|https://github.com/apache/flink/commit/d9f8c419b89c2011320ef941fafd15526b5715aa];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove HBase connector from master branch,FLINK-32999,13549141,13503028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/23 10:56,11/Sep/23 20:09,04/Jun/24 20:40,11/Sep/23 20:09,,,,,,1.18.0,1.19.0,,,Connectors / HBase,,,,,,0,pull-request-available,,,"The connector was externalized at FLINK-30061
Once it is released it would make sense to remove it from master branch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 20:08:54 UTC 2023,,,,,,,,,,"0|z1k3gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 13:30;ferenc-csaky;Thanks for creating this! Feel free to assign this to me, I'd be happy to help wrap-up the HBase externalization process.;;;","30/Aug/23 13:42;Sergey Nuyanzin;Thanks for volontieering, assigned to you [~ferenc-csaky] ;;;","11/Sep/23 20:08;Sergey Nuyanzin;Merged to 1.18 as [53981d81d378770f8a1f87717c5d80b35e4010d7|https://github.com/apache/flink/commit/53981d81d378770f8a1f87717c5d80b35e4010d7]
master: [62e41acdf56cccde53cc6226b2cc7c0bc3cda3e6|https://github.com/apache/flink/commit/62e41acdf56cccde53cc6226b2cc7c0bc3cda3e6];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
if function result not correct,FLINK-32998,13549135,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhou_yb,zhou_yb,30/Aug/23 10:32,09/Mar/24 22:25,04/Jun/24 20:40,,1.15.4,,,,,,,,,API / Core,,,,,,0,,,,"*if function result not correct,not result in origin field value, cut off the filed(word) value* 

code :

!image-2023-08-30-18-29-16-277.png!

result:

!image-2023-08-30-18-30-05-568.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/23 10:29;zhou_yb;image-2023-08-30-18-29-16-277.png;https://issues.apache.org/jira/secure/attachment/13062595/image-2023-08-30-18-29-16-277.png","30/Aug/23 10:30;zhou_yb;image-2023-08-30-18-30-05-568.png;https://issues.apache.org/jira/secure/attachment/13062594/image-2023-08-30-18-30-05-568.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 09 22:21:44 UTC 2024,,,,,,,,,,"0|z1k3f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 10:59;martijnvisser;[~zhou_yb] Please verify this with a release that's supported, given that https://issues.apache.org/jira/browse/FLINK-30966 has been fixed for newer versions;;;","19/Sep/23 06:43;zhou_yb;(y);;;","09/Mar/24 22:21;jeyhunkarimov;[~martijnvisser] [~zhou_yb]  I verified that as of master (d6a4eb966fbc47277e07b79e7c64939a62eb1d54) the issue seems to be fixed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (StreamingTestBase),FLINK-32997,13549131,13485220,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,jiabao.sun,jiabao.sun,30/Aug/23 10:01,22/Nov/23 06:09,04/Jun/24 20:40,22/Nov/23 06:09,1.18.0,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,JUnit5 Migration Module: flink-table-planner (StreamingTestBase),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 22 06:09:19 UTC 2023,,,,,,,,,,"0|z1k3e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 11:37;337361684@qq.com;Hi, [~jiabao.sun] , thanks for your contribution. IMO, there are too many tests in the table-planner module. Migrating all tests to junit5 at once has a significant impact to current pr and tests. Can we complete the migration in a smoother way? For example, adding StreamingTestBaseV2 (junit5) and BatchTestBaseV2 (junit5), and then new tests extended StreamingTestBaseV2 when modifying and exists tests gradually migrates. This may have a smaller impact on existing code and developers.  cc [~lincoln.86xy]  [~jark] ;;;","22/Nov/23 06:09;leonard;Implemented by master(1.19): dcc47907354a0cd86a4d319d9cea3ad01d2104b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 CheckpointAfterAllTasksFinishedITCase.testFailoverAfterSomeTasksFinished fails on AZP,FLINK-32996,13549127,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiang Xin,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/23 09:43,01/Sep/23 10:42,04/Jun/24 20:40,01/Sep/23 10:42,1.18.0,,,,,1.18.0,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52810&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8366
fails as
{noformat}
Aug 30 02:20:32 02:20:32.726 [ERROR] Failures: 
Aug 30 02:20:32 02:20:32.726 [ERROR]   CheckpointAfterAllTasksFinishedITCase.testFailoverAfterSomeTasksFinished:162 
Aug 30 02:20:32 expected: 20
Aug 30 02:20:32  but was: 40
Aug 30 02:20:32 02:20:32.726 [INFO] 

{noformat}

it is very likely it is related to FLINK-32907",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32907,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 10:41:57 UTC 2023,,,,,,,,,,"0|z1k3dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 09:44;Sergey Nuyanzin;[~Jiang Xin], [~lindong] could you have a look please since you were involved in  FLINK-32907;;;","30/Aug/23 11:40;Sergey Nuyanzin;I checked that current build already contains a fix made in FLINK-32907;;;","30/Aug/23 12:19;Jiang Xin;I tried to reproduce it locally, but the failure didn't appear after running 500 times.

However, I found a potential problem that might cause the issue. I will create a PR to fix it and run the tests on AZP multiple times to make sure the issue is resolved.;;;","31/Aug/23 23:04;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52874&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8691;;;","01/Sep/23 10:41;lindong;Merged to apache/flink master branch 6608d873db5fd90fce321293f264b098e2de51ad

Merged to apche/flink release-1.18 branch 5a8d42345cd740332bb731369039230a64c5d088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS end-to-end test fails with chmod: cannot access '../target/generator/dsdgen_linux': ,FLINK-32995,13549121,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/23 08:58,21/Sep/23 22:35,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,,0,auto-deprioritized-critical,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52773&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=5504
 fails as
{noformat}
Aug 29 10:03:20 [INFO] 10:03:20 Generating TPC-DS qualification data, this need several minutes, please wait...
chmod: cannot access '../target/generator/dsdgen_linux': No such file or directory
Aug 29 10:03:20 [FAIL] Test script contains errors.
Aug 29 10:03:20 Checking for errors...

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 21 22:35:12 UTC 2023,,,,,,,,,,"0|z1k3c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderElectionDriver.toString() is not implemented,FLINK-32994,13549102,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,30/Aug/23 06:21,01/Sep/23 09:09,04/Jun/24 20:40,31/Aug/23 07:26,1.18.0,1.19.0,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,starter,,"We noticed in FLINK-32678 that the {{toString()}} method of {{LeaderElectionDriver}} wasn't implemented with the FLINK-26522 changes. The legacy implementations actually provided a proper implementation. The {{MultipleComponentLeaderElectionDriver}}  implementations (which we reused in FLINK-26522) didn't provide such a method.

See [ZooKeeperLeaderElectionDriver.toString()|https://github.com/apache/flink/blob/release-1.17/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperLeaderElectionDriver.java#L236] and [KubernetesLeaderElectionDriver.toString()|https://github.com/apache/flink/blob/release-1.17/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L257] for comparison.

I'm marking this as a critical because it's a regression. But I'm not marking it as a blocker because it's only affecting the log output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26522,,,,,,,,,,,,,,FLINK-32678,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 07:26:55 UTC 2023,,,,,,,,,,"0|z1k37s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 07:26;mapohl;master (1.19): 11259ef52466889157ef473f422ecced72bab169
1.18: 5ec31f1214367e181b2f0c189159a674230962f4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datagen connector handles length-constrained fields according to the schema definition by default,FLINK-32993,13549087,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,30/Aug/23 03:50,18/Dec/23 13:21,04/Jun/24 20:40,18/Dec/23 13:21,1.19.0,,,,,1.19.0,,,,Connectors / Common,,,,,,0,pull-request-available,,,"create table as follows:

!image-2023-08-30-11-51-44-498.png!

results:

!image-2023-08-30-11-47-05-887.png!

I have found that Char type data length is 100 instead of defined 50.

we could help users to deal with the following data types in a proper way, thus allow the datagen connector to automatically generate data that conforms to the schema definition without additional manual configuration.
 # fixed-length data types (char, binary) , the length should be defined by schema definion, and could not be user-defined length.
 # variable-length data types (varchar, varbinary), the length should be defined by schema definition, and could be the user-defined length which less than schema definition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/23 03:47;liyubin117;image-2023-08-30-11-47-05-887.png;https://issues.apache.org/jira/secure/attachment/13062580/image-2023-08-30-11-47-05-887.png","30/Aug/23 03:47;liyubin117;image-2023-08-30-11-47-43-719.png;https://issues.apache.org/jira/secure/attachment/13062579/image-2023-08-30-11-47-43-719.png","30/Aug/23 03:51;liyubin117;image-2023-08-30-11-51-44-498.png;https://issues.apache.org/jira/secure/attachment/13062581/image-2023-08-30-11-51-44-498.png","30/Aug/23 05:44;liyubin117;image-2023-08-30-13-44-10-106.png;https://issues.apache.org/jira/secure/attachment/13062583/image-2023-08-30-13-44-10-106.png","30/Aug/23 05:56;liyubin117;image-2023-08-30-13-56-12-468.png;https://issues.apache.org/jira/secure/attachment/13062584/image-2023-08-30-13-56-12-468.png","30/Aug/23 05:56;liyubin117;image-2023-08-30-13-56-28-403.png;https://issues.apache.org/jira/secure/attachment/13062585/image-2023-08-30-13-56-28-403.png",,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 13:20:57 UTC 2023,,,,,,,,,,"0|z1k34g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 05:05;337361684@qq.com;Hi, [~liyubin117] . it's a by design behavior. If you want to specify the length of char, u need to add 'fieds.#.length' into table options as the document requirement: [document|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/datagen/#fields-length]. specify char(50) in the schema is invalid(default 100).  In your case, your ddl need change to:
{code:java}
create table t1(name string, addr string) with ('connector' = 'datagen', 'fields.name.length' = '50');{code};;;","30/Aug/23 06:00;liyubin117;[~337361684@qq.com] Char type is different obviously with others, field length has been explictly configured in the type definition by users, and others not. the current behavior is confusing users. maybe we can help users to configure it as you pointed automaticly.

!image-2023-08-30-13-44-10-106.png!

besides, the following results show that the behavior has made wrong data in the downstream tables.

!image-2023-08-30-13-56-12-468.png!

!image-2023-08-30-13-56-28-403.png!;;;","18/Dec/23 13:19;qingyue;Fixed in master 7cd6547a9027dfdc7ea97e496bb0e15213150529;;;","18/Dec/23 13:20;qingyue;Discussion thread https://lists.apache.org/thread/kp6popo4cnhl6vx31sdn6mlscpzj9tgc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recommended parallelism metric is a duplicate of Parallelism metric,FLINK-32992,13549027,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,mxm,mxm,mxm,29/Aug/23 17:03,31/Aug/23 13:32,04/Jun/24 20:40,31/Aug/23 13:32,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Deployment / Kubernetes,,,,,0,,,,The two metrics are the same. Recommended parallelism seems to have been added as a way to report real-time parallelism updates before we changed all metrics to be reported in real time.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 13:32:19 UTC 2023,,,,,,,,,,"0|z1k2r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 13:32;gyfora;cc [~morhidi] ;;;","30/Aug/23 14:19;gyfora;I think they are not the same when scaling is turned off (when the autoscaler only suggests parallelisms). In that case parallelism will stay the same while recommended parallelism changes. That was the original intention here as well.;;;","31/Aug/23 13:32;mxm;That makes sense. Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some metrics from autoscaler never get registered,FLINK-32991,13549026,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,29/Aug/23 17:03,31/Aug/23 13:42,04/Jun/24 20:40,31/Aug/23 13:42,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Deployment / Kubernetes,,,,,0,pull-request-available,,,"Not all metrics appear in the latest 1.6 release. This is because we report metrics as soon as they are available and the registration code assumes that they will all be available at once. In practice, some are only available after sampling data multiple times. For example, TARGET_DATA_RATE is only available after the source metrics have been aggregated and the lag has been computed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-29 17:03:08.0,,,,,,,,,,"0|z1k2qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when execute Plan#translate function with CREATE TABLE AS statement,  the CreateTableASOperation  as Plan.translate function parameter exception",FLINK-32990,13548950,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Licho,Licho,29/Aug/23 09:19,31/Aug/23 01:42,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"The `translate` function comment description `ModifyOperation` could be a parameter, but in the implementation function, there isn't a process for the `CreateTableASOperation` type.

I think at code PlannerBase.scala:L191(private[flink] def translateToRel(modifyOperation: ModifyOperation): RelNode) function no item for

`CreateTableASOperation`  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-29 09:19:47.0,,,,,,,,,,"0|z1k2a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink wheel package build failed,FLINK-32989,13548935,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaborgsomogyi,dianfu,dianfu,29/Aug/23 07:31,31/Aug/23 06:49,04/Jun/24 20:40,31/Aug/23 06:45,1.19.0,,,,,1.19.0,,,,API / Python,,,,,,0,pull-request-available,,,"{code}
Compiling pyflink/fn_execution/coder_impl_fast.pyx because it changed. 
Compiling pyflink/fn_execution/table/aggregate_fast.pyx because it changed. 
Compiling pyflink/fn_execution/table/window_aggregate_fast.pyx because it changed. 
Compiling pyflink/fn_execution/stream_fast.pyx because it changed. 
Compiling pyflink/fn_execution/beam/beam_stream_fast.pyx because it changed. 
Compiling pyflink/fn_execution/beam/beam_coder_impl_fast.pyx because it changed. 
Compiling pyflink/fn_execution/beam/beam_operations_fast.pyx because it changed. 
[1/7] Cythonizing pyflink/fn_execution/beam/beam_coder_impl_fast.pyx 
[2/7] Cythonizing pyflink/fn_execution/beam/beam_operations_fast.pyx 
[3/7] Cythonizing pyflink/fn_execution/beam/beam_stream_fast.pyx 
[4/7] Cythonizing pyflink/fn_execution/coder_impl_fast.pyx 
[5/7] Cythonizing pyflink/fn_execution/stream_fast.pyx 
[6/7] Cythonizing pyflink/fn_execution/table/aggregate_fast.pyx 
[7/7] Cythonizing pyflink/fn_execution/table/window_aggregate_fast.pyx 
/home/vsts/work/1/s/flink-python/dev/.conda/envs/3.7/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/vsts/work/1/s/flink-python/pyflink/fn_execution/table/window_aggregate_fast.pxd 
 tree = Parsing.p_module(s, pxd, full_module_name) 
Exactly one Flink home directory must exist, but found: []
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52740&view=logs&j=d15e2b2e-10cd-5f59-7734-42d57dc5564d&t=4a86776f-e6e1-598a-f75a-c43d8b819662",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 06:45:53 UTC 2023,,,,,,,,,,"0|z1k26o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/23 08:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52740&view=logs&j=d15e2b2e-10cd-5f59-7734-42d57dc5564d&t=4a86776f-e6e1-598a-f75a-c43d8b819662&l=880;;;","29/Aug/23 16:33;gaborgsomogyi;Started to have a look...;;;","29/Aug/23 18:59;gaborgsomogyi;I've found a clean solution but I would like to test it in the morning more extensively not to trap into the same issue.
I'm going to file a PR...;;;","29/Aug/23 19:01;gaborgsomogyi;Can we try the mentioned job somehow with a PR? It would be good to merge something which is rock stable.;;;","31/Aug/23 06:45;gaborgsomogyi;3975dd0 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveITCase failed due to TestContainer not coming up,FLINK-32988,13548931,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,mapohl,mapohl,29/Aug/23 07:04,12/Sep/23 07:22,04/Jun/24 20:40,12/Sep/23 07:22,1.18.0,1.19.0,,,,,,,,Connectors / Hive,,,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52740&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=15866

{code}
Aug 29 02:47:56 org.testcontainers.containers.ContainerLaunchException: Container startup failed for image prestodb/hive3.1-hive:10
Aug 29 02:47:56 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:349)
Aug 29 02:47:56 	at org.apache.flink.tests.hive.containers.HiveContainer.doStart(HiveContainer.java:81)
Aug 29 02:47:56 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:322)
Aug 29 02:47:56 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1131)
Aug 29 02:47:56 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28)
Aug 29 02:47:56 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 29 02:47:56 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 29 02:47:56 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 29 02:47:56 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 29 02:47:56 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 29 02:47:56 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 29 02:47:56 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Aug 29 02:47:56 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Aug 29 02:47:56 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Aug 29 02:47:56 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Aug 29 02:47:56 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Aug 29 02:47:56 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Aug 29 02:47:56 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32731,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 01:55:39 UTC 2023,,,,,,,,,,"0|z1k25s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/23 07:41;Sergey Nuyanzin;cc [~yuxia], [~fsk119] ;;;","30/Aug/23 09:00;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52807&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=16403;;;","31/Aug/23 09:28;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52871&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=16395;;;","01/Sep/23 11:31;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52912&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=16146;;;","04/Sep/23 09:03;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52952&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50;;;","04/Sep/23 09:18;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52963&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=16231;;;","04/Sep/23 12:55;mapohl;[~fsk119] There is a temporal dependency between the test failures appearing and FLINK-32731. I linked the issue.;;;","05/Sep/23 06:24;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52973&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=15347;;;","06/Sep/23 06:48;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52994&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=15999;;;","07/Sep/23 07:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53020&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=16201;;;","12/Sep/23 01:55;fsk119;Merged into master: 649b7fe197c8b03cce9595adcfea33c8d708a8b4
Merged into release-1.18: 9e5659ea65278b2b699ab0c0f0eafc918a0107bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobClientSslTest>BlobClientTest.testSocketTimeout expected SocketTimeoutException but identified SSLException,FLINK-32987,13548930,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ferenc-csaky,mapohl,mapohl,29/Aug/23 06:58,31/Aug/23 07:25,04/Jun/24 20:40,31/Aug/23 07:25,1.19.0,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52740&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8692

{code}
Aug 29 03:28:11 03:28:11.280 [ERROR]   BlobClientSslTest>BlobClientTest.testSocketTimeout:512 
Aug 29 03:28:11 Expecting a throwable with cause being an instance of:
Aug 29 03:28:11   java.net.SocketTimeoutException
Aug 29 03:28:11 but was an instance of:
Aug 29 03:28:11   javax.net.ssl.SSLException
Aug 29 03:28:11 Throwable that failed the check:
Aug 29 03:28:11 
Aug 29 03:28:11 java.io.IOException: GET operation failed: Read timed out
Aug 29 03:28:11 	at org.apache.flink.runtime.blob.BlobClient.getInternal(BlobClient.java:231)
Aug 29 03:28:11 	at org.apache.flink.runtime.blob.BlobClientTest.lambda$testSocketTimeout$2(BlobClientTest.java:510)
Aug 29 03:28:11 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
Aug 29 03:28:11 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
Aug 29 03:28:11 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
Aug 29 03:28:11 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
Aug 29 03:28:11 	at org.apache.flink.runtime.blob.BlobClientTest.testSocketTimeout(BlobClientTest.java:508)
Aug 29 03:28:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 29 03:28:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 29 03:28:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 29 03:28:11 	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32835,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 07:25:27 UTC 2023,,,,,,,,,,"0|z1k25k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 14:50;mapohl;The issue is caused by FLINK-32835 where we migrate from JUnit4 to 5. It looks like we're did a migration of the code that checks for {{SocketTimeoutException}} but the assert became more strict (the exception has to be the direct cause whereas the old code just checked from some cause being the {{SocketTimeoutException}}). I removed the 1.18.0 affected version from the ticket.

[~ferenc-csaky] can you pick this up?;;;","30/Aug/23 17:58;ferenc-csaky;Definitely, pls. assign it to me, I'll take care of it. Thanks for the detailed description!;;;","31/Aug/23 07:25;fanrui;Thanks [~mapohl]  for the report and [~ferenc-csaky]  for this fix.

 

Merged <master:1.19> 926b79f6f4444e1962a61d016c3d8122e864b213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The new createTemporaryFunction has some regression of type inference compare to the deprecated registerFunction,FLINK-32986,13548904,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,lincoln.86xy,lincoln.86xy,29/Aug/23 02:38,01/Feb/24 09:39,04/Jun/24 20:40,,1.17.1,1.18.0,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,"Current `LookupJoinITCase#testJoinTemporalTableWithUdfFilter` uses a legacy form function registration:
{code}
tEnv.registerFunction(""add"", new TestAddWithOpen)
{code}
it works fine with the SQL call `add(T.id, 2) > 3` but fails when swith to the new api:
{code}
tEnv.createTemporaryFunction(""add"", classOf[TestAddWithOpen])
// or this
tEnv.createTemporaryFunction(""add"", new TestAddWithOpen)
{code}
exception:
{code}
Caused by: org.apache.flink.table.api.ValidationException: Invalid function call:
default_catalog.default_database.add(BIGINT, INT NOT NULL)
	at org.apache.flink.table.types.inference.TypeInferenceUtil.createInvalidCallException(TypeInferenceUtil.java:193)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypes(TypeInferenceOperandChecker.java:89)
	at org.apache.calcite.sql.SqlOperator.checkOperandTypes(SqlOperator.java:753)
	at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:499)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:335)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:231)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6302)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6287)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:161)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1869)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1860)
	at org.apache.calcite.sql.type.SqlTypeUtil.deriveType(SqlTypeUtil.java:200)
	at org.apache.calcite.sql.type.InferTypes.lambda$static$0(InferTypes.java:47)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:2050)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:2055)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereOrOn(SqlValidatorImpl.java:4338)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3410)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:154)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3282)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3603)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:64)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:89)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1050)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1025)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:248)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1000)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:749)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:196)
	... 49 more
Caused by: org.apache.flink.table.api.ValidationException: Invalid input arguments. Expected signatures are:
default_catalog.default_database.add(a BIGINT NOT NULL, b INT NOT NULL)
default_catalog.default_database.add(a BIGINT NOT NULL, b BIGINT NOT NULL)
	at org.apache.flink.table.types.inference.TypeInferenceUtil.createInvalidInputException(TypeInferenceUtil.java:180)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypesOrError(TypeInferenceOperandChecker.java:124)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypes(TypeInferenceOperandChecker.java:86)
	... 75 more
Caused by: org.apache.flink.table.api.ValidationException: Invalid input arguments.
	at org.apache.flink.table.types.inference.TypeInferenceUtil.inferInputTypes(TypeInferenceUtil.java:442)
	at org.apache.flink.table.types.inference.TypeInferenceUtil.adaptArguments(TypeInferenceUtil.java:123)
	at org.apache.flink.table.types.inference.TypeInferenceUtil.adaptArguments(TypeInferenceUtil.java:100)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypesOrError(TypeInferenceOperandChecker.java:122)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 03 15:59:31 UTC 2023,,,,,,,,,,"0|z1k1zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/23 16:57;jeyhunkarimov;Hi [~lincoln.86xy]  could you please assign this task to me or give me an access to self-assign the task? Thanks!;;;","03/Dec/23 15:59;lincoln.86xy;[~jeyhunkarimov] assigned to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support setting env.java.opts.sql-gateway to specify the jvm opts for sql gateway,FLINK-32985,13548844,13548014,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,guoyangze,xiangyu0xf,xiangyu0xf,28/Aug/23 13:12,07/Oct/23 07:35,04/Jun/24 20:40,07/Oct/23 07:35,,,,,,,,,,Runtime / Configuration,Table SQL / Gateway,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-28 13:12:54.0,,,,,,,,,,"0|z1k1mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Display the host port information on the suttasks page of SubtaskExecution,FLINK-32984,13548843,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,wangm92,wangm92,28/Aug/23 13:05,29/Aug/23 02:52,04/Jun/24 20:40,29/Aug/23 02:52,1.7.2,1.8.0,,,,1.19.0,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,"Currently in the Flink UI, the Host in the task SubTasks page will not display TM port information, but the Host in TaskManagers will display port information. 

!image-2023-08-28-21-04-05-297.png|width=850,height=144!
!image-2023-08-28-21-04-27-396.png|width=949,height=109!

Since multiple TMs will run on one Host, I think the port information of the TM should also be displayed in SubTasks, so that it is convenient to locate the TM running on the subtask",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25371,,,,,,,,,,,,,,,,,"28/Aug/23 13:04;wangm92;image-2023-08-28-21-04-05-297.png;https://issues.apache.org/jira/secure/attachment/13062523/image-2023-08-28-21-04-05-297.png","28/Aug/23 13:04;wangm92;image-2023-08-28-21-04-27-396.png;https://issues.apache.org/jira/secure/attachment/13062522/image-2023-08-28-21-04-27-396.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-28 13:05:54.0,,,,,,,,,,"0|z1k1m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support setting env.java.opts.all & env.java.opts.cli configs via dynamic properties on the CLI side,FLINK-32983,13548834,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Zhanghao Chen,Zhanghao Chen,28/Aug/23 11:45,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,,,,Command Line Client,Deployment / Scripts,,,,,0,,,,"*Problem*

The following configs are supposed to be supported:
|h5. env.java.opts.all|(none)|String|Java options to start the JVM of all Flink processes with.|
|h5. env.java.opts.client|(none)|String|Java options to start the JVM of the Flink Client with.|

However, the two configs only takes effect on the Client side when they are set in the flink-conf files. In other words, configs set via -D or-yD on the CLI will not take effect, which is counter-intuitive and makes configuration less flexible.

 

*Proposal*

Add logic to parse configs set via -D or-yD in config.sh and make them has a higher precedence over configs set in flink-conf.yaml for env.java.opts.all & env.java.opts.client.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-28 11:45:48.0,,,,,,,,,,"0|z1k1k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not to suggest to swap flink-table-planner lib for using Hive dialect,FLINK-32982,13548833,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,28/Aug/23 11:36,31/Aug/23 08:00,04/Jun/24 20:40,31/Aug/23 08:00,,,,,,1.18.0,1.19.0,,,Connectors / Hive,Documentation,,,,,0,pull-request-available,,,"After FLINK-26603, FLINK-31575, to use hive dialect, we then won't need to swap the flink-table-planner jar in FLINK_HOME/opt with flink-table-planner-loader in FLINK_HOME/lib.

And it has been verified in FLINK-32799, so we need to remove the suggestion of swapping planner jar in code base & doc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 02:02:45 UTC 2023,,,,,,,,,,"0|z1k1k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 02:02;luoyuxia;master:

8e6c667128aa451ecc913939be57335dc5ba9fab

1.18:

ade9b5f32cda1e76aa3c88cb4ba018c0f33b4c96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add python dynamic Flink home detection,FLINK-32981,13548828,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,28/Aug/23 11:18,29/Aug/23 07:34,04/Jun/24 20:40,28/Aug/23 14:17,1.19.0,,,,,1.19.0,,,,API / Python,,,,,,0,pull-request-available,,,"During `pyflink` library compilation Flink home is calculated from the provided `pyflink` version which is normally something like: `1.19.dev0`.  Such case `.dev0` is replaced to `-SNAPSHOT` which ends-up in hardcoded home directory: `../../flink-dist/target/flink-1.18-SNAPSHOT-bin/flink-1.18-SNAPSHOT`. This is fine as long as one uses the basic version types described [here](https://peps.python.org/pep-0440/#developmental-releases). In order to support any kind of `pyflink` version one can dynamically find out the Flink home directory through globbing.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32989,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 07:34:32 UTC 2023,,,,,,,,,,"0|z1k1iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 14:17;mbalassi;63a4db2 in master;;;","29/Aug/23 07:34;dianfu;[~gaborgsomogyi] [~mbalassi] It seems that this PR breaks the PyFlink wheel package build (https://issues.apache.org/jira/browse/FLINK-32989). Could you help to take a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support env.java.opts.all & env.java.opts.cli config for starting Session cluster CLIs,FLINK-32980,13548827,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,28/Aug/23 11:17,29/Aug/23 02:49,04/Jun/24 20:40,29/Aug/23 02:48,1.18.0,,,,,1.19.0,,,,Command Line Client,Deployment / Scripts,,,,,0,pull-request-available,,,"*Problem*

The following configs are supposed to be supported:
|h5. env.java.opts.all|(none)|String|Java options to start the JVM of all Flink processes with.|
|h5. env.java.opts.client|(none)|String|Java options to start the JVM of the Flink Client with.|

However, the two configs do not take effect for starting Flink session clusters using kubernetes-session.sh and yarn-session.sh. This can lead to problems in complex production envs. For example, in my company, some nodes are IPv6-only, and the connection between Flink client and K8s/YARN control plane is via a domain name whose backend is on IPv4/v6 dual stack, and the JVM arg -Djava.net.preferIPv6Addresses=true needs to be set to make Java connect to IPv6 addresses in favor of IPv4 ones otherwise the K8s/YARN control plane is inaccessible.

 

*Proposal*

The fix is straight-forward, simply apply the following changes to the session scripts:

`
FLINK_ENV_JAVA_OPTS=""${FLINK_ENV_JAVA_OPTS} ${FLINK_ENV_JAVA_OPTS_CLI}""
exec $JAVA_RUN $JVM_ARGS $FLINK_ENV_JAVA_OPTS xxx
`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 02:49:10 UTC 2023,,,,,,,,,,"0|z1k1io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 11:19;Zhanghao Chen;Hi [~huweihua], do you think it valid? If so, could you assign this to me? I'd be willing to volunteer.;;;","28/Aug/23 11:29;huweihua;[~Zhanghao Chen] Thanks for reporting this. you are assigned;;;","29/Aug/23 02:49;huweihua;Resolved in master: 6dbb9b8d3fcfe88a85b22e2f177e8fea7585702e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate WindowAssigner#getDefaultTrigger(StreamExecutionEnvironment env),FLINK-32979,13548822,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,Wencong Liu,Wencong Liu,28/Aug/23 10:21,11/Sep/23 11:37,04/Jun/24 20:40,11/Sep/23 11:37,1.19.0,,,,,1.19.0,,,,API / Core,,,,,,0,pull-request-available,,,"The [FLIP-343|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=263425229] has decided that the parameter in WindowAssigner#getDefaultTrigger() will be removed in the next major version. We should deprecate it now and remove it in Flink 2.0. The removal will be tracked in [FLINK-4675|https://issues.apache.org/jira/browse/FLINK-4675].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-4675,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 11:37:35 UTC 2023,,,,,,,,,,"0|z1k1hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/23 11:37;xtsong;master (1.19): 831cb8eff022db5543052c96716518c86aaaa2c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate RichFunction#open(Configuration parameters),FLINK-32978,13548821,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Wencong Liu,Wencong Liu,28/Aug/23 10:19,30/Jan/24 09:00,04/Jun/24 20:40,30/Jan/24 08:59,1.19.0,,,,,1.19.0,,,,API / Core,,,,,,0,pull-request-available,,,"The [FLIP-344|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=263425231] has decided that the parameter in RichFunction#open will be removed in the next major version. We should deprecate it now and remove it in Flink 2.0. The removal will be tracked in [FLINK-6912|https://issues.apache.org/jira/browse/FLINK-6912].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-6912,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 09:00:41 UTC 2024,,,,,,,,,,"0|z1k1hc:",9223372036854775807,"The RichFunction#open(Configuration parameters) method has been deprecated and will be removed in future versions. Users are encouraged to migrate to the new RichFunction#open(OpenContext openContext) method, which provides a more comprehensive context for initialization.

Here are the key changes and recommendations for migration:

The open(Configuration parameters) method is now marked as deprecated.
A new method open(OpenContext openContext) has been added as a default method to the RichFunction interface.
Users should implement the new open(OpenContext openContext) method for function initialization tasks. The new method will be called automatically before the execution of any processing methods (map, join, etc.).
If the new open(OpenContext openContext) method is not implemented, Flink will fall back to invoking the deprecated open(Configuration parameters) method.",,,,,,,,,,,,,,,,,,,"12/Sep/23 03:48;xtsong;master (1.19): e9353319ad625baa5b2c20fa709ab5b23f83c0f4;;;","08/Jan/24 20:46;Sergey Nuyanzin;[~Wencong Liu], [~xtsong] this looks like a breaking change especially for classes where signature changed from {{public void open(Configuration parameters)}} to {{public void open(OpenContext openContext)}}

e.g.this change for {{org.apache.flink.streaming.api.functions.source.MultipleIdsMessageAcknowledgingSourceBase}}
was before the commit
{code:java}
    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
        sessionIds = new ArrayList<>(64);
        sessionIdsPerSnapshot = new ArrayDeque<>();
    }
{code}
and after
{code:java}
    @Override
    public void open(OpenContext openContext) throws Exception {
        super.open(openContext);
        sessionIds = new ArrayList<>(64);
        sessionIdsPerSnapshot = new ArrayDeque<>();
    }
{code}
also changes could be seen at [https://github.com/apache/flink/pull/23058/files#diff-59e8c6f938bd5fed55424ceb712a4617c3bfd8b45ab6a941af51521f7eae69c6L96-L98]

the problem with this change is that if someone extends from this class {{MultipleIdsMessageAcknowledgingSourceBase}} and calls {{super.open(<Configuration>)}} then now it will be redirected to {{org.apache.flink.api.common.functions.AbstractRichFunction#open}} and now variables stopped being initialized...

Exactly this happened with flink-connector-rabbitmq at [https://github.com/apache/flink-connector-rabbitmq/pull/22]

Since there are several places like that I think it should be explicitly mentioned what to do and how to check it during updates. The tricky thing is that the code still continues being compiled however some tests unexpectedly behave in unpredictable way...;;;","09/Jan/24 04:16;xtsong;Thanks for reporting this, [~Sergey Nuyanzin].

You're right, this is indeed a problem.

I think the interface change itself is non-breaking. The problem is that we also migrated the built-in implementations from the old interface to the new one. That should be fine for internal classes, but would become a breaking change for @Public / @PublicEvolving classes which might be overridden by user codes.

[~Wencong Liu], could you please look into this?;;;","09/Jan/24 04:19;xtsong;Reopening the ticket to fix the unexpected breaking changes.;;;","09/Jan/24 09:45;Wencong Liu;Thanks for proposing this issue 😄. I will investigate all modified implementation classes annotated by @Public or @PublicEvolving and open a pull request to revert the error changes.;;;","15/Jan/24 03:29;xtsong;Breaking changes reverted.
master (1.19): 1d6150f386d9c9ec61f4ab30853b915de7712047;;;","30/Jan/24 08:45;martijnvisser;[~Wencong Liu] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;","30/Jan/24 09:00;Wencong Liu;[~martijnvisser] Thanks for the reminding. I've added the release notes information.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate IOReadableWritable serialization in Path,FLINK-32977,13548820,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,Wencong Liu,Wencong Liu,28/Aug/23 10:17,07/Sep/23 02:20,04/Jun/24 20:40,07/Sep/23 02:20,1.19.0,,,,,1.19.0,,,,API / Core,,,,,,0,,,,"The [FLIP-347 |https://cwiki.apache.org/confluence/display/FLINK/FLIP-347%3A+Remove+IOReadableWritable+serialization+in+Path]has decided that the IOReadableWritable serialization will be removed in the next major version. We should deprecate it now and remove it in Flink 2.0. The removal will be tracked in [FLINK-5336|https://issues.apache.org/jira/browse/FLINK-5336].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-5336,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 07 02:20:49 UTC 2023,,,,,,,,,,"0|z1k1h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/23 02:20;xtsong;master (1.19): 7052251b22d3fcb7b34f29b13d19cd49eaff4aaf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointException when starting flink cluster in standalone mode,FLINK-32976,13548816,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,28/Aug/23 09:55,22/Sep/23 08:51,04/Jun/24 20:40,22/Sep/23 07:59,1.17.1,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,"It can be reproduced when starting flink cluster with hadoop configuration. 

 
{code:java}
//代码占位符

// Set up hadoop conf , hadoop classpath

// start jobManager


./jobmanager.sh start-foreground {code}
 

The error message as follows: 

 
{code:java}
//代码占位符
Caused by: java.ang.NullPointerException
at org.apache.flink. runtime. security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:173)~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvidertionTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.1.jar:1.17.1
at java.security.AccessController.doprivileged(Native Method)~[?:1.8.0 281]
at javax.security.auth.Subject.doAs(Subject.java:422)~[?:1.8.0 281]
at org. apache.hadoop . security.UserGroupInformation.doAs(UserGroupInformation. java:1876) ~flink-shacd-hadoop-3-uber-3.1.1.7.2.1.0-327-9.0.jar:3.1.1.7.2.1.0-327-9.0]
at org. apache.flink. runtime.security.token .hadoop .HadoopFSDelegationTokenProvider.obtainDelegationTcens(HadoopFSDelegationTokenProvider.java:108)~flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink. runtime. security.token.DefaultDelegationTokenManager . lambda$obtainDel
SAndGetNextRenewal$1(DefaultDelegationTokenManager .java:264)~ flink-dist-1.17.1.jar:1.17.1]
at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~?:1.8.0 281
at java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1628)~[?:1.8.0 281]at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)~?:1.8.0 281]
at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~?:1.8.0 281at java,util.stream.Reduce0ps$Reduce0p.evaluateSequential(Reduce0ps.java:708)~?:1.8.0 281]
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)~[?:1.8.0 281]at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:479) ~?:1.8.0 281
at java.util.stream.ReferencePipeline.min(ReferencePipeline.java:520)~?:1.8.0 281at org. apache. flink. runtime. security.token.DefaultDelegationTokenManager .obtainDelegationTokensAndGeNextRenewal(DefaultDelegationTokenManager .java:286)~[flink-dist-1.17.1.jar:1.17.1
at org.apache. flink.runtime. security.token.DefaultDelegationTokenManager. obtainDelegationTokens(DefaltDelegationTokenManager.java:242)~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializes@) ~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.entrypoint.clusterEntrypoint.nk-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint:232) ~[flink-dist-1.17.1.jar:1.17.1]
at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8. 281]
at javax.security.auth.Subject.doAs(Subject.java:422)~?:1.8.0 281]
at org. apache.hadoop . security.UserGroupInformation. doAs (UserGroupInformation. java:1876)~[flink-shadd-hadoop-3-uber-3.1.1.7.2.1.0-327-9.0.jar:3.1.1.7.2.1.0-327-9.0]
at org.apache.flink.runtime.security. contexts .HadoopSecurityContext.runSecured(HadoopSecurijava:41) ~[flink-dist-1.17.1.jar:1.17.1
at org. apache.flink. runtime. entrypoint. ClusterEntrypoint . startCluster(clusterEntrypoint. java:229)link-dist-1.17.1.jar:1.17.1]...2 more{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 22 07:59:05 UTC 2023,,,,,,,,,,"0|z1k1g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 14:27;martijnvisser;[~hackergin] Please provide info on what you've tried and how to reproduce this issue: there's currently not enough information to determine what's the issue;;;","28/Aug/23 17:24;hackergin;[~martijnvisser]  Sorry for the not clearly description. I have updated the reproducible steps.   

 

The corresponding code for  is as follows:

https://github.com/hackergin/flink/blob/78136133fbec4ca145dec66d4bc0c324c8e16d82/flink-runtime/src/main/java/org/apache/flink/runtime/security/token/hadoop/HadoopFSDelegationTokenProvider.java#L173
{code:java}
//代码占位符

if (flinkConfiguration.getString(DeploymentOptions.TARGET).toLowerCase().contains(""yarn"")) {
    LOG.debug(""Running on YARN, trying to add staging directory to file systems to access"");
    String yarnStagingDirectory =
            flinkConfiguration.getString(""yarn.staging-directory"", """");
    if (!StringUtils.isBlank(yarnStagingDirectory)) {
        LOG.debug(
                ""Adding staging directory to file systems to access {}"",
                yarnStagingDirectory);
        result.add(new Path(yarnStagingDirectory).getFileSystem(hadoopConfiguration));
        LOG.debug(""Staging directory added to file systems to access successfully"");
    } else {
        LOG.debug(
                ""Staging directory is not set or empty so not added to file systems to access"");
    }
} {code}
When starting the standalone cluster, since the TARGET parameter is not set, there is no check for the existence of TARGET, resulting in a null pointer error.

 

Therefore, we can easily fix this issue by adding a check. 

 

Flink stand alone cluster with Hadoop configuration is not a common practice. Most Flink jobs are either run on YARN or on Kubernetes. In these two modes, the TARGET parameter is correctly set, so this issue does not occur.

 ;;;","29/Aug/23 11:13;martijnvisser;[~gaborgsomogyi] WDYT?;;;","29/Aug/23 16:28;gaborgsomogyi;Yeah, DeploymentOptions.TARGET null check is the way to go. Thanks for reporting it.;;;","29/Aug/23 17:20;hackergin;[~gaborgsomogyi] [~martijnvisser]  I would like to fix this this, please assign this issue to me. ;;;","22/Sep/23 07:59;gaborgsomogyi;1fb95c3 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance equal() for all MapState's iterator,FLINK-32975,13548815,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xiarui,xiarui,xiarui,28/Aug/23 09:45,31/Aug/23 03:25,04/Jun/24 20:40,31/Aug/23 03:25,,,,,,1.19.0,,,,Runtime / State Backends,Tests,,,,,1,pull-request-available,,,"This ticket is originated from the junit version upgrade of Changelog module.

The assertThat() in junit5 uses Object#equals to compare two Map.Entry. The unnamed class Map.Entry<UK, UV> in ChangelogMapState uses the default Object#equals(), which does not compares the contents of two entries.

This ticket is to add a basic equal() implementation for the Map.Entry<UK, UV> in ChangelogMapState.

EDIT: To be more general, the equal() for RocksDB MapState's iterator is also vacant. It would better align the behavior of the comparsion of all MapState#Entry. This ticket will correct them together (RocksDB's and Changelog's).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 03:25:59 UTC 2023,,,,,,,,,,"0|z1k1g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 03:25;masteryhx;merge [aa8d93ea|https://github.com/apache/flink/commit/aa8d93ea239f5be79066b7e5caad08d966c86ab2] into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestClusterClient always leaks flink-rest-client-jobgraphs* directories,FLINK-32974,13548810,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,wanglijie,wanglijie,28/Aug/23 09:16,21/Sep/23 02:30,04/Jun/24 20:40,21/Sep/23 02:30,1.17.2,1.18.0,,,,1.17.2,1.18.0,,,Client / Job Submission,,,,,,0,,,,"After FLINK-32226, a temporary directory(named {{flink-rest-client-jobgraphs*}}) is created when creating a new RestClusterClient, but this directory will never be cleaned up.

This will result in a lot of {{flink-rest-client-jobgraphs*}} directories under {{/tmp}}, especially when using CollectDynamicSink/CollectResultFetcher, which may cause the inode to be used up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 21 02:30:12 UTC 2023,,,,,,,,,,"0|z1k1ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/23 02:30;wanglijie;Fix via
master: 6034d5ff335dc970672c290851811235399452fb
release-1.18: 2aeb99804ba56c008df0a1730f3246d3fea856b9
release-1.17: 7c9e05ea8c67b12c657b60cd5e6d1bea52b4f9a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation for kubernetes.operator.job.savepoint-on-deletion indicate that it can be set on per resource level while it can't,FLINK-32973,13548809,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,28/Aug/23 08:52,30/Aug/23 12:54,04/Jun/24 20:40,30/Aug/23 12:54,,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"Documentation for kubernetes.operator.job.savepoint-on-deletion indicate that it can be set on [per resource level|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#resourceuser-configuration] while it can't.

The config is only retrieved from operator configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 12:54:57 UTC 2023,,,,,,,,,,"0|z1k1eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 08:53;nfraison.datadog;I think that we should support the per-resource level configuration and not only update the documentation.

Will provide a patch for this;;;","28/Aug/23 08:58;gyfora;Ah yes, this should not be part of the operator configuration. We should fix that;;;","30/Aug/23 12:54;gyfora;merged to main 61ff9f59c14e012ea0237da0c1cc2f7297483293;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTest.testInterruptibleSharedLockInInvokeAndCancel causes a JVM shutdown with exit code 239,FLINK-32972,13548808,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,28/Aug/23 08:48,20/Nov/23 07:24,04/Jun/24 20:40,,1.17.2,,,,,,,,,Runtime / Coordination,,,,,,0,test-stability,,,"Within this build [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52668&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=8677]

it looks like task {{1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0}} was canceled
{noformat}
================================================================================
Test testInterruptibleSharedLockInInvokeAndCancel(org.apache.flink.runtime.taskmanager.TaskTest) is running.
--------------------------------------------------------------------------------
01:30:05,140 [                main] INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	/tmp/flink-netty-shuffle-82415974-782a-46db-afbc-8f18f30a4ec5
01:30:05,177 [                main] INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 32 MB for network buffer pool (number of memory segments: 1024, bytes per segment: 32768).
01:30:05,181 [   Test Task (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0) switched from CREATED to DEPLOYING.
01:30:05,190 [   Test Task (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0) [DEPLOYING].
01:30:05,192 [   Test Task (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0) switched from DEPLOYING to INITIALIZING.
01:30:05,192 [   Test Task (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0) switched from INITIALIZING to RUNNING.
01:30:05,195 [                main] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0).
01:30:05,196 [                main] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0) switched from RUNNING to CANCELING.
01:30:05,196 [                main] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0).
01:30:05,197 [   Test Task (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0) switched from CANCELING to CANCELED.
01:30:05,198 [   Test Task (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0).
{noformat}
and after that there are records in logs complaining htat task did not react
{noformat}
01:30:05,337 [Canceler/Interrupts for Test Task (1/1)#0 (1ec32305eb0f926acae926007429c142_00000000000000000000000000000000_0_0).] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Task 'Test Task (1/1)#0' did not react to cancelling signal - interrupting; it is stuck for 0 seconds in method:
 app//org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.close(AbstractMetricGroup.java:322)
app//org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.close(AbstractMetricGroup.java:327)
app//org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.close(AbstractMetricGroup.java:327)
app//org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.close(AbstractMetricGroup.java:327)
app//org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.close(AbstractMetricGroup.java:327)
app//org.apache.flink.runtime.metrics.groups.ComponentMetricGroup.close(ComponentMetricGroup.java:62)
app//org.apache.flink.runtime.metrics.groups.TaskMetricGroup.close(TaskMetricGroup.java:179)
app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:866)
app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
java.base@11.0.11/java.lang.Thread.run(Thread.java:829)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30844,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 07:24:09 UTC 2023,,,,,,,,,,"0|z1k1eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 09:01;Sergey Nuyanzin;1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52682&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8716];;;","30/Aug/23 14:27;mapohl;[~akalashnikov] Can you have a look into it? We're running into {{FatalExitExceptionHandler}} again similar to FLINK-30844. We might want to change the test so that we don't have this issue anymore. The 1s timeout which you set in FLINK-30844 might not be the right approach?;;;","31/Aug/23 09:51;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52873&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8265;;;","14/Nov/23 07:39;mapohl;1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54198&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=8428];;;","20/Nov/23 07:24;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54681&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8750;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add pyflink proper development version support,FLINK-32971,13548805,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,28/Aug/23 08:40,28/Aug/23 14:10,04/Jun/24 20:40,28/Aug/23 14:10,1.19.0,,,,,1.19.0,,,,API / Python,,,,,,0,pull-request-available,,,"The following doc describes the python version specification: https://peps.python.org/pep-0440/#developmental-releases

Extract:
{code:java}
Developmental releases are also permitted for pre-releases and post-releases:

X.YaN.devM       # Developmental release of an alpha release
X.YbN.devM       # Developmental release of a beta release
X.YrcN.devM      # Developmental release of a release candidate
X.Y.postN.devM   # Developmental release of a post-release
{code}

At the moment pyflink supports only `.dev0` development version.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 14:10:54 UTC 2023,,,,,,,,,,"0|z1k1ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 14:10;mbalassi;171524f in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"could not load external class using ""-C"" option",FLINK-32970,13548785,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,SpongebobZ,SpongebobZ,28/Aug/23 07:03,01/Sep/23 05:45,04/Jun/24 20:40,01/Sep/23 05:45,1.14.6,,,,,,,,,API / Core,,,,,,0,,,,"Firstly, the ""connectors.jar"" contains ""test-connector"" and I put it in user libs.

Then, I started a tableEvironment in one operator function of streamExecutionEnvironment.

In the tableEvironment I declared a table using the ""test-connector"".

Finally, I run the application and load the ""connectors.jar"" using ""-C connectors.jar"", when the table's creation statement was executed, I got an class not found exception which like below(please notice that if I put the ""connectors.jar"" in flink lib, the application would run normally):
{code:java}
SLF4J: Found binding in [jar:file:/data/hadoop-3.3.5/tmpdata/nm-local-dir/usercache/root/appcache/application_1690443774859_0439/filecache/13/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/data/hadoop-3.3.5/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:129)	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:92)	at com.xctech.cone.data.sql.model.runner.ModelRunner.executeStatementSet(ModelRunner.java:58)	at com.xctech.cone.data.versionedStarRocks.MicroBatchModelRunner.run(MicroBatchModelRunner.java:60)	at com.xctech.cone.data.versionedStarRocks.ExecuteSQLFunction.invoke(ExecuteSQLFunction.java:103)	at com.xctech.cone.data.versionedStarRocks.ExecuteSQLFunction.invoke(ExecuteSQLFunction.java:25)	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)	at org.apache.flink.streaming.api.functions.windowing.PassThroughAllWindowFunction.apply(PassThroughAllWindowFunction.java:35)	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueAllWindowFunction.process(InternalSingleValueAllWindowFunction.java:48)	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueAllWindowFunction.process(InternalSingleValueAllWindowFunction.java:34)	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:568)	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:524)	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1693)	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1684)	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)	at java.lang.Thread.run(Thread.java:750)Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish	at org.apache.flink.table.api.internal.InsertResultIterator.hasNext(InsertResultIterator.java:56)	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.isFirstRowReady(TableResultImpl.java:383)	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:116)	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	... 1 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)	at org.apache.flink.table.api.internal.InsertResultIterator.hasNext(InsertResultIterator.java:54)	... 7 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)	at akka.dispatch.OnComplete.internal(Future.scala:300)	at akka.dispatch.OnComplete.internal(Future.scala:297)	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)	at akka.actor.Actor.aroundReceive(Actor.scala:537)	at akka.actor.Actor.aroundReceive$(Actor.scala:535)	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)	at akka.actor.ActorCell.invoke(ActorCell.scala:548)	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)	at akka.dispatch.Mailbox.run(Mailbox.scala:231)	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)	... 4 moreCaused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: com.xctech.cone.connector.starrocks.table.StarRocksBatchSinkFunctionClassLoader info: URL ClassLoader:Class not resolvable through given classloader.	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:338)	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:155)	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63)	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:666)	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654)	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)	at java.lang.Thread.run(Thread.java:750)Caused by: java.lang.ClassNotFoundException: com.xctech.cone.connector.starrocks.table.StarRocksBatchSinkFunction	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:64)	at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74)	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:48)	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:172)	at java.lang.Class.forName0(Native Method)	at java.lang.Class.forName(Class.java:348)	at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1875)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2209)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:508)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:466)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543)	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:324)	... 9 more taskmanager.log 28532023-08-25 16:39:34,248 INFO  com.xctech.cone.data.versionedStarRocks.ExecuteSQLFunction   [] - 瀹氬埗鍖?starRocks鐗堟湰鍖栧井鎵瑰悓姝ュ凡鍚姩2023-08-25 16:41:00,463 INFO  com.xctech.cone.data.versionedStarRocks.ExecuteSQLFunction   [] - 瑙﹀彂SR寰壒, 褰撳墠鍏ㄥ眬鐗堟湰鍙? 22032023-08-25 16:41:03,799 ERROR com.xctech.cone.data.sql.model.runner.ModelRunner            [] - 璇QL ID鍒楄〃涔嬩竴鎵ц澶辫触: [1]2023-08-25 16:41:05,195 ERROR org.apache.flink.runtime.util.ClusterUncaughtExceptionHandler [] - WARNING: Thread 'Thread-8' produced an uncaught exception. If you want to fail on uncaught exceptions, then configure cluster.uncaught-exception-handling accordinglyjava.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164) ~[flink-dist_2.12-1.14.5.jar:1.14.5]	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183) ~[flink-dist_2.12-1.14.5.jar:1.14.5]	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2839) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3113) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102) ~[hadoop-common-3.3.5.jar:?] taskmanager.out 26592023-08-25 16:39:34,248 INFO  瀹氬埗鍖?starRocks鐗堟湰鍖栧井鎵瑰悓姝ュ凡鍚姩{A_SINK_1={1=SinkData(sqlId=1, pkBuffer=[[BOB]], sinkTable=A_SINK_1)}}2023-08-25 16:41:00,463 INFO  瑙﹀彂SR寰壒, 褰撳墠鍏ㄥ眬鐗堟湰鍙? 22032023-08-25 16:41:03,799 ERROR 璇QL ID鍒楄〃涔嬩竴鎵ц澶辫触: [1]2023-08-25 16:41:05,195 ERROR WARNING: Thread 'Thread-8' produced an uncaught exception. If you want to fail on uncaught exceptions, then configure cluster.uncaught-exception-handling accordinglyjava.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164) ~[flink-dist_2.12-1.14.5.jar:1.14.5]	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183) ~[flink-dist_2.12-1.14.5.jar:1.14.5]	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2839) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3113) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65) ~[hadoop-common-3.3.5.jar:?]	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102) ~[hadoop-common-3.3.5.jar:?]    	 VERSION*( &container_1690443774859_0439_01_000002none?$?$data:BCFile.indexnone?p
data:TFile.indexnone?:66data:TFile.metanone?4     }   ?觝懙锥9逜@捄酨 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 09:00:57 UTC 2023,,,,,,,,,,"0|z1k19c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 14:27;martijnvisser;[~SpongebobZ] Can you please try this with the latest versions of Flink, given that the community doesn't support 1.14 anymore?;;;","31/Aug/23 09:00;SpongebobZ;Hi [~martijnvisser] , I think I should not create another Environment in operation functions. Because that would lauch a local Environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IfCallGen NullPointException Bug,FLINK-32969,13548763,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zicat,zicat,28/Aug/23 02:49,28/Aug/23 03:37,04/Jun/24 20:40,28/Aug/23 03:37,1.17.1,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,,,,"If Function will cause NullPointException when the third param is a udf.
h1. Example:
{code:java}
CREATE TABLE source (
    name    STRING ,
    score   INT
) WITH (
  'connector' = 'socket',
  'hostname' = 'localhost:9999',
  'port' = '9999',
  'byte-delimiter' = '10',
  'format' = 'json'
);

CREATE TABLE print(
  name  STRING,
  score INT
) WITH ('connector' = 'print');

INSERT INTO print
SELECT IF(name = 'aa', 'null', CONCAT(name,'x')),score
FROM source;{code}
–- \{""name"":""aa"",""score"":10}

 
h1. Code Generator Result

[^if_function_codegen.java]

!image-2023-08-28-10-55-26-228.png|width=648,height=317!
h1. Exception Stack

!image-2023-08-28-11-02-03-842.png|width=707,height=161!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/23 02:52;zicat;if_function_codegen.java;https://issues.apache.org/jira/secure/attachment/13062494/if_function_codegen.java","28/Aug/23 02:55;zicat;image-2023-08-28-10-55-26-228.png;https://issues.apache.org/jira/secure/attachment/13062493/image-2023-08-28-10-55-26-228.png","28/Aug/23 03:02;zicat;image-2023-08-28-11-02-03-842.png;https://issues.apache.org/jira/secure/attachment/13062495/image-2023-08-28-11-02-03-842.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 03:36:51 UTC 2023,,,,,,,,,,"0|z1k14g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 02:58;zicat;[~jark] [~godfreyhe]  Please help to check this issue;;;","28/Aug/23 03:13;luoyuxia;[~zicat] Thanks for reporting. Is it similar to FLINK-30966?;;;","28/Aug/23 03:36;zicat;[~luoyuxia] looks like it has been fixed in 1.17.2 and 1.18, I will try to run this sql in those versions, this issue closed first.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix doc for customized catalog listener,FLINK-32968,13548759,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,28/Aug/23 02:16,01/Sep/23 03:27,04/Jun/24 20:40,01/Sep/23 03:27,1.18.0,1.19.0,,,,1.18.0,1.19.0,,,Documentation,,,,,,0,pull-request-available,,,Refer to https://issues.apache.org/jira/browse/FLINK-32798 for more details,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32798,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 03:27:13 UTC 2023,,,,,,,,,,"0|z1k13k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/23 03:27;zjureel;Fixed by d62feeca45290da610aa8aa1790e80f57397cd6d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner,FLINK-32967,13548753,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jiabao.sun,jiabao.sun,27/Aug/23 14:43,27/Aug/23 15:13,04/Jun/24 20:40,27/Aug/23 15:12,,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 27 15:12:27 UTC 2023,,,,,,,,,,"0|z1k128:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/23 14:46;jiabao.sun;Hi [~fanrui], could you help assign this ticket to me?;;;","27/Aug/23 15:12;Sergey Nuyanzin;closing since it is a duplicate of https://issues.apache.org/jira/browse/FLINK-29541;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner,FLINK-32966,13548751,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jiabao.sun,jiabao.sun,27/Aug/23 14:29,27/Aug/23 14:45,04/Jun/24 20:40,27/Aug/23 14:44,1.17.1,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 27 14:44:29 UTC 2023,,,,,,,,,,"0|z1k11s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/23 14:29;jiabao.sun;Hi [~fanrui], could you help assign this ticket to me?

Thanks a lot.;;;","27/Aug/23 14:44;jiabao.sun;Duplicate with https://issues.apache.org/jira/browse/FLINK-32967;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the deprecated MutableObjectMode,FLINK-32965,13548723,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,26/Aug/23 17:19,29/Aug/23 02:14,04/Jun/24 20:40,29/Aug/23 02:06,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,"FLINK-1285 updated the MutableObjectMode to ObjectReuse. The TaskConfig#getMutableObjectMode() is not used for a long long time, and it's internal class, so it can be removed directly.

Also, we should update the GroupReduceDriverTest:
 * Updating the _testAllReduceDriverAccumulatingImmutable()_ from `context.setMutableObjectMode({color:#cc7832}false{color}){color:#cc7832};{color}` to `context.getExecutionConfig().disableObjectReuse(){color:#cc7832};{color}`.
 * Updating the _testAllReduceDriverIncorrectlyAccumulatingMutable()_ to `context.getExecutionConfig().disableObjectReuse(){color:#cc7832};{color}`, and fix the assert.

 

More details can be get from [https://github.com/apache/flink/pull/23233#discussion_r1304595960]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 02:06:46 UTC 2023,,,,,,,,,,"0|z1k0vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/23 02:06;fanrui;Merged <master:1.19> via 3fa046ef7c461554b50d2d791dc4846cd8ff75b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisStreamsSink cant renew credentials with WebIdentityTokenFileCredentialsProvider,FLINK-32964,13548716,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,pbenaoun,pbenaoun,26/Aug/23 14:34,06/Mar/24 14:27,04/Jun/24 20:40,06/Mar/24 14:22,1.15.4,1.16.2,1.17.1,,,aws-connector-4.3.0,,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,"(First time filling a ticket in Flink community, please let me know if there are any guidelinges I need to follow)

I noticed a very strange behavior with the Kinesis Sink. I actually using Flink in containerized and Application (reactive) mode on EKS with high availability on S3. 
Kinesis is configured with IAM role and appropried policies. 
{code:java}
//Here a part of my flink-config.yaml:
parallelism.default: 2
scheduler-mode: reactive
execution.checkpointing.interval: 10s
env.java.opts.jobmanager: -Dkubernetes.max.concurrent.requests=200
containerized.master.env.KUBERNETES_MAX_CONCURRENT_REQUESTS: 200
aws.credentials.provider: WEB_IDENTITY_TOKEN
aws.credentials.role.arn: role
aws.credentials.role.sessionName: session
aws.credentials.webIdentityToken.file: /var/run/secrets/eks.amazonaws.com/serviceaccount/token {code}
When my project is deployed the application and cluster are working well but when the project has been started for about an hour, I suppose the IAM roles session need to be renew, then the job become to crashing continuously.
{code:java}
2023-08-24 10:35:55
java.lang.IllegalStateException: Connection pool shut down
    at org.apache.flink.kinesis.shaded.org.apache.http.util.Asserts.check(Asserts.java:34)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.requestConnection(PoolingHttpClientConnectionManager.java:269)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$DelegatingHttpClientConnectionManager.requestConnection(ClientConnectionManagerFactory.java:75)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$InstrumentedHttpClientConnectionManager.requestConnection(ClientConnectionManagerFactory.java:57)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:176)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.internal.impl.ApacheSdkHttpClient.execute(ApacheSdkHttpClient.java:72)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient.execute(ApacheHttpClient.java:254)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient.access$500(ApacheHttpClient.java:104)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:231)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:228)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.util.MetricUtils.measureDurationUnsafe(MetricUtils.java:63)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.executeHttpRequest(MakeHttpRequestStage.java:77)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:39)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:171)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:179)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.DefaultStsClient.assumeRoleWithWebIdentity(DefaultStsClient.java:760)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsAssumeRoleWithWebIdentityCredentialsProvider.getUpdatedCredentials(StsAssumeRoleWithWebIdentityCredentialsProvider.java:73)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsCredentialsProvider.updateSessionCredentials(StsCredentialsProvider.java:88)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.lambda$jitteredPrefetchValueSupplier$3(CachedSupplier.java:283)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier$PrefetchStrategy.fetch(CachedSupplier.java:419)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.refreshCache(CachedSupplier.java:198)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.get(CachedSupplier.java:127)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsCredentialsProvider.resolveCredentials(StsCredentialsProvider.java:99)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsAssumeRoleWithWebIdentityCredentialsProvider.resolveCredentials(StsAssumeRoleWithWebIdentityCredentialsProvider.java:44)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.internal.StsWebIdentityCredentialsProviderFactory$StsWebIdentityCredentialsProvider.resolveCredentials(StsWebIdentityCredentialsProviderFactory.java:93)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider.resolveCredentials(WebIdentityTokenFileCredentialsProvider.java:113)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.AwsCredentialsProviderChain.resolveCredentials(AwsCredentialsProviderChain.java:90)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.internal.LazyAwsCredentialsProvider.resolveCredentials(LazyAwsCredentialsProvider.java:45)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider.resolveCredentials(DefaultCredentialsProvider.java:123)
    at org.apache.flink.kinesis.shaded.org.apache.flink.connector.aws.util.AWSGeneralUtil.validateAwsCredentials(AWSGeneralUtil.java:404)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.buildClient(KinesisStreamsSinkWriter.java:159)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.<init>(KinesisStreamsSinkWriter.java:154)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSink.restoreWriter(KinesisStreamsSink.java:154)
    at org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler.createWriter(StatefulSinkWriterStateHandler.java:115)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.initializeState(SinkWriterOperator.java:146)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:274)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Unknown Source)
 {code}

I tested my project in many flink version with 1.15.4, 1.16.2 and 1.17.1 the same issues is happening.

Please let me know if this can be filled as a bug or If you can helping me to figure out my misunderstood.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 15 17:56:28 UTC 2024,,,,,,,,,,"0|z1k0u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 14:06;mapohl;Thanks for raising the issue, [~pbenaoun]. The information you provided seems to be reasonable. [~danny.cranmer] [~Hong Teoh] can you have a look at this?;;;","31/Aug/23 07:30;pbenaoun;Hello [~mapohl] thanks for the response. I can help for adding or completing some information on the issue.

The connector Kinesis is actually shade some package from apache client and sdk v2. The probleme is actually hitting who using WebIdentityToken on EKS.

I figure out some issue from aws-sdk-v2 project and can be the root cause.

[https://github.com/aws/aws-sdk-java/issues/1282]

[https://github.com/aws/aws-sdk-java-v2/issues/4221]

 ;;;","14/Sep/23 10:36;a.pilipenko;Hi [~pbenaoun],

I was not able to reproduce described issue using WEB_IDENTITY_TOKEN on EKS. Tested in Flink 1.17.1, Flink Kubernetes Operator 1.6, and Kinesis connector 4.1.

Both source and sink were able to successfully authenticate for over a day without any issues, max IAM session duration was configured to 1 hour.

Based on this I don't think that issue caused by credential provider being unable to renew credentials.

 

Exception you've shared indicates that connector is already in the process of shutting down and not the reason for job failure.

Could you share any additional logs around the time of the error? ;;;","18/Dec/23 14:33;jank;Hi,

we have the same issue in the same setup.
Flink 1.17 with Kinesis connector org.apache.flink:flink-connector-kinesis:4.1.0-1.17  running in EKS with access provided by an IAM role in a k8s service account.
After running the app, it is caught in a crash loop with the following stack trace after a couple of hours. This happened multiple times. After restarting the job manager, the app continued from last checkpoint until the next crash loop a couple of hours later.
{code:java}
java.lang.IllegalStateException: Connection pool shut down
    at org.apache.flink.kinesis.shaded.org.apache.http.util.Asserts.check(Asserts.java:34)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.requestConnection(PoolingHttpClientConnectionManager.java:269)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$DelegatingHttpClientConnectionManager.requestConnection(ClientConnectionManagerFactory.java:75)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$InstrumentedHttpClientConnectionManager.requestConnection(ClientConnectionManagerFactory.java:57)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:176)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
    at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.internal.impl.ApacheSdkHttpClient.execute(ApacheSdkHttpClient.java:72)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient.execute(ApacheHttpClient.java:254)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient.access$500(ApacheHttpClient.java:104)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:231)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:228)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.util.MetricUtils.measureDurationUnsafe(MetricUtils.java:63)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.executeHttpRequest(MakeHttpRequestStage.java:77)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:39)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:171)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:179)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.DefaultStsClient.assumeRoleWithWebIdentity(DefaultStsClient.java:760)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsAssumeRoleWithWebIdentityCredentialsProvider.getUpdatedCredentials(StsAssumeRoleWithWebIdentityCredentialsProvider.java:73)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsCredentialsProvider.updateSessionCredentials(StsCredentialsProvider.java:88)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.lambda$jitteredPrefetchValueSupplier$3(CachedSupplier.java:283)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier$PrefetchStrategy.fetch(CachedSupplier.java:419)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.refreshCache(CachedSupplier.java:198)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.get(CachedSupplier.java:127)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsCredentialsProvider.resolveCredentials(StsCredentialsProvider.java:99)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsAssumeRoleWithWebIdentityCredentialsProvider.resolveCredentials(StsAssumeRoleWithWebIdentityCredentialsProvider.java:44)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.internal.StsWebIdentityCredentialsProviderFactory$StsWebIdentityCredentialsProvider.resolveCredentials(StsWebIdentityCredentialsProviderFactory.java:93)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider.resolveCredentials(WebIdentityTokenFileCredentialsProvider.java:113)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.AwsCredentialsProviderChain.resolveCredentials(AwsCredentialsProviderChain.java:90)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.internal.LazyAwsCredentialsProvider.resolveCredentials(LazyAwsCredentialsProvider.java:45)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider.resolveCredentials(DefaultCredentialsProvider.java:123)
    at org.apache.flink.kinesis.shaded.org.apache.flink.connector.aws.util.AWSGeneralUtil.validateAwsCredentials(AWSGeneralUtil.java:404)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.buildClient(KinesisStreamsSinkWriter.java:159)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.<init>(KinesisStreamsSinkWriter.java:154)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSink.restoreWriter(KinesisStreamsSink.java:154)
    at org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler.createWriter(StatefulSinkWriterStateHandler.java:115)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.initializeState(SinkWriterOperator.java:146)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:274)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Unknown Source) {code}
I'm quite sure that the *StsAssumeRoleWithWebIdentityCredentialsProvider* or at least the AWS authentication is the root cause of this issue, as we also deployed the same app with basic aws access key and secret access key credentials afterwards with which the app was running stable for multiple days.

The Iceberg project has a similar issue: [https://github.com/apache/iceberg/issues/8601]
Their issue occurred roughly around the same time.
They are stating that the cause is the *DefaultCredentialsProvider* which is used as a singleton. 
How I understand the code in *org/apache/flink/kinesis/shaded/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.class* flink uses a singleton here, too.
The suggested fix is to not use a singleton but to recreate the credentials provider on every credentials refresh.

I hope this helps.;;;","12/Feb/24 18:47;a.pilipenko;Hi [~jank], thank you for the detailed info, this was very helpful.

My attempt to reproduce the issue was unsuccessful because I also configured source and sink to use *WEB_IDENTITY_TOKEN* - this way *WebIdentityTokenFileCredentialsProvider* is used directly, with a new instance created for each client.

There is also a bug filed in AWS SDK repository related to this: https://github.com/aws/aws-sdk-java-v2/issues/3493;;;","15/Feb/24 17:56;hong; merged commit [{{5e1d76d}}|https://github.com/apache/flink-connector-aws/commit/5e1d76d3d935627cc542fafef4df6c8604a3713d] into   apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make the test ""testKeyedMapStateStateMigration"" stable",FLINK-32963,13548702,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,asha-boyapati,asha-boyapati,asha-boyapati,26/Aug/23 02:16,31/Aug/23 06:46,04/Jun/24 20:40,28/Aug/23 13:21,1.17.1,,,,,1.19.0,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,"We are proposing to make the following test stable:

{{org.apache.flink.runtime.state.FileStateBackendMigrationTest.testKeyedMapStateStateMigration}}

The test is currently flaky because the order of elements returned by the iterator is non-deterministic.

The following PR fixes the flaky test by making it independent of the order of elements returned by the iterator:
[https://github.com/apache/flink/pull/23298]

We detected this using the NonDex tool using the following command:

{{mvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -pl flink-runtime -DnondexRuns=10 -Dtest=org.apache.flink.runtime.state.FileStateBackendMigrationTest#testKeyedMapStateStateMigration}}

Please see the following Continuous Integration log that shows the flakiness:
[https://github.com/asha-boyapati/flink/actions/runs/5909136145/job/16029377793]

Please see the following Continuous Integration log that shows that the flakiness is fixed by this change:
[https://github.com/asha-boyapati/flink/actions/runs/5909183468/job/16029467973]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 06:46:37 UTC 2023,,,,,,,,,,"0|z1k0qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 02:22;masteryhx;Thanks for reporting this. You're right.

Just assigned to you, please go ahead.;;;","28/Aug/23 13:21;masteryhx;merge [ab41e4f4|https://github.com/apache/flink/commit/ab41e4f4f1406ca349f0407cbbe05e0647132c81] into master;;;","30/Aug/23 14:04;mapohl;[~masteryhx] The Jira issue is labeled with 1.17.1 as the affected version. This would assume that we should create backports for older version (and 1.18), shouldn't we?;;;","31/Aug/23 03:20;masteryhx;This is just a simple improvement to make this UT case more reasonable which doesn't affect the main code path and CI.

So I think it's fine that we just resolved it in the master.;;;","31/Aug/23 06:46;mapohl;Ok, thanks for the clarification :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to install python dependencies from requirements file,FLINK-32962,13548692,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,25/Aug/23 20:27,19/Sep/23 10:28,04/Jun/24 20:40,19/Sep/23 10:28,1.15.4,1.16.2,1.17.1,1.18.0,,1.17.2,1.18.0,1.19.0,,API / Python,,,,,,0,pull-request-available,,,"We have encountered an issue when Flink fails to install python dependencies from requirements file if python environment contains setuptools dependency version 67.5.0 or above.
Flink job fails with following error:
{code:java}
py4j.protocol.Py4JJavaError: An error occurred while calling o118.await.
: java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
    at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
    at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
    ...
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 2ca4026944022ac4537c503464d4c47f)
    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
    at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
    ... 6 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 2ca4026944022ac4537c503464d4c47f)
    at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
    at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
    ...

Caused by: java.io.IOException: java.io.IOException: Failed to execute the command: /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install --ignore-installed -r /var/folders/rb/q_3h54_94b57gz_qkbp593vw0000gn/T/tm_localhost:63904-4178d3/blobStorage/job_2ca4026944022ac4537c503464d4c47f/blob_p-feb04bed919e628d98b1ef085111482f05bf43a1-1f5c3fda7cbdd6842e950e04d9d807c5 --install-option --prefix=/var/folders/rb/q_3h54_94b57gz_qkbp593vw0000gn/T/python-dist-c17912db-5aba-439f-9e39-69cb8c957bec/python-requirements
output:
Usage:
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] [-e] <vcs project url> ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] [-e] <local project path> ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] <archive url/path> ...

no such option: --install-option

    at org.apache.flink.python.util.PythonEnvironmentManagerUtils.pipInstallRequirements(PythonEnvironmentManagerUtils.java:144)
    at org.apache.flink.python.env.AbstractPythonEnvironmentManager.installRequirements(AbstractPythonEnvironmentManager.java:215)
    at org.apache.flink.python.env.AbstractPythonEnvironmentManager.lambda$open$0(AbstractPythonEnvironmentManager.java:126)
    at org.apache.flink.python.env.AbstractPythonEnvironmentManager$PythonEnvResources.createResource(AbstractPythonEnvironmentManager.java:435)
    at org.apache.flink.python.env.AbstractPythonEnvironmentManager$PythonEnvResources.getOrAllocateSharedResource(AbstractPythonEnvironmentManager.java:402)
    at org.apache.flink.python.env.AbstractPythonEnvironmentManager.open(AbstractPythonEnvironmentManager.java:114)
    at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:238)
    at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.open(AbstractExternalPythonFunctionOperator.java:57)
    at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:92)
    at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:101)
    at org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperator.open(PythonScalarFunctionOperator.java:71)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
    ...
Caused by: java.io.IOException: Failed to execute the command: /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install --ignore-installed -r /var/folders/rb/q_3h54_94b57gz_qkbp593vw0000gn/T/tm_localhost:63904-4178d3/blobStorage/job_2ca4026944022ac4537c503464d4c47f/blob_p-feb04bed919e628d98b1ef085111482f05bf43a1-1f5c3fda7cbdd6842e950e04d9d807c5 --install-option --prefix=/var/folders/rb/q_3h54_94b57gz_qkbp593vw0000gn/T/python-dist-c17912db-5aba-439f-9e39-69cb8c957bec/python-requirements
output:
Usage:
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] [-e] <vcs project url> ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] [-e] <local project path> ...
  /Users/z3d1k/.pyenv/versions/3.9.17/bin/python -m pip install [options] <archive url/path> ...

no such option: --install-option

    at org.apache.flink.python.util.PythonEnvironmentManagerUtils.execute(PythonEnvironmentManagerUtils.java:220)
    at org.apache.flink.python.util.PythonEnvironmentManagerUtils.pipInstallRequirements(PythonEnvironmentManagerUtils.java:130)
    ... 20 more
{code}
Before installing dependencies provided in the requirements.txt file, Flink checks version of pip, and based on result constructs cli command to execute [1]. Version check is performed by running following command [2]:
{code:java}
python -c ""import sys;from pkg_resources import get_distribution, parse_version;pip_version = get_distribution('pip').version;print(parse_version(pip_version) >= parse_version(sys.args[1]))""{code}
 

Starting from this version, python will print deprecation warning when package pkg_resources is used [3]:
{code:java}
<string>:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
True {code}
This warning breaks output parsing, causing Flink to assume that python environment has pip with version 8.0.0 or below, adding ""--install-option"" argument. This argument had been deprecated and removed from recent pip versions.

 

*Proposed solution:*

Remove pip version check since pip of version `8.0.0` does not support recent python versions: 
Documentation states that Flink supports python 3.7 and above for Flink 1.17 [4] and earliest version to support python 3.7 is pip 18.1 [5].

 

*References:*

[1]: [https://github.com/apache/flink/blob/c1fba732738aaaa0d576f80881eb464a954e12c5/flink-python/src/main/java/org/apache/flink/python/util/PythonEnvironmentManagerUtils.java#L61-L65]
[2]: [https://github.com/apache/flink/blob/c1fba732738aaaa0d576f80881eb464a954e12c5/flink-python/src/main/java/org/apache/flink/python/util/PythonEnvironmentManagerUtils.java#L109-L117]
[3]: [https://github.com/pypa/setuptools/pull/3843]
[4]: [https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/python/installation/]
[5]: [https://pip.pypa.io/en/stable/news/#v18-1]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 19 10:28:15 UTC 2023,,,,,,,,,,"0|z1k0oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 11:52;a.pilipenko;cc [~dianfu], [~hxbks2ks];;;","07/Sep/23 16:28;hong;merged commit [{{1193e17}}|https://github.com/apache/flink/commit/1193e178f6c478bf493231fc53c74f03158c0ca9] into apache:master;;;","07/Sep/23 16:29;hong;[~a.pilipenko]  should we also backport this to Flink 1.17 and 1.18?;;;","08/Sep/23 02:21;dianfu;+1 to backport to Flink 1.17 and 1.18, especially Flink 1.18.;;;","08/Sep/23 11:08;hong;merged commit [{{7f99a27}}|https://github.com/apache/flink/commit/7f99a274f9912513479a6eeb8ed82d721a8aeb7f] into apache:release-1.18;;;","09/Sep/23 10:32;a.pilipenko;Fix for 1.18 has been merged, added [PR|https://github.com/apache/flink/pull/23384] for 1.17 branch.;;;","19/Sep/23 10:28;hong;merged commit [{{f786dbe}}|https://github.com/apache/flink/commit/f786dbea0be25cc62dc10311a8d64d9de2ee20dd] into apache:release-1.17 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A new FileSystemFactory that support two high available hdfs,FLINK-32961,13548660,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lovemelovemycode,lovemelovemycode,25/Aug/23 13:52,28/Aug/23 14:28,04/Jun/24 20:40,,,,,,,,,,,FileSystems,,,,,,0,,,,"I run realtime ETL program by flink on yarn. The ETL program sink user log to master hdfs, and sink checkpoint to anather micro hdfs.The master hdfs and micro hdfs are both high available.

By default, the ETL program can not understand the dfs.nameservices of the micro hdfs.

I prepare to write a custom org.apache.flink.core.fs.FileSystemFactory that support two or more ha hdfs.So that , i can sink user log to master hdfs, and save checkpoint data to micro hdfs",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-25 13:52:46.0,,,,,,,,,,"0|z1k0hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logic to log vertex exclusion only once does not work correctly,FLINK-32960,13548657,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Later,mxm,mxm,mxm,25/Aug/23 12:45,15/Nov/23 15:01,04/Jun/24 20:40,15/Nov/23 15:01,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,"As part of daeb4b6559f0d26b1b0f23be5e8230f895b0a03e we wanted to log vertex exclusion only once. This logic does not work because the vertices without busy time are excluded in memory for every run. So we print ""No busyTimeMsPerSecond metric available for {}. No scaling will be performed for this vertex."" for every run.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-25 12:45:46.0,,,,,,,,,,"0|z1k0gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator standalone mode throws NoClassDefFoundError,FLINK-32959,13548656,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,mxm,mxm,mxm,25/Aug/23 12:22,28/Aug/23 10:55,04/Jun/24 20:40,28/Aug/23 10:55,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,,,,"The standalone mode throws the following error:
{noformat}
Exception in thread ""pool-7-thread-2"" org.apache.flink.shaded.guava30.com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/apache/flink/kubernetes/operator/standalone/StandaloneKubernetesConfigOptionsInternal
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2049)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)
	at org.apache.flink.kubernetes.operator.config.FlinkConfigManager.getConfig(FlinkConfigManager.java:298)
	at org.apache.flink.kubernetes.operator.config.FlinkConfigManager.getDeployConfig(FlinkConfigManager.java:219)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentContext.getDeployConfig(FlinkDeploymentContext.java:48)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:118)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:136)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:138)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:96)
	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:95)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:414)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/kubernetes/operator/standalone/StandaloneKubernetesConfigOptionsInternal
	at org.apache.flink.kubernetes.operator.config.FlinkConfigBuilder.applyTaskManagerSpec(FlinkConfigBuilder.java:279)
	at org.apache.flink.kubernetes.operator.config.FlinkConfigBuilder.buildFrom(FlinkConfigBuilder.java:405)
	at org.apache.flink.kubernetes.operator.config.FlinkConfigManager.generateConfig(FlinkConfigManager.java:305)
	at org.apache.flink.kubernetes.operator.config.FlinkConfigManager$1.load(FlinkConfigManager.java:106)
	at org.apache.flink.kubernetes.operator.config.FlinkConfigManager$1.load(FlinkConfigManager.java:103)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	``` {noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 10:55:24 UTC 2023,,,,,,,,,,"0|z1k0go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 12:24;mxm;I think I figured out what caused this. We changed the shading settings in this commit: https://github.com/apache/flink-kubernetes-operator/commit/d9dae4576168eca486e1d0ee006a714186edbada;;;","25/Aug/23 16:28;gyfora;But still this error should surface in the e2e, is it possible that you have a custom docker image for the operator which was not updated based on the changed jar names? (so it could be an error on your side);;;","28/Aug/23 10:55;mxm;This is spot-on! The issue is repacking logic on our side. I'll close this for now. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support VIEW as a source table in CREATE TABLE ... Like statement,FLINK-32958,13548628,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fornaix,fornaix,25/Aug/23 09:11,03/Jan/24 07:03,04/Jun/24 20:40,,1.17.1,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"We can't create a table from a view through CREATE TABLE LIKE statement

 

case 1:
{code:sql}
create view source_view as select id,val from source;
create table sink with ('connector' = 'print') like source_view (excluding all);
insert into sink select * from source_view;{code}
case 2
{code:java}
DataStreamSource<Entity> source = ...;
tEnv.createTemporaryView(""source"", source);
tEnv.executeSql(""create table sink with ('connector' = 'print') like source (excluding all)"");
tEnv.executeSql(""insert into sink select * from source"");{code}
 

The above cases will throw an exception:
{code:java}
Source table '`default_catalog`.`default_database`.`source`' of the LIKE clause can not be a VIEW{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 07:03:27 UTC 2024,,,,,,,,,,"0|z1k0ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 01:49;lsy;[~fornaix] Can you explain why we need to support this syntax?;;;","28/Aug/23 02:41;fornaix;[~lsy] Sure
 # In the above case, we just want to print the table content. It's convenient to use the CREATE TABLE LIKE statement to copy schema and create a print sink table. But now if the source table is a view, we had to give up this idea in frustration;
 # Other engines, such as hive and spark, support CREATE TABLE LIKE a view. So maybe supporting this syntax is not a strange thing.;;;","28/Aug/23 09:21;yesorno;[~fornaix]  I think CTAS syntax may be helpful in your case. 

https://issues.apache.org/jira/browse/FLINK-24567;;;","28/Aug/23 10:31;fornaix;[~yesorno] Yes, CTAS is a good alternative, thanks~

But I still insist on providing CREATE TABLE LIKE VIEW for the convenience of users.

Is there any other reason why this syntax cannot be provided?;;;","12/Oct/23 09:40;martijnvisser;[~fornaix] I can't find the ""CREATE TABLE LIKE VIEW"" in the Hive and Spark documentation, can you share the details on that?;;;","03/Jan/24 02:58;fornaix;[~martijnvisser]  sure.

spark doc: [https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table-like.html]

> The {{CREATE TABLE}} statement defines a new table using the definition/metadata of an existing table or {*}view{*}.

 

hive doc: [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableLike]

> Before Hive 0.8.0, CREATE TABLE LIKE view_name would make a copy of the view. In Hive 0.8.0 and later releases, CREATE TABLE LIKE view_name creates a table by adopting the schema of view_name (fields and partition columns) using defaults for SerDe and file formats.

 

They all support ""CREATE TABLE LIKE VIEW"" syntax;;;","03/Jan/24 07:03;martijnvisser;Thanks [~fornaix] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add current timer trigger lag to metrics,FLINK-32957,13548627,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xiarui,xiarui,25/Aug/23 09:06,02/Apr/24 12:52,04/Jun/24 20:40,,,,,,,,,,,Runtime / Metrics,Runtime / State Backends,,,,,0,,,,"Timer trigger lag denotes the gap between the actual trigger timestamp and the expected trigger timestamp (registered timestamp to `TimeService`). This metric can aid users to find out whether there is a backlog of timers. 



The backlog of timers may affect downstream data processing. Users customize the trigger logic, which may interact with downstream data processing. For example, a trigger logic can inject some records to downstream operators. The backlog of timers blocks the record injection. 



On the other side, The backlog of timers makes jobs unstable. Timers are used by window operators, which leverage a timer to remove the window state of a triggered window. The backlog of timers blocks data removal, and the state size may grow unexpectedly large. The large state size affects the performance of state-backend. In cloud-native environment, a k8s pod is prone to reach local disk limit due to large state files (RocksDB SST).



Currently, users are hard to observe the backlog of timers. As far as I known, heap dump is the only way to learn the backlog of timers. Thus, users cannot notice the backlog of timers in time. FLINK-32954 (https://issues.apache.org/jira/browse/FLINK-32954) exposes number of heap timers, but is not suitable for RocksDB timer due to performance loss.



Compare with FLINK-32954, timer trigger lag is much more lightweight for RocksDB timer. 
 * Reason 1: Timer trigger lag does not affect timer registering. 
 * Reason 2: The effect on timer triggering is limited. Timer registering is a hot code-path, while timer triggering is much colder. In general, the trigger interval is tens of second, and the timer trigger code-path is invoked every tens of second. Thus, the addition of timer trigger lag calculation has little performance overhead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 12:52:48 UTC 2024,,,,,,,,,,"0|z1k0a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 06:17;masteryhx;IMO, This metric is not so necessary.

Users cannot get more infos from this metric, they may only know that ""the timer trigger is delayed"" which they could get from the backpressure metric (IIUC, delayed timer trigger usually means backpressure), but they still need to figure out why (maybe it's not related to timer)

It's a bit different with FLINK-32954 which could help us to know whether heap timer is the bottleneck (large heap timer size is the most common case from my experience)

BTW, if we want to know the rocksdb timer size, some metrics in RocksDBProperty which reports in column family level (e.g. estimate-num-keys, estimate-live-data-size) could help to estimate.;;;","02/Apr/24 12:52;pnowojski;{{mailboxLatencyMs}} shows basically the same thing AFAIK. That is sampled time how long things are waiting in the mailbox queue before being executed, and timers are fired via the mailbox.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceITCase.testCloseUninterruptedOperation failed on AZP,FLINK-32956,13548622,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,25/Aug/23 08:21,25/Aug/23 08:22,04/Jun/24 20:40,,1.16.3,,,,,,,,,Table SQL / Client,,,,,,0,test-stability,,,"This build
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52620&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13979

failed as
{noformat}
Aug 25 06:05:38 [ERROR] Tests run: 28, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.212 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceITCase
Aug 25 06:05:38 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCloseUninterruptedOperation  Time elapsed: 0.025 s  <<< FAILURE!
Aug 25 06:05:38 java.lang.AssertionError: 
Aug 25 06:05:38 
Aug 25 06:05:38 Expecting code to raise a throwable.
Aug 25 06:05:38 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCloseUninterruptedOperation(SqlGatewayServiceITCase.java:320)
Aug 25 06:05:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 25 06:05:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 25 06:05:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 25 06:05:38 	at java.lang.reflect.Method.invoke(Method.java:498)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-25 08:21:43.0,,,,,,,,,,"0|z1k094:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support state compatibility between enabling TTL and disabling TTL,FLINK-32955,13548597,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lijinzhong,lijinzhong,lijinzhong,25/Aug/23 04:12,29/Aug/23 06:37,04/Jun/24 20:40,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,"Currently, trying to restore state, which was previously configured without TTL, using TTL enabled descriptor or vice versa will lead to compatibility failure and StateMigrationException.

In some scenario, user may enable state ttl and restore from old state which was previously configured without TTL;  or vice versa.

It would be useful for users if we support state compatibility between enabling TTL and disabling TTL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 06:37:41 UTC 2023,,,,,,,,,,"0|z1k03k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/23 06:37;yunta;+1 for this improvement, especially for setting the TTL on a job that forgets to set previously.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metrics expose number of heap timer,FLINK-32954,13548594,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiarui,xiarui,xiarui,25/Aug/23 03:38,09/Apr/24 09:07,04/Jun/24 20:40,,,,,,,,,,,Runtime / Metrics,Runtime / State Backends,,,,,0,pull-request-available,,,"Expose the current number of heap timers to metric reporter. Internally, expose the size of `InternalPriorityQueue` to `MetricGroup`. 

 

This metric can aid in debugging the memory consumption of heap timer. Currently, we need to dump jvm heap to identify the memory consumption of heap timer. With this metric, users can quickly get a basic knowledge on the working condition of heap timers. 

 

The *numOfTimers* metric is only suitable for heap timer. Heap priority queue (`AbstractHeapPriorityQueue`) has an off-the-shelf member to get size (`AbstractHeapPriorityQueue#size`). Expose it to metric reporter is zero-cost.

 

The *numOfTimers* metric for RocksDB timer brings performance loss, and is not supported right now. RocksDB (cached) priority queue does *not* has an off-the-shelf member to get size. Intuitively, we can add an exclusive counter for RocksDB priority queue. This counter affects the runtime per-record code path. Thus, the *numOfTimers* metric for RocksDB timer is currently not supported. I think there may exist some better/lightweight metrics to let user learn the work condition of RocksDB timers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35065,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 05:30:29 UTC 2023,,,,,,,,,,"0|z1k02w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 04:35;xiarui;[~masteryhx] Could you have a look at this ticket?;;;","25/Aug/23 05:30;masteryhx;Thanks for reporting this.

Heap timer could improve the performance than rocksdb timer, but increase the pressure on the GC.

We have seen heap timer causing frequent GC many times in the production environment.

We have to heap dump to see the root cause.

It could help us to find the bottleneck of heap timer quickly.


So I think it makes sense if not introducing complex codes and extra cost.

Already assigned to you, please go ahead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[State TTL]resolve data correctness problem after ttl was changed ,FLINK-32953,13548517,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lijinzhong,lijinzhong,lijinzhong,24/Aug/23 12:52,13/Mar/24 04:39,04/Jun/24 20:40,,,,,,,,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,"Because expired data is cleaned up in background on a best effort basis (hashmap use INCREMENTAL_CLEANUP strategy, rocksdb use ROCKSDB_COMPACTION_FILTER strategy), some expired state is often persisted into snapshots.

 

In some scenarios, user changes the state ttl of the job and then restore job from the old state. If the user adjust the state ttl from a short value to a long value (eg, from 12 hours to 24 hours),  some expired data that was not cleaned up will be alive after restore. Obviously this is unreasonable, and may break data regulatory requirements. 

 

Particularly, rocksdb stateBackend may cause data correctness problems due to level compaction in this case.(eg. One key has two versions at level-1 and level-2，both of which are ttl expired. Then level-1 version is cleaned up by compaction,  and level-2 version isn't.  If we adjust state ttl and restart job, the incorrect data of level-2 will become valid after restore)

 

To solve this problem, I think we can
1) persist old state ttl into snapshot meta info; (eg. RegisteredKeyValueStateBackendMetaInfo or others)
2) During state restore, check the size between the current ttl and old ttl;
3) If current ttl is longer than old ttl, we need to iterate over all data, filter out expired data with old ttl, and wirte valid data into stateBackend.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 15 02:23:07 UTC 2023,,,,,,,,,,"0|z1jzls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 03:48;masteryhx;[~lijinzhong] Thanks for reporting this, this makes sense to me.

It could work by reusing the logic of state migration.

BTW, We could go a step further, I think we could also support the compatibility between enabling TTL and disabling TTL using similar logic, this will be useful for users.;;;","25/Aug/23 04:10;lijinzhong;Thanks for your reply and advice.  [~masteryhx] 

I also think it meaningful to support state compatibility between enabling TTL and disabling TTL.

I will create another ticket to track this work.;;;","28/Aug/23 06:21;masteryhx;Thanks. [~lijinzhong] 
Would you like to fix this ? I could assign to you if you'd like.;;;","28/Aug/23 07:58;lijinzhong;[~masteryhx]  Yes, I want to do this work. Thanks.;;;","12/Sep/23 05:37;lijinzhong;[~masteryhx]  Currently, the stateBackend layer (HeapStateBackend/RocksDBStateBackend) doesn't contain information about state-ttl.  To solve this problem, i think we have to pass a ttl-state restore transformer from TtlStateFactory to StateBackend which is like [StateSnapshotTransformer|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateSnapshotTransformer.java].  

 

And this requires changing the interface KeyedStateFactory#createOrUpdateInternalState (see my [draft commit|https://github.com/ljz2051/flink/commit/7a058fd0f9d34a8fb42254a7ca1d6f32419c69e1] )

 

I think that fixing this corner case but changing the general interface of stateBackend layer may not be worthwhile. But at least，i think we should alert users to this situation in the documentation.

 

WDYT? [~masteryhx] 

 ;;;","12/Sep/23 07:15;masteryhx;[~lijinzhong] Thanks for the effort.

As disscussed offline, I think it's fine we resolve this by adding alert infos in the documentation firstly.

We could continute to dive into this problem when we have more feedback from users or other developers.

 ;;;","13/Sep/23 07:09;lijinzhong;[~masteryhx]  I publish a pr which add some notes about this issue in document. Could you please help review the pr?;;;","15/Sep/23 02:23;masteryhx;merged 9a891f117c3a44a633316b84f9cbf2a541b80d11 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan reuse with readable metadata and watermark push down will get wrong watermark ,FLINK-32952,13548503,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,24/Aug/23 11:37,12/Sep/23 03:08,04/Jun/24 20:40,12/Sep/23 03:08,1.18.0,,,,,1.18.0,1.19.0,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"Scan reuse with readable metadata and watermark push down will get wrong result. In class ScanReuser, we will re-build watermark spec after projection push down. However, we will get wrong index while try to find index in new source type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 03:07:58 UTC 2023,,,,,,,,,,"0|z1jzio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 14:40;jark;Fixed in 
- master: 103de5bf136816ce1e520f372e17b162e4aa2ba7
- release-1.18: TODO;;;","12/Sep/23 03:07;fsk119;Merged into release-1.18: f66679bfe5f5b344eec71a7579504762cc3c04ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp failed on AZP,FLINK-32951,13548494,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,24/Aug/23 10:15,24/Aug/23 19:48,04/Jun/24 20:40,24/Aug/23 19:48,1.19.0,,,,,1.19.0,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,test-stability,,"This build fails [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52564&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=8747]
{noformat}
Aug 24 01:35:46 01:35:46.537 [ERROR] org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp  Time elapsed: 0.024 s  <<< FAILURE!
Aug 24 01:35:46 java.lang.AssertionError: 
Aug 24 01:35:46 
Aug 24 01:35:46 Expecting actual:
Aug 24 01:35:46   ""org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$0/0x0""
Aug 24 01:35:46 to end with:
Aug 24 01:35:46   ""$Lambda$0/0""
Aug 24 01:35:46 
Aug 24 01:35:46 	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.verifyRecursively(VertexFlameGraphFactoryTest.java:70)
Aug 24 01:35:46 	at java.base/java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:212)
Aug 24 01:35:46 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
Aug 24 01:35:46 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
Aug 24 01:35:46 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
Aug 24 01:35:46 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
Aug 24 01:35:46 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 24 01:35:46 	at java.base/java.util.stream.IntPipeline.reduce(IntPipeline.java:491)
Aug 24 01:35:46 	at java.base/java.util.stream.IntPipeline.sum(IntPipeline.java:449)
Aug 24 01:35:46 	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.verifyRecursively(VertexFlameGraphFactoryTest.java:72)
Aug 24 01:35:46 	at java.base/java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:212)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32137,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 19:48:06 UTC 2023,,,,,,,,,,"0|z1jzgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/23 10:16;Sergey Nuyanzin;[~netvl] , [~fanrui]
could you have a look please?
It looks like it was introduced in FLINK-32137;;;","24/Aug/23 11:40;Sergey Nuyanzin;it looks like it could be reproduced with jdk11 and jdk17 (same are failing on AZP) and passing with jdk8;;;","24/Aug/23 11:42;Sergey Nuyanzin;blocker since now it fails every nightly;;;","24/Aug/23 19:48;Sergey Nuyanzin;Merged to master as [b3fb1421fe86129a4e0b10bf3a46704b7132e775|https://github.com/apache/flink/commit/b3fb1421fe86129a4e0b10bf3a46704b7132e775];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statsd reporter does not follow spec for counters,FLINK-32950,13548470,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,paymahngoldsky,paymahngoldsky,24/Aug/23 06:30,24/Aug/23 06:30,04/Jun/24 20:40,,,,,,,,,,,Runtime / Metrics,,,,,,0,,,,"The [statsd|https://github.com/statsd/statsd/blob/master/docs/metric_types.md] spec says the following:

> At each flush the current count is sent and reset to 0.

 

The flink [statsd reporter|https://github.com/apache/flink/blob/e5f78352a29df0d4dfe0c34369193896e7a1b4be/flink-metrics/flink-metrics-statsd/src/main/java/org/apache/flink/metrics/statsd/StatsDReporter.java#L129-L131] does not reset the counter to 0 after each flush. Instead it reports cumulative values. This is not correct and causes issues with downstream clients which consume these statsd metrics.

 

One possible fix would be do add the following as a class variable

 
{code:java}
  protected final Map<Counter, Long> lastKnownCounterValues = new ConcurrentHashMap<>();{code}
and then modify the {{reportCounter}} function like so
{code:java}
  private void reportCounter(final DMetric metric, final Counter counter) {
    // the statsd protocol says that counters should be set to 0 after flushing
    // https://github.com/statsd/statsd/blob/master/docs/metric_types.md#counting
    // we don't want to actually change the value of the counter because it could have uninteded
    // consequences. Instead, we keep track of the last known value and report the delta
    long curCount = counter.getCount();
    long lastKnownCount = this.lastKnownCounterValues.getOrDefault(counter, 0L);
    send(metric.getName(), curCount - lastKnownCount, DMetricType.COUNTER, metric.getTags());
    this.lastKnownCounterValues.put(counter, curCount);
  }{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-24 06:30:18.0,,,,,,,,,,"0|z1jzbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow specifying the ServerSocket port for the collect function when accessing the TaskManager from the client.,FLINK-32949,13548453,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jingge,hzjhjjyy,hzjhjjyy,24/Aug/23 01:25,18/Dec/23 14:09,04/Jun/24 20:40,18/Dec/23 14:08,,,,,,1.19.0,,,,API / Core,API / DataStream,Runtime / Configuration,,,,0,pull-request-available,,,"In the context of [#12069|https://github.com/apache/flink/pull/12069], the initialization of the {{CollectSinkFunction$ServerThread}} currently uses port 0, which corresponds to a random port assignment.

Issues might arise under the following circumstances:
 # When the JobManager and TaskManager are deployed on different servers.
 # When network communication between servers requires specific ports to be open.
 # When using {{sql-client.sh}} at the JobManager to execute operations like selecting data, the CollectSinkFunction$ServerThread running on the TaskManager using a random port can lead to data retrieval failures.

The purpose of this pull request is to address this problem by introducing a configuration parameter, 'taskmanager.collect.port', which allows specifying the port for the {{{}CollectSinkFunction$ServerThread{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 14:08:43 UTC 2023,,,,,,,,,,"0|z1jz7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 10:53;jingge;[~hzjhjjyy] would you like to review the PR? Thanks!;;;","18/Dec/23 14:08;jingge;master: e6169cee12d66ce22f7907b8dc947d4e96228b30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minimize visibility of parameterized tests,FLINK-32948,13548410,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jiabao.sun,jiabao.sun,jiabao.sun,23/Aug/23 15:04,23/Aug/23 15:12,04/Jun/24 20:40,,1.17.1,,,,,,,,,Tests,,,,,,0,,,,"Since  was [FLINK-32942|https://issues.apache.org/jira/browse/FLINK-32942] was merged into master, we can minimize visibility of parameterized tests.

Before
{code:java}
@ExtendWith(ParameterizedTestExtension.class)
public class ParameterizedTestExtensionTest {

    private static final List<Integer> PARAMETERS = Arrays.asList(1, 2);

    @Parameter
    public Integer parameter;

    @Parameters
    public static List<Integer> parameters() {
        return PARAMETERS;
    }

    @TestTemplate
    void testWithParameters() {
        assertThat(parameter).isIn(PARAMETERS);
    }
}
{code}

Now
{code:java}
@ExtendWith(ParameterizedTestExtension.class)
class ParameterizedTestExtensionTest {

    private static final List<Integer> PARAMETERS = Arrays.asList(1, 2);

    @Parameter
    private Integer parameter;

    @Parameters
    private static List<Integer> parameters() {
        return PARAMETERS;
    }

    @TestTemplate
    void testWithParameters() {
        assertThat(parameter).isIn(PARAMETERS);
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 15:12:38 UTC 2023,,,,,,,,,,"0|z1jyy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 15:05;jiabao.sun;cc [~fanrui] ;;;","23/Aug/23 15:12;fanrui;Looks make sense to me, and assigned to you, please go ahead! :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone autoscaler supports the metric reporter,FLINK-32947,13548409,13556703,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,23/Aug/23 15:02,03/Nov/23 14:00,04/Jun/24 20:40,,,,,,,,,,,Autoscaler,,,,,,0,,,,"The flink kubernetes operator itself sets up the metrics reporters and provides the base metric groups, for standalone implementation we need to create a new metric reporter otherwise we cannot report the metrics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-23 15:02:48.0,,,,,,,,,,"0|z1jyxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start End of Life discussion thread for now outdated Flink minor version,FLINK-32946,13548368,13548190,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,23/Aug/23 10:36,23/Aug/23 10:36,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"The idea is to discuss whether we should do a final release for the now not supported minor version in the community. Such a minor release shouldn't be covered by the current minor version release managers. Their only responsibility is to trigger the discussion.

The intention of a final patch release for the now unsupported Flink minor version is to flush out all the fixes that didn't end up in the previous release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-23 10:36:23.0,,,,,,,,,,"0|z1jyoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when executing TopSpeedWindowing example with checkpointing enabled,FLINK-32945,13548357,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Jiang Xin,mapohl,mapohl,23/Aug/23 09:41,30/Aug/23 10:09,04/Jun/24 20:40,30/Aug/23 10:09,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"I tried running the TopSpeedWindowing example with a checkpoint interval of 10s and the data input file that I attached to this Jira. I run into the following NullPointerException:
{code}
2023-08-23 09:56:03,111 ERROR org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor       [] - Error while executing remote procedure call public void org.apache.flink.runtime.jobmaster.JobMaster.notifyEndOfData(org.apache.flink.runtime.executiongraph.ExecutionAttemptID).
java.lang.reflect.InvocationTargetException: null
	at jdk.internal.reflect.GeneratedMethodAccessor20.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:568) ~[?:?]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$0(PekkoRpcActor.java:301) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[classes/:?]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:300) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373) [?:?]
	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182) [?:?]
	at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655) [?:?]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622) [?:?]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165) [?:?]
Caused by: java.lang.NullPointerException: Cannot invoke ""java.util.BitSet.set(int)"" because ""subtaskStatus"" is null
	at org.apache.flink.runtime.scheduler.VertexEndOfDataListener.recordTaskEndOfData(VertexEndOfDataListener.java:51) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.notifyEndOfData(SchedulerBase.java:1079) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.notifyEndOfData(JobMaster.java:508) ~[classes/:?]
	... 30 more
{code}

I adapted the {{TopSpeedWindowing}} example slightly (see [gist|https://gist.github.com/XComp/8a2bae96c7d9312427ed71cbee5370c2]) and executed it from within the IDE with the [attached input data|https://issues.apache.org/jira/secure/attachment/13062371/data.txt.gz].",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28386,,,,,,,,,,,,,,FLINK-32911,,"23/Aug/23 09:40;mapohl;data.txt.gz;https://issues.apache.org/jira/secure/attachment/13062371/data.txt.gz",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 10:09:05 UTC 2023,,,,,,,,,,"0|z1jym8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 10:09;lindong;Merged to apache/flink master branch 6c9bb3716a3a92f3b5326558c6238432c669556d

Merged to apache/flink release-1.18 branch 4868da924976fa8f38acd37dc11aec7c1ea46299

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.,FLINK-32944,13548333,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,tanee.kim,tanee.kim,23/Aug/23 07:50,12/Oct/23 09:42,04/Jun/24 20:40,12/Oct/23 09:42,,,,,,,,,,Table SQL / API,,,,,,0,,,,"When I try to change a Table to a DataStream through the Bridge API of the Table API, the following error occurs when I try to convert the List variable of the Pojo class.

 
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: Error while generating structured type converter.
    at org.apache.flink.table.data.conversion.StructuredObjectConverter.open(StructuredObjectConverter.java:89)
    at org.apache.flink.table.data.conversion.StructuredObjectConverter.open(StructuredObjectConverter.java:76)
    at org.apache.flink.table.runtime.typeutils.ExternalSerializer.initializeConverter(ExternalSerializer.java:217)
    at org.apache.flink.table.runtime.typeutils.ExternalSerializer.<init>(ExternalSerializer.java:78)
    at org.apache.flink.table.runtime.typeutils.ExternalSerializer.of(ExternalSerializer.java:93)
    at org.apache.flink.table.runtime.typeutils.ExternalTypeInfo.createExternalTypeSerializer(ExternalTypeInfo.java:97)
    at org.apache.flink.table.runtime.typeutils.ExternalTypeInfo.of(ExternalTypeInfo.java:67)
    at org.apache.flink.table.planner.connectors.ExternalDynamicSink.lambda$getSinkRuntimeProvider$2(ExternalDynamicSink.java:115)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.applySinkProvider(CommonExecSink.java:512)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.createSinkTransformation(CommonExecSink.java:218)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:176)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161)
    at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
    at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:151)

..

Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
    at org.apache.flink.table.data.conversion.StructuredObjectConverter.open(StructuredObjectConverter.java:80)
    ... 31 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
    ... 32 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
    ... 35 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 14, Column 112: Cannot cast ""java.util.List"" to ""com.woowahan.ds.pojo.OrderItem[]""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5051)
    at org.codehaus.janino.UnitCompiler.access$8600(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4418)
    at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$Cast.accept(Java.java:4898)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5057)
    at org.codehaus.janino.UnitCompiler.access$8100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4409)
    at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4400)
    at org.codehaus.janino.Java$ParenthesizedExpression.accept(Java.java:4924)
    at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400)
    at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5182)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5182)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
    ... 41 more
  {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 09:42:56 UTC 2023,,,,,,,,,,"0|z1jygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 12:53;mapohl;Thanks for reporting this, [~tanee.kim]. Can you add the Flink version you're using to this Jira under ""Affected Version""?

[~snuyanzin] is this something you can help out with? ...I'm asking because it's caused by Janino;;;","30/Aug/23 13:08;Sergey Nuyanzin;It would be great to see the code which leads to this behaviour
in best case the minimum reproducing test case;;;","12/Oct/23 09:42;martijnvisser;Closing this ticket due to lack of input for a reproducer;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"run batch tasks concurrently, the tasks still in the initialization status",FLINK-32943,13548328,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,zhuyinjun,zhuyinjun,23/Aug/23 07:24,01/Nov/23 03:34,04/Jun/24 20:40,01/Nov/23 03:34,1.15.2,,,,,,,,,,,,,,,0,,,,"run 1.15.2 flink session on k8s，In most cases, there is no problem. Sometimes, tasks are initialized continuously, and subsequent tasks are also initialized continuously，and

i find jobmanager thread dump   jobmanager-io thread all blocked，

I run batch job with 6 concurrent,jobmanage with 2cpu and 3g Memory

When this situation occurs， i find  this source code will still loop

public static void waitUntilJobInitializationFinished(
            SupplierWithException<JobStatus, Exception> jobStatusSupplier,
            SupplierWithException<JobResult, Exception> jobResultSupplier,
            ClassLoader userCodeClassloader)
            throws JobInitializationException {
        LOG.debug(""Wait until job initialization is finished"");
        WaitStrategy waitStrategy = new ExponentialWaitStrategy(50, 2000);
        try {
            JobStatus status = jobStatusSupplier.get();
            long attempt = 0;
            while (status == JobStatus.INITIALIZING) {
                Thread.sleep(waitStrategy.sleepTime(attempt++));
                status = jobStatusSupplier.get();
            }
            if (status == JobStatus.FAILED) {
                JobResult result = jobResultSupplier.get();
                Optional<SerializedThrowable> throwable = result.getSerializedThrowable();
                if (throwable.isPresent()) {
                    Throwable t = throwable.get().deserializeError(userCodeClassloader);
                    if (t instanceof JobInitializationException) {
                        throw t;
                    }
                }
            }
        } catch (JobInitializationException initializationException) {
            throw initializationException;
        } catch (Throwable throwable) {
            ExceptionUtils.checkInterrupted(throwable);
            throw new RuntimeException(""Error while waiting for job to be initialized"", throwable);
        }
    }

 ","flink 1.15.2

 
|*lob.server.port*|6124|
|*classloader.resolve-order*|parent-first|
|*jobmanager.execution.failover-strategy*|region|
|*jobmanager.memory.heap.size*|2228014280b|
|*jobmanager.memory.jvm-metaspace.size*|536870912b|
|*jobmanager.memory.jvm-overhead.max*|322122552b|
|*jobmanager.memory.jvm-overhead.min*|322122552b|
|*jobmanager.memory.off-heap.size*|134217728b|
|*jobmanager.memory.process.size*|3gb|
|*jobmanager.rpc.address*|naf-flink-ms-flink-manager-1-4gcwz|
|*jobmanager.rpc.port*|6123|
|*parallelism.default*|1|
|*query.server.port*|6125|
|*rest.address*|0.0.0.0|
|*rest.bind-address*|0.0.0.0|
|*rest.connection-timeout*|60000|
|*rest.server.numThreads*|8|
|*slot.request.timeout*|3000000|
|*state.backend.rocksdb.localdir*|/home/nafplat/data/flinkStateStore|
|*state.backend.type*|rocksdb|
|*taskmanager.bind-host*|0.0.0.0|
|*taskmanager.host*|0.0.0.0|
|*taskmanager.memory.framework.off-heap.batch-shuffle.size*|256mb|
|*taskmanager.memory.framework.off-heap.size*|512mb|
|*taskmanager.memory.managed.fraction*|0.4|
|*taskmanager.memory.network.fraction*|0.2|
|*taskmanager.memory.process.size*|16gb|
|*taskmanager.memory.task.off-heap.size*|268435456bytes|
|*taskmanager.numberOfTaskSlots*|6|
|*taskmanager.runtime.large-record-handler*|true|
|*web.submit.enable*|true|
|*web.tmpdir*|/tmp/flink-web-4be192ba-870a-4f88-8185-d07fa6303cca|
|*web.upload.dir*|/opt/flink/nafJar|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 09:43:25 UTC 2023,,,,,,,,,,"0|z1jyfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 03:14;zhuyinjun;[~mapohl] ;;;","30/Aug/23 05:48;mapohl;Hi [~zhuyinjun], thanks for bringing this up. But have you tried reproducing the issue in newer versions of Flink? Officially, Flink 1.15.2 has reached its end-of-life. Newer version might have a fix for your issue.

Generally, it's hard to tell what's blocking your job with the information that's provided. Flink's log files would help investigating the issue.;;;","30/Aug/23 06:21;zhuyinjun;[~mapohl] 

Thank you for your reply,This problem has been bothering me for a long time. I now rely on restarting the cluster to avoid it. We plan to update to version 1.17.1 in the next few days, and I will verify if this problem still exists. Thank you again;;;","12/Oct/23 09:43;martijnvisser;[~zhuyinjun] Can this ticket be closed?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JUnit5 ParameterizedTestExtension's parameter provider can be private. ,FLINK-32942,13548322,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,23/Aug/23 06:37,23/Aug/23 13:47,04/Jun/24 20:40,23/Aug/23 13:47,1.17.1,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,"Currently parameters provider must be public.
If we make the parameterProvider accessible before invocation, the test case will get better isolation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:47:05 UTC 2023,,,,,,,,,,"0|z1jyeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 13:47;fanrui;Merged <master:1.19> c3609890ebf3028c865bc67bcd2a84553b6c411e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API Bridge `toDataStream(targetDataType)` function not working correctly for Java List,FLINK-32941,13548308,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanee.kim,tanee.kim,23/Aug/23 01:10,14/Sep/23 22:35,04/Jun/24 20:40,,,,,,,,,,,Table SQL / API,,,,,,0,auto-deprioritized-critical,bridge,,"When the code below is executed, only the first element of the list is assigned to the List variable in MyPoJo repeatedly.
{code:java}
case class Item(
  name: String
)
case class MyPojo(
  @DataTypeHist(""RAW"") items: java.util.List[Item]
)

...

tableEnv
  .sqlQuery(""select items from table"")
  .toDataStream(DataTypes.of(classOf[MyPoJo])) {code}
 

For example, if you have the following list coming in as input,
[""a"",""b"",""c""]
The value actually stored in MyPojo's list variable is
[""a"",""a"",""a""] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 14 22:35:22 UTC 2023,,,,,,,,,,"0|z1jybc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support projection pushdown to table source for column projections through UDTF,FLINK-32940,13548304,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vsowrirajan,vsowrirajan,23/Aug/23 00:32,01/Feb/24 09:41,04/Jun/24 20:40,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,"Currently, Flink doesn't push down columns projected through UDTF like _UNNEST_ to the table source.

For eg:
{code:java}
SELECT t1.deptno, t2.ename FROM db.dept_nested t1, UNNEST(t1.employees) AS t2{code}
For the above SQL, Flink projects all the columns for DEPT_NESTED rather than only _name_ and {_}employees{_}. If the table source supports nested fields column projection, ideally it should project only _t1.employees.ename_ from the table source.

Query plan:
{code:java}
== Abstract Syntax Tree ==
LogicalProject(deptno=[$0], ename=[$5])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{3}])
   :- LogicalTableScan(table=[[hive_catalog, db, dept_nested]])
   +- Uncollect
      +- LogicalProject(employees=[$cor1.employees])
         +- LogicalValues(tuples=[[{ 0 }]]){code}
{code:java}
== Optimized Physical Plan ==
Calc(select=[deptno, ename])
+- Correlate(invocation=[$UNNEST_ROWS$1($cor1.employees)], correlate=[table($UNNEST_ROWS$1($cor1.employees))], select=[deptno,name,skillrecord,employees,empno,ename,skills], rowType=[RecordType(BIGINT deptno, VARCHAR(2147483647) name, RecordType:peek_no_expand(VARCHAR(2147483647) skilltype, VARCHAR(2147483647) desc, RecordType:peek_no_expand(VARCHAR(2147483647) a, VARCHAR(2147483647) b) others) skillrecord, RecordType:peek_no_expand(BIGINT empno, VARCHAR(2147483647) ename, RecordType:peek_no_expand(VARCHAR(2147483647) type, VARCHAR(2147483647) desc, RecordType:peek_no_expand(VARCHAR(2147483647) a, VARCHAR(2147483647) b) others) ARRAY skills) ARRAY employees, BIGINT empno, VARCHAR(2147483647) ename, RecordType:peek_no_expand(VARCHAR(2147483647) type, VARCHAR(2147483647) desc, RecordType:peek_no_expand(VARCHAR(2147483647) a, VARCHAR(2147483647) b) others) ARRAY skills)], joinType=[INNER])
   +- TableSourceScan(table=[[hive_catalog, db, dept_nested]], fields=[deptno, name, skillrecord, employees]){code}
{code:java}
== Optimized Execution Plan ==
Calc(select=[deptno, ename])
+- Correlate(invocation=[$UNNEST_ROWS$1($cor1.employees)], correlate=[table($UNNEST_ROWS$1($cor1.employees))], select=[deptno,name,skillrecord,employees,empno,ename,skills], rowType=[RecordType(BIGINT deptno, VARCHAR(2147483647) name, RecordType:peek_no_expand(VARCHAR(2147483647) skilltype, VARCHAR(2147483647) desc, RecordType:peek_no_expand(VARCHAR(2147483647) a, VARCHAR(2147483647) b) others) skillrecord, RecordType:peek_no_expand(BIGINT empno, VARCHAR(2147483647) ename, RecordType:peek_no_expand(VARCHAR(2147483647) type, VARCHAR(2147483647) desc, RecordType:peek_no_expand(VARCHAR(2147483647) a, VARCHAR(2147483647) b) others) ARRAY skills) ARRAY employees, BIGINT empno, VARCHAR(2147483647) ename, RecordType:peek_no_expand(VARCHAR(2147483647) type, VARCHAR(2147483647) desc, RecordType:peek_no_expand(VARCHAR(2147483647) a, VARCHAR(2147483647) b) others) ARRAY skills)], joinType=[INNER])
   +- TableSourceScan(table=[[hive_catalog, db, dept_nested]], fields=[deptno, name, skillrecord, employees]) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 18:48:23 UTC 2024,,,,,,,,,,"0|z1jyag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 12:20;jark;Hi [~vsowrirajan], thank you for reporting this problem. Which Flink version are you using? And could you share the execution plan of the query by executing `EXPLAIN` statement on the query?;;;","23/Aug/23 12:22;jark;[~337361684@qq.com] do you have time to help take a look?;;;","23/Aug/23 12:25;337361684@qq.com;Sure, [~jark] . Please assign to me. Thanks!;;;","23/Aug/23 21:07;vsowrirajan;[~jark] Added the SQL and query plan output. This is happening in the master branch of flink.

My observations and attempt at fixing the issue:
 * Apply calcite's {{CoreRules.ProjectCorrelateTransposeRule}} to push project down to the {{Correlate}} `s inputs
 * Fix {{BatchPhysicalCorrelateRule}} to handle the new logical plan with {{projects}} pushed through {{Correlate}}

Adding CoreRules.ProjectCorrelateTransposeRule in FlinkBatchRuleSets. It fails with the below exception
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule BatchPhysicalCorrelateRule(in:LOGICAL,out:BATCH_PHYSICAL), args [rel#372:FlinkLogicalCorrelate.LOGICAL.any.[](left=RelSubset#368,right=RelSubset#371,correlation=$cor2,joinType=inner,requiredColumns={1})]
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:250)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:59)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:523)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:318)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:93)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:324)
	at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:536)
	at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:115)
	at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:47)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:696)
	at org.apache.flink.table.api.internal.TableImpl.explain(TableImpl.java:482)
	at org.apache.flink.table.api.Explainable.explain(Explainable.java:40)
	at org.apache.flink.table.api.Explainable.printExplain(Explainable.java:57)
	at com.linkedin.tracking3.flink.FlinkTest.runJoinUsingTableAPI(FlinkTest.java:129)
	at com.linkedin.tracking3.flink.FlinkTest.main(FlinkTest.java:316)
Caused by: java.lang.NullPointerException
	at org.apache.calcite.rex.RexProgram.expandLocalRef(RexProgram.java:549)
	at org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalCorrelateRule.convertToCorrelate$1(BatchPhysicalCorrelateRule.scala:67)
	at org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalCorrelateRule.convert(BatchPhysicalCorrelateRule.scala:80)
	at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:172)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223)
{code}
Looking for pointers to fix the above issue. Appreciate your inputs. Thanks.;;;","25/Aug/23 02:07;lsy;[~vsowrirajan] Your attempt look goods to me;;;","25/Aug/23 04:17;337361684@qq.com;Hi, [~vsowrirajan] . I think your idea is good, but the biggest problem currently is how to pass the column cropping condition of LogicalTableFunctionScan to LogicalTableScan and rewrite LogicalTableFunctionScan. So I think we need to add a rule in project_rewrite stage. 
 # Actually, first we need add  rule CoreRules.ProjectCorrelateTransposeRule in FlinkBatchRuleSets to push project into LogicalCorrelate.
 # And we need add a rule in project_rewrite stage to pass by this project into LogicalTableScan side and rewrite LogicalTableFunctionScan.
 # For this npe problem, you can add if logical to avoid it.

Adding one example to explain step: 2

for this ddl

 
{code:java}
String ddl =
        ""CREATE TABLE NestedItemTable1 (\n""
                + ""  `deptno` INT,\n""
                + ""  `employees` MAP<varchar, varchar>\n""
                + "") WITH (\n""
                + "" 'connector' = 'values',\n""
                + "" 'nested-projection-supported' = 'true',""
                + "" 'bounded' = 'true'\n""
                + "")"";
util.tableEnv().executeSql(ddl);

util.verifyRelPlan(
        ""SELECT t1.deptno, k FROM NestedItemTable1 t1, UNNEST(t1.employees) as f(k, v)"");{code}
 

we will get the below plan after add CoreRules.ProjectCorrelateTransposeRule:

 
{code:java}
optimize project_rewrite cost 413675 ms.
optimize result: 
LogicalProject(inputs=[0], exprs=[[$2]])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{1}])
   :- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable1]])
   +- LogicalProject(inputs=[0])
      +- LogicalTableFunctionScan(invocation=[$UNNEST_ROWS$1($cor0.employees)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) f0, VARCHAR(2147483647) f1)]){code}
 

I think for this pattern, we need add a new rule to match this.:

 
{code:java}
+- LogicalCorrelate
   :- LogicalTableScan
   +- LogicalProject
      +- LogicalTableFunctionScan{code}
 

In this rule, we first need to create a new LogicalTableFunctionScan after merge LogicalProject and LogicalTableFunctionScan.

second, we need add a new LogicalProject for LogicalTableScan, which will be push down to LogicalTableScan in logical stage.

IMO, the new plan after match this rule will be (just an example, not correct plan):

 
{code:java}
LogicalProject(inputs=[0], exprs=[[$2]])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{1}])
   :- LogicalProject(inputs=[employees.k])    
      +- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable1]])
   :- LogicalTableFunctionScan(invocation=[$UNNEST_ROWS$1($cor0.employees.k)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) f0)]){code}
 

WDYT?  [~vsowrirajan]. Once the solution is determined and u complete the development, you can ping me to review it.;;;","28/Aug/23 05:17;vsowrirajan;Thanks [~337361684@qq.com] and [~lsy] . [~337361684@qq.com] , it makes sense to me.

To summarize:
 # Apply _CoreRules.ProjectCorrelateTransposeRule_ to {_}FlinkBatchRuleSets{_}. But this only pushes the projects that are referenced by the children of _Correlate_ and not the other projects that needs to be pushed to the TableScan.
 # Introduce another rule that pushes the projects to the TableScan but only if the _Correlate_ is on a known UDTF like {_}UNNEST{_}. Otherwise, for user defined UDTFs we won't know for sure whether it is safe to push to the _TableScan_ or not. What do you think? I'm still thinking about how to write this rule so that it pushes all the projects to the TableScan operator.
 # Merge the _LogicalProject_ and _LogicalTableFunctionScan_ to LogicalTableFunctionScan.;;;","22/Oct/23 23:44;jeyhunkarimov;Hi [~vsowrirajan] [~337361684@qq.com] [~lsy] [~jark], throwing my two cents here:

Adding _CoreRules.ProjectCorrelateTransposeRule_ is not enough to solve the problem because of several reasons:

* Calcite will add two projections (one for the left and one for the right input) [1]. Sometimes some of these projections can be no-op (e.g, without expressions). This will cause null reference error in _BatchPhysicalCorrelateRule.scala: 67_ (_Some(calc.getProgram.expandLocalRef(calc.getProgram.getCondition))_). That is why probably you get this error. 
* However, solving the above issue is probably not enough to get this rule working, mainly because how _CoreRules.ProjectCorrelateTransposeRule_ works. Basically, this rule pushes down projects, without further handling/correcting the references (e.g., LogicalTableFunctionScan will have stale function invocation expression - getCall()). As a result, LogicalTableFunctionScan will try to access some field, however this field is already projected by Calcite rule (there is a LogicalProject operator(s) on top). 
* The above issue will get even complicated, when there are more operators (e.g., filters and projections) which has dangling references after Calcite rule is applied or many nested fields are accessed (this will result in LogicalCorrelate operators nested in each other)


  
About solution, IMO we should either:
# Create a rule that inherits from _CoreRules.ProjectCorrelateTransposeRule_ and overrides its _onMatch_ method. We should gracefully handle the downstream tree of operators when pushing down projections down to the LogicalCorrelate.
# Alternatively, we can use _CoreRules.ProjectCorrelateTransposeRule_ and our own rule to match


{code:java}
+- LogicalCorrelate
   :- LogicalProject
{code}

We cannot force matching LogicalTableFunctionScan or LogicalTableScan because dangling references can be in anywhere of the query plan. We need to 1) find all RexCall fields of LogicalTableFunctionScan, 2) check if they exists after projection pushdown, 3) if not, find to which [new] project expressions they correspond, and 4) rewire them. This potentially requires to rewrite expressions thought the whole query plan until the leaf node. 

Also, we do not need to merge LogicalProject and LogicalTableScan as part of this rule, since other rules will already do it. 

What do you guys think?

[1] https://github.com/apache/calcite/blob/c83ac69111fd9e75af5e3615af29a72284667a4a/core/src/main/java/org/apache/calcite/rel/rules/ProjectCorrelateTransposeRule.java#L126;;;","18/Jan/24 18:48;vsowrirajan;[~jeyhun] Thanks for the detailed response. Sorry for the delayed response.

Yes, you're correct that _CoreRules.ProjectCorrelateTransposeRule_ is not enough and it results in the above error you mentioned and even after resolving it by other means it still won't work as you described above.

I was actually taking the approach of extending the _CoreRules.ProjectCorrelateTransposeRule_ and overriding the _onMatch_ method. The issue I was facing there was Calcite doesn't support expressing nested fields on a collection type (Map or Array) for eg: arr.a.b where arr is an Array type. Nested fields are typically represented through _RexFieldAccess_ but it is not supported on a collection type (Array) holding a bunch of {_}structs{_}. I was trying to extend the _RexFieldAccess_ internally with in Flink that handles the collection case. The new _RexFieldAccess_ (say {_}FlinkRexFieldAccess{_}) would be used in the new rule to push the nested projections on the collection down.

 

Additionally, it also requires the above adjustments you mentioned to be done to make sure there are no dangling references once the plan is rewritten. Let me know your thoughts. Thanks!

 

I got pulled in to other important things for the time being and not able to return to this problem. Hopefully, I will get some spare cycles in the coming weeks to address this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink 1.17.0 has missing transitive dependency for pyopenssl,FLINK-32939,13548273,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,engnatha,engnatha,22/Aug/23 17:20,22/Aug/23 17:20,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"When running a pyflink job recently, we got an error about not being able to import something from pyopenssl correctly. Here's the traceback.
{code:bash}
E                   Caused by: java.lang.RuntimeException: Failed to create stage bundle factory! Traceback (most recent call last):
E                     File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
E                       return _run_code(code, main_globals, None,
E                     File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
E                       exec(code, run_globals)
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_boot.py"", line 36, in <module>
E                       from apache_beam.portability.api.org.apache.beam.model.fn_execution.v1.beam_fn_api_pb2 import \
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/apache_beam/__init__.py"", line 93, in <module>
E                       from apache_beam import io
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/apache_beam/io/__init__.py"", line 27, in <module>
E                       from apache_beam.io.mongodbio import *
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/apache_beam/io/mongodbio.py"", line 93, in <module>
E                       from bson import json_util
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/bson/json_util.py"", line 130, in <module>
E                       from pymongo.errors import ConfigurationError
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/pymongo/__init__.py"", line 114, in <module>
E                       from pymongo.collection import ReturnDocument
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/pymongo/collection.py"", line 26, in <module>
E                       from pymongo import common, helpers, message
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/pymongo/common.py"", line 38, in <module>
E                       from pymongo.ssl_support import validate_allow_invalid_certs, validate_cert_reqs
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/pymongo/ssl_support.py"", line 27, in <module>
E                       import pymongo.pyopenssl_context as _ssl
E                     File ""/home/buildbot/.cache/pants/named_caches/pex_root/venvs/s/361094c4/venv/lib/python3.8/site-packages/pymongo/pyopenssl_context.py"", line 27, in <module>
E                       from OpenSSL import SSL as _SSL
E                     File ""/usr/local/lib/python3.8/dist-packages/OpenSSL/__init__.py"", line 8, in <module>
E                       from OpenSSL import crypto, SSL
E                     File ""/usr/local/lib/python3.8/dist-packages/OpenSSL/crypto.py"", line 1556, in <module>
E                       class X509StoreFlags(object):
E                     File ""/usr/local/lib/python3.8/dist-packages/OpenSSL/crypto.py"", line 1577, in X509StoreFlags
E                       CB_ISSUER_CHECK = _lib.X509_V_FLAG_CB_ISSUER_CHECK
E                   AttributeError: module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK'
{code}
It seems to be the case from this traceback that apache-flink depends on apache-beam which depends on pymongo which wants to depend on pyopenssl. In order to do that within the pymongo library, users need to specify `pymongo[ocsp]` as their dependency instead of just `pymongo`. It looks like apache-beam is just specifying `pymongo` and then doing some horrible python path mutilation to find some random installation on the system path. The tool we are using (pantsbuild) modifies python path at the start, so it shouldn't have been possible to find this installation.

I believe this is an Apache Beam problem, but Jira will not let me make an issue there. Since this affects all Flink python users, though, it seems appropriate to be here as whatever fix comes to Beam should be worked downstream into Flink.","Ubuntu 20.04
Flink 1.17.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 17:20:18.0,,,,,,,,,,"0|z1jy3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-pulsar should remove all `PulsarAdmin` calls,FLINK-32938,13548269,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nlu90,nlu90,nlu90,22/Aug/23 16:45,21/Sep/23 03:16,04/Jun/24 20:40,21/Sep/23 03:16,,,,,,pulsar-4.1.0,,,,Connectors / Pulsar,,,,,,1,pull-request-available,,,"The flink-connector-pulsar should not access and interact with the admin endpoint. This could introduce potential security issues.

In a production environment, a Pulsar cluster admin will not grant the permissions for the flink application to conduct any admin operations. 

Currently, the connector does various admin calls:

```{{{}{}}}{{{}{}}}
PulsarAdmin.topics().getPartitionedTopicMetadata(topic)
PulsarAdmin.namespaces().getTopics(namespace)
PulsarAdmin.topics().getLastMessageId(topic)
PulsarAdmin.topics().getMessageIdByTimestamp(topic, timestamp)
PulsarAdmin.topics().getSubscriptions(topic)
PulsarAdmin.topics().createSubscription(topic, subscription, MessageId.earliest)
PulsarAdmin.topics().resetCursor(topic, subscription, initial, !include)
```

We need to replace these calls with consumer or client calls.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 21 03:16:12 UTC 2023,,,,,,,,,,"0|z1jy2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 21:05;nlu90;I have prepared a draft PR for solve this issue:
https://github.com/apache/flink-connector-pulsar/pull/59;;;","21/Sep/23 03:16;tison;master via 78d00ea9e3e278d4ce2fbb0c8a8d380abef7b858;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a new version in JIRA,FLINK-32937,13548253,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:52,23/Aug/23 10:10,04/Jun/24 20:40,22/Aug/23 14:54,,,,,,,,,,,,,,,,0,,,,"When contributors resolve an issue in JIRA, they are tagging it with a release that will contain their changes. With the release currently underway, new issues should be resolved against a subsequent future release. Therefore, you should create a release item for this subsequent release, as follows:
 # In JIRA, navigate to the [Flink > Administration > Versions|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions].
 # Add a new release: choose the next minor version number compared to the one currently underway, select today’s date as the Start Date, and choose Add.
(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

 
----
h3. Expectations
 * The new version should be listed in the dropdown menu of {{fixVersion}} or {{affectedVersion}} under ""unreleased versions"" when creating a new Jira issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 22 14:53:58 UTC 2023,,,,,,,,,,"0|z1jxz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 14:53;Sergey Nuyanzin;Checked in affected/fixed version dropdown and in https://issues.apache.org/jira/projects/FLINK?selectedItem=com.atlassian.jira.jira-projects-plugin%3Arelease-page&status=unreleased
it is present;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vote on the release candidate,FLINK-32936,13548250,13548246,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:40,22/Aug/23 14:42,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,,0,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:40:17.0,,,,,,,,,,"0|z1jxyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propose a pull request for website updates,FLINK-32935,13548249,13548246,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:40,22/Aug/23 14:41,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:40:17.0,,,,,,,,,,"0|z1jxy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage source and binary releases on dist.apache.org,FLINK-32934,13548248,13548246,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:40,22/Aug/23 14:41,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:40:17.0,,,,,,,,,,"0|z1jxy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build and stage Java and Python artifacts,FLINK-32933,13548247,13548246,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:40,22/Aug/23 14:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:40:17.0,,,,,,,,,,"0|z1jxxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.18.0-rc1,FLINK-32932,13548246,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:40,14/Dec/23 15:17,04/Jun/24 20:40,25/Oct/23 12:46,1.18.0,,,,,,,,,,,,,,,0,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.

h4. Prerequisites
Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code}
RC_NUM=""1""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,FLINK-32925,FLINK-32921,FLINK-32726,FLINK-32920,,,,,,,FLINK-33271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:40:17.0,,,,,,,,,,"0|z1jxxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish the Dockerfiles for the new release,FLINK-32931,13548242,13548198,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:18,29/Oct/23 15:53,04/Jun/24 20:40,29/Oct/23 15:53,,,,,,,,,,,,,,,,0,pull-request-available,,,"Note: the official Dockerfiles fetch the binary distribution of the target Flink version from an Apache mirror. After publishing the binary release artifacts, mirrors can take some hours to start serving the new artifacts, so you may want to wait to do this step until you are ready to continue with the ""Promote the release"" steps in the follow-up Jira.

Follow the [release instructions in the flink-docker repo|https://github.com/apache/flink-docker#release-workflow] to build the new Dockerfiles and send an updated manifest to Docker Hub so the new images are built and published.

 
----
h3. Expectations

 * Dockerfiles in [flink-docker|https://github.com/apache/flink-docker] updated for the new Flink release and pull request opened on the Docker official-images with an updated manifest",,,,,,,,,,,FLINK-32928,FLINK-32929,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 29 15:53:21 UTC 2023,,,,,,,,,,"0|z1jxwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 12:07;jingge;Opened a PR with the official docker library: [https://github.com/docker-library/official-images/pull/15627]

Asked [~xtsong]  to help with releasing from the apache/flink dockerHub repo;;;","27/Oct/23 23:19;jingge;[https://hub.docker.com/r/apache/flink] is online;;;","29/Oct/23 15:53;jingge;PR merged. The official images are online: https://hub.docker.com/_/flink/tags;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Git tag and mark version as released in Jira,FLINK-32930,13548241,13548198,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:17,02/Nov/23 10:23,04/Jun/24 20:40,24/Oct/23 12:45,,,,,,,,,,,,,,,,0,,,,"Create and push a new Git tag for the released version by copying the tag for the final release candidate, as follows:
{code:java}
$ git tag -s ""release-${RELEASE_VERSION}"" refs/tags/${TAG}^{} -m ""Release Flink ${RELEASE_VERSION}""
$ git push <remote> refs/tags/release-${RELEASE_VERSION}
{code}
In JIRA, inside [version management|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions], hover over the current release and a settings menu will appear. Click Release, and select today’s date.

(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

If PRs have been merged to the release branch after the the last release candidate was tagged, make sure that the corresponding Jira tickets have the correct Fix Version set.

 
----
h3. Expectations
 * Release tagged in the source code repository
 * Release version finalized in JIRA. (Note: Not all committers have administrator access to JIRA. If you end up getting permissions errors ask on the mailing list for assistance)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 02 10:23:42 UTC 2023,,,,,,,,,,"0|z1jxwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 10:23;mapohl;[~jingge] is anyone reviewing the changes? I'm just concerned that we're missing things, e.g. we missed marking the 1.18.0 as released in Jira even though it should be covered by this issue.

I'm gonna go ahead and mark 1.18.0 as released in Jira. ...to have this FLINK-32930 fully resolved. I hope that's ok with you;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy artifacts to Maven Central Repository,FLINK-32929,13548240,13548198,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:17,24/Oct/23 12:46,04/Jun/24 20:40,24/Oct/23 12:45,,,,,,,,,,,,,,,,0,,,,"Use the [Apache Nexus repository|https://repository.apache.org/] to release the staged binary artifacts to the Maven Central repository. In the Staging Repositories section, find the relevant release candidate orgapacheflink-XXX entry and click Release. Drop all other release candidates that are not being released.
h3. Deploy source and binary releases to dist.apache.org

Copy the source and binary releases from the dev repository to the release repository at [dist.apache.org|http://dist.apache.org/] using Subversion.
{code:java}
$ svn move -m ""Release Flink ${RELEASE_VERSION}"" https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM} https://dist.apache.org/repos/dist/release/flink/flink-${RELEASE_VERSION}
{code}
(Note: Only PMC members have access to the release repository. If you do not have access, ask on the mailing list for assistance.)
h3. Remove old release candidates from [dist.apache.org|http://dist.apache.org/]

Remove the old release candidates from [https://dist.apache.org/repos/dist/dev/flink] using Subversion.
{code:java}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
$ cd flink
$ svn remove flink-${RELEASE_VERSION}-rc*
$ svn commit -m ""Remove old release candidates for Apache Flink ${RELEASE_VERSION}
{code}
 
----
h3. Expectations
 * Maven artifacts released and indexed in the [Maven Central Repository|https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.flink%22] (usually takes about a day to show up)
 * Source & binary distributions available in the release repository of [https://dist.apache.org/repos/dist/release/flink/]
 * Dev repository [https://dist.apache.org/repos/dist/dev/flink/] is empty
 * Website contains links to new release binaries and sources in download page
 * (for minor version updates) the front page references the correct new major release version and directs to the correct link",,,,,,,,,,,,,,FLINK-32931,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:17:20.0,,,,,,,,,,"0|z1jxw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy Python artifacts to PyPI,FLINK-32928,13548239,13548198,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 14:16,24/Oct/23 12:46,04/Jun/24 20:40,24/Oct/23 12:45,,,,,,,,,,,,,,,,0,,,,"Release manager should create a PyPI account and ask the PMC add this account to pyflink collaborator list with Maintainer role (The PyPI admin account info can be found here. NOTE, only visible to PMC members) to deploy the Python artifacts to PyPI. The artifacts could be uploaded using twine([https://pypi.org/project/twine/]). To install twine, just run:
{code:java}
pip install --upgrade twine==1.12.0
{code}
Download the python artifacts from dist.apache.org and upload it to pypi.org:
{code:java}
svn checkout https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
cd flink-${RELEASE_VERSION}-rc${RC_NUM}
 
cd python
 
#uploads wheels
for f in *.whl; do twine upload --repository-url https://upload.pypi.org/legacy/ $f $f.asc; done
 
#upload source packages
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-libraries-${RELEASE_VERSION}.tar.gz apache-flink-libraries-${RELEASE_VERSION}.tar.gz.asc
 
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-${RELEASE_VERSION}.tar.gz apache-flink-${RELEASE_VERSION}.tar.gz.asc
{code}
If upload failed or incorrect for some reason (e.g. network transmission problem), you need to delete the uploaded release package of the same version (if exists) and rename the artifact to \{{{}apache-flink-${RELEASE_VERSION}.post0.tar.gz{}}}, then re-upload.

(!) Note: re-uploading to pypi.org must be avoided as much as possible because it will cause some irreparable problems. If that happens, users cannot install the apache-flink package by explicitly specifying the package version, i.e. the following command ""pip install apache-flink==${RELEASE_VERSION}"" will fail. Instead they have to run ""pip install apache-flink"" or ""pip install apache-flink==${RELEASE_VERSION}.post0"" to install the apache-flink package.

 
----
h3. Expectations
 * Python artifacts released and indexed in the [PyPI|https://pypi.org/project/apache-flink/] Repository",,,,,,,,,,,,,,FLINK-32931,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 14:16:44.0,,,,,,,,,,"0|z1jxw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify that no exclusions were erroneously added to the japicmp plugin,FLINK-32927,13548228,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 13:26,15/Nov/23 23:27,04/Jun/24 20:40,15/Nov/23 23:27,,,,,,,,,,,,,,,,0,,,,"Verify that no exclusions were erroneously added to the japicmp plugin that break compatibility guarantees. Check the exclusions for the japicmp-maven-plugin in the root pom (see [apache/flink:pom.xml:2175ff|https://github.com/apache/flink/blob/3856c49af77601cf7943a5072d8c932279ce46b4/pom.xml#L2175] for exclusions that:
* For minor releases: break source compatibility for \{{@Public}} APIs
* For patch releases: break source/binary compatibility for \{{@Public}}/\{{@PublicEvolving}}  APIs
Any such exclusion must be properly justified, in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 14 11:24:02 UTC 2023,,,,,,,,,,"0|z1jxtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 11:24;Sergey Nuyanzin;Based on local check there is no exclusions within 1,18 release ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a release branch,FLINK-32926,13548227,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 13:26,06/Sep/23 23:27,04/Jun/24 20:40,06/Sep/23 23:27,,,,,,,,,,,,,,,,0,,,,"If you are doing a new minor release, you need to update Flink version in the following repositories and the [AzureCI project configuration|https://dev.azure.com/apache-flink/apache-flink/]:
 * [apache/flink|https://github.com/apache/flink]
 * [apache/flink-docker|https://github.com/apache/flink-docker]
 * [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks]

Patch releases don't require the these repositories to be touched. Simply checkout the already existing branch for that version:
{code:java}
$ git checkout release-$SHORT_RELEASE_VERSION
{code}
h4. Flink repository

Create a branch for the new version that we want to release before updating the master branch to the next development version:
{code:bash}
$ cd ./tools
tools $ releasing/create_snapshot_branch.sh
tools $ git checkout master
tools $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$NEXT_SNAPSHOT_VERSION releasing/update_branch_version.sh
{code}
In the {{master}} branch, add a new value (e.g. {{{}v1_16(""1.16""){}}}) to [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] as the last entry:
{code:java}
// ...
v1_12(""1.12""),
v1_13(""1.13""),
v1_14(""1.14""),
v1_15(""1.15""),
v1_16(""1.16"");
{code}
The newly created branch and updated {{master}} branch need to be pushed to the official repository.
h4. Flink Docker Repository

Afterwards fork off from {{dev-master}} a {{dev-x.y}} branch in the [apache/flink-docker|https://github.com/apache/flink-docker] repository. Make sure that [apache/flink-docker:.github/workflows/ci.yml|https://github.com/apache/flink-docker/blob/dev-master/.github/workflows/ci.yml] points to the correct snapshot version; for {{dev-x.y}} it should point to {{{}x.y-SNAPSHOT{}}}, while for {{dev-master}} it should point to the most recent snapshot version (
{[$NEXT_SNAPSHOT_VERSION}}).

After pushing the new minor release branch, as the last step you should also update the documentation workflow to also build the documentation for the new release branch. Check [Managing Documentation|https://cwiki.apache.org/confluence/display/FLINK/Managing+Documentation] on details on how to do that. You may also want to manually trigger a build to make the changes visible as soon as possible.
h4. Flink Benchmark Repository

First of all, checkout the {{master}} branch to {{dev-x.y}} branch in [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks], so that we can have a branch named {{dev-x.y}} which could be built on top of (${{{}CURRENT_SNAPSHOT_VERSION{}}}).

Then, inside the repository you need to manually update the {{flink.version}} property inside the parent *pom.xml* file. It should be pointing to the most recent snapshot version ($NEXT_SNAPSHOT_VERSION). For example:
{code:xml}
<flink.version>1.18-SNAPSHOT</flink.version>
{code}
h4. AzureCI Project Configuration

The new release branch needs to be configured within AzureCI to make azure aware of the new release branch. This matter can only be handled by Ververica employees since they are owning the AzureCI setup.
 
----
h3. Expectations (Minor Version only if not stated otherwise)
 * Release branch has been created and pushed
 * Changes on the new release branch are picked up by [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary]
 * {{master}} branch has the version information updated to the new version (check pom.xml files and 
 * [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum)
 * New version is added to the [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum.
 * Make sure [flink-docker|https://github.com/apache/flink-docker/] has {{dev-x.y}} branch and docker e2e tests run against this branch in the corresponding Apache Flink release branch (see [apache/flink:flink-end-to-end-tests/test-scripts/common_docker.sh:51|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/common_docker.sh#L51])
 * [apache-flink:docs/config.toml|https://github.com/apache/flink/blob/release-1.17/docs/config.toml] has been updated appropriately in the new Apache Flink release branch.
 * The {{flink.version}} property (see [apache/flink-benchmarks:pom.xml|https://github.com/apache/flink-benchmarks/blob/master/pom.xml#L48] of Flink Benchmark repo has been updated to the latest snapshot version.
 * Make announcement in Flink's dev ML",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 13:26:22.0,,,,,,,,,,"0|z1jxtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select executing Release Manager,FLINK-32925,13548226,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 13:11,24/Oct/23 12:42,04/Jun/24 20:40,14/Sep/23 11:51,,,,,,,,,,,,,,,,0,,,,"h4. GPG Key

You need to have a GPG key to sign the release artifacts. Please be aware of the ASF-wide [release signing guidelines|https://www.apache.org/dev/release-signing.html]. If you don’t have a GPG key associated with your Apache account, please create one according to the guidelines.

Determine your Apache GPG Key and Key ID, as follows:
{code:java}
$ gpg --list-keys
{code}
This will list your GPG keys. One of these should reflect your Apache account, for example:
{code:java}
--------------------------------------------------
pub   2048R/845E6689 2016-02-23
uid                  Nomen Nescio <anonymous@apache.org>
sub   2048R/BA4D50BE 2016-02-23
{code}
In the example above, the key ID is the 8-digit hex string in the \{{pub}} line: \{{{}845E6689{}}}.

Now, add your Apache GPG key to the Flink’s \{{KEYS}} file in the [Apache Flink release KEYS file|https://dist.apache.org/repos/dist/release/flink/KEYS] repository at [dist.apache.org|http://dist.apache.org/]. Follow the instructions listed at the top of these files. (Note: Only PMC members have write access to the release repository. If you end up getting 403 errors ask on the mailing list for assistance.)

Configure \{{git}} to use this key when signing code by giving it your key ID, as follows:
{code:java}
$ git config --global user.signingkey 845E6689
{code}
You may drop the \{{--global}} option if you’d prefer to use this key for the current repository only.

You may wish to start \{{gpg-agent}} to unlock your GPG key only once using your passphrase. Otherwise, you may need to enter this passphrase hundreds of times. The setup for \{{gpg-agent}} varies based on operating system, but may be something like this:
{code:bash}
$ eval $(gpg-agent --daemon --no-grab --write-env-file $HOME/.gpg-agent-info)
$ export GPG_TTY=$(tty)
$ export GPG_AGENT_INFO
{code}
h4. Access to Apache Nexus repository

Configure access to the [Apache Nexus repository|https://repository.apache.org/], which enables final deployment of releases to the Maven Central Repository.
 # You log in with your Apache account.
 # Confirm you have appropriate access by finding \{{org.apache.flink}} under \{{{}Staging Profiles{}}}.
 # Navigate to your \{{Profile}} (top right drop-down menu of the page).
 # Choose \{{User Token}} from the dropdown, then click \{{{}Access User Token{}}}. Copy a snippet of the Maven XML configuration block.
 # Insert this snippet twice into your global Maven \{{settings.xml}} file, typically \{{{}${HOME}/.m2/settings.xml{}}}. The end result should look like this, where \{{TOKEN_NAME}} and \{{TOKEN_PASSWORD}} are your secret tokens:
{code:xml}
<settings>
   <servers>
     <server>
       <id>apache.releases.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
     <server>
       <id>apache.snapshots.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
   </servers>
 </settings>
{code}

h4. Website development setup

Get ready for updating the Flink website by following the [website development instructions|https://flink.apache.org/contributing/improve-website.html].
h4. GNU Tar Setup for Mac (Skip this step if you are not using a Mac)

The default tar application on Mac does not support GNU archive format and defaults to Pax. This bloats the archive with unnecessary metadata that can result in additional files when decompressing (see [1.15.2-RC2 vote thread|https://lists.apache.org/thread/mzbgsb7y9vdp9bs00gsgscsjv2ygy58q]). Install gnu-tar and create a symbolic link to use in preference of the default tar program.
{code:bash}
$ brew install gnu-tar
$ ln -s /usr/local/bin/gtar /usr/local/bin/tar
$ which tar
{code}
 
----
h3. Expectations
 * Release Manager’s GPG key is published to [dist.apache.org|http://dist.apache.org/]
 * Release Manager’s GPG key is configured in git configuration
 * Release Manager's GPG key is configured as the default gpg key.
 * Release Manager has \{{org.apache.flink}} listed under Staging Profiles in Nexus
 * Release Manager’s Nexus User Token is configured in settings.xml",,,,,,,,,,,,,,FLINK-32932,FLINK-33271,FLINK-33347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 14 11:12:25 UTC 2023,,,,,,,,,,"0|z1jxt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/23 02:31;renqs;Jing's public key has been published: [https://dist.apache.org/repos/dist/release/flink/KEYS]

[~jingge] could you check if all steps described above has been done and close this issue before creating the first RC? Thanks;;;","14/Sep/23 11:12;jingge;I am not sure if you have the same issue, but the symbolic link does not work for me on mac:

$ ln -s /usr/local/bin/gtar /usr/local/bin/tar

it should be /opt/homebrew/bin/gtar

And we don't need that if we can just use gtar.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review Release Notes in JIRA,FLINK-32924,13548225,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 13:09,13/Nov/23 22:35,04/Jun/24 20:40,13/Nov/23 22:35,,,,,,,,,,,,,,,,0,,,,"JIRA automatically generates Release Notes based on the \{{Fix Version}} field applied to issues. Release Notes are intended for Flink users (not Flink committers/contributors). You should ensure that Release Notes are informative and useful.

Open the release notes from the version status page by choosing the release underway and clicking Release Notes.

You should verify that the issues listed automatically by JIRA are appropriate to appear in the Release Notes. Specifically, issues should:
 * Be appropriately classified as \{{{}Bug{}}}, \{{{}New Feature{}}}, \{{{}Improvement{}}}, etc.
 * Represent noteworthy user-facing changes, such as new functionality, backward-incompatible API changes, or performance improvements.
 * Have occurred since the previous release; an issue that was introduced and fixed between releases should not appear in the Release Notes.
 * Have an issue title that makes sense when read on its own.

Adjust any of the above properties to the improve clarity and presentation of the Release Notes.

Ensure that the JIRA release notes are also included in the release notes of the documentation (see section ""Review and update documentation"").
h4. Content of Release Notes field from JIRA tickets 

To get the list of ""release notes"" field from JIRA, you can ran the following script using JIRA REST API (notes the maxResults limits the number of entries):
{code:bash}
curl -s https://issues.apache.org/jira//rest/api/2/search?maxResults=200&jql=project%20%3D%20FLINK%20AND%20%22Release%20Note%22%20is%20not%20EMPTY%20and%20fixVersion%20%3D%20${RELEASE_VERSION} | jq '.issues[]|.key,.fields.summary,.fields.customfield_12310192' | paste - - -
{code}
{\{jq}}  is present in most Linux distributions and on MacOS can be installed via brew.

 
----
h3. Expectations
 * Release Notes in JIRA have been audited and adjusted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 22:35:27 UTC 2023,,,,,,,,,,"0|z1jxsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 22:35;Sergey Nuyanzin;PR for release notes has been created and reviewed
https://github.com/apache/flink/pull/23527;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triage release-blocking issues in JIRA,FLINK-32923,13548222,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 13:04,13/Nov/23 22:33,04/Jun/24 20:40,13/Nov/23 22:33,,,,,,,,,,,,,,,,0,,,,"There could be outstanding release-blocking issues, which should be triaged before proceeding to build a release candidate. We track them by assigning a specific Fix version field even before the issue resolved.

The list of release-blocking issues is available at the version status page. Triage each unresolved issue with one of the following resolutions:
 * If the issue has been resolved and JIRA was not updated, resolve it accordingly.
 * If the issue has not been resolved and it is acceptable to defer this until the next release, update the Fix Version field to the new version you just created. Please consider discussing this with stakeholders and the dev@ mailing list, as appropriate.
 ** When using ""Bulk Change"" functionality of Jira
 *** First, add the newly created version to Fix Version for all unresolved tickets that have old the old version among its Fix Versions.
 *** Afterwards, remove the old version from the Fix Version.
 * If the issue has not been resolved and it is not acceptable to release until it is fixed, the release cannot proceed. Instead, work with the Flink community to resolve the issue.

 
----
h3. Expectations
 * There are no release blocking JIRA issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 22:33:19 UTC 2023,,,,,,,,,,"0|z1jxs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 22:33;Sergey Nuyanzin;All blockers of 1.18.0 has been swiped out. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review and update documentation,FLINK-32922,13548220,13548217,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 12:57,15/Nov/23 23:27,04/Jun/24 20:40,15/Nov/23 23:27,1.18.0,,,,,1.18.1,1.19.0,,,,,,,,,0,pull-request-available,,,"There are a few pages in the documentation that need to be reviewed and updated for each release.
 * Ensure that there exists a release notes page for each non-bugfix release (e.g., 1.5.0) in {{{}./docs/release-notes/{}}}, that it is up-to-date, and linked from the start page of the documentation.
 * Upgrading Applications and Flink Versions: [https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html]
 * ...

 
----
h3. Expectations
 * Update upgrade compatibility table ([apache-flink:./docs/content/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content/docs/ops/upgrading.md#compatibility-table] and [apache-flink:./docs/content.zh/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content.zh/docs/ops/upgrading.md#compatibility-table]).
 * Update [Release Overview in Confluence|https://cwiki.apache.org/confluence/display/FLINK/Release+Management+and+Feature+Plan]
 * (minor only) The documentation for the new major release is visible under [https://nightlies.apache.org/flink/flink-docs-release-$SHORT_RELEASE_VERSION] (after at least one [doc build|https://github.com/apache/flink/actions/workflows/docs.yml] succeeded).
 * (minor only) The documentation for the new major release does not contain ""-SNAPSHOT"" in its version title, and all links refer to the corresponding version docs instead of {{{}master{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 15 23:26:51 UTC 2023,,,,,,,,,,"0|z1jxrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 23:26;Sergey Nuyanzin;Merged as 
master: [012704d9884f92274495fbf6fdb7234373944212|https://github.com/apache/flink/commit/012704d9884f92274495fbf6fdb7234373944212]
1.18: [f0c7c97ead78efd2d5d48fd81c937c7488a787a9|https://github.com/apache/flink/commit/f0c7c97ead78efd2d5d48fd81c937c7488a787a9];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepare Flink 1.18 Release,FLINK-32921,13548217,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 12:54,24/Oct/23 12:42,04/Jun/24 20:40,13/Oct/23 13:41,1.18.0,,,,,,,,,Release System,,,,,,0,,,,"This umbrella issue is meant as a test balloon for moving the [release documentation|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release] into Jira.
h3. Prerequisites
h4. Environment Variables

Commands in the subtasks might expect some of the following enviroment variables to be set accordingly to the version that is about to be released:
{code:bash}
RELEASE_VERSION=""1.5.0""
SHORT_RELEASE_VERSION=""1.5""
CURRENT_SNAPSHOT_VERSION=""$SHORT_RELEASE_VERSION-SNAPSHOT""
NEXT_SNAPSHOT_VERSION=""1.6-SNAPSHOT""
SHORT_NEXT_SNAPSHOT_VERSION=""1.6""
{code}
h4. Build Tools

All of the following steps require to use Maven 3.8.6 and Java 8. Modify your PATH environment variable accordingly if needed.
h4. Flink Source
 * Create a new directory for this release and clone the Flink repository from Github to ensure you have a clean workspace (this step is optional).
 * Run {{mvn -Prelease clean install}} to ensure that the build processes that are specific to that profile are in good shape (this step is optional).

The rest of this instructions assumes that commands are run in the root (or {{./tools}} directory) of a repository on the branch of the release version with the above environment variables set.",,,,,,,,,,,,,,FLINK-32932,FLINK-33271,FLINK-33347,,FLINK-32726,,,,FLINK-31146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 12:54:55.0,,,,,,,,,,"0|z1jxr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Finalize release 1.18.0,FLINK-32920,13548198,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:47,16/Mar/24 00:53,04/Jun/24 20:40,29/Oct/23 15:54,,,,,,,,,,,,,,,,0,,,,"Once the release candidate has been reviewed and approved by the community, the release should be finalized. This involves the final deployment of the release candidate to the release repositories, merging of the website changes, etc.",,,,,,,,,,,FLINK-32932,FLINK-33271,FLINK-33347,FLINK-32912,,,,,,,FLINK-34697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 11:47:27.0,,,,,,,,,,"0|z1jxn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates the docs stable version,FLINK-32919,13548197,13548190,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,13/Nov/23 23:12,04/Jun/24 20:40,13/Nov/23 23:12,,,,,,,,,,,,,,,,0,,,,"Update docs to ""stable"" in {{docs/config.toml}} in the branch of the _just-released_ version:
 * Change V{{{}ersion{}}} from {{{}x.y-SNAPSHOT }}to \{{{}x.y.z{}}}, i.e. {{1.6-SNAPSHOT}} to {{1.6.0}}
 * Change V{{{}ersionTitle{}}} from {{x.y-SNAPSHOT}} to {{{}x.y{}}}, i.e. {{1.6-SNAPSHOT}} to {{1.6}}
 * Change Branch from {{master}} to {{{}release-x.y{}}}, i.e. {{master}} to {{release-1.6}}
 * Change {{baseURL}} from {{//[ci.apache.org/projects/flink/flink-docs-master|http://ci.apache.org/projects/flink/flink-docs-master]}} to {{//[ci.apache.org/projects/flink/flink-docs-release-x.y|http://ci.apache.org/projects/flink/flink-docs-release-x.y]}}
 * Change {{javadocs_baseurl}} from {{//[ci.apache.org/projects/flink/flink-docs-master|http://ci.apache.org/projects/flink/flink-docs-master]}} to {{//[ci.apache.org/projects/flink/flink-docs-release-x.y|http://ci.apache.org/projects/flink/flink-docs-release-x.y]}}
 * Change {{IsStable}} to {{true}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 23:12:50 UTC 2023,,,,,,,,,,"0|z1jxmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 23:12;Sergey Nuyanzin;done within https://github.com/apache/flink/commit/cfa4a9c35563fd8a5973ec2f35251190e365be14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update reference data for Migration Tests,FLINK-32918,13548196,13548190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,21/Nov/23 22:59,04/Jun/24 20:40,21/Nov/23 22:59,,,,,,1.18.1,1.19.0,,,,,,,,,0,pull-request-available,,,"Update migration tests in master to cover migration from new version. Since 1.18, this step could be done automatically with the following steps. For more information please refer to [this page.|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/README.md]
 # {*}On the published release tag (e.g., release-1.16.0){*}, run 
{panel}
{panel}
|{{$ mvn clean }}{{package}} {{{}-Pgenerate-migration-test-data -Dgenerate.version={}}}{{{}1.16{}}} {{-nsu -Dfast -DskipTests}}|

The version (1.16 in the command above) should be replaced with the target one.

 # Modify the content of the file [apache/flink:flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version] to the latest version (it would be ""v1_16"" if sticking to the example where 1.16.0 was released). 
 # Commit the modification in step a and b with ""{_}[release] Generate reference data for state migration tests based on release-1.xx.0{_}"" to the corresponding release branch (e.g. {{release-1.16}} in our example), replace ""xx"" with the actual version (in this example ""16""). You should use the Jira issue ID in case of [release]  as the commit message's prefix if you have a dedicated Jira issue for this task.

 # Cherry-pick the commit to the master branch. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 22:58:57 UTC 2023,,,,,,,,,,"0|z1jxmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 22:58;Sergey Nuyanzin;Merged to master

as [5720d69e40e20bbdc56d4c93f70a01333a99078d|https://github.com/apache/flink/commit/5720d69e40e20bbdc56d4c93f70a01333a99078d]
1.18 [c96095c4672c2329424c2473a0e16bbe7579d4fc|https://github.com/apache/flink/commit/c96095c4672c2329424c2473a0e16bbe7579d4fc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Other announcements,FLINK-32917,13548195,13548190,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,14/Nov/23 09:28,04/Jun/24 20:40,14/Nov/23 09:28,,,,,,,,,,,,,,,,0,,,,"h3. Recordkeeping

Use [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink] to seed the information about the release into future project reports.

(Note: Only PMC members have access report releases. If you do not have access, ask on the mailing list for assistance.)
h3. Flink blog

Major or otherwise important releases should have a blog post. Write one if needed for this particular release. Minor releases that don’t introduce new major functionality don’t necessarily need to be blogged (see [flink-web PR #581 for Flink 1.15.3|https://github.com/apache/flink-web/pull/581] as an example for a minor release blog post).

Please make sure that the release notes of the documentation (see section ""Review and update documentation"") are linked from the blog post of a major release.
We usually include the names of all contributors in the announcement blog post. Use the following command to get the list of contributors:
{code}
# first line is required to make sort first with uppercase and then lower
export LC_ALL=C
export FLINK_PREVIOUS_RELEASE_BRANCH=<previousReleaseBranch>
export FLINK_CURRENT_RELEASE_BRANCH=<currentReleaseBranch>
# e.g.
# export FLINK_PREVIOUS_RELEASE_BRANCH=release-1.17
# export FLINK_CURRENT_RELEASE_BRANCH=release-1.18
git log $(git merge-base master $FLINK_PREVIOUS_RELEASE_BRANCH)..$(git show-ref --hash ${FLINK_CURRENT_RELEASE_BRANCH}) --pretty=format:""%an%n%cn"" | sort  -u | paste -sd, | sed ""s/\,/\, /g""
{code}
h3. Social media

Tweet, post on Facebook, LinkedIn, and other platforms. Ask other contributors to do the same.
h3. Flink Release Wiki page

Add a summary of things that went well or that went not so well during the release process. This can include feedback from contributors but also more generic things like the release have taken longer than initially anticipated (and why) to give a bit of context to the release process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 14 09:25:19 UTC 2023,,,,,,,,,,"0|z1jxmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 09:25;Sergey Nuyanzin;*     1.18.0 has been registered with release date 2023/10/27: https://reporter.apache.org/addrelease.html?flink
*     Blog is online: https://flink.apache.org/2023/10/24/announcing-the-release-of-apache-flink-1.18/
* retrospective done within first meeting for 1.19
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache mailing lists announcements,FLINK-32916,13548194,13548190,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,13/Nov/23 22:47,04/Jun/24 20:40,13/Nov/23 22:47,,,,,,,,,,,,,,,,0,,,,"Announce on the {{dev@}} mailing list that the release has been finished.

Announce on the release on the {{user@}} mailing list, listing major improvements and contributions.

Announce the release on the [announce@apache.org|mailto:announce@apache.org] mailing list.
{panel}
{panel}
|{{From: Release Manager}}
{{To: dev@flink.apache.org, user@flink.apache.org, user-zh@flink.apache.org, announce@apache.org}}
{{Subject: [ANNOUNCE] Apache Flink 1.2.3 released}}
 
{{The Apache Flink community is very happy to announce the release of Apache Flink 1.2.3, which is the third bugfix release for the Apache Flink 1.2 series.}}
 
{{Apache Flink® is an open-source stream processing framework for distributed, high-performing, always-available, and accurate data streaming applications.}}
 
{{The release is available for download at:}}
{{[https://flink.apache.org/downloads.html]}}
 
{{Please check out the release blog post for an overview of the improvements for this bugfix release:}}
{{<blob post link>}}
 
{{The full release notes are available in Jira:}}
{{<jira release notes link>}}
 
{{We would like to thank all contributors of the Apache Flink community who made this release possible!}}
 
{{Feel free to reach out to the release managers (or respond to this thread) with feedback on the release process. Our goal is to constantly improve the release process. Feedback on what could be improved or things that didn't go so well are appreciated.}}
 
{{Regards,}}
{{Release Manager}}|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 22:47:30 UTC 2023,,,,,,,,,,"0|z1jxm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 22:47;Sergey Nuyanzin;Announcement in the mailing list: https://lists.apache.org/thread/f4dlwx3c4tb9fbk5r2zbg0mqts1o9c8l;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove outdated versions,FLINK-32915,13548193,13548190,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,22/Aug/23 11:51,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"h4. dist.apache.org

For a new major release remove all release files older than 2 versions, e.g., when releasing 1.7, remove all releases <= 1.5.

For a new bugfix version remove all release files for previous bugfix releases in the same series, e.g., when releasing 1.7.1, remove the 1.7.0 release.
# If you have not already, check out the Flink section of the {{release}} repository on {{[dist.apache.org|http://dist.apache.org/]}} via Subversion. In a fresh directory:
{code}
svn checkout https://dist.apache.org/repos/dist/release/flink --depth=immediates
cd flink
{code}
# Remove files for outdated releases and commit the changes.
{code}
svn remove flink-<version_to_remove>
svn commit
{code}
# Verify that files  are [removed|https://dist.apache.org/repos/dist/release/flink]
(!) Remember to remove the corresponding download links from the website.

h4. CI

Disable the cron job for the now-unsupported version from (tools/azure-pipelines/[build-apache-repo.yml|https://github.com/apache/flink/blob/master/tools/azure-pipelines/build-apache-repo.yml]) in the respective branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 11:45:10.0,,,,,,,,,,"0|z1jxm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge website pull request,FLINK-32914,13548192,13548190,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,13/Nov/23 22:44,04/Jun/24 20:40,13/Nov/23 22:44,,,,,,,,,,,,,,,,0,,,,"Merge the website pull request to [list the release|http://flink.apache.org/downloads.html]. Make sure to regenerate the website as well, as it isn't build automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 22:44:10 UTC 2023,,,,,,,,,,"0|z1jxls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 22:44;Sergey Nuyanzin;The website is on and correctly shows 1.18.0 informations:

Download page: [https://flink.apache.org/downloads/]

Announcement: [https://flink.apache.org/2023/10/24/announcing-the-release-of-apache-flink-1.18/]

Release note: [https://nightlies.apache.org/flink/flink-docs-release-1.18/release-notes/flink-1.18/];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update japicmp configuration,FLINK-32913,13548191,13548190,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,15/Nov/23 22:00,04/Jun/24 20:40,15/Nov/23 21:51,,,,,,1.18.1,1.19.0,,,,,,,,,0,pull-request-available,,,"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 15 21:51:04 UTC 2023,,,,,,,,,,"0|z1jxlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 21:51;Sergey Nuyanzin;Merged to master as [5b8d7dfc7cfb31fcaa62cb5760e46047cab75fbd|https://github.com/apache/flink/commit/5b8d7dfc7cfb31fcaa62cb5760e46047cab75fbd]
1.18: [07067f3fe7fc2353ceecaab0ce001c004d6fa247|https://github.com/apache/flink/commit/07067f3fe7fc2353ceecaab0ce001c004d6fa247];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote release 1.18,FLINK-32912,13548190,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jingge,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 11:45,18/Mar/24 06:25,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,"Once the release has been finalized (FLINK-32920), the last step of the process is to promote the release within the project and beyond. Please wait for 24h after finalizing the release in accordance with the [ASF release policy|http://www.apache.org/legal/release-policy.html#release-announcements].

*Final checklist to declare this issue resolved:*
 # Website pull request to [list the release|http://flink.apache.org/downloads.html] merged
 # Release announced on the user@ mailing list.
 # Blog post published, if applicable.
 # Release recorded in [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink].
 # Release announced on social media.
 # Completion declared on the dev@ mailing list.
 # Update Homebrew: [https://docs.brew.sh/How-To-Open-a-Homebrew-Pull-Request] (seems to be done automatically - at least for minor releases  for both minor and major releases)
 # Updated the japicmp configuration
 ** corresponding SNAPSHOT branch japicmp reference version set to the just released version, and API compatibiltity checks for {{@PublicEvolving}}  was enabled
 ** (minor version release only) master branch japicmp reference version set to the just released version
 ** (minor version release only) master branch japicmp exclusions have been cleared
 # Update the list of previous version in {{docs/config.toml}} on the master branch.
 # Set {{show_outdated_warning: true}} in {{docs/config.toml}} in the branch of the _now deprecated_ Flink version (i.e. 1.16 if 1.18.0 is released)
 # Update stable and master alias in [https://github.com/apache/flink/blob/master/.github/workflows/docs.yml]
 # Open discussion thread for End of Life for Unsupported version (i.e. 1.16)",,,,,,,,,,,FLINK-32920,,,,,,,,,,FLINK-34706,FLINK-31567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 11:45:10.0,,,,,,,,,,"0|z1jxlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-28386: Trigger an immediate checkpoint after all sinks finished,FLINK-32911,13548178,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,Jiang Xin,Jiang Xin,22/Aug/23 10:10,30/Aug/23 11:59,04/Jun/24 20:40,30/Aug/23 11:59,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,"The feature is described in the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/#waiting-for-the-final-checkpoint-before-task-exit] .

To test the feature, we should run a Flink program on a bounded source and configure it with a large checkpoint interval. With this feature, the program should end up immediately when no more records need to be processed, instead of waiting for one more periodic checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28386,,,,,,,,,,,,,,,FLINK-32945,"23/Aug/23 08:24;mapohl;data.txt.gz;https://issues.apache.org/jira/secure/attachment/13062368/data.txt.gz","30/Aug/23 11:21;mapohl;flink-1.17.1.log;https://issues.apache.org/jira/secure/attachment/13062596/flink-1.17.1.log","30/Aug/23 11:21;mapohl;flink-1.18-6e4c2d1-SNAPSHOT.log;https://issues.apache.org/jira/secure/attachment/13062597/flink-1.18-6e4c2d1-SNAPSHOT.log",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 11:59:05 UTC 2023,,,,,,,,,,"0|z1jxio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 07:58;mapohl;I tried running the {{TopSpeedWindowing}} example with a checkpoint interval of 10s and the data input file that I [attached to this Jira|https://issues.apache.org/jira/secure/attachment/13062368/data.txt.gz]. I run into the following {{NullPointerException}}:
{code}
2023-08-23 09:56:03,111 ERROR org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor       [] - Error while executing remote procedure call public void org.apache.flink.runtime.jobmaster.JobMaster.notifyEndOfData(org.apache.flink.runtime.executiongraph.ExecutionAttemptID).
java.lang.reflect.InvocationTargetException: null
	at jdk.internal.reflect.GeneratedMethodAccessor20.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:568) ~[?:?]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$0(PekkoRpcActor.java:301) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[classes/:?]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:300) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akka4947976d-da31-483f-88c7-8f3ce057edfe.jar:1.18-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373) [?:?]
	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182) [?:?]
	at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655) [?:?]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622) [?:?]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165) [?:?]
Caused by: java.lang.NullPointerException: Cannot invoke ""java.util.BitSet.set(int)"" because ""subtaskStatus"" is null
	at org.apache.flink.runtime.scheduler.VertexEndOfDataListener.recordTaskEndOfData(VertexEndOfDataListener.java:51) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.notifyEndOfData(SchedulerBase.java:1079) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.notifyEndOfData(JobMaster.java:508) ~[classes/:?]
	... 30 more
{code}

It looks like {{VertexEndOfDataListener}} is related to the change of FLINK-28386. [~Jiang Xin] Shall I create a bug issue?;;;","23/Aug/23 08:23;Jiang Xin;[~mapohl] Thanks for reporting this. I didn't find the attached file, could you upload it again so I can try to reproduce the error?;;;","23/Aug/23 08:24;mapohl;Because I forgot to attach it in the end. 8) Now it's attached.;;;","23/Aug/23 08:38;mapohl;Here's a gist with the example code I used and some information on how I executed it: https://gist.github.com/XComp/8a2bae96c7d9312427ed71cbee5370c2;;;","23/Aug/23 09:36;Jiang Xin;[~mapohl] Yes, it is a bug. Please help create a bug issue and I'll fix it ASAP.;;;","23/Aug/23 09:42;mapohl;I created FLINK-32945 to cover it. I collected all the information in FLINK-32945.;;;","30/Aug/23 11:28;mapohl;Thanks for fixing the issue. I reran the test on [6e4c2d1|https://github.com/apache/flink/commit/6e4c2d105a17322e1655d5760dd5967de1c866a5] which included the fix of FLINK-32945 (and used Flink 1.17.1 as a baseline). I ran [TopSpeedWindowing|https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/TopSpeedWindowing.java] with a slightly different configuration to enable batch processing on both checkouts:
{code:diff}
diff --git a/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/TopSpeedWindowing.java b/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/TopSpeedWindowing.java
index 3cca4cc8f8f..41204acfbcc 100644
--- a/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/TopSpeedWindowing.java
+++ b/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/TopSpeedWindowing.java
@@ -17,6 +17,7 @@
 
 package org.apache.flink.streaming.examples.windowing;
 
+import org.apache.flink.api.common.RuntimeExecutionMode;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.functions.RichMapFunction;
 import org.apache.flink.api.common.serialization.SimpleStringEncoder;
@@ -24,6 +25,8 @@ import org.apache.flink.api.common.typeinfo.TypeHint;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.connector.source.util.ratelimit.GuavaRateLimiter;
 import org.apache.flink.api.java.tuple.Tuple4;
+import org.apache.flink.configuration.CheckpointingOptions;
+import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.MemorySize;
 import org.apache.flink.connector.datagen.source.DataGeneratorSource;
 import org.apache.flink.connector.file.sink.FileSink;
@@ -61,7 +64,13 @@ public class TopSpeedWindowing {
 
         // Create the execution environment. This is the main entrypoint
         // to building a Flink application.
-        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+        final Configuration config = new Configuration();
+        config.set(CheckpointingOptions.CHECKPOINT_STORAGE, ""filesystem"");
+        config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, ""file:///tmp/checkpoints"");
+        final StreamExecutionEnvironment env =
+                StreamExecutionEnvironment.getExecutionEnvironment(config);
+        env.enableCheckpointing(20000);
+        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
 
         // Apache Flink’s unified approach to stream and batch processing means that a DataStream
         // application executed over bounded input will produce the same final results regardless
{code}

The job ran on the input [attached (uncompressed!) input data|https://issues.apache.org/jira/secure/attachment/13062368/data.txt.gz]. I attached the logs of both runs to this Jira issue ([flink-1.17.1.log|https://issues.apache.org/jira/secure/attachment/13062596/flink-1.17.1.log] and [flink-1.18-6e4c2d1-SNAPSHOT.log|https://issues.apache.org/jira/secure/attachment/13062597/flink-1.18-6e4c2d1-SNAPSHOT.log]).

{code:title=flink-1.17.1.log}
[...]
2023-08-30 13:14:14,952 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
2023-08-30 13:14:33,525 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1693394073515 for job 35b1393706f05b74ff74355e4d080c78.
2023-08-30 13:14:33,695 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job 35b1393706f05b74ff74355e4d080c78 (30002681 bytes, checkpointDuration=177 ms, finalizationTime=3 ms).
2023-08-30 13:14:33,697 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 1 as completed for source Source: file-input.
2023-08-30 13:14:33,698 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
[...]
{code}

{code:title=flink-1.18-6e4c2d1-SNAPSHOT.log}
[...]
2023-08-30 13:14:12,559 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
2023-08-30 13:14:12,563 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering a manual checkpoint for job 02520a50b9464b94d7fa6908929fef45.
2023-08-30 13:14:12,571 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1693394052565 for job 02520a50b9464b94d7fa6908929fef45.
2023-08-30 13:14:12,905 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job 02520a50b9464b94d7fa6908929fef45 (30002705 bytes, checkpointDuration=337 ms, finalizationTime=3 ms).
2023-08-30 13:14:12,907 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 1 as completed for source Source: file-input.
2023-08-30 13:14:12,909 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
[...]
{code}

The logs show the difference in runtime (1.17.1 has a ~18s gap between end of data and checkpoint triggering; 1.18 triggers a checkpoint right away with an appropriate INFO log message).

I consider this issue to be resolved and tested. [~Jiang Xin] feel free to resolve this issue or ping me if you need me to do additional testing.;;;","30/Aug/23 11:59;Jiang Xin;[~mapohl], thanks for your efforts. The test result looks good and the issue will be resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silence curls in test code,FLINK-32910,13548177,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,22/Aug/23 10:02,30/Sep/23 22:35,04/Jun/24 20:40,,1.16.2,1.17.1,1.18.0,,,,,,,Test Infrastructure,,,,,,0,pull-request-available,stale-assigned,starter,"We use {{curl}} in several locations to download artifacts. Usually, the a progress bar is printed which spams the console output of the test execution. This issue is about cleaning this up.

Parameters to consider (depending on the usecase):
 * {{\-L}}/{{\-\-location}} redirects the curl command and retries if the server reported that the artifact was moved
 * {{\-O}}/{{\-\-remote-name}} writes output to file matching the remote name (which was extracted from the URL) instead of stdout; alternative: {{\-o}}/{{\-\-output}} writes output to a file with the given name instead of stdout
* {{\-f}}/{{\-\-fail}} makes curl command fail with non-0 exit code for HTTP error codes
* {{\-s \-S}}/{{\-\-silent \-\-show-error}} doesn't print progress bar but shows error
* {{\-r}}/{{\-\-retry}} Retries certain errors

{{curl}} uses a default config file {{${user.home}/.curlrc}}. But one could make it more explicit using {{\-K}}/{{\-\-config}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 30 22:35:05 UTC 2023,,,,,,,,,,"0|z1jxig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 13:38;mapohl;The same applies for {{wget}};;;","30/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The jobmanager.sh pass arguments failed,FLINK-32909,13548172,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wuqqq,wuqqq,wuqqq,22/Aug/23 09:28,01/Sep/23 09:20,04/Jun/24 20:40,01/Sep/23 09:20,1.16.2,1.17.1,1.18.0,,,1.16.3,1.17.2,1.18.0,,Deployment / Scripts,,,,,,0,pull-request-available,,,"I' m trying to use the jobmanager.sh script to create a jobmanager instance manually, and I need to pass arugments to the script dynamically, rather than through flink-conf.yaml. But I found that I didn't succeed in doing that when I commented out all configurations in the flink-conf.yaml,  I typed command like:

 
{code:java}
./bin/jobmanager.sh start -D jobmanager.memory.flink.size=1024m -D jobmanager.rpc.address=xx.xx.xx.xx -D jobmanager.rpc.port=xxx -D jobmanager.bind-host=0.0.0.0 -D rest.address=xx.xx.xx.xx -D rest.port=xxx -D rest.bind-address=0.0.0.0{code}
but I got some errors below:

 
{code:java}
[ERROR] The execution result is empty.
[ERROR] Could not get JVM parameters and dynamic configurations properly.
[ERROR] Raw output from BashJavaUtils:
WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: JobManager memory configuration failed: Either required fine-grained memory (jobmanager.memory.heap.size), or Total Flink Memory size (Key: 'jobmanager.memory.flink.size' , default: null (fallback keys: [])), or Total Process Memory size (Key: 'jobmanager.memory.process.size' , default: null (fallback keys: [])) need to be configured explicitly.
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:78)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:98)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:69)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:56)
Caused by: org.apache.flink.configuration.IllegalConfigurationException: Either required fine-grained memory (jobmanager.memory.heap.size), or Total Flink Memory size (Key: 'jobmanager.memory.flink.size' , default: null (fallback keys: [])), or Total Process Memory size (Key: 'jobmanager.memory.process.size' , default: null (fallback keys: [])) need to be configured explicitly.
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.failBecauseRequiredOptionsNotConfigured(ProcessMemoryUtils.java:129)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:86)
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:83)
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobM {code}
It seems to remind me to configure memory for jobmanager instance explicitly, but I had already passed the jobmanager.memory.flink.size parameter. So I debug the script, and found a spelling error in the jobmanager.sh script at line 54:

 
{code:java}
parseJmArgsAndExportLogs ""${ARGS[@]}""
{code}
the uppercase ""$\{ARGS[@]}"" is a wrong variable name here from a contextual perspective, and causing an empty string passed to the function. I changed to ""$\{args[@]}"" and It works fine.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 09:20:05 UTC 2023,,,,,,,,,,"0|z1jxhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 13:24;mapohl;Thanks for reporting this issue, [~wuqqq]. Do you want to work on a fix for it?;;;","23/Aug/23 02:35;wuqqq;Yes, I would be happy to. Emmm, I should prepare a PR next, right?;;;","01/Sep/23 09:20;mapohl;Thanks for your contribution and sorry for the delayed merge. It slipped through for a bit in my todo list.

master (1.19): 37b6559fedac5078b2ea5c29ffc34e36b2b029bb
1.18: c4093b80643a641c16a943486ce21a88e6f91cd0
1.17: ee018eb3989dac2023569386e20319e28988ed20
1.16: e028fbda9aeee892daee62083aa6bf9d6344a82b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong materialization id setting when switching from non-log-based checkpoint to log-based checkpoint,FLINK-32908,13548163,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Feifan Wang,Feifan Wang,22/Aug/23 08:04,22/Aug/23 09:07,04/Jun/24 20:40,,,,,,,,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,"Initial materialization ID should be set to checkpointID when switching from non-log-based checkpoint to log-based checkpoint. Currently initial id will be set to 0, which will cause the incremental snapshot of inner backend go wrong.

PTAL [~Yanfei Lei] , [~roman] .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-22 08:04:43.0,,,,,,,,,,"0|z1jxfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointAfterAllTasksFinishedITCase hangs on AZP,FLINK-32907,13548161,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiang Xin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/23 08:00,30/Aug/23 09:44,04/Jun/24 20:40,29/Aug/23 08:46,1.18.0,,,,,1.18.0,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,test-stability,,"With this build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52478&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8867
{{CheckpointAfterAllTasksFinishedITCase}} hangs on AZP as
{noformat}
2023-08-22T05:15:37.5810740Z Aug 22 05:15:37 ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007f459ce44800 nid=0xbb3b waiting on condition [0x00007f454f500000]
2023-08-22T05:15:37.5811908Z Aug 22 05:15:37    java.lang.Thread.State: WAITING (parking)
2023-08-22T05:15:37.5814427Z Aug 22 05:15:37 	at sun.misc.Unsafe.park(Native Method)
2023-08-22T05:15:37.5816487Z Aug 22 05:15:37 	- parking to wait for  <0x00000000a7923e60> (a java.util.concurrent.CompletableFuture$Signaller)
2023-08-22T05:15:37.5818424Z Aug 22 05:15:37 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)

2023-08-22T05:15:37.5820556Z Aug 22 05:15:37 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2023-08-22T05:15:37.5822534Z Aug 22 05:15:37 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-08-22T05:15:37.5824626Z Aug 22 05:15:37 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2023-08-22T05:15:37.5826636Z Aug 22 05:15:37 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2023-08-22T05:15:37.5828836Z Aug 22 05:15:37 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
2023-08-22T05:15:37.5830778Z Aug 22 05:15:37 	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:68)
2023-08-22T05:15:37.5832645Z Aug 22 05:15:37 	at org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testImmediateCheckpointing(CheckpointAfterAllTasksFinishedITCase.java:83)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32996,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 08:45:56 UTC 2023,,,,,,,,,,"0|z1jxew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:31;renqs;[~Jiang Xin] Could you take a look at this issue? Thanks;;;","29/Aug/23 08:45;lindong;Merged into apache/flink master branch fbcf6bffc870813d07a5d215dde8cf5dc55da620.

Merged into apache/flink release-1.18 branch 94277e187464c56be1144a183dab0756d5f1800b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-279 Unified the max display column width for SqlClient and Table APi in both Streaming and Batch execMode,FLINK-32906,13548133,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,jingge,jingge,22/Aug/23 05:11,12/Sep/23 06:55,04/Jun/24 20:40,12/Sep/23 06:55,,,,,,1.18.0,,,,Tests,,,,,,0,,,,"more info could be found at 

FLIP-279 [https://cwiki.apache.org/confluence/display/FLINK/FLIP-279+Unified+the+max+display+column+width+for+SqlClient+and+Table+APi+in+both+Streaming+and+Batch+execMode]

 

Tests:

Both configs could be set with new value and following behaviours are expected : 
| | |sql-client.display.max-column-width, default value is 30|table.display.max-column-width, default value is 30|
|sqlclient|Streaming|text longer than the value will be truncated and replaced with “...”|Text longer than the value will be truncated and replaced with “...”|
|sqlclient|Batch|text longer than the value will be truncated and replaced with “...”|Text longer than the value will be truncated and replaced with “...”|
|Table API|Streaming|No effect. 
table.display.max-column-width with the default value 30 will be used|Text longer than the value will be truncated and replaced with “...”|
|Table API|Batch|No effect. 
table.display.max-column-width with the default value 30 will be used|Text longer than the value will be truncated and replaced with “...”|

 

Please pay attention that this task offers a backward compatible solution and deprecated sql-client.display.max-column-width, which means once sql-client.display.max-column-width is used, it falls into the old scenario where table.display.max-column-width didn't exist, any changes of table.display.max-column-width won't take effect. You should test it either by only using table.display.max-column-width or by using sql-client.display.max-column-width, but not both of them back and forth.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30025,,,,,,,,,,,"22/Aug/23 10:54;liyubin117;image-2023-08-22-18-54-45-122.png;https://issues.apache.org/jira/secure/attachment/13062347/image-2023-08-22-18-54-45-122.png","22/Aug/23 10:54;liyubin117;image-2023-08-22-18-54-57-699.png;https://issues.apache.org/jira/secure/attachment/13062348/image-2023-08-22-18-54-57-699.png","22/Aug/23 10:58;liyubin117;image-2023-08-22-18-58-11-241.png;https://issues.apache.org/jira/secure/attachment/13062349/image-2023-08-22-18-58-11-241.png","22/Aug/23 10:58;liyubin117;image-2023-08-22-18-58-20-509.png;https://issues.apache.org/jira/secure/attachment/13062350/image-2023-08-22-18-58-20-509.png","22/Aug/23 11:04;liyubin117;image-2023-08-22-19-04-49-344.png;https://issues.apache.org/jira/secure/attachment/13062351/image-2023-08-22-19-04-49-344.png","22/Aug/23 11:06;liyubin117;image-2023-08-22-19-06-49-335.png;https://issues.apache.org/jira/secure/attachment/13062352/image-2023-08-22-19-06-49-335.png","23/Aug/23 02:39;liyubin117;image-2023-08-23-10-39-10-225.png;https://issues.apache.org/jira/secure/attachment/13062359/image-2023-08-23-10-39-10-225.png","23/Aug/23 02:39;liyubin117;image-2023-08-23-10-39-26-602.png;https://issues.apache.org/jira/secure/attachment/13062360/image-2023-08-23-10-39-26-602.png","23/Aug/23 02:41;liyubin117;image-2023-08-23-10-41-30-913.png;https://issues.apache.org/jira/secure/attachment/13062361/image-2023-08-23-10-41-30-913.png","23/Aug/23 02:41;liyubin117;image-2023-08-23-10-41-42-513.png;https://issues.apache.org/jira/secure/attachment/13062362/image-2023-08-23-10-41-42-513.png","30/Aug/23 09:09;liyubin117;image-2023-08-30-17-09-32-728.png;https://issues.apache.org/jira/secure/attachment/13062592/image-2023-08-30-17-09-32-728.png","30/Aug/23 09:06;liyubin117;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13062591/screenshot-1.png",,12.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 02:31:01 UTC 2023,,,,,,,,,,"0|z1jx8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:28;liyubin117;[~jingge] I would like to take this, could you please assign this to me? thanks:);;;","22/Aug/23 08:58;renqs;[~liyubin117] Thanks for volunteering! Assigned to you just now.;;;","22/Aug/23 11:00;liyubin117;in batch mode, use sql-client with default sql-client.display.max-column-width, print results as expected:

!image-2023-08-22-18-54-45-122.png!!image-2023-08-22-18-54-57-699.png!

but use specified sql-client.display.max-column-width, the configuration also take effects, it seems conflict with expected `No effect`:

!image-2023-08-22-18-58-11-241.png!

!image-2023-08-22-18-58-20-509.png!

use specified table.display.max-column-width,  the configuration take no effect, it conflicts with 'Text longer than the value will be truncated and replaced with “...”'.

!image-2023-08-22-19-04-49-344.png!

!image-2023-08-22-19-06-49-335.png!

[~jingge] Do we need to fix it?;;;","22/Aug/23 18:34;jingge;[~liyubin117] Thanks for testing.

""but use specified sql-client.display.max-column-width, the configuration also take effects, it seems conflict with expected `No effect`"" - the description was wrong. ""No effect"" was a bug and has been fixed alongside the PR. I have updated the description. Please check it again.

 

""use specified table.display.max-column-width,  the configuration take no effect, it conflicts with 'Text longer than the value will be truncated and replaced with “...”'."" - this task offers a backward compatible solution and deprecated sql-client.display.max-column-width, which means once sql-client.display.max-column-width is used, it falls into the old scenario where table.display.max-column-width didn't exist, any changes of table.display.max-column-width won't take effect. You should test it either by only using table.display.max-column-width or by using sql-client.display.max-column-width, but not both of them back and forth.

 

 

 ;;;","23/Aug/23 02:42;liyubin117;[~jingge] Thanks for claring that.

I found that  both configurations  also takes effect in fixed-length type (eg. char), Is It expected? i am very glad to contribute :)

!image-2023-08-23-10-39-26-602.png!

!image-2023-08-23-10-39-10-225.png!

in another session,

!image-2023-08-23-10-41-30-913.png!

!image-2023-08-23-10-41-42-513.png!;;;","29/Aug/23 14:09;jingge;The idea behind it is to enable users to show content of table in a feasible way. Afiak, it is expected to truncate char. [~jark] would you please double confirm? Thanks!;;;","30/Aug/23 09:08;liyubin117;If it is reasonable, the doc may need to be updated.

!image-2023-08-30-17-09-32-728.png!

!screenshot-1.png!;;;","31/Aug/23 09:06;renqs;[~jingge] Any followup on this one? Thanks;;;","05/Sep/23 08:51;jingge;[~liyubin117] fair enough. Thanks! I have to take a deep dive into it.;;;","07/Sep/23 21:38;jingge;[~liyubin117] e.g. means ""for example"", it does not need to contain all cases. Nevertheless, I will improve the description with more precise content.;;;","11/Sep/23 02:31;liyubin117;[~jingge] glad to hear that! (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of broadcast hash join doesn't support spill to disk when enable operator fusion codegn,FLINK-32905,13548131,13532997,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,22/Aug/23 03:55,24/Aug/23 11:22,04/Jun/24 20:40,24/Aug/23 11:22,1.18.0,,,,,1.18.0,1.19.0,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 12:32:24 UTC 2023,,,,,,,,,,"0|z1jx88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 12:32;jark;Fixed in 
 - master: 2cf32e82773254b79af4dad7ee317db4878d6f8a
 - release-1.18: c78f440533efe68687ab1e6e04d3407e40303de7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support cron expressions for periodic snapshots triggering,FLINK-32904,13548086,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,21/Aug/23 14:41,04/Sep/23 06:05,04/Jun/24 20:40,04/Sep/23 06:05,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"Current functionality to trigger savepoints and checkpoints is based on specifying a fixed interval. This approach gives little control to prevent potentially overlapping snapshots running simultaneously in adjacent deployments and can cause issues for high scale, large state jobs. In order to have better control over the exact schedule, triggering based on cron expressions should be added.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 06:05:53 UTC 2023,,,,,,,,,,"0|z1jwy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 06:05;gyfora;merged to main b39797300875b241598042834d96a20e7809adfc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a load simulation example pipeline for canary testing,FLINK-32903,13548065,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,21/Aug/23 11:57,25/Aug/23 14:44,04/Jun/24 20:40,25/Aug/23 14:44,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,"The current provided example pipeline generates an infinite amount of load, only yielding only scaleup decisions. It would be desirable to have a pipeline which scales up and down in a periodic fashion. Users do not always have enough data available to do that. Hence, the load needs to be simulated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-21 11:57:22.0,,,,,,,,,,"0|z1jwtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs creation and checks in multiple locations,FLINK-32902,13548048,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,21/Aug/23 10:16,07/Nov/23 08:23,04/Jun/24 20:40,,1.16.2,1.17.1,1.18.0,,,,,,,Documentation,Test Infrastructure,,,,,0,starter,,,"Currently, Flink has two CI pipeline configs dealing with documentation ([tools/ci/compile.sh:83ff|https://github.com/apache/flink/blob/28aaa4b80c571fe497dd76f79e617c01e2a25869/tools/ci/compile.sh#L83] that's used by Flink's Azure pipeline; the docs are generated in a separate step in [azure-pipelines.yml:89|https://github.com/apache/flink/blob/29f009b7e8c714cd5af0626e9725eb8538a4bd0f/azure-pipelines.yml#L89]; there's a Github Actions workflow that runs the generation and verification as well using a separate script in [.github/workflows/docs.yml:51|https://github.com/apache/flink/blob/fbb215e57919ad05f4c48b4ef79ee2f808f62ac8/.github/workflows/docs.yml#L51]). This leads to duplicate code for documentation generation and verification.

This is redundant code which we could unify: Both pipelines should ideally rely on the same script for generating and verifying the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26658,,,,,,,,,,,,,,FLINK-30923,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 07 08:23:45 UTC 2023,,,,,,,,,,"0|z1jwps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/23 00:56;tzy_0xffffffff;[~mapohl] I can merge the scrips to reduce redundancy. Could you assign it to me ;) ?;;;","07/Nov/23 08:23;mapohl;Hi [~tzy_0xffffffff], thanks for offering your help. I'm just wondering whether it's worth the effort. I'm working on FLINK-27075. The goal is to move entirely to Github Actions. That would enable us to do even more refactorings in the end. Aligning the two code paths wouldn't be necessary anymore. So, I'd suggest to focus on other tasks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should set initial initialSequenceNumber of StateChangelogWriter after restore,FLINK-32901,13548046,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,Feifan Wang,Feifan Wang,21/Aug/23 09:51,28/Aug/23 10:09,04/Jun/24 20:40,28/Aug/23 10:09,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,0,,,,"The current StateChangelogWriter lacks an interface to set the initial sequence number, and FsStateChangelogWriter always sets the initial sequence number to 0. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 10:08:51 UTC 2023,,,,,,,,,,"0|z1jwpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 08:35;masteryhx;Hi, [~Feifan Wang]

IIUC, current sequence number is not related to restore so that the init logic is maintained in the StateChangelogWriter, right ?

Do you have other cases to pass your defined seq number ? If not, I'd remain this interface as before to reduce unnecessary work of caller.;;;","28/Aug/23 10:08;Feifan Wang;Sorry [~masteryhx] , before I thought that the sequence number starting from 0 after restore was the cause of a bug, but later I found that it was caused by another reason （[FLINK-32908|https://issues.apache.org/jira/browse/FLINK-32908]）. Sorry for not closing this ticket in time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL doesn't find any built-in SQL functions,FLINK-32900,13548030,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,tanee.kim,tanee.kim,21/Aug/23 06:58,22/Aug/23 14:37,04/Jun/24 20:40,22/Aug/23 14:37,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,,,0,,,,"I wrote an app that uses a Hive-Glue catalog to run a query that joins a Hive table with a Kafka table via the Flink Table API.
It doesn't find the REGEXP_EXTRACT function, so it tries to find the UDF in the glue catalog.
The REGEXP_EXTRACT function is a built-in SQL function that doesn't need to look up UDFs.
I have no idea why this is happening, is there something I'm missing?

 
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. Failed to get function default.REGEXP_EXTRACT{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 22 11:46:46 UTC 2023,,,,,,,,,,"0|z1jwls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 13:31;Sergey Nuyanzin;Hi [~tanee.kim]could you please share more details e.g. more complete trace ?
Also if you share a minimal reproducing code snippet it would be great;;;","21/Aug/23 13:34;tanee.kim;Thank you for the reply! I will share the stacktrace as below.
{code:java}
Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
    at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist-1.17.0.jar:1.17.0]
    ... 52 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist-1.17.0.jar:1.17.0]
    ... 52 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: SQL validation failed. Failed to get function default.REGEXP_EXTRACT
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:254) ~[flink-dist-1.17.0.jar:1.17.0]
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
    at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    ... 7 more
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. Failed to get function default.REGEXP_EXTRACT
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:187) ~[?:?]
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:281) ~[?:?]
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:738) ~[flink-table-api-java-uber-1.17.0.jar:1.17.0]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:254) ~[flink-dist-1.17.0.jar:1.17.0]
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
    at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    ... 7 more
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get function default.REGEXP_EXTRACT
    at org.apache.flink.table.catalog.hive.HiveCatalog.getFunction(HiveCatalog.java:1355) ~[?:?]
    at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:578) ~[flink-table-api-java-uber-1.17.0.jar:1.17.0]
    at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$5(FunctionCatalog.java:637) ~[flink-table-api-java-uber-1.17.0.jar:1.17.0]
    at java.util.Optional.orElseGet(Unknown Source) ~[?:?]
    at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:637) ~[flink-table-api-java-uber-1.17.0.jar:1.17.0]
    at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:398) ~[flink-table-api-java-uber-1.17.0.jar:1.17.0]
    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:91) ~[?:?]
    at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:69) ~[?:?]
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1302) ~[?:?]
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1288) ~[?:?]
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1288) ~[?:?]
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1288) ~[?:?]
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:985) ~[?:?]
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:741) ~[?:?]
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:183) ~[?:?]
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:281) ~[?:?]
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:738) ~[flink-table-api-java-uber-1.17.0.jar:1.17.0]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.17.0.jar:1.17.0]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:254) ~[flink-dist-1.17.0.jar:1.17.0]
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
    at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41) ~[flink-rpc-akka_3b58c44b-378d-4e38-aa64-c8b6ee8f8d9a.jar:1.17.0]
    ... 7 more {code};;;","21/Aug/23 13:37;tanee.kim;[~Sergey Nuyanzin] 

I just used REGEXP_EXTRACT function, it's working in local environment but when I deployed to flink kuberentes operator environment, I've got error like above.
{code:java}
tableEnv.executeSql(""select REGEXP_EXTRACT(data, '([0-9]+)', 1) as cd from table"") {code}
Actually, I'm using hive for aws glue catalog, so the version of hiveCatalog is '2.3.6-glue'.

And I'm building client jar with own code not flink hive dependency due to some of class incompatibility.

Could this have an impact? The SQL function not being found seems to be a calcite issue, so I added a separate calcite dependency, but it didn't fix it.;;;","22/Aug/23 10:11;tanee.kim;[~Sergey Nuyanzin], Thank you for your attention to this issue.

I found the cause of this issue.

It was an issue caused by a Resolution Order related to SQL Function Reference.

When running Flink SQL, it would look up the catalog function before looking up the sql function of calcite, and the error was caused by not being able to access catalog.

I solved it by giving flink access to the catalog, which allowed it to move on to the next step in the resolution order (the calcite sql function).;;;","22/Aug/23 11:44;Sergey Nuyanzin;great that you were able to solve and thanks for sharing this.

One more question from my side: is there anything we should/could do with this issue or we can closed it as resolved?;;;","22/Aug/23 11:46;tanee.kim;[~Sergey Nuyanzin] 

You can close it as resolved. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not access inner/nested objects in MATCH_RECOGNIZE DEFINE clause and in Aggregate function,FLINK-32899,13548024,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,krutikgadhiya,krutikgadhiya,21/Aug/23 06:23,22/Aug/23 12:31,04/Jun/24 20:40,,1.17.1,,,,,,,,,Library / CEP,,,,,,0,,,,"Note: we are implementing everything in java.

example event

 

 
{code:java}
{
  ""config"": ""conf1"",
  ""id"": 1,
  ""action"": ""login"",
  ""size"": 10,
  ""message"": {
    ""action"": ""login"",
    ""size"": 10,
    ""event"": {
       ""action"": ""login"", ""size"": 10,
       ""event"": {
    }
  }
}{code}
we are tried to run query, it does not works and gives bellow exception
{code:java}
SELECT * FROM Log
MATCH_RECOGNIZE (
  PARTITION BY config
  ORDER BY time_ltz
  MEASURES
    A.id AS aID,
    COUNT(B.id) AS cnt
  AFTER MATCH SKIP PAST LAST ROW
  PATTERN (A B+ C)
  DEFINE
    A AS A.message.event.action = 'login'
    B AS B.message.event.action = 'data' AND SUM(B.message.event.size) >= 20""
);{code}
 
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 11, column 22 to line 11, column 26: Table 'event' not found
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:187)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:281)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:738)
    at org.abs.security.StaticRuleEvaluator.main(StaticRuleEvaluator.java:151)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 11, column 22 to line 11, column 26: Table 'event' not found
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:932)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:917)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5163)
    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:344)  {code}
The problem here is that if I try to run the same query replacing *A.message.event.action* with *A.message.action* without Aggregate function it works with Aggregate function it does not works and if I replace *A.message.event.action* with *A.action* with Aggregate function it works fine.

 

without Aggregate function:
 * A.action (/)
 * A.message.action(/)
 * A.message.event.action (x)

with Aggregate function:
 * A.action (/)
 * A.message.action (x)
 * A.message.event.action (x)

 

 ",Implemented everything in java maven project on a Windows 11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,2023-08-21 06:23:18.0,,,,,,,,,,"0|z1jwkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improvement of usability for Flink OLAP,FLINK-32898,13548014,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,21/Aug/23 04:26,06/Dec/23 07:43,04/Jun/24 20:40,,,,,,,,,,,Client / Job Submission,Runtime / Configuration,Runtime / Coordination,Table SQL / JDBC,Table SQL / Runtime,,0,pull-request-available,,,"By building Flink OLAP services internally using Flink master branch, we realized that if we want to involve more community users to use Flink OLAP, besides scheduling and execution performance improvement mentioned in [FLINK-25318|https://issues.apache.org/jira/browse/FLINK-25318], we also need to improve some basic usability issues in OLAP scenarios, including providing quick start for users to build Flink OLAP service, allocating minimum resources when cluster start, displaying detailed job sql in flink ui, etc.

 

Meanwhile, we believe this can also optimize the user experience of running batch jobs in session cluster mode. So after talk with [~zjureel] , we create this jira to track the progress of related work.",,,,,,,,,,,,,,,,,,FLINK-25015,FLINK-32863,FLINK-15959,,,,,,,,,,,,,FLINK-25318,FLINK-32488,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-21 04:26:47.0,,,,,,,,,,"0|z1jwi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowToRowCastRule will get wrong result while cast Array(Row()) type,FLINK-32897,13548003,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,337361684@qq.com,337361684@qq.com,337361684@qq.com,21/Aug/23 00:49,24/Aug/23 06:39,04/Jun/24 20:40,24/Aug/23 06:04,1.16.2,1.17.1,1.18.0,,,1.19.0,,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,"{code:java}
// code placeholder
CREATE TABLE kafkaTest (
  a STRING NOT NULL,
  config ARRAY<ROW<notificationTypeOptInReference STRING NOT NULL,cso STRING    NOT NULL,autocreate BOOLEAN NOT NULL> NOT NULL> NOT NULL,
  ingestionTime TIMESTAMP(3) METADATA FROM 'timestamp',
  PRIMARY KEY (businessEvent) NOT ENFORCED) 
WITH (
     'connector' = 'kafka',
      'topic' = 'test',
      'properties.group.id' = 'testGroup',
      'scan.startup.mode' = 'earliest-offset',
      'properties.bootstrap.servers' = '', 
      'properties.security.protocol' = 'SASL_SSL',
      'properties.sasl.mechanism' = 'PLAIN',
      'properties.sasl.jaas.config' = ';',
      'value.format' = 'json',
      'sink.partitioner' = 'fixed'
); {code}
If we with the following INSERT, it will see that the last item in the array is placed in the topic 3 times and the first two are igniored.
{code:java}
// code placeholder
INSERT INTO kafkaTest VALUES ('Transaction', ARRAY[ROW('G', 'IT', true),ROW('H', 'FR', true), ROW('I', 'IT', false)], TIMESTAMP '2023-08-30 14:01:00'); {code}
The result:

!image-2023-08-21-08-48-25-116.png|width=296,height=175!

If I use the 'print' sink, I can get the right result. So I think this is a bug of 'kafka' connector.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32296,,,,,,,,,,,,,,,,,"21/Aug/23 07:45;renqs;codegen.java;https://issues.apache.org/jira/secure/attachment/13062306/codegen.java","21/Aug/23 00:48;337361684@qq.com;image-2023-08-21-08-48-25-116.png;https://issues.apache.org/jira/secure/attachment/13062303/image-2023-08-21-08-48-25-116.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 06:05:34 UTC 2023,,,,,,,,,,"0|z1jwfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 07:51;renqs;[~zhengyunhong97] The issue can be also reproduced with print sink:
{code:java}
// code placeholder
CREATE TABLE print_sink (
  a STRING NOT NULL,
  config ARRAY<ROW<notificationTypeOptInReference STRING NOT NULL,cso STRING NOT NULL,autocreate BOOLEAN NOT NULL> NOT NULL> NOT NULL,
  ingestionTime TIMESTAMP(3)
) WITH (
    'connector' = 'print'
);

INSERT INTO print_sink VALUES ('Transaction', ARRAY[ROW('G', 'IT', true),ROW('H', 'FR', true), ROW('I', 'IT', false)], TIMESTAMP '2023-08-30 14:01:00'); {code}
I dumped the generated code (see the attachment [^codegen.java]). The problem locates in the generated {{{}processElement{}}}, which converts {{BinaryArrayData}} to {{{}RowData{}}}. The logic reuses the same writer object to process 3 rows in the array, so all rows are finally replaced by the last one. I'm not quite familiar with SQL codegen so maybe some SQL experts can take a look at the issue [~lincoln.86xy] [~337361684@qq.com] 

 ;;;","21/Aug/23 08:03;337361684@qq.com;Thanks, [~renqs] . Can you assign this issue to me, I will try to fix it.;;;","21/Aug/23 08:03;renqs;[~337361684@qq.com] Thanks for taking the issue! Assigned to you just now;;;","23/Aug/23 18:50;Sergey Nuyanzin;This looks like a duplicate of https://issues.apache.org/jira/browse/FLINK-32296;;;","24/Aug/23 03:24;337361684@qq.com;Hi, [~Sergey Nuyanzin]. Sorry for I'm not checking whether there are same issue in jira at the beginning, because initially I thought it was a bug in Kafka-connector. This is indeed a duplicate issue, it can be closed. Thanks for your contribution!;;;","24/Aug/23 06:05;renqs;[~337361684@qq.com] Thanks for the contribution anyway! I marked the issue as duplicated and closed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Incorrect `Map.computeIfAbsent(..., ...::new)` usage which misinterprets key as initial capacity",FLINK-32896,13547965,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tzy_0xffffffff,Marcono1234,Marcono1234,19/Aug/23 21:55,25/Oct/23 07:36,04/Jun/24 20:40,24/Oct/23 09:34,1.17.1,1.18.0,,,,1.19.0,,,,Runtime / Coordination,,,,,,2,pull-request-available,starter,,"The are multiple cases in the code which look like this:
{code}
map.computeIfAbsent(..., ArrayList::new)
{code}

Not only does this create a new collection (here an {{ArrayList}}), but {{computeIfAbsent}} also passes the map key as argument to the mapping function, so instead of calling the no-args constructor such as {{new ArrayList<>()}}, this actually calls the constructor with {{int}} initial capacity parameter, such as {{new ArrayList<>(initialCapacity)}}.

For some cases where this occurs in the Flink code I am not sure if it is intended, but in same cases this does not seem to be intended. Here are the cases I found:

- [{{HsFileDataIndexImpl:163}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImpl.java#L163C70-L163C84]
- [{{HsSpillingStrategy:128}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategy.java#L128C68-L128C82]
- [{{HsSpillingStrategy:134}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategy.java#L134C63-L134C77]
- [{{HsSpillingStrategy:140}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategy.java#L140C63-L140C77]
- [{{HsSpillingStrategy:145}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategy.java#L145C70-L145C84]
- [{{HsSpillingStrategy:151}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategy.java#L151C65-L151C79]
- [{{HsSpillingStrategy:157}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategy.java#L157C65-L157C79]
- [{{HsSpillingStrategyUtils:76}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSpillingStrategyUtils.java#L76C74-L76C88]
- [{{ProducerMergedPartitionFileIndex:171}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/tiered/file/ProducerMergedPartitionFileIndex.java#L171C75-L171C89]
- [{{TestingSpillingInfoProvider:201}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingSpillingInfoProvider.java#L201C56-L201C70]
- [{{TestingSpillingInfoProvider:208}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingSpillingInfoProvider.java#L208C54-L208C66]
- [{{TestingSpillingInfoProvider:216}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingSpillingInfoProvider.java#L216C54-L216C66]
- [{{SourceCoordinatorConcurrentAttemptsTest:269}}|https://github.com/apache/flink/blob/9546f8243a24e7b45582b6de6702f819f1d73f97/flink-runtime/src/test/java/org/apache/flink/runtime/source/coordinator/SourceCoordinatorConcurrentAttemptsTest.java#L269C53-L269C65]

This can lead either to runtime exceptions in case the map key is negative, since the collection constructors reject negative initial capacity values, or it can lead to bad performance if the key (which is misinterpreted as initial capacity) is pretty low, such as 0, or is pretty large and therefore pre-allocates a lot of memory for the collection.


Regardless of whether or not these cases are intended it might be good to replace them with lambda expressions to make this more explicit:
{code}
map.computeIfAbsent(..., capacity -> new ArrayList<>(capacity))
{code}
or
{code}
map.computeIfAbsent(..., k -> new ArrayList<>())
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 24 09:34:56 UTC 2023,,,,,,,,,,"0|z1jw7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/23 23:28;f.pompermaier;Good catch;;;","21/Aug/23 10:33;mapohl;This seems to be a local issue. I don't now about the id ranges that are used in the specific scenarios. But I'd assume that we would have run into issues during testing if those ranges would have covered negative int values. Anyway, it's a good catch. We should fix that. I updated the issues metadata. [~Marcono1234] do you have capacity to work on this one?;;;","22/Aug/23 21:34;Marcono1234;{quote}
Marcono1234 do you have capacity to work on this one?
{quote}
The main problem is that I am not familiar with Flink, neither the source code nor the project itself. It would therefore be great if one of you could do this, because it will most likely be much easier and way more efficient than me trying to get familiar with the project and how to build it. Sorry if this is causing trouble for you.
I came across these issues while [writing a custom CodeQL query|https://github.com/Marcono1234/codeql-java-queries/blob/master/codeql-custom-queries-java/queries/likely-bugs/method-ref-creating-collection-with-capacity.ql] and running it against popular Java projects on GitHub to verify that it works correctly. The query results included those listed above for Flink, and I thought it might be useful for you if I notified you about these issues.;;;","11/Oct/23 19:11;tzy_0xffffffff;Hi [~mapohl], I can fix that as a beginner to familiarize myself with the process. Could you assign it to me?;;;","12/Oct/23 07:51;mapohl;Sure, go ahead. Thanks for picking this up. :);;;","24/Oct/23 09:34;mapohl;I didn't push for backports because it doesn't seem to cause issues. Therefore, it's rather a code cleanup than a bug.

master (1.19): 30e8b3de05c1d6b75d8f27b9188a1d34f1589ac5
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the max attempts for Exponential Delay Restart Strategy,FLINK-32895,13547931,13560425,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,19/Aug/23 04:23,17/Dec/23 10:38,04/Jun/24 20:40,15/Dec/23 04:50,,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"Currently, Flink has 3 restart strategies, they are: fixed-delay, failure-rate and exponential-delay.

The exponential-delay is suitable if a job continues to fail for a period of time. The fixed-delay and failure-rate has the max attempts mechanism, that means, the job won't restart and go to fail after the attempt exceeds the threshold of max attempts. 

The max attempts mechanism is reasonable, flink should not or need to infinitely restart the job if the job keeps failing. However, the exponential-delay doesn't have the max attempts mechanism.

I propose introducing the `restart-strategy.exponential-delay.max-attempts-before-reset` to support the max attempts mechanism for exponential-delay. It means flink won't restart job if the number of job failures before reset exceeds max-attempts-before-reset when is exponential-delay is enabled.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33865,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 04:50:34 UTC 2023,,,,,,,,,,"0|z1jvzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 02:39;fanrui;Hi [~zhuzh][~wanglijie], would you mind helping take a look this JIRA in your free time? thanks~

BTW, if the improvement is reasonable, it will add a new option and a new `exponentialDelayRestart` method in `RestartStrategies` (It's a PublicEvolving class), it's a small feature.

I'm not sure whether the FLIP is necessary when adding any options or changing any public classes even if it's a small feature. If yes, I can start a FLIP, if no, I will follow it here.;;;","21/Aug/23 07:47;zhuzh;Thanks for the proposal! A FLIP is required because it includes changes to pubic interfaces (config options). And it is proposing a new feature which needs to be seen and set by users.

And maybe we can re-consider the new config option, to make to easier for understanding. e.g. introduce a {{restart-strategy.exponential-delay.fail-on-exceeding-max-backoff}}.

I would also suggest to not change {{RestartStrategies}} any more because we are considering to deprecate it later when improving Flink configuration. {{RestartStrategies}} is not flexible for custom restart strategy and can be superseded by config options.
Can we delay this work a bit, waiting for the result of the FLIP and ML discussion of the deprecation? It should happen soon. ;;;","21/Aug/23 08:38;fanrui;Thanks [~zhuzh] for the quick feedback!

{quote}A FLIP is required because it includes changes to pubic interfaces (config options). And it is proposing a new feature which needs to be seen and set by users.{quote}

Got it, thanks for the clarification!

{quote}And maybe we can re-consider the new config option, to make to easier for understanding. e.g. introduce a restart-strategy.exponential-delay.fail-on-exceeding-max-backoff.{quote}

Good suggestion, and I will record this suggestion, and we can discuss the option in the mail list later.

{quote}I would also suggest to not change RestartStrategies any more because we are considering to deprecate it later when improving Flink configuration. RestartStrategies is not flexible for custom restart strategy and can be superseded by config options.
 {quote}

To be honest, I and our internal flink platform always use the config option instead of Java code for flink configuration. So I totally agree deprecating the RestartStrategies.

{quote}Can we delay this work a bit, waiting for the result of the FLIP and ML discussion of the deprecation? It should happen soon.{quote}

Sure, this improvement can wait for deprecating the RestartStrategies, and could you ping me if the discussion is started? thanks a lot :)
;;;","22/Aug/23 06:55;zhuzh;Sure I will ping you in time. [~fanrui];;;","10/Sep/23 05:28;fanrui;Hi [~zhuzh] , I created the FLIP-364 in advance due to I found several points in the restart strategy that need to be improved. We can discuss them in the mail list in the future.

There are 2 option for discussion:
 * Option1: Start discuss FLIP-364 after deprecating the RestartStrategies is discussed.
 * Option2: FLIP-364 has serveral points need to be discussed, we can first discuss other parts of FLIP-364 besides RestartStrategies. And the RestartStrategies part can be followed by your separate FLIP. 

WDYT?

BTW, after some more thought: restart-strategy.exponential-delay.fail-on-exceeding-max-backoff may not work well. Because the user may want to restart this job multiple times using max-backoff before failing it.

For example, users don't want the delay-time to be too long, so they set the initial-backoff=1s, backoff-multiplier=2, max-backoff=30s. So the delay time is 1s, 2s, 4s, 8s, 16s, 30s, 30s, 30s, 30s, 30s, etc. If we introduced the `fail-on-exceeding-max-backoff`, it means that the job won't restart when the delay-time is extended to 30s at first time. right?

Please correct me if I'm wrong, and looking forward to more feedbacks from community, thanks~

 

[1]https://cwiki.apache.org/confluence/display/FLINK/FLIP-364%3A+Improve+the+restart-strategy;;;","11/Sep/23 02:58;zhuzh;Either option sounds good to me. Feel free to start the discussion earlier if you feel there are much uncertainty which needs to be addressed earlier.

> user may want to restart this job multiple times using max-backoff before failing it.
Yes you are right. Yet maybe we can give it a more user friendly name. e.g. `max-attempts-before-reset-backoff` looks better to me.;;;","15/Dec/23 04:50;fanrui;Merged to master(1.19) via :

1c7b10873be475b73d78a083eda6be71fbb13c2b

80e71a47662c70e5cc0d96bfa3962bd37a6d020d

3d4d396e68e6cf7f49b7cc8d94b2c9516ffc2b96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-parent should use maven-shade-plugin 3.3.0+ for Java 17,FLINK-32894,13547844,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,18/Aug/23 09:24,17/Jan/24 11:27,04/Jun/24 20:40,07/Dec/23 09:51,connector-parent-1.0.0,,,,,connector-parent-1.1.0,,,,Connectors / Parent,,,,,,0,pull-request-available,,,"When I tried to compile {{flink-sql-connector-kafka}} with Java 17 and using profile {{{}-Pjava17 -Pjava17-target{}}}:

 
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.2.4:shade (shade-flink) on project flink-sql-connector-kafka: Error creating shaded jar: Problem shading JAR flink-connectors/flink-connector-kafka/flink-connector-kafka/target/flink-connector-kafka-3.1-SNAPSHOT.jar entry org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducerBase.class: java.lang.IllegalArgumentException: Unsupported class file major version 61 {code}
{{maven-shade-plugin}} supports Java 17 starting from 3.3.0 (see MSHADE-407). We need to set the version of {{maven-shade-plugin}} to at least 3.3.0 for profile {{java17}} in {{flink-connector-parent}} pom.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 09:51:12 UTC 2023,,,,,,,,,,"0|z1jvgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 09:51;Sergey Nuyanzin;Merged  as [7c2745af777c6681b9eb14f0b05b2136fb141784|https://github.com/apache/flink-connector-shared-utils/commit/7c2745af777c6681b9eb14f0b05b2136fb141784];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make client.id configurable from KafkaSource,FLINK-32893,13547786,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,17/Aug/23 21:34,18/Jan/24 21:47,04/Jun/24 20:40,,kafka-3.0.0,,,,,,,,,,,,,,,0,pull-request-available,,,"The client id is not strictly configurable from the KafkaSource because it appends a configurable prefix and subtask information to avoid the mbean conflict exception messages that are internal to the Kafka client.

 

However, various users reported that they need use this client.id for Kafka quotas and they need to have control over the client.id to enforce quotas properly.

 

Affects Kafka connector 3.1 and below.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 20 00:01:52 UTC 2023,,,,,,,,,,"0|z1jv3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 21:37;mason6345;I was talking to [~ryanvanhuuksloot] offline, he's interested in picking this up

 ;;;","20/Sep/23 00:01;mason6345;We discussed this offline. It is unclear whether or not this is actually required since Kafka quotas seem to be able to use an opaque principal name as per [https://docs.confluent.io/kafka/design/quotas.html]. The client.id parameter is optional from the Kafka perspective and I'm not sure we should violate this. 

In addition, it doesn't make sense to use quotas without proper auth and proper auth as noted in the docs should be able to identify a client without the `client.id` parameter.

I would opt to mark this ticket as ""not a problem"" unless there are differing opinions;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle Kafka clients JMX mBean errors by disabling JMX metrics,FLINK-32892,13547785,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masc,mason6345,mason6345,17/Aug/23 21:26,16/Apr/24 08:03,04/Jun/24 20:40,,1.17.1,,,,,kafka-4.0.0,,,,Connectors / Kafka,,,,,,0,,,,"3.4.x includes an implementation for KIP-830. [https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]

Many people on the mailing list have complained about confusing warning logs about mbean conflicts. We should be safe to disable this JMX reporter since Flink has its own metric system and the internal kafka-clients JMX reporter should not be used. 

This affects Kafka connector release 3.1 and below (for some reason I cannot enter 3.1 in the affects version/s box).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 19 23:33:29 UTC 2023,,,,,,,,,,"0|z1jv3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 23:33;mason6345;Renamed the title as I just realized https://issues.apache.org/jira/browse/FLINK-31599 is completed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Opensearch SQL connector crash job on upsert from multiple sources (409 version conflict),FLINK-32891,13547753,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,thebranchnotmerged,thebranchnotmerged,17/Aug/23 16:28,17/Aug/23 20:15,04/Jun/24 20:40,,opensearch-1.0.1,,,,,,,,,,,,,,,0,,,,"Using Opensearch SQL Connector for flink , An attempt to perform an Upsert for the same document ID from multiple jobs at the same time has resulted in job crush with 409 version conflict error message, In our environment we cannot guarantee that messages will arrive separately.

I suggest in RowOpensearchEmitter.processUpsert() , when an UpdateRequest object is being created it will also call an existing method of the UpdateRequest ""retryOnConflict(int retries)"" which is designed to remedy this issue where 'retries' will be set by OpensearchConnectorOptions class , default 0.

suggested parameter name : 'sink.upsert.retry-on-conflict' 

Also, I do not see evidence of bulk-retries being Effective in this case",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 20:15:53 UTC 2023,,,,,,,,,,"0|z1juw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 20:15;reta;Thanks [~thebranchnotmerged] , I think it could be useful enhancements, [~martijnvisser] if you have no objections, please assign it to me;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink app rolled back with old savepoints (3 hours back in time) while some checkpoints have been taken in between,FLINK-32890,13547727,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nfraison.datadog,nfraison.datadog,17/Aug/23 12:49,30/Oct/23 10:37,04/Jun/24 20:40,22/Aug/23 20:52,,,,,,kubernetes-operator-1.6.1,kubernetes-operator-1.7.0,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"Here are all details about the issue:
 * Deployed new release of a flink app with a new operator
 * Flink Operator set the app as stable
 * After some time the app failed and stay in failed state (due to some issue with our kafka clusters)
 * Finally decided to rollback the new release just in case it could be the root cause of the issue on kafka
 * Operator detect: {{Job is not running but HA metadata is available for last state restore, ready for upgrade, Deleting JobManager deployment while preserving HA metadata.}}  -> rely on last-state (as we do not disable fallback), no savepoint taken
 * Flink start JM and deployment of the app. It well find the 3 checkpoints

 * {{Using '/flink-kafka-job-apache-nico/flink-kafka-job-apache-nico' as Zookeeper namespace.}}
 * {{Initializing job 'flink-kafka-job' (6b24a364c1905e924a69f3dbff0d26a6).}}
 * {{Recovering checkpoints from ZooKeeperStateHandleStore\{namespace='flink-kafka-job-apache-nico/flink-kafka-job-apache-nico/jobs/6b24a364c1905e924a69f3dbff0d26a6/checkpoints'}.}}
 * {{Found 3 checkpoints in ZooKeeperStateHandleStore\{namespace='flink-kafka-job-apache-nico/flink-kafka-job-apache-nico/jobs/6b24a364c1905e924a69f3dbff0d26a6/checkpoints'}.}}
 * {{{}Restoring job 6b24a364c1905e924a69f3dbff0d26a6 from Checkpoint 19 @ 1692268003920 for 6b24a364c1905e924a69f3dbff0d26a6 located at }}\{{{}s3p://.../flink-kafka-job-apache-nico/checkpoints/6b24a364c1905e924a69f3dbff0d26a6/chk-19{}}}{{{}.{}}}

 * Job failed because of the missing operator

{code:java}
Job 6b24a364c1905e924a69f3dbff0d26a6 reached terminal state FAILED.
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: There is no operator for the state f298e8715b4d85e6f965b60e1c848cbe * Job 6b24a364c1905e924a69f3dbff0d26a6 has been registered for cleanup in the JobResultStore after reaching a terminal state.{code}
 * {{Clean up the high availability data for job 6b24a364c1905e924a69f3dbff0d26a6.}}
 * {{Removed job graph 6b24a364c1905e924a69f3dbff0d26a6 from ZooKeeperStateHandleStore\{namespace='flink-kafka-job-apache-nico/flink-kafka-job-apache-nico/jobgraphs'}.}}

 * JobManager restart and try to resubmit the job but the job was already submitted so finished

 * {{Received JobGraph submission 'flink-kafka-job' (6b24a364c1905e924a69f3dbff0d26a6).}}
 * {{Ignoring JobGraph submission 'flink-kafka-job' (6b24a364c1905e924a69f3dbff0d26a6) because the job already reached a globally-terminal state (i.e. FAILED, CANCELED, FINISHED) in a previous execution.}}
 * {{Application completed SUCCESSFULLY}}

 * Finally the operator rollback the deployment and still indicate that {{Job is not running but HA metadata is available for last state restore, ready for upgrade}}
 * But the job metadata are not anymore there (clean previously)

 
{code:java}
(CONNECTED [zookeeper-data-eng-multi-cloud.zookeeper-flink.svc:2181]) /> ls /flink-kafka-job-apache-nico/flink-kafka-job-apache-nico/jobs/6b24a364c1905e924a69f3dbff0d26a6/checkpoints
Path /flink-kafka-job-apache-nico/flink-kafka-job-apache-nico/jobs/6b24a364c1905e924a69f3dbff0d26a6/checkpoints doesn't exist
(CONNECTED [zookeeper-data-eng-multi-cloud.zookeeper-flink.svc:2181]) /> ls /flink-kafka-job-apache-nico/flink-kafka-job-apache-nico/jobs
(CONNECTED [zookeeper-data-eng-multi-cloud.zookeeper-flink.svc:2181]) /> ls /flink-kafka-job-apache-nico/flink-kafka-job-apache-nico
jobgraphs
jobs
leader
{code}
 

The rolled back app from flink operator finally take the last provided savepoint as no metadata/checkpoints are available. But this last savepoint is an old one as during the upgrade the operator decided to rely on last-state (The old savepoint taken is a scheduled one)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 08:04:40 UTC 2023,,,,,,,,,,"0|z1juqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 08:59;nfraison.datadog;The HA metadata check for zookeeper is not fine. It should have check for availability of entries in the /jobs znode.

With such a test the rollback would have not being done requiring manual intervention to ensure last taken checkpoint is used to restore the flink app;;;","22/Aug/23 20:52;gyfora;merged to main 301b179c1e38904124fbfa22cacac1ee46602dce;;;","25/Aug/23 08:04;gyfora;release-1.6: 25cadc34bb3b4b2cbbffbffeadaeb9e337b27516;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryClassificationEvaluator gives wrong weighted AUC value,FLINK-32889,13547691,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,17/Aug/23 08:50,24/Aug/23 06:50,04/Jun/24 20:40,24/Aug/23 06:50,ml-2.3.0,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,"BinaryClassificationEvaluator gives wrong AUC value when a weight column provided.

Here is an case from the unit test. The (score, label, weight) of data are:
{code:java}
(0.9, 1.0,  0.8),
(0.9, 1.0,  0.7),
(0.9, 1.0,  0.5),
(0.75, 0.0,  1.2),
(0.6, 0.0,  1.3),
(0.9, 1.0,  1.5),
(0.9, 1.0,  1.4),
(0.4, 0.0,  0.3),
(0.3, 0.0,  0.5),
(0.9, 1.0,  1.9),
(0.2, 0.0,  1.2),
(0.1, 1.0,  1.0)
{code}
PySpark and scikit-learn gives a AUC score of 0.87179, while Flink ML implementation gives 0.891168.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 06:49:47 UTC 2023,,,,,,,,,,"0|z1juig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 02:53;hongfanxo;BTW, the area under PRC is also found incorrect. PySpark and scikit-learn give

0.9510202726261435,  while current implementation gives 0.9377705627705628.;;;","24/Aug/23 06:49;zhangzp;Solved on master via 5619c3b8591b220e78a0a792c1f940e06149c8f0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File upload runs into EndOfDataDecoderException,FLINK-32888,13547690,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Aug/23 08:45,18/Aug/23 12:27,04/Jun/24 20:40,18/Aug/23 12:27,1.17.0,,,,,1.16.3,1.17.2,1.18.0,,Runtime / REST,,,,,,0,pull-request-available,,,With the right request the FIleUploadHandler runs into a EndOfDataDecoderException although everything is fine.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 11:57:29 UTC 2023,,,,,,,,,,"0|z1jui8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 11:57;chesnay;master:
329ee55e2e3ef5a6cf21009f6e321a99b1c91452
9546f8243a24e7b45582b6de6702f819f1d73f97
1.17:
4a368707162760fc39208d4a4d4bac2c6c728802
3e1e32bd89a2ed871e79cd16634c6f66d5ff3db8
1.16:
333a6e67b4bbe78b8c5695f8cd52ea2ea7dc0b20
2d3c142eb036f2cf65b0a4f81caddd7e4c943fd5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinatorContext#workerExecutor may cause task initializing slowly ,FLINK-32887,13547668,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,baldur,baldur,17/Aug/23 06:14,17/Aug/23 12:13,04/Jun/24 20:40,,1.15.2,,,,,,,,,Connectors / Common,Runtime / Coordination,,,,,0,pull-request-available,,,"SourceCoordinatorContext#workerExecutor is typically used to calculate partitions of a source task and is implemented by a ScheduledExecutorService with only 1 core (hard coded).Tasks to calculate partitions with be executed through the function 'workerExecutor.scheduleAtFixedRate'.
--- 
In some case, for example, 'getSubscribedPartitions' method will take quite a long time(e.g. 5min) because of lots of topics are included in the same task or requests to outer systems timeout etc. And partitionDiscoveryInterval is set to a short intervel e.g. 1min.
In this case, 'getSubscribedPartitions' runnable tasks will be triggered repeatedly and be queued in the queue of workerExecutor, during the first 'getSubscribedPartitions' task running duration, which causing 'checkPartitionChanges' tasks will be queued too. Each 'checkPartitionChanges' task needs to wait for 25mins(5 * 'getSubscribedPartitions' task execution duration) before it was executed.
---
In my view, tasks of workerExecutor should be scheduled with fix deley rather than at fixed rate. Because there is no meaning that 'getSubscribedPartitions' tasks being repeatedly executed without a 'checkPartitionChanges' execution.",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-17 06:14:21.0,,,,,,,,,,"0|z1judc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with volumeMounts when creating OLM for Flink Operator 1.6.0,FLINK-32886,13547633,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jbusche,jbusche,jbusche,16/Aug/23 21:10,17/Aug/23 07:23,04/Jun/24 20:40,,kubernetes-operator-1.6.0,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"I notice a volumemount problem when trying to deploy the OLM CSV for the 1.6.0 Flink Kubernetes Operator.  (Following the directions from [OLM Verification of a Flink Kubernetes Operator Release|https://cwiki.apache.org/confluence/display/FLINK/OLM+Verification+of+a+Flink+Kubernetes+Operator+Release]]

 

^{{oc describe csv}}^

^{{...}}^

^{{Warning  InstallComponentFailed  46s (x7 over 49s)  operator-lifecycle-manager  install strategy failed: Deployment.apps ""flink-kubernetes-operator"" is invalid: [spec.template.spec.volumes[2].name: Duplicate value: ""keystore"", spec.template.spec.containers[0].volumeMounts[1].name: Not found: ""flink-artifacts-volume""]}}^

 

My current workaround is to change [line 88|https://github.com/apache/flink-kubernetes-operator/blob/main/tools/olm/docker-entry.sh#L88] to look like this:

 

{{  ^yq ea -i '.spec.install.spec.deployments[0].spec.template.spec.volumes[1] = \{""name"": ""flink-artifacts-volume"",""emptyDir"": {}}' ""${CSV_FILE}""^}}  ^{{yq ea -i '.spec.install.spec.deployments[0].spec.template.spec.volumes[2] = \{""name"": ""keystore"",""emptyDir"": {}}' ""${CSV_FILE}""}}^

 

And then the operator deploys without error:

^oc get csv                                                                                                                                          NAME                               DISPLAY                     VERSION   REPLACES                           PHASEflink-kubernetes-operator.v1.6.0   Flink Kubernetes Operator   1.6.0     flink-kubernetes-operator.v1.5.0   Succeeded^",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-16 21:10:16.0,,,,,,,,,,"0|z1ju5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactoring: Moving UrlPrefixDecorator into flink-clients so it can be used by RestClusterClient for PyFlink remote execution,FLINK-32885,13547621,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,elkhand,elkhand,elkhand,16/Aug/23 19:08,12/Sep/23 22:02,04/Jun/24 20:40,12/Sep/23 22:01,1.17.1,,,,,1.19.0,,,,Client / Job Submission,Table SQL / Gateway,,,,,0,pull-request-available,,,"UrlPrefixDecorator is introduced in `flink-sql-gateway` module, which has dependency on `flink-clients` module. RestClusterClient will also need to use UrlPrefixDecorator for supporting PyFlink remote execution. Will refactor related classes to achieve this.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-16 19:08:53.0,,,,,,,,,,"0|z1ju2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink remote execution should support URLs with paths and https scheme,FLINK-32884,13547620,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,elkhand,elkhand,elkhand,16/Aug/23 19:04,27/Sep/23 05:54,04/Jun/24 20:40,27/Sep/23 05:54,1.17.1,,,,,1.19.0,,,,Client / Job Submission,Runtime / REST,,,,,0,pull-request-available,,,"Currently, the `SUBMIT_ARGS=remote -m http://<hostname>:<port>` format. For local execution it works fine `SUBMIT_ARGS=remote -m http://localhost:8081/`, but it does not support the placement of the JobManager behind a proxy or using an Ingress for routing to a specific Flink cluster based on the URL path. In the current scenario, it expects JobManager to access PyFlink jobs at `http://<hostname>:<port>/v1/jobs` endpoint. Mapping to a non-root location, `https://<hostname>:<port>/flink-clusters/namespace/flink_job_deployment/v1/jobs` is not supported.

This will use changes from [FLINK-32885](https://issues.apache.org/jira/browse/FLINK-32885)(https://issues.apache.org/jira/browse/FLINK-32885)

Since RestClusterClient talks to the JobManager via its REST endpoint, the right format for `SUBMIT_ARGS` is a URL with a path (also support for https scheme).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2023-08-16 19:04:05.0,,,,,,,,,,"0|z1ju2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for standby task managers,FLINK-32883,13547599,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,laughingman7743,laughingman7743,16/Aug/23 16:02,31/Aug/23 06:32,04/Jun/24 20:40,,kubernetes-operator-1.6.0,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"[https://docs.ververica.com/user_guide/application_operations/deployments/scaling.html#run-with-standby-taskmanager]
I would like to be able to support standby task managers. Because on K8s, pods are often evicted or deleted due to node failure or autoscaling.

With the current implementation, it is not possible to set up a standby task manager, and jobs cannot run until all task managers are up and running. If a standby task manager could be supported, jobs could continue to run without downtime using the standby task manager, even if the task manager is unexpectedly deleted.

[https://github.com/apache/flink-kubernetes-operator/blob/release-1.6.0/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java#L370-L380|https://github.com/apache/flink-kubernetes-operator/blob/release-1.6.0/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java#L370-L380]
If the task manager's number of replicas is set, the job's parallelism setting is ignored, but it should be possible to support a standby task manager by automatically setting parallelism to the replicas*task slot only if the job's parallelism is not set (i.e. 0) and using that value if parallelism is set. 

If this change looks good, I will send a PR on GitHub.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 02:28:56 UTC 2023,,,,,,,,,,"0|z1jty0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 02:06;guoyangze;Hi, [~laughingman7743]. IIUC, your requirement should be fulfill by the [redundant taskmanager|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/config/#slotmanager-redundant-taskmanager-num]. Also, [~xiangyu0xf] and I is working on FLINK-15959, in which Flink should support to retain min resources even if the cluster is idle. I think these two feature should cover the functionality of the standby TM yon mentioned in this ticket. WDYT?;;;","17/Aug/23 02:18;laughingman7743;[~guoyangze] Thank you.	It looks like the slotmanager.redundant-taskmanager-num option only supports Native K8s and YARN.
I am running the Flink application in standalone mode with flink-kubernetes-operator, I will change to Native K8s mode and try it.
It would be nice if the standalone mode also supported the standby task manager.;;;","17/Aug/23 02:28;guoyangze;ok. I see your problem. The k8s operator will enforce the job parallelism to align with the total slot num. Not sure about the context of this enforcement. Maybe [~wangyang0918] can give more information about it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager cause NPE when clear pending taskmanager twice,FLINK-32882,13547528,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huweihua,huweihua,huweihua,16/Aug/23 09:40,16/Aug/23 14:34,04/Jun/24 20:40,16/Aug/23 14:34,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"When job finished we call processResourceRequirements(ResourceRequirements.empty) and clearResourceRequirements. Both methods trigger 
taskManagerTracker.clearPendingAllocationsOfJob(jobId) to release pending task manager early.
 
This causes NPE, we need to add a safety net for PendingTaskManager#clearPendingAllocationsOfJob",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/23 09:34;huweihua;image-2023-08-16-17-34-32-619.png;https://issues.apache.org/jira/secure/attachment/13062221/image-2023-08-16-17-34-32-619.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 14:34:16 UTC 2023,,,,,,,,,,"0|z1jti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 14:34;huweihua;master: b56ae7822f6e4ff3ad67991f1c7916a435c960e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client supports making savepoints in detach mode,FLINK-32881,13547526,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhourenxiang,zhourenxiang,zhourenxiang,16/Aug/23 09:28,27/Dec/23 02:50,04/Jun/24 20:40,27/Dec/23 02:50,1.19.0,,,,,1.19.0,,,,Client / Job Submission,Runtime / Checkpointing,,,,,3,detach-savepoint,pull-request-available,,"When triggering a savepoint using the command-line tool, the client needs to wait for the job to finish creating the savepoint before it can exit. For jobs with large state, the savepoint creation process can be time-consuming, leading to the following problems:
 # Platform users may need to manage thousands of Flink tasks on a single client machine. With the current savepoint triggering mode, all savepoint creation threads on that machine have to wait for the job to finish the snapshot, resulting in significant resource waste;
 # If the savepoint producing time exceeds the client's timeout duration, the client will throw a timeout exception and report that the triggering savepoint process fails. Since different jobs have varying savepoint durations, it is difficult to adjust the timeout parameter on the client side.

Therefore, we propose adding a detach mode to trigger savepoints on the client side, just similar to the detach mode behavior when submitting jobs. Here are some specific details:
 # The savepoint UUID will be generated on the client side. After successfully triggering the savepoint, the client immediately returns the UUID information and exits.
 # Add a ""dump-pending-savepoints"" API that allows the client to check whether the triggered savepoint has been successfully created.

By implementing these changes, the client can detach from the savepoint creation process, reducing resource waste, and providing a way to check the status of savepoint creation.

!image-2023-08-16-17-14-34-740.png|width=2129,height=625!!image-2023-08-16-17-14-44-212.png|width=1530,height=445!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/23 09:14;zhourenxiang;image-2023-08-16-17-14-34-740.png;https://issues.apache.org/jira/secure/attachment/13062220/image-2023-08-16-17-14-34-740.png","16/Aug/23 09:14;zhourenxiang;image-2023-08-16-17-14-44-212.png;https://issues.apache.org/jira/secure/attachment/13062219/image-2023-08-16-17-14-44-212.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,Wed Dec 27 02:50:30 UTC 2023,,,,,,,,,,"0|z1jths:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 13:12;legendtkl;+1 for this proposal.;;;","17/Aug/23 07:05;masteryhx;Hi, [~zhourenxiang] 
We often use asyc thread to trigger savepoint by REST API to resolve this.

Of course I think It's still useful for users to trigger savepoint by CLI.
Would you like to contribute your pr ?;;;","17/Aug/23 14:20;zhourenxiang;Hi, [~masteryhx]

I am very glad to contribute my code, so could you please assign this ticket to me, thank you~;;;","18/Aug/23 02:15;masteryhx;[~zhourenxiang] Sure, assigned to you, please go ahead.;;;","18/Aug/23 02:22;zhourenxiang;[~masteryhx] Many thanks~ ;);;;","22/Aug/23 09:18;zhourenxiang;[~masteryhx] PR has been submitted, please take a look, thank you~

https://github.com/apache/flink/pull/23253;;;","27/Dec/23 02:50;masteryhx;merged into master via 81598f8a...d0dbd51c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant taskManagers should always be fulfilled in FineGrainedSlotManager,FLINK-32880,13547507,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xiangyu0xf,xiangyu0xf,xiangyu0xf,16/Aug/23 07:05,25/Dec/23 11:17,04/Jun/24 20:40,18/Aug/23 08:35,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"Currently, if you are using FineGrainedSlotManager, when a redundant taskmanager exit abnormally, no extra taskmanager will be replenished during the
periodical check in FineGrainedSlotManager.
 ",,,,,,,,,,,,,,FLINK-15959,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 25 11:17:27 UTC 2023,,,,,,,,,,"0|z1jtdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 08:35;xiangyu0xf;Hi [~guoyangze], I found this issue in the process of completing [FLINK-15959|https://issues.apache.org/jira/browse/FLINK-15959]. I'd like to fix this issue first before proceeding [FLINK-15959|https://issues.apache.org/jira/browse/FLINK-15959]. Would u kindly assign this to me?;;;","16/Aug/23 09:14;guoyangze;ok, I assign it to you.;;;","23/Dec/23 09:19;mapohl;small reminder for the future [~huweihua]: Please add the merge information as a comment. The release notes are meant to be used for adding information that's relevant to the user (this field is used to generate the release notes of the corresponding release, i.e. 1.18.0 in that case). :-);;;","23/Dec/23 09:19;mapohl;master: 7299da4cf688a2d87fd918b6327a0573bc88cbd8;;;","25/Dec/23 11:17;huweihua;[~mapohl]  Sorry for missing the merge information. I will remember it. Thanks for reminding.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
slotmanager.max-total-resource.cpu/memory should be ignored in standalone mode.,FLINK-32879,13547495,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangyu0xf,guoyangze,guoyangze,16/Aug/23 05:57,18/Aug/23 03:22,04/Jun/24 20:40,18/Aug/23 03:22,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"Just as slotmanager.number-of-slots.max, we should also ignore the cpu and memory limitation in standalone mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 03:21:46 UTC 2023,,,,,,,,,,"0|z1jtaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 03:21;huweihua;Resolved in master: c8cbb0e2dec6ed505ce749c453e4f69ff0aecc95;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add entropy to gcs path for better scalability,FLINK-32878,13547475,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cheena_budhiraja,jjayadeep,jjayadeep,16/Aug/23 03:19,01/Dec/23 15:39,04/Jun/24 20:40,01/Dec/23 15:39,1.15.4,1.16.2,1.17.1,,,1.19.0,,,,Connectors / FileSystem,,,,,,0,pull-request-available,,,"Currently GCS is used as a backend for both checkpointing and sink. In both these cases the file names are sequential which causes hotspotting in GCS and results in HTTP 5XX status code.

 

As per [GCS best practices]([https://cloud.google.com/storage/docs/request-rate)] it is advisable to spread out the object creation with random names close to the beginning of the file name. 

 

There is similar work done already for S3 as part of FLINK-9061 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 01 15:39:09 UTC 2023,,,,,,,,,,"0|z1jt6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/23 13:23;martijnvisser;Fixed in apache/flink:master dc1db12137ecad921ae90969d7bfbf1ee7d3d2ef;;;","01/Dec/23 15:39;martijnvisser;I reverted this change via c7da98f23c3f86de3a3b12355fa7a1289200f93d because CI indicated that it had failed and I couldn't compile it locally. However, CI was failing because of an unrelated error (download issue on NPM) and my local version failed because of my mistake.

So re-enabled it via 87d7b4abd0a1ec92433603c83401cc8ad00fd500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for HTTP connect and timeout options while writes in GCS connector,FLINK-32877,13547473,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,singhravidutt,jjayadeep,jjayadeep,16/Aug/23 02:46,26/Apr/24 09:10,04/Jun/24 20:40,26/Apr/24 09:10,1.15.3,1.16.2,1.17.1,1.18.1,1.19.0,1.20.0,,,,Connectors / FileSystem,,,,,,0,pull-request-available,,,"The current GCS connector uses the gcs java storage library and bypasses the hadoop gcs connector which supports multiple http options. There are situations where GCS takes longer to provide a response for a PUT operation than the default value.

This change will allow users to customize their connect time and read timeout based on their application",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35232,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 04:41:24 UTC 2023,,,,,,,,,,"0|z1jt60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 04:41;jjayadeep;[~martijnvisser] - can you please take a look.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionTimeBasedSlowTaskDetector treats unscheduled tasks as slow tasks and causes speculative execution to fail.,FLINK-32876,13547470,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,16/Aug/23 02:36,18/Aug/23 06:13,04/Jun/24 20:40,18/Aug/23 06:13,1.17.1,1.18.0,,,,1.17.2,1.18.0,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"When we enable speculative execution and configure job with the following configuration:
{code:java}
execution.batch.speculative.enabled: true
slow-task-detector.execution-time.baseline-ratio: 0.0
slow-task-detector.execution-time.baseline-lower-bound: 0s{code}
The ExecutionTimeBasedSlowTaskDetector will identify ExecutionJobVertex that has not yet been scheduled as slow tasks and notify them to the SpeculativeScheduler. However, the SpeculativeScheduler requires that the corresponding ExecutionVertex has entered the scheduled state before scheduling backup tasks. If this requirement is not met, it will result in speculative execution failure.

The exception stack trace is as follows:
{code:java}
java.lang.IllegalStateException: Execution vertex b3f44e8b1dc132ff2a47f7955c75ef7d_0 does not have a recorded version  at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.scheduler.ExecutionVertexVersioner.getCurrentVersion(ExecutionVertexVersioner.java:71) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.scheduler.ExecutionVertexVersioner.lambda$getExecutionVertexVersions$1(ExecutionVertexVersioner.java:89) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_333]  at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1580) ~[?:1.8.0_333]  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_333]  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_333]  at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_333]  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_333]  at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_333]  at org.apache.flink.runtime.scheduler.ExecutionVertexVersioner.getExecutionVertexVersions(ExecutionVertexVersioner.java:90) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.notifySlowTasks(SpeculativeScheduler.java:377) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.scheduler.slowtaskdetector.ExecutionTimeBasedSlowTaskDetector.lambda$scheduleTask$1(ExecutionTimeBasedSlowTaskDetector.java:129) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_333]  at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_333]  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451) ~[flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451) ~[flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218) ~[flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85) ~[flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akka48a43f0a-d73c-494a-a57b-ded9f5d82a84.jar:1.18-SNAPSHOT]  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_333]  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067) [?:1.8.0_333]  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703) [?:1.8.0_333]  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172) [?:1.8.0_333] {code}
 ",,,,,,,,,,,,,,FLINK-32788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 06:13:59 UTC 2023,,,,,,,,,,"0|z1jt5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 06:13;zhuzh;master/1.18: 4bce9c09ca4fdf3ad8bf95ba5cf4ca361acea156
1.17: 38b9c280128981b3e809df1f963bdaf8c0491804;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable limiting number of retained jobs for the local job archive directory in flink history server,FLINK-32875,13547447,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,achang52255,achang52255,15/Aug/23 21:08,17/Aug/23 08:33,04/Jun/24 20:40,,1.16.1,,,,,,,,,,,,,,,0,,,,"Currently the Flink history server limits the number of job archives stored using the history.server.retained-jobs configuration, which sets the limit for both the local cache as well as remote directories. We want to decouple the local cache limit from the remote directory which stores the job archives so that we are not limited by the size of a local filesystem where Flink History server is deployed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-15 21:08:50.0,,,,,,,,,,"0|z1jt08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Janino to current,FLINK-32874,13547424,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,satanicmechanic,satanicmechanic,15/Aug/23 16:58,31/Aug/23 15:52,04/Jun/24 20:40,16/Aug/23 07:16,1.17.1,,,,,,,,,,,,,,,0,security,,,Janino 3.1.9 is vulnerable to CVE-2023-33546,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 15:52:28 UTC 2023,,,,,,,,,,"0|z1jsv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 21:18;Sergey Nuyanzin;[~satanicmechanic] could you please clarify where does 3.1.9 come from?
Master depends on 3.1.10
1.17. depends on 3.0.11

Moreover description states
{quote}
 If the parser runs on user-supplied input, an attacker could supply content that causes the parser to crash due to a stack overflow.
{quote}
Janino is used only internally without exposing to user... So it is unclear how it is possible in Flink to supply such content from user side...
Nice to have it updated however not sure that it impacts Flink.
;;;","16/Aug/23 07:16;Sergey Nuyanzin;anyway it is a duplicate of FLINK-30984
so closing it;;;","31/Aug/23 15:52;satanicmechanic;Thank you for the triage and for the link.  I didn't recognize there was an update from the ticket but I see it in the merge.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add configuration to allow disabling SQL query hints,FLINK-32873,13547417,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,bvarghese,bvarghese,bvarghese,15/Aug/23 16:14,11/Mar/24 12:43,04/Jun/24 20:40,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"Platform providers may want to disable hints completely for security reasons.

Currently, there is a configuration to disable OPTIONS hint - [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config/#table-dynamic-table-options-enabled]

We need a new configuration to also disable QUERY hints - [https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/queries/hints/#query-hints]

The proposal is to add a new configuration:

 
{code:java}
Name: table.optimizer.query-hints.enabled
Description: Enable or disable the QUERY hint, if disabled, an exception would be thrown if any QUERY hints are specified
Note: The default value will be set to true.
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 16:01:38 UTC 2023,,,,,,,,,,"0|z1jstk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 06:20;lsy;[~bvarghese] Does there have some discuss introducing this option?;;;","16/Aug/23 06:44;xuyangzhong;[~bvarghese] Although I think this option is necessary, what about getting a discussion in dev mail list ? ;;;","16/Aug/23 16:01;bvarghese;[~xuyangzhong] [~lsy] Created a discussion in the dev mailing list - https://lists.apache.org/thread/vqx2809g2z0kzbq4cj1lfx76rdy5cvp1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to control the default partitioner when the parallelism of upstream and downstream operator does not match,FLINK-32872,13547397,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,15/Aug/23 14:35,06/Feb/24 08:34,04/Jun/24 20:40,,1.17.0,,,,,,,,,Runtime / Configuration,,,,,,0,,,,"*Problem*

Currently, when the no partitioner is specified, FORWARD partitioner is used if the parallelism of upstream and downstream operator matches, REBALANCE partitioner used otherwise. However, this behavior is not configurable and can be undesirable in certain cases:
 #  REBALANCE partitioner will create an all-to-all connection between upstream and downstream operators and consume a lot of extra CPU and memory resources when the parallelism is high in pipelining mode and RESCALE partitioner is desirable in this case.
 # For Flink SQL jobs, users cannot specify the partitioner directly so far. And for DataStream jobs, users may not want to explicitly set the partitioner everywhere.

*Proposal*

Add an option to control the default partitioner when the parallelism of upstream and downstream operator does not match. The option can have the name ""pipeline.default-partitioner-with-unmatched-parallelism"" with REBALANCE as the default value.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 05 11:26:18 UTC 2023,,,,,,,,,,"0|z1jsp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 14:36;Zhanghao Chen;[~huwh] [~guoyangze] What do you think about this?;;;","05/Sep/23 04:59;Zhanghao Chen;[~huweihua] Looking forward to your suggestions on this issue~;;;","05/Sep/23 11:26;huweihua;[~Zhanghao Chen] Thanks for bring this. We need a FLIP and public discussion in the dev ML to adding a new config options.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support BuiltInMethod TO_TIMESTAMP with timezone options,FLINK-32871,13547369,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuzifu,xuzifu,15/Aug/23 11:26,11/Mar/24 12:43,04/Jun/24 20:40,,,,,,,1.20.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"Support BuiltInMethod TO_TIMESTAMP with timezone options，TO_TIMESTAMPS now only use utc timezone，but many scenarios we need timzone to choose，so need a pr to support it as TO_TIMESTAMP('2023-08-10', 'yyyy-MM-dd', 'Asia/Shanghai')

this scenario in presto，starrocks，trino：

as presto,trino,starrocks:
SELECT timestamp '2012-10-31 01:00 UTC' AT TIME ZONE 'America/Los_Angeles';
2012-10-30 18:00:00.000 America/Los_Angeles

so we maybe need this function in to_timestamps",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33594,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 09:46:15 UTC 2023,,,,,,,,,,"0|z1jsiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 12:10;xuzifu;cc [~twalthr] have a review please;;;","21/Aug/23 04:43;libenchao;{{timestamp '2012-10-31 01:00 UTC' AT TIME ZONE 'America/Los_Angeles'}} is constructing a {{TIMESTAMP}} literal, it's different from the function {{TO_TIMESTAMP}}. Are u seeing any databases that contains {{TO_TIMESTAMP}} that have such capability?;;;","21/Aug/23 04:47;xuzifu;other engines have no functions named to_timestamps，but have function like to_timestamps in flink，so we support the function to be compatible [~libenchao] ;;;","21/Aug/23 04:54;libenchao;I don't think they are the same thing, and I believe {{TO_TIMESTAMP}} exists in some other databases. I searched it in Google, and it seems that {{SQL Server}}/{{Oracle}}/{{RedShift}} all have it, it would be better to keep it aligned with these engines.;;;","21/Aug/23 06:15;xuzifu;engines I mean some engines in bigdata scope，such as presto、trino、starrocks and so on，these engine had function to compute timestamp but name is not to_timestamp，and these engines support timezone argument. Oracle/SQL Server may not used in bigdata scope，so i add the function with it，otherwise user can only use udf for it，and it would increase maintenance cost [~libenchao] ;;;","12/Oct/23 09:46;martijnvisser;I'm +1 for [~libenchao] his comment: I don't think this is correct;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading multiple small buffers by reading and slicing one large buffer for tiered storage,FLINK-32870,13547364,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,15/Aug/23 10:30,12/Sep/23 03:43,04/Jun/24 20:40,12/Sep/23 03:43,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,"Currently, when the file reader of tiered storage loads data from the disk file, it reads data in buffer granularity. Before compression, each buffer is 32K by default. After compressed, the size will become smaller (may less than 5K), which is pretty small for the network buffer and the file IO. 
We should read multiple small buffers by reading and slicing one large buffer to decrease the buffer competition and the file IO, leading to better performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 03:43:26 UTC 2023,,,,,,,,,,"0|z1jshs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 03:43;xtsong;- master(1.19): ade583cf80478ac60e9b83fc0a97f36bc2b26f1c
- release-1.18: d100ab65367fe0b3d74a9901bcaaa26049c996b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kubernetes related tests failed to run on aarch64,FLINK-32869,13547347,13247523,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mimaomao,mimaomao,mimaomao,15/Aug/23 08:16,01/Sep/23 09:20,04/Jun/24 20:40,25/Aug/23 06:37,1.16.2,1.17.1,1.18.0,,,1.16.3,1.17.2,1.18.0,,Tests,,,,,,0,pull-request-available,,,"Both _crictl_ and _cri-dockerd_ only applies to *amd64* currently. See [here|https://github.com/apache/flink/blob/fc2b5d8f53a41695117f6eaf4c798cc183cf1e36/flink-end-to-end-tests/test-scripts/common_kubernetes.sh#L62] for example. Thus, the kubernetes related tests will fail in aarch64 environment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 06:37:20 UTC 2023,,,,,,,,,,"0|z1jse0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 08:17;mimaomao;I have already had a fix for this. Please assign this issue to me.;;;","15/Aug/23 10:09;mapohl;Hi [~mimaomao], thanks for creating the issue. I guess, the 1.8.0 affected version and fix version is only a typo? I'm gonna go ahead and fix that one.

Additionally, why do you think that this is a blocker (see [Flink's Jira Process documentation|https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process])?
{quote}
infrastructure failures, bugs that block us from releasing
{quote}

FLINK-13448 covers ARM support (which is partially resolved). But we don't have ARM support in our CI infrastructure right now. I'm gonna go ahead and adapt the issue accordingly and move it under FLINK-13448 as a subtask. ;;;","15/Aug/23 10:15;mapohl;The code snippet you refer to in the Jira issue's description was introduced in FLINK-28269 (i.e. Flink 1.16). In this sense, it's also not considered a blocker because it's not newly introduced but was already missed in previous releases. Please correct me if I'm wrong.

I'm gonna assign the issue to you. Thanks for picking it up.

fyi: [~snuyanzin] [~renqs] [~jingge] [~knaufk];;;","18/Aug/23 03:11;mimaomao;Thanks [~mapohl] ! I have already created a PR here: https://github.com/apache/flink/pull/23235.;;;","25/Aug/23 06:37;mapohl;master (1.19): d91b40698f33fae65900342730c1ffa689b97ff9
1.18: 1b250f7128aa1b91311e7206ff03dd89ceca7801
1.17: d640cc64671a70bbd194b4dec24a4cfb955f4aee
1.16: 9b5c4c9b242ce53f8c26a661d3c3a9eeee64cb99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document the need to backport FLINK-30213 for using autoscaler with older version Flinks,FLINK-32868,13547333,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,15/Aug/23 05:25,30/Oct/23 10:37,04/Jun/24 20:40,15/Aug/23 13:11,,,,,,kubernetes-operator-1.6.1,kubernetes-operator-1.7.0,,,Autoscaler,,,,,,0,pull-request-available,,,"The current Autoscaler doc states on job requirements as the following:

Job requirements:
 * The autoscaler currently only works with the latest [Flink 1.17|https://hub.docker.com/_/flink] or after backporting the following fixes to your 1.15/1.16 Flink image
 ** [Job vertex parallelism overrides|https://github.com/apache/flink/commit/23ce2281a0bb4047c64def9af7ddd5f19d88e2a9] (must have)
 ** [Support timespan for busyTime metrics|https://github.com/apache/flink/commit/a7fdab8b23cddf568fa32ee7eb804d7c3eb23a35] (good to have)

However, https://issues.apache.org/jira/browse/FLINK-30213 is also crucial and need to be backported to 1.15/1.16 to enable autoscaling. We should add it to the doc as well, and marked as must have.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-15 05:25:21.0,,,,,,,,,,"0|z1jsaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suspending job and triggering savepoint when only updating restartNonce,FLINK-32867,13547283,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Bug,,haiqingchen,haiqingchen,14/Aug/23 15:35,15/Aug/23 13:39,04/Jun/24 20:40,15/Aug/23 13:39,kubernetes-operator-1.4.0,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"When I tried to restart Flink Deployment without any configuration change,

I updated restartNonce to current time mills.

However, kubernetes operator suspend the Flink Deployment and trigger a savepoint but we didn't update savepointTriggerNonce column.

It's not what we expected as we thought if we didn't update savepointTriggerNonce it would never trigger savepoint.

Is there any possibility to add a precheck(if savepointTriggerNonce is updated) when canceling the job?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 13:39:43 UTC 2023,,,,,,,,,,"0|z1jrzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 16:01;gyfora;If you change the restartNonce a full job restart / upgrade will be performed. If you have savepoint upgradeMode this will trigger a shutdown with savepoint , then restart the job from the savepoint. Sounds like normal behaviour to me;;;","15/Aug/23 13:36;haiqingchen;Thanks, I got the idea about savepoint upgradeMode. But if I want a full job restart with a predefined checkpoint path, the only option is to delete the deployment, wait until the deployment deleted completely and recreate the deployment with initial savepoint path. I'm wondering if we could introduce another upgrade mode option for this kind of situation: If I update restartNonce with a predefined checkpoint path, the job will restart from the checkpoint path. 

 ;;;","15/Aug/23 13:39;gyfora;restartNonce is meant to trigger a simple redeployment using the latest checkpoint. Adding this new functionality would be confusing to the existing users. I think deleting and recreating is the way to go if you want to use a new predefined checkpoint path currently.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify the TestLoggerExtension config of junit5,FLINK-32866,13547281,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,fanrui,fanrui,fanrui,14/Aug/23 15:33,16/Oct/23 09:31,04/Jun/24 20:40,,,,,,,,,,,Tests,,,,,,1,pull-request-available,starter,,"Some modules added the {{TestLoggerExtension}} to the {{org.junit.jupiter.api.extension.Extension}} resource file. All test classes of these modules don't need add the {{@ExtendWith(TestLoggerExtension.class)}}  at class level.


This JIRA propose clean up the {{@ExtendWith(TestLoggerExtension.class)}}  for modules that added the {{TestLoggerExtension}} to the {{org.junit.jupiter.api.extension.Extension}} resource file.

Update: We could also investigate in what extend we could remove the {{org.junit.jupiter.api.extension.Extension}} resource files from certain modules. It only has to be present once on the classpath. A single location for this configuration would be nice (e.g. putting it in {{flink-test-utils-parent/flink-test-utils-junit}}). But I think not all modules have this one as there dependency. So, having a single resource file for configuring this might be not possible. But at least, we can reduce the number of files to a minimum.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 27 22:35:12 UTC 2023,,,,,,,,,,"0|z1jrzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 09:11;fanrui;Hi [~mapohl] , thanks for the update.
{quote}A single location for this configuration would be nice
{quote}
I think it's a great solution if we want to enable {{TestLoggerExtension }}for all tests of the whole flink repo. And I found a good place for it, we can put it in the same module as TestLoggerExtension.

It means all test classes will enable TestLoggerExtension if the module can find the TestLoggerExtension class. I have tried it, it works. You can take a look the draft PR: https://github.com/apache/flink/pull/23217;;;","24/Aug/23 14:16;fanrui;Hi [~mapohl] , would you mind helping take a look this idea in your free time? Does it make sense? thanks a lot~;;;","27/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicFilteringDataCollectorOperator can't chain with the upstream operator when the parallelism is inconsistent,FLINK-32865,13547239,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,lsy,lsy,14/Aug/23 11:21,30/Aug/23 13:13,04/Jun/24 20:40,30/Aug/23 13:13,1.16.2,1.17.1,1.18.0,,,1.18.0,1.19.0,,,Table SQL / Planner,,,,,,0,,,,"!image-2023-08-14-19-17-22-109.png!

 

If the DynamicFilteringDataCollectorOperator parallelism is not consistent with the upstream operator, they can't chain together, this will the DynamicFilteringDataCollectorOperator to execute after the fact source, so the dpp won't work. Due to the operator parallelism being decided during runtime, so we should add scheduler dependency forcibly in compile phase.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 11:17;lsy;image-2023-08-14-19-17-22-109.png;https://issues.apache.org/jira/secure/attachment/13062187/image-2023-08-14-19-17-22-109.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 13:12:56 UTC 2023,,,,,,,,,,"0|z1jrq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 13:12;wanglijie;Fixed via:
master(1.19):
5ce5c8142c8c360bc10804fa1e52b13a3376a50f
3fffed00df3234106464642f555275b846ff74b3

release-1.18:
83915d909dce3832ba2c3dbd9b1624715466aa2d
94ba97a13abc7e70a55d929fd5fdf85d52417639;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamDependencyTests.test_set_requirements_with_cached_directory  fails on AZP,FLINK-32864,13547214,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,14/Aug/23 08:32,15/Aug/23 09:08,04/Jun/24 20:40,,1.16.2,1.17.1,1.18.0,,,,,,,API / Python,,,,,,0,test-stability,,,"This build [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52209&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25208]
fails as below.
The error is very similar to one of the old issues FLINK-15929, probably some dependencies in requirements should be fixed...

 {noformat}
Aug 13 01:41:17 =================================== FAILURES ===================================
Aug 13 01:41:17 ______ StreamDependencyTests.test_set_requirements_with_cached_directory _______
Aug 13 01:41:17 
Aug 13 01:41:17 self = <pyflink.table.tests.test_dependency.StreamDependencyTests testMethod=test_set_requirements_with_cached_directory>
Aug 13 01:41:17 
Aug 13 01:41:17     def test_set_requirements_with_cached_directory(self):
Aug 13 01:41:17         tmp_dir = self.tempdir
Aug 13 01:41:17         requirements_txt_path = os.path.join(tmp_dir, ""requirements_txt_"" + str(uuid.uuid4()))
Aug 13 01:41:17         with open(requirements_txt_path, 'w') as f:
Aug 13 01:41:17             f.write(""python-package1==0.0.0"")
Aug 13 01:41:17     
Aug 13 01:41:17         requirements_dir_path = os.path.join(tmp_dir, ""requirements_dir_"" + str(uuid.uuid4()))
Aug 13 01:41:17         os.mkdir(requirements_dir_path)
Aug 13 01:41:17         package_file_name = ""python-package1-0.0.0.tar.gz""

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 09:08:18 UTC 2023,,,,,,,,,,"0|z1jrkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 08:33;Sergey Nuyanzin;cc [~dianfu], [~hxbks2ks];;;","14/Aug/23 13:06;mapohl;{quote}
ModuleNotFoundError: No module named 'python_package1'
{quote}
Not sure about the close relationship with FLINK-15929. Looks like there is a module not appearing on the classpath:
{code}
[...]
Aug 13 01:41:17 E                   Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.invokeFinishBundle(AbstractExternalPythonFunctionOperator.java:107)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByCount(AbstractPythonFunctionOperator.java:292)
Aug 13 01:41:17 E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.processElement(AbstractStatelessFunctionOperator.java:101)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:425)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:520)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:110)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:99)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338)
Aug 13 01:41:17 E                   Caused by: java.lang.RuntimeException: Failed to close remote bundle
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:423)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:407)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.lambda$invokeFinishBundle$0(AbstractExternalPythonFunctionOperator.java:86)
Aug 13 01:41:17 E                   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Aug 13 01:41:17 E                   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 13 01:41:17 E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Aug 13 01:41:17 E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Aug 13 01:41:17 E                   	at java.lang.Thread.run(Thread.java:748)
Aug 13 01:41:17 E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 287, in _execute
Aug 13 01:41:17 E                       response = task()
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 360, in <lambda>
Aug 13 01:41:17 E                       lambda: self.create_worker().do_instruction(request), request)
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 597, in do_instruction
Aug 13 01:41:17 E                       getattr(request, request_type), request.instruction_id)
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 634, in process_bundle
Aug 13 01:41:17 E                       bundle_processor.process_bundle(instruction_id))
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1004, in process_bundle
Aug 13 01:41:17 E                       element.data)
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 227, in process_encoded
Aug 13 01:41:17 E                       self.output(decoded_value)
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 526, in apache_beam.runners.worker.operations.Operation.output
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 528, in apache_beam.runners.worker.operations.Operation.output
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Aug 13 01:41:17 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 169, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Aug 13 01:41:17 E                       with self.scoped_process_state:
Aug 13 01:41:17 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 196, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Aug 13 01:41:17 E                       self.process_element(input_processor.next())
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/pyflink/fn_execution/table/operations.py"", line 102, in process_element
Aug 13 01:41:17 E                       return self.func(value)
Aug 13 01:41:17 E                     File ""<string>"", line 1, in <lambda>
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/pyflink/table/tests/test_dependency.py"", line 184, in add_one
Aug 13 01:41:17 E                       from python_package1 import plus
Aug 13 01:41:17 E                   ModuleNotFoundError: No module named 'python_package1'
Aug 13 01:41:17 E                   
Aug 13 01:41:17 E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Aug 13 01:41:17 E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Aug 13 01:41:17 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:61)
Aug 13 01:41:17 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
Aug 13 01:41:17 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
Aug 13 01:41:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:421)
Aug 13 01:41:17 E                   	... 7 more
Aug 13 01:41:17 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 287, in _execute
Aug 13 01:41:17 E                       response = task()
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 360, in <lambda>
Aug 13 01:41:17 E                       lambda: self.create_worker().do_instruction(request), request)
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 597, in do_instruction
Aug 13 01:41:17 E                       getattr(request, request_type), request.instruction_id)
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 634, in process_bundle
Aug 13 01:41:17 E                       bundle_processor.process_bundle(instruction_id))
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1004, in process_bundle
Aug 13 01:41:17 E                       element.data)
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 227, in process_encoded
Aug 13 01:41:17 E                       self.output(decoded_value)
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 526, in apache_beam.runners.worker.operations.Operation.output
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 528, in apache_beam.runners.worker.operations.Operation.output
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Aug 13 01:41:17 E                     File ""apache_beam/runners/worker/operations.py"", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Aug 13 01:41:17 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 169, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Aug 13 01:41:17 E                       with self.scoped_process_state:
Aug 13 01:41:17 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 196, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Aug 13 01:41:17 E                       self.process_element(input_processor.next())
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/pyflink/fn_execution/table/operations.py"", line 102, in process_element
Aug 13 01:41:17 E                       return self.func(value)
Aug 13 01:41:17 E                     File ""<string>"", line 1, in <lambda>
Aug 13 01:41:17 E                     File ""/__w/3/s/flink-python/pyflink/table/tests/test_dependency.py"", line 184, in add_one
Aug 13 01:41:17 E                       from python_package1 import plus
Aug 13 01:41:17 E                   ModuleNotFoundError: No module named 'python_package1'
Aug 13 01:41:17 E                   
Aug 13 01:41:17 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
Aug 13 01:41:17 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:332)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:315)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:834)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
Aug 13 01:41:17 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
Aug 13 01:41:17 E                   	... 3 more
{code};;;","14/Aug/23 13:25;mapohl;We had the same error appearing in 1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44275&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29859

I noticed this build failure in FLINK-26644 where it was added even though the error cause was different to what was reported in FLINK-26644. I'm gonna update the affected versions.;;;","15/Aug/23 09:08;Sergey Nuyanzin;Thanks for the info
based on that i will downgrade priority to major;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Flink UI's time precision from second level to millisecond level,FLINK-32863,13547203,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hejufang001,Runking,Runking,14/Aug/23 07:42,14/Sep/23 02:34,04/Jun/24 20:40,14/Sep/23 02:34,1.17.1,,,,,1.19.0,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,"This an UI improvement for OLAP jobs.

OLAP queries are generally small queries which will finish at the seconds or milliseconds, but currently the time precision displayed is second level and not enough for OLAP queries. Millisecond part of time is very important for users and developers, to see accurate time, for performance measurement and optimization. The displayed time includes job duration, task duration, task start time, end time and so on.

It would be nice to improve this for better OLAP user experience.",,,,,,,,,,,,,,,,,FLINK-32898,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 14 02:34:12 UTC 2023,,,,,,,,,,"0|z1jri0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 03:04;xiangyu0xf;+1, this is very useful for short-lived queries;;;","13/Sep/23 03:16;hejufang001;Hi [~guoyangze], I have created a PR([https://github.com/apache/flink/pull/23403)] for this issue, could you assign this to me?;;;","14/Sep/23 02:34;guoyangze;master: 2d74260f41275add3c16ea72ee4ecedbec45f9d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support INIT operation type to be compatible with DTS on Alibaba Cloud,FLINK-32862,13547196,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,ruanhang1993,ruanhang1993,ruanhang1993,14/Aug/23 06:47,15/Dec/23 01:58,04/Jun/24 20:40,15/Dec/23 01:58,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,stale-assigned,,The operation type of canal json messages from DTS on Alibaba Cloud may contain a new type `INIT`. We cannot handle these messages.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 14 22:35:19 UTC 2023,,,,,,,,,,"0|z1jrgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Batch mode, sink to multables the task graph of web page monitor jobs , lack sink task graph block",FLINK-32861,13547156,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhengyuan,zhengyuan,14/Aug/23 03:26,14/Aug/23 03:26,04/Jun/24 20:40,,1.16.3,,,,,,,,,Project Website,,,,,,0,,,,"Batch mode, sink to multables (mysql, paimon) the task graph of web page monitor jobs , lack sink task graph block. Expect 2 sink Task graphs . but the results is correct.


flink sql detail see attachements flink-mult-out.sql and screenshot.

flink-mult-out.sql:

======================

SET execution.checkpointing.interval=10000;
SET state.checkpoints.dir=hdfs://hadoop01:9000/flink/checkpoints/20230814103606840;
SET execution.runtime-mode=batch;
CREATE TABLE source_jdbc_9kT_QLyGtM(
`id` BIGINT,
`tenant_code` STRING,
`ces2` STRING,
`ces1` STRING,
`address` STRING,
`amount2` FLOAT,
`bizdate` DATE)  WITH ( 
    'connector'='jdbc',
    'scan.fetch-size'='300000',
    'url'='jdbc:mysql://10.x.x.22:3306/data_storage?autoReconnect=true&useUnicode=true&characterEncoding=utf-8&allowMultiQueries=true&serverTimezone=Asia/Shanghai',
    'username'='xxx',
    'password'='xxxx',
    'table-name'='v_csmx_129_default'
);

 

CREATE VIEW tranform_sql_mapping AS select `id`,`tenant_code`,`ces2`,`ces1`,`address`,`amount2`,`bizdate` from source_jdbc_9kT_QLyGtM where  ( `id`<50000 );


CREATE TABLE data_processing_out_1(
`id` BIGINT,
`tenant_code` STRING,
`ces2` STRING,
`ces1` STRING,
`address` STRING,
`amount2` FLOAT,
`bizdate` DATE)  WITH ( 
    'connector'='jdbc',
    'sink.buffer-flush.max-rows'='50000',
    'sink.buffer-flush.interval'='0',
    'url'='jdbc:mysql://10.x.x.22:3306/data_storage?autoReconnect=true&useUnicode=true&characterEncoding=utf-8&allowMultiQueries=true&serverTimezone=Asia/Shanghai',
    'username'='xxxx',
    'password'='xxxxx',
    'table-name'='data_processing_out_1'
);

INSERT INTO data_processing_out_1 select `id`,`tenant_code`,`ces2`,`ces1`,`address`,`amount2`,`bizdate` from tranform_sql_mapping;

CREATE CATALOG paimon WITH (
    'type' = 'paimon',
    'warehouse' = 'hdfs://hadoop01:9000/painmon/data-processing/paimon_ods'   
);

USE CATALOG paimon;
create database if not exists paimon.paimon_ods_db;
drop table if exists paimon_ods_db.paimon_mysql_test01;
CREATE TABLE if not exists paimon_ods_db.paimon_mysql_test01(
`id` BIGINT,
`tenant_code` STRING,
`ces2` STRING,
`ces1` STRING,
`address` STRING,
`amount2` FLOAT,
`bizdate` DATE
)  WITH (
   'sink.parallelism'='8',
   'bucket'='8',
   'bucket-key'='tenant_code',
   'sink.use-managed-memory-allocator'='true',
   'sink.managed.writer-buffer-memory'='512MB',
   'num-sorted-run.compaction-trigger'='20',
   'write-buffer-size'='1024MB',
   'write-buffer-spillable'='true',
   'write-mode'='append-only'
);

INSERT INTO paimon_ods_db.paimon_mysql_test01 select `id`,`tenant_code`,`ces2`,`ces1`,`address`,`amount2`,`bizdate` from default_catalog.default_database.tranform_sql_mapping;

 

==================================================

results

======================

trino> use paimon.paimon_ods_db;
USE
trino:paimon_ods_db> select count(*) from paimon_mysql_test01;
 _col0 
-------
 49999 
(1 row)

 

mysql> SELECT count(*) FROM data_storage.data_processing_out_1;
+----------+
| count(*) |
+----------+
|    49999 |
+----------+
1 row in set (0.06 sec)

mysql> ","flink 1.16

centos 7 64

mysql 5.7

paimon 0.5

open jdk 1.8 64",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 03:13;zhengyuan;flink-mult-out.sql;https://issues.apache.org/jira/secure/attachment/13062173/flink-mult-out.sql","14/Aug/23 03:18;zhengyuan;screenshot.png;https://issues.apache.org/jira/secure/attachment/13062172/screenshot.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-14 03:26:26.0,,,,,,,,,,"0|z1jr7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing visibility annotation for Table APIs,FLINK-32860,13547147,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,14/Aug/23 01:59,24/Aug/23 12:32,04/Jun/24 20:40,24/Aug/23 12:32,1.19.0,,,,,1.19.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"Based on the [discussion|https://lists.apache.org/thread/zl2rmodsjsdb49tt4hn6wv3gdwo0m31o], all classes in flink-table-api-java, flink-table-api-java-bridge, and flink-table-common should be marked with proper visibility annotations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 12:32:00 UTC 2023,,,,,,,,,,"0|z1jr5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/23 12:32;lincoln.86xy;fixed in master: 84044f45830542d13d6c9baeb2bfe30de3eac4ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the state of adaptive sheduler with StateWithoutExecutionGraph,FLINK-32859,13547138,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,13/Aug/23 16:34,23/Aug/23 13:42,04/Jun/24 20:40,23/Aug/23 13:42,,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"Currently, most of states of adaptive scheduler extends StateWithExecutionGraph, and it can reduce a lots of repeated code.

We can define the State{color:red}Without{color}ExecutionGraph and the corresponding Context as well, then the rest of states can extend the State{color:red}Without{color}ExecutionGraph, such as: Created, WaitingForResources, CreatingExecutionGraph.

This improvement can reduce a lot of code for these states, such as: cancel(), suspend(), getJob(), handleGlobalFailure() and getLogger().

These methods of Created, WaitingForResources and CreatingExecutionGraph are same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:42:15 UTC 2023,,,,,,,,,,"0|z1jr3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 13:42;mapohl;master (1.19): 4aebc43721873480dfbbaf2cfe784c3833f1cdc9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The util package of flink-runtime module,FLINK-32858,13546951,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,fanrui,fanrui,11/Aug/23 09:00,24/Aug/23 14:20,04/Jun/24 20:40,24/Aug/23 14:19,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 14:19:34 UTC 2023,,,,,,,,,,"0|z1jqb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:49;Zhanghao Chen;Hi [~fanrui], I'd be willing to volunteer, could you assign it to me?;;;","11/Aug/23 11:00;fanrui;Hi [~Zhanghao Chen], thanks for the warm, assigned to you. :);;;","13/Aug/23 08:17;Zhanghao Chen;Hi [~fanrui], I've prepared a MR, help review when you are free~ Caution: the MR size is huge.;;;","24/Aug/23 14:19;fanrui;Merged <1.19, master> via 8e5bdd9405b15b21e209518e3c5881984bd16ce9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The webmonitor and zookeeper packages of flink-runtime module,FLINK-32857,13546950,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,11/Aug/23 08:59,23/Aug/23 13:43,04/Jun/24 20:40,23/Aug/23 13:43,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:43:42 UTC 2023,,,,,,,,,,"0|z1jqaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 13:43;mapohl;master (1.19): 1ae8e401b0ad7c185b1d166d6fcfd879a877bf9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The testtasks, testutils, throughput and throwable packages of flink-runtime module",FLINK-32856,13546949,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,11/Aug/23 08:58,23/Aug/23 13:49,04/Jun/24 20:40,23/Aug/23 13:49,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:49:51 UTC 2023,,,,,,,,,,"0|z1jqao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 13:49;fanrui;Merged <master:1.19> aae023f48c8b92cde37e0e7765d821844ace036b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The taskexecutor package of flink-runtime module,FLINK-32855,13546948,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jiabao.sun,fanrui,fanrui,11/Aug/23 08:56,23/Aug/23 13:54,04/Jun/24 20:40,23/Aug/23 13:54,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:54:51 UTC 2023,,,,,,,,,,"0|z1jqag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 09:55;jiabao.sun;Hi [~fanrui], may I help with this task as well?;;;","13/Aug/23 11:32;fanrui;Assigned, thanks for your warm! :);;;","23/Aug/23 13:54;fanrui;Merged <master:1.19> 2aa59c78e72190b139bab5fb497c2571fe7ac71c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The state package of flink-runtime module,FLINK-32854,13546947,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jiabao.sun,fanrui,fanrui,11/Aug/23 08:56,11/Sep/23 15:00,04/Jun/24 20:40,11/Sep/23 15:00,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 15:00:35 UTC 2023,,,,,,,,,,"0|z1jqa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 09:55;jiabao.sun;Hi [~fanrui], may I help with this task as well?;;;","13/Aug/23 11:32;fanrui;Assigned, thanks for your warm! :);;;","11/Sep/23 15:00;fanrui;Merged via <master:1.19> 554810abbbd44a656a446d051b3dd199b0df9055;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The security, taskmanager and source packages of flink-runtime module",FLINK-32853,13546946,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,zhangyy91,fanrui,fanrui,11/Aug/23 08:56,13/Oct/23 04:42,04/Jun/24 20:40,,,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 11:30:25 UTC 2023,,,,,,,,,,"0|z1jqa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 11:25;zhangyy91;[~fanrui] I would like to migrate these tests. Can I take this ticket?;;;","15/Aug/23 11:30;fanrui;[~zhangyy91]  , assigned to you, please go ahead! :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The scheduler package of flink-runtime module,FLINK-32852,13546945,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,fanrui,fanrui,11/Aug/23 08:54,07/May/24 02:01,04/Jun/24 20:40,07/May/24 02:01,,,,,,1.20.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 02:28:30 UTC 2024,,,,,,,,,,"0|z1jq9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 02:28;fanrui;Merged to master(1.19) via a6412b8497d1fdb3c0137a1651767db33836d966

Merged to master(1.20) via ca441b847e85b29de118ff72280eecfbc68deeb7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The rest package of flink-runtime module,FLINK-32851,13546944,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wangm92,fanrui,fanrui,11/Aug/23 08:54,22/Sep/23 05:04,04/Jun/24 20:40,22/Sep/23 05:04,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 22 05:04:37 UTC 2023,,,,,,,,,,"0|z1jq9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","22/Sep/23 05:04;huweihua;master: 4817db916889a7701481fb04333a7f8da9f5b583;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The io package of flink-runtime module,FLINK-32850,13546943,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,jiabao.sun,fanrui,fanrui,11/Aug/23 08:54,22/Dec/23 03:49,04/Jun/24 20:40,22/Dec/23 03:49,1.18.0,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 22 03:49:28 UTC 2023,,,,,,,,,,"0|z1jq9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 09:12;jiabao.sun;Hi [~fanrui], can I help with this task?;;;","11/Aug/23 09:50;fanrui;Thanks a lot for the warm, assigned to you:);;;","23/Oct/23 12:30;jiabao.sun;Since the original PR is too large to review, I'm going to split it into multiple PRs.;;;","26/Oct/23 11:03;fanrui;Merged master (1.19) via:

0ccd95ef48bcd7246f8c88c9aa7b69ffa268c865
1c884ab48372f7a66f86c28aeaf9518000c7f357;;;","18/Dec/23 16:13;jiabao.sun;Merged master (1.19) via:

1c67cccd2fdd6c674a38e0c26fe990e1dd7b62ae

8a8cceda4ced7f71d739e9f5f0bd7dadb094242d;;;","22/Dec/23 03:15;Weijie Guo;master(1.19) via 462708228df253de09e0a002ee0f2bee94e701c5.;;;","22/Dec/23 03:40;jiabao.sun;Thanks [~Weijie Guo], this task is finished.
Could you help close this issue as well?;;;","22/Dec/23 03:49;leonard;All PRs merged, close this ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The resourcemanager package of flink-runtime module,FLINK-32849,13546942,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,fanrui,fanrui,11/Aug/23 08:53,26/Dec/23 14:46,04/Jun/24 20:40,26/Dec/23 14:46,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 26 14:45:53 UTC 2023,,,,,,,,,,"0|z1jq94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/23 14:45;fanrui;Merged to master(1.19) via : 11cdf7e7adacfe64d961a48844841a24b918257a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The persistence, query, registration, rpc and shuffle packages of flink-runtime module",FLINK-32848,13546940,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,fanrui,fanrui,11/Aug/23 08:51,21/Sep/23 00:48,04/Jun/24 20:40,21/Sep/23 00:48,,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 21 00:48:44 UTC 2023,,,,,,,,,,"0|z1jq8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 08:26;Zhanghao Chen;Hi [~fanrui], could you assign it to me as well? I'm done with the flink-runtime/utils tests.;;;","13/Aug/23 11:34;fanrui;Assigned, thanks for your warm! :);;;","26/Aug/23 11:00;Zhanghao Chen;Hi [~fanrui], to simplify the code review, I've created 4 PRs for migrating tests in packages query, registration, rpc and shuffle respectively. The persistence package is omitted as no changes needed there. Take your time to review them, thanks~;;;","05/Sep/23 04:48;Zhanghao Chen;Hi [~fanrui], if you have time, could you help check the PRs? Thanks in advance!;;;","21/Sep/23 00:48;zjureel;Fixed by 2ced46b6ed4ebfb20f9b392cba4e789e16e4da7d...f2cb1d247283344e9194e63931a2948e09f73c93;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The operators package of flink-runtime module,FLINK-32847,13546939,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jiabao.sun,fanrui,fanrui,11/Aug/23 08:46,27/Aug/23 04:23,04/Jun/24 20:40,27/Aug/23 03:55,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 27 03:54:56 UTC 2023,,,,,,,,,,"0|z1jq8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 11:27;jiabao.sun;Hi [~fanrui], may I help with this task?;;;","15/Aug/23 11:29;fanrui;[~jiabao.sun] Assigned to you, please go ahead!;;;","27/Aug/23 03:54;fanrui;Merged <master:1.19> via ac61e83dad8ea7d0fc91ad98315b2987275586dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The metrics, minicluster and net packages of flink-runtime module",FLINK-32846,13546938,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wangm92,fanrui,fanrui,11/Aug/23 08:45,04/Jan/24 06:55,04/Jun/24 20:40,20/Sep/23 09:10,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33414,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 20 09:09:29 UTC 2023,,,,,,,,,,"0|z1jq88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/23 04:38;fanrui;Merged <master:1.19>
 * 42f170b192049bc573efddd3f82169bb327b3fe4;;;","20/Sep/23 09:09;huweihua;Metrics merged in master: 5a8321f6385f9a108773989a4ca176af1f3c72d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The leaderelection, leaderretrieval, mailbox, memory and messages packages of flink-runtime module",FLINK-32845,13546937,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,11/Aug/23 08:44,23/Aug/23 13:51,04/Jun/24 20:40,23/Aug/23 13:51,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:50:53 UTC 2023,,,,,,,,,,"0|z1jq80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 13:50;fanrui;Merged <master:1.19> 8dd4576cb16a0b2e6f896bbf3a7eeec51f109d24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runtime Filter should not be applied if the field is already filtered by DPP,FLINK-32844,13546936,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,11/Aug/23 08:42,16/Aug/23 06:24,04/Jun/24 20:40,16/Aug/23 06:24,1.18.0,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"Currently, the runtime filter and DPP may take effect on the same key. In this case, the runtime filter may be redundant because the data may have been filtered out by the DPP. We should avoid this because redundant runtime filters can have negative effects.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32486,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 06:22:13 UTC 2023,,,,,,,,,,"0|z1jq7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 06:22;wanglijie;Fix via master(1.18): 6b7d6d67808b7fcdd6fc93222c6c7055c4343475;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The jobmaster package of flink-runtime module,FLINK-32843,13546935,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,fanrui,fanrui,11/Aug/23 08:42,06/May/24 01:16,04/Jun/24 20:40,06/May/24 01:16,,,,,,1.20.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 06 01:16:16 UTC 2024,,,,,,,,,,"0|z1jq7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/24 01:16;jiabaosun;Resolved via master: beb0b167bdcf95f27be87a214a69a174fd49d256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The iterative, jobgraph and jobmanager packages of flink-runtime module",FLINK-32842,13546934,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,fanrui,fanrui,fanrui,11/Aug/23 08:41,24/Aug/23 14:57,04/Jun/24 20:40,,,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-11 08:41:30.0,,,,,,,,,,"0|z1jq7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The failure, filecache, hadoop, heartbeat, highavailability and instance packages of flink-runtime module",FLINK-32841,13546932,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,fanrui,fanrui,fanrui,11/Aug/23 08:38,24/Aug/23 14:57,04/Jun/24 20:40,,,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-11 08:38:51.0,,,,,,,,,,"0|z1jq6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The executiongraph package of flink-runtime module,FLINK-32840,13546931,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wangm92,fanrui,fanrui,11/Aug/23 08:37,23/Aug/23 23:19,04/Jun/24 20:40,23/Aug/23 23:19,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 23:19:44 UTC 2023,,,,,,,,,,"0|z1jq6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 09:29;wangm92;[~fanrui] i I am interested in this, can you assign it to me;;;","23/Aug/23 23:19;fanrui;Merged <master:1.19> 4532e3494dd97fdb7c5ad86f63458ba7024279b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The entrypoint, event, execution and externalresource packages of flink-runtime module",FLINK-32839,13546930,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,fanrui,fanrui,fanrui,11/Aug/23 08:36,24/Aug/23 14:57,04/Jun/24 20:40,,,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-11 08:36:22.0,,,,,,,,,,"0|z1jq6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The deployment and dispatcher packages of flink-runtime module,FLINK-32838,13546929,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,fanrui,fanrui,fanrui,11/Aug/23 08:34,24/Aug/23 14:57,04/Jun/24 20:40,,,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-11 08:34:10.0,,,,,,,,,,"0|z1jq68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The client, clusterframework and concurrent packages of flink-runtime module",FLINK-32837,13546928,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,11/Aug/23 08:33,23/Aug/23 13:52,04/Jun/24 20:40,23/Aug/23 13:52,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:52:22 UTC 2023,,,,,,,,,,"0|z1jq60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 13:52;fanrui;Merged <master:1.19> 64c725f4a6ed0073a0559f4b2eadaf909608abf2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] The checkpoint package of flink-runtime module,FLINK-32836,13546927,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,11/Aug/23 08:32,25/Aug/23 03:25,04/Jun/24 20:40,25/Aug/23 03:25,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 03:24:59 UTC 2023,,,,,,,,,,"0|z1jq5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 03:24;fanrui;Merged <master:1.19> af22a07e7810fa120178ffce964016f459e51547;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JUnit5 Migration] The accumulators, blob and blocklist packages of flink-runtime module",FLINK-32835,13546926,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ferenc-csaky,fanrui,fanrui,11/Aug/23 08:30,30/Aug/23 14:48,04/Jun/24 20:40,28/Aug/23 03:48,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32987,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 03:48:00 UTC 2023,,,,,,,,,,"0|z1jq5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 14:03;ferenc-csaky;FYI: Jira contains the ""blocklist"" package, as I just listed A-B in the other jira when we defined the scope for this one, alothugh those tests are already JUnit5, so the PR only includes ""accumultars"" and ""blob"" unit tests.;;;","16/Aug/23 12:24;ferenc-csaky;[~fanrui] if you have time, can you check on the PR? Thanks in advance!;;;","28/Aug/23 03:48;fanrui;Merged <master:1.19> 3ef713271771f9367166bcb31a6c08075f1d9617;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow compile.sh to be used manually,FLINK-32834,13546925,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Aug/23 07:42,17/Aug/23 17:08,04/Jun/24 20:40,17/Aug/23 17:08,,,,,,1.18.0,,,,Build System / CI,,,,,,0,pull-request-available,,,For debugging purposes it would be nice if you could run compile.sh locally.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 17:08:09 UTC 2023,,,,,,,,,,"0|z1jq5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 17:08;chesnay;master: 
5d163dd39d6179c3618dcada86d42b2b332569f3..5bf5003f5c7baf19b0164f78558e495d8bb62b04;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rocksdb CacheIndexAndFilterBlocks must be true when using shared memory,FLINK-32833,13546924,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mayuehappy,mayuehappy,11/Aug/23 07:35,14/Aug/23 06:03,04/Jun/24 20:40,,1.17.1,,,,,,,,,Runtime / State Backends,,,,,,0,,,,"Currently in RocksDBResourceContainer#getColumnOptions, if sharedResources is used, blockBasedTableConfig will add the following configuration by default.
{code:java}
blockBasedTableConfig.setBlockCache(blockCache);
blockBasedTableConfig.setCacheIndexAndFilterBlocks(true);
blockBasedTableConfig.setCacheIndexAndFilterBlocksWithHighPriority(true);
blockBasedTableConfig.setPinL0FilterAndIndexBlocksInCache(true);{code}
In my understanding, these configurations can help flink better manage the memory of rocksdb and save some memory overhead in some scenarios. But this may not be the best practice, mainly for the following reasons:
1. After CacheIndexAndFilterBlocks is set to true, it may cause index and filter miss when reading, resulting in performance degradation.
2. These parameters may not be bound together with whether shared memory is used, or some configurations should be supported separately to decide whether to enable these features",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-7289,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 06:03:28 UTC 2023,,,,,,,,,,"0|z1jq54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:33;yunta;Hi [~mayuehappy], {{setCacheIndexAndFilterBlocks}}  is necessary as we want to limit memory usage. We would cost three parts (index & filter blocks, memtables, data blocks) in block cache to limit the whole memory usage. Setting this option is for stability instead of performance.
And the other two options {{setCacheIndexAndFilterBlocksWithHighPriority}} and {{setPinL0FilterAndIndexBlocksInCache}} are introduced to mitigate the performance regression when we cache the filter and index blocks.;;;","13/Aug/23 03:45;mayuehappy;hi [~yunta]  thank you for your replying.
I understand the original purpose of these codes. These configurations can help we limit memory usage better. 

What I want to say is that the user may need to decide to configure these parameters instead of hardcoding them in the flink code. In our previous tests, we found that indexAndFiltlers has a great impact on performance, especially in hdd environment. In the current flink version, if the user uses shared memory at the same time and wants to ensure high performance, it may also need to set an appropriate WRITE_BUFFER_RATIO or HIGH_PRIORITY_POOL_RATIO , which may be difficult for the user mode. In other words, if the user only wants to put the datablock in the cache, and wants the meta information of indexAndFilter to be resident in memory, it also sounds reasonable. What do you think ? ;;;","14/Aug/23 06:03;yunta;FLINK-7289 is the original ticket to introduce these options. The purpose is to limit the memory usage as the previous Flink's implementation would easily cause OOMKilled when using many RocksDB instances within the same slots in the k8s environment. If users could only put the data block in the cache and let the index & filter block residents in the memory, the flink process would easily run out of limited memory (especially considering we would not limit the opened SST files), and we cannot say the memory for rocksdb is *managed* anymore.
If you have other solutions to manage the RocksDB memory usage without caching the index and filter blocks, we should discuss those candidates.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row对象不兼容scala int,FLINK-32832,13546914,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,luca.yang,luca.yang,11/Aug/23 06:31,18/Aug/23 01:48,04/Jun/24 20:40,,1.17.1,,,,,,,,,Table SQL / API,,,,,,0,,,,"val table = tEnv.fromValues(
DataTypes.ROW(
DataTypes.FIELD(""id"", DataTypes.INT().notNull())
, DataTypes.FIELD(""name2"", DataTypes.INT().notNull())
),
row(1, Integer.valueOf(11)) // 离谱的是 第一个int可以使scala int,后面的int就必须是java.lang.Integer
)

table.execute().print(){*}{*}",flink1.17 macos m2芯片,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 01:48:05 UTC 2023,,,,,,,,,,"0|z1jq2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 06:33;luca.yang;不兼容 我使用 java Integer就可以,这个我理解. 但是为什么第一个int字段就不需要Interger.

而且 flink int对应 sql int not null, flink integer 对应sql中 int.   这块代码看,没有体现出来.;;;","16/Aug/23 06:27;lsy;Hi, [~luca.yang] Please use the English here.;;;","18/Aug/23 01:48;luca.yang;The paradox is that the first int can make scala int, and the subsequent int must be java.lang.Integer;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeFilterProgram should aware join type when looking for the build side,FLINK-32831,13546910,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,11/Aug/23 03:55,28/Aug/23 01:47,04/Jun/24 20:40,28/Aug/23 01:47,1.18.0,,,,,1.18.0,1.19.0,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"Currently, runtime filter program will try to look for an {{Exchange}} as build side to avoid affecting {{MultiInput}}. It will try to push down the runtime filter builder if the original build side is not {{Exchange}}.

Currenlty, the builder-push-down does not aware the join type, which may lead to incorrect results(For example, push down the builder to the right input of left-join).

We should only support following cases:

1. Inner join: builder can push to left + right input
2. semi join: builder can push to left + right input
3. left join: builder can only push to the left input
4. right join: builder can only push to the right input",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 01:46:52 UTC 2023,,,,,,,,,,"0|z1jq20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 01:46;wanglijie;Fixed via
master:
43bcf319ff6104e1af829e34ae8a430c6417622d
f518f8a5bdfcde4912961f60075e530399160f43
07888af59bbc2d24afa049e8a6aedcd9eb822986
a68dd419718b4304343c2b27dab94394c88c67b5

release-1.18:
ac3f3cf4802d0271349636322dbd16772e86453b
e48a02628349cdfdecf7a1aedd2a576fa2a7caf3
6938b928e0104f6b222a4281fbdc7bde20260f3b
fa2ab5e3bfe401d0c599c8ffd224aafb69d6d503;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Histogram function,FLINK-32830,13546891,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hanyuzheng,hanyuzheng,10/Aug/23 22:57,10/Aug/23 23:00,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"This is an implementation of HISTOGRAM

Returns a map containing the distinct values of col1 mapped to the number of times each one occurs for the given window. This version limits the number of distinct values which can be counted to 1000, beyond which any additional entries are ignored.
h3. Brief change log

HISTOGRAM for Table API and SQL

Syntax:
{code:java}
HISTOGRAM(col1){code}


Arguments:
col1: the data in col1

Examples:


{code:java}
Flink SQL> create temporary table orders (
> orderId INT,
> price DECIMAL(10,3)
> )with(
> 'connector' = 'datagen',
> 'rows-per-second' = '5',
> 'fields.orderId.min' = '1',
> 'fields.orderId.max' = '20',
> 'fields.price.min' = '1',
> 'fields.price.max' = '200'
> );

Flink SQL> select histogram(price) as map from orders;
res: {147.451 = 1， 65.765 = 1， 41.662 = 1 …}
{code}
 

see also:
KsqlDB: [https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/aggregate-functions/#histogram]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-10 22:57:38.0,,,,,,,,,,"0|z1jpxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow setting different values for CPU requests and limits,FLINK-32829,13546861,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ivan.stoiev,ivan.stoiev,10/Aug/23 18:19,14/Aug/23 14:21,04/Jun/24 20:40,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"In order to optimize kubernetes nodes CPU usage before operator, we did usually set higher CPU limit than request at kubernetes level.

This configuration allowed a faster rollout/restart of job/task managers because PODs can use unused CPU from node until they start processing.

In operator, the only possible way of setting CPU request/limit is under job/taskmanager.resource.cpu, this sets kubernetes pod.spec.resource.requests.cpu equals to pod.spec.resource.limits.cpu, and there is no way to configure those settings differently.

Here is a ilustration of CPU usage of PODs (managers) as percent of CPU limits under a rollout, using limits ≠ requests:

!image-2023-08-10-15-18-22-705.png!

 

Is this by design, or a possible missing feature?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 18:18;ivan.stoiev;image-2023-08-10-15-18-22-705.png;https://issues.apache.org/jira/secure/attachment/13062041/image-2023-08-10-15-18-22-705.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 14:21:59 UTC 2023,,,,,,,,,,"0|z1jpr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 14:21;haiqingchen;Hi [~ivan.stoiev], you can use  
|h5. kubernetes.jobmanager.cpu.limit-factor |

in flink configuration to define factor between requests and limit, default value is 1. But I found out the configuration for jobmanager works while it doesn't work for taskmanager.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition aware watermark not handled correctly shortly after job start up from checkpoint or savepoint,FLINK-32828,13546843,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,gliter,gliter,10/Aug/23 14:37,21/May/24 07:54,04/Jun/24 20:40,17/May/24 08:08,1.17.1,1.18.1,1.19.0,,,1.18.2,1.19.1,1.20.0,,API / DataStream,Connectors / Kafka,,,,,5,pull-request-available,,,"When using KafkaSource with partition aware watermarks. Watermarks are being emitted even when only one partition has some events just after job startup from savepoint/checkpoint. After it has some events on other partitions the watermark behaviour is correct and watermark is emited as a minimum watarmark from all partition.

 

Steps to reproduce:
 # Setup a Kafka cluster with a topic that has 2 or more partitions. (see attached docker-compose.yml)

 # 
 ## {{./kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test-2 --partitions 4}}
 # Create a job that (see attached `test-job.java`):
 ## uses a KafkaSource with `WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10L)`
 ## has parallelism lower than number of partitions
 ## stores checkpoint/savepoint
 # Start job
 # Send events only on single partition
 ## {{./kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test-2 --property ""parse.key=true"" --property ""key.separator=:""}}

 

{{14:51:19,883 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:51:18.849Z, watermark -292275055-05-16T16:47:04.192Z}}
{{14:51:32,484 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:51:31.475Z, watermark -292275055-05-16T16:47:04.192Z}}
{{14:51:35,914 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:51:34.909Z, watermark -292275055-05-16T16:47:04.192Z}}

Expected: Watermark does not progress. Actual: Watermark does not progress.

5. Stop the job

6. Startup job from last checkpoint/savepoint

7. Send events only on single partitions

{{14:53:41,693 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:53:40.662Z, watermark -292275055-05-16T16:47:04.192Z}}
{{14:53:46,088 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:53:45.078Z, watermark 2023-08-10T12:53:30.661Z}}
{{14:53:49,520 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:53:48.511Z, watermark 2023-08-10T12:53:35.077Z}}

Expected: Watermark does not progress. {color:#ff0000}*Actual: Watermark has progress*{color}

 

{color:#172b4d}To add bit more of context:{color}

{color:#172b4d}8. Send events on other partitions and then send events only on single partitions{color}

{{{color:#172b4d}14:54:55,112 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/0: 2 -> a, timestamp 2023-08-10T12:54:54.104Z, watermark 2023-08-10T12:53:38.510Z
14:54:57,673 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/1: 4 -> a, timestamp 2023-08-10T12:54:56.665Z, watermark 2023-08-10T12:53:38.510Z
14:54:57,673 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/2: 5 -> a, timestamp 2023-08-10T12:54:57.554Z, watermark 2023-08-10T12:53:38.510Z
14:55:12,821 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:55:11.814Z, watermark 2023-08-10T12:54:44.103Z
14:55:16,099 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:55:15.091Z, watermark 2023-08-10T12:54:44.103Z
14:55:19,122 WARN  com.example.TestJob6$InputSink2                   [] - == Received: test-2/3: 1 -> a, timestamp 2023-08-10T12:55:18.114Z, watermark 2023-08-10T12:54:44.103Z{color}}}

{color:#172b4d}Expected: Watermark should progress a bit and then should not progress when receiving events only on single partition. {color}

{color:#172b4d}Actual: As expected{color}

 

 

{color:#172b4d}This behavior also shows as a burst of late events just after startup and then no more late events when job operates normally. {color}","Affected environments:
 * Local MiniCluster + Confluent Kafka run in docker
 ** See attached files
 * Flink job run in Kubernetes using Flink Operator 1.15 + 3 node Kafka cluster run in Kubernetes cluster",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 14:33;gliter;docker-compose.yml;https://issues.apache.org/jira/secure/attachment/13062033/docker-compose.yml","10/Aug/23 14:26;gliter;test-job.java;https://issues.apache.org/jira/secure/attachment/13062034/test-job.java",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 08:08:46 UTC 2024,,,,,,,,,,"0|z1jpn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 18:43;mason6345;From the logs, it looks like you have a key by and watermark is progressing because that one active partition is moving data to all other operators. I would start by setting and tuning the  idleness ([https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/#dealing-with-idle-sources)|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/#dealing-with-idle-sources] to account for the idle partitions;;;","10/Aug/23 18:59;gliter;[~mason6345] its not about idle partitions, its about a quite opposite situation. During normal operation all partitions are active and more or less aligned with timestamp. Now imagine a situation during startup when one thread starts to consume some partitions faster than others. If it will progress through events from that single partition fast enough to consume events with timestamps that are bigger than some events in other partitions + watermark time, it will start dropping those events as late events. 

To mitigate this situation Kafka partition aware watermarks were implemented. There is a clear bug here where in case of running fresh job or after having job settled and having traffic on all partition works correctly. That means the watermark emitted by source is a minimal watermark of all partitions. But in in short window just after startup from checkpoint/savepoint watermark incorrectly progresses just based on traffic from single partitions, where it should wait until traffic on all partitions to take a minimal watermark of all partitions.

Please not that the logs are from the minimal example I have created (attached to the tickets) and events are send by hand.

In production restarting job causes around 1 - 10% of events be dropped at the very start of the job. Even thou in scope of single partition the out of ordness is well below allowed one. ;;;","15/May/24 15:57;pnowojski;We have just stumbled upon the same issue and I can confirm that it's quite critical bug that can leads to all kinds of incorrect results. 

The problem is that the initial splits recovered from state are initial are registered in the {{WatermarkOutputMultiplexer}} only when first record from that split has been emitted. Resulting watermark was not properly combined from the initial splits, but only from the splits that have already emitted at least one record.

I will publish a fix for that shortly.

edit: Also the problem doesn't affect only Kafka, but all FLIP-27 sources (anything that uses {{SourceOperator}}.;;;","17/May/24 08:08;pnowojski;Merged to master as 7fd6c6da26c
Merged to release-1.19 as a160f8db1f7
Merged to release-1.18 as 4848a50d735;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator fusion codegen may not take effect when enable runtime filter,FLINK-32827,13546804,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,wanglijie,wanglijie,10/Aug/23 09:28,24/Aug/23 02:40,04/Jun/24 20:40,24/Aug/23 02:40,1.18.0,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"Currently, the RuntimeFilterOperator does not support operator fusion codegen(OFCG), which means the Runtime Filter and OFCG can not take affect together, we should fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 02:40:13 UTC 2023,,,,,,,,,,"0|z1jpeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 09:29;wanglijie;cc [~lsy];;;","24/Aug/23 02:40;wanglijie;Fixed via
master: 679436390db2ac1b54584dbafb6e1091f2f16ada
release-1.18: 9cd04bd004fbf2d0fa4266cf07909c0bbcc94813;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Our job is stuck on requestMemorySegmentBlocking,FLINK-32826,13546787,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,cndpzc,cndpzc,10/Aug/23 08:21,19/Apr/24 09:33,04/Jun/24 20:40,,1.16.0,1.16.2,,,,,,,,Runtime / Network,,,,,,0,,,,"We have a Flink job and find it often gets stuck on requesting memory segment.

 
{code:java}
""shardConsumers-Source: xxx[1] -> Calc[2] (1/1)#0-thread-0"" Id=107 WAITING on java.util.concurrent.CompletableFuture$Signaller@62e01595
    at java.base@11.0.20/jdk.internal.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.CompletableFuture$Signaller@62e01595
    at java.base@11.0.20/java.util.concurrent.locks.LockSupport.park(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.CompletableFuture$Signaller.block(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.ForkJoinPool.managedBlock(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.CompletableFuture.waitingGet(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.CompletableFuture.get(Unknown Source)
    at app//org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:383)
    at app//org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:355)
    at app//org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:414)
    at app//org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:390)
    at app//org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForRecordContinuation(BufferWritingResultPartition.java:328)
    at app//org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:161)
    at app//org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
    at app//org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54)
    at app//org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:105)
    at app//org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:91)
    at app//org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45)
    at app//org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at app//org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at StreamExecCalc$23.processElement_split2(Unknown Source)
    at StreamExecCalc$23.processElement(Unknown Source)
    at app//org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at app//org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at app//org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at app//org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at app//org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at app//org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:423)
    at app//org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:528)
    at app//org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collectWithTimestamp(StreamSourceContexts.java:108)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:1028)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.access$000(KinesisDataFetcher.java:113)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$AsyncKinesisRecordEmitter.emit(KinesisDataFetcher.java:315)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:332)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:329)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:1012)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.deserializeRecordForCollectionAndUpdateState(ShardConsumer.java:219)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.lambda$run$0(ShardConsumer.java:126)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer$$Lambda$1780/0x0000000840f89440.accept(Unknown Source)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.lambda$run$0(FanOutRecordPublisher.java:120)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher$$Lambda$1781/0x0000000840f89840.accept(Unknown Source)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.consumeAllRecordsFromKinesisShard(FanOutShardSubscriber.java:360)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:189)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:169)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:124)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
    at java.base@11.0.20/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.FutureTask.run(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base@11.0.20/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base@11.0.20/java.lang.Thread.run(Unknown Source) {code}
 

We also noticed high backpressure, but we couldn't find the reason. The downstream writer thread was waiting for a message from mailbox.
{code:java}
""xxx[3]: Writer (1/1)#0"" Id=91 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2072929    at java.base@11.0.20/jdk.internal.misc.Unsafe.park(Native Method)    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2072929    at java.base@11.0.20/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)    at java.base@11.0.20/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)    at app//org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)    at app//org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:363)    at app//org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)    at app//org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)    at app//org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:836) {code}
Some observations:
 * The job logic is pretty simple: it consumes AWS Kinesis, does some filtering and writes results to another Kinesis.
 * The job gets stuck after running for 3-4 days.
 ** If we restart from last checkpoint, the job will get stuck again soon.
 ** If we restart without checkpoint, the job will recover, and may be stuck in a few days again.
 * We have several jobs consuming different Kinesis, but only this one has problem. This Kinesis has only one shard, and data volume is small.
 * At first we were using 1.16.0, after found some issues like FLINK-29298, FLINK-31293 related to LocalBufferPool, we upgraded to the latest 1.16.2, but the issue was not solved.

The heap dump of LocalBufferPool:

 
{code:java}
@LocalBufferPool[
        LOG=@Log4jLogger[
            FQCN=@String[org.apache.logging.slf4j.Log4jLogger],
            serialVersionUID=@Long[7869000638091304316],
            EVENT_MARKER=@Log4jMarker[org.apache.logging.slf4j.Log4jMarker@3f47a99],
            CONVERTER=null,
            eventLogger=@Boolean[false],
            logger=@Logger[org.apache.flink.runtime.io.network.buffer.LocalBufferPool:INFO in 4783da3f],
            name=@String[org.apache.flink.runtime.io.network.buffer.LocalBufferPool],
        ],
        UNKNOWN_CHANNEL=@Integer[-1],
        networkBufferPool=@NetworkBufferPool[
            UNBOUNDED_POOL_SIZE=@Integer[2147483647],
            USAGE_WARNING_THRESHOLD=@Integer[100],
            LOG=@Log4jLogger[org.apache.logging.slf4j.Log4jLogger@16f3a390],
            totalNumberOfMemorySegments=@Integer[5079],
            memorySegmentSize=@Integer[32768],
            availableMemorySegments=@ArrayDeque[isEmpty=false;size=5068],
            isDestroyed=@Boolean[false],
            factoryLock=@Object[java.lang.Object@5151b0a4],
            allBufferPools=@HashSet[isEmpty=false;size=2],
            resizableBufferPools=@HashSet[isEmpty=false;size=2],
            numTotalRequiredBuffers=@Integer[3],
            requestSegmentsTimeout=@Duration[PT30S],
            availabilityHelper=@AvailabilityHelper[AVAILABLE],
            lastCheckedUsage=@Integer[0],
            $assertionsDisabled=@Boolean[true],
        ],
        numberOfRequiredMemorySegments=@Integer[2],
        availableMemorySegments=@ArrayDeque[isEmpty=true;size=0],
        registeredListeners=@ArrayDeque[isEmpty=true;size=0],
        maxNumberOfMemorySegments=@Integer[10],
        currentPoolSize=@Integer[10],
        numberOfRequestedMemorySegments=@Integer[10],
        maxBuffersPerChannel=@Integer[10],
        subpartitionBuffersCount=@int[][
            @Integer[10],
        ],
        subpartitionBufferRecyclers=@BufferRecycler[][
            @SubpartitionBufferRecycler[org.apache.flink.runtime.io.network.buffer.LocalBufferPool$SubpartitionBufferRecycler@41b3ff09],
        ],
        unavailableSubpartitionsCount=@Integer[1],
        maxOverdraftBuffersPerGate=@Integer[0],
        isDestroyed=@Boolean[false],
        availabilityHelper=@AvailabilityHelper[
            availableFuture=@CompletableFuture[java.util.concurrent.CompletableFuture@4570da85[Not completed, 1 dependents]],
        ],
        requestingNotificationOfGlobalPoolAvailable=@Boolean[false],
        $assertionsDisabled=@Boolean[true],
    ] {code}
Could you help give some clues on how to troubleshoot such problem? Or if you need more information, please let me know, thank you!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 09:33:58 UTC 2024,,,,,,,,,,"0|z1jpao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/24 03:21;liusz;[~cndpzc]   do u resolve this problem ?  ;;;","19/Apr/24 09:33;nuafonso;Could this be related to https://issues.apache.org/jira/browse/FLINK-35073?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortingDataInput may introduce unnecessary timestamps for record,FLINK-32825,13546765,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,10/Aug/23 06:01,10/Aug/23 07:48,04/Jun/24 20:40,,,,,,,,,,,Runtime / Task,,,,,,0,,,,"SortingDataInput will always attach timestamp for record during deserialization, even though the sorted data without timestamps before.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 07:47;Weijie Guo;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13062023/screenshot-1.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 07:48:32 UTC 2023,,,,,,,,,,"0|z1jp5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 07:26;Weijie Guo;This bug has been existed for a long time and will not affect the correctness of job (the incorrect timestamp is Long.MIN_VALUE, which will not affect time-dependent operator). Considering that 1.18 is already code freeze, I tend to fix this in 1.19.;;;","10/Aug/23 07:48;Weijie Guo;This will also cause the numBytesIn and numBytesOut of the identity map to be unequal, and each record will differ by 8B (timestamp type is long).

 !screenshot-1.png! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port Calcite's fix for the sql like operator,FLINK-32824,13546760,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,lincoln.86xy,lincoln.86xy,10/Aug/23 04:33,29/Aug/23 03:53,04/Jun/24 20:40,29/Aug/23 03:53,1.17.1,1.18.0,,,,1.18.0,1.19.0,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,"we should port the bugfix of sql like operator https://issues.apache.org/jira/browse/CALCITE-1898
{code}
The LIKE operator must match '.' (period) literally, not treat it as a wild-card. Currently it treats it the same as '_'.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 03:53:23 UTC 2023,,,,,,,,,,"0|z1jp4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 04:33;xu_shuai_;Hi, lincoln. I‘d like to fix this issue, could you assign it to me?;;;","10/Aug/23 05:20;lincoln.86xy;[~xu_shuai_] assigned to you.;;;","25/Aug/23 12:22;lincoln.86xy;fixed in master: c1fba732738aaaa0d576f80881eb464a954e12c5;;;","29/Aug/23 03:53;lincoln.86xy;fixed in release-1.18: e9566267639a33adfd6ced6df0c44d19f435366d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the adaptive scheduler doc about batch job limitation,FLINK-32823,13546745,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,10/Aug/23 02:38,10/Aug/23 08:37,04/Jun/24 20:40,10/Aug/23 08:37,1.18.0,,,,,1.18.0,,,,Documentation,Runtime / Coordination,,,,,0,pull-request-available,,,"FLINK-30846 [1] updated the fall back strategy from default Scheduler to AdaptiveBatch Scheduler when batch job enable `jobmanager.scheduler: adaptive`.

However, the doc[2] isn't updated.

 !screenshot-1.png! 



[1] https://github.com/apache/flink/pull/21814/files
[2] https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/#limitations-1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 02:38;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13062016/screenshot-1.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 08:37:00 UTC 2023,,,,,,,,,,"0|z1jp1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 08:37;fanrui;Merged via <master:1.18> 545d9c69d6a6bb94c624c772594bec9c9709b63a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add connector option to control whether to enable auto-commit of offsets when checkpoints is enabled,FLINK-32822,13546703,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Zhanghao Chen,Zhanghao Chen,09/Aug/23 17:16,16/Oct/23 12:38,04/Jun/24 20:40,16/Oct/23 12:38,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,"When checkpointing is enabled, Flink Kafka connector commits the current consuming offset when checkpoints are *completed* although Kafka source does *NOT* rely on committed offsets for fault tolerance. When the checkpoint interval is long, the lag curve will behave in a zig-zag way: the lag will keep increasing, and suddenly drops on a complete checkpoint. It have led to some confusion for users as in [https://stackoverflow.com/questions/76419633/flink-kafka-source-commit-offset-to-error-offset-suddenly-increase-or-decrease] and may also affect external monitoring for setting up alarms (you'll have to set up with a high threshold due to the non-realtime commit of offsets) and autoscaling (the algorithm would need to pay extra effort to distinguish whether the backlog is actually growing or just because the checkpoint is not completed yet).

Therefore, I think it is worthwhile to add an option to enable auto-commit of offsets when checkpoints is enabled. For DataStream API, it will be adding a configuration method. For Table API, it will be adding a new connector option which wires to the DataStream API configuration underneath.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 16 12:38:09 UTC 2023,,,,,,,,,,"0|z1jos0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 18:22;mason6345;This auto-commit is disabled for conscious reasons as it would violate consistency guarantees of a checkpoint and I'm not sure we should introduce this feature. If you are monitoring lag from server side, is it possible for you the monitor lag from the client side? Flink has a good set of metric reporters that integrate with external systems?

Also, this does sound like more of issue with the checkpoint. Is there a reason why the interval is long?

 ;;;","11/Aug/23 10:06;Zhanghao Chen;Hi [~mason6345] , the pain point of monitoring lag from the client side is that the metrics will stop being reported once the job failed, so we find it more convenient to use server-side lag metrics for monitoring and alarms in particular. I totally agree with you that auto-commit violates consistency guarantees of a checkpoint, but after all, as you said, it's just a conscious thing. Therefore, I think it valuable to add an option for it and we can disable it by default for conscious reasons.;;;","16/Oct/23 12:38;martijnvisser;I'm +1 with [~mason6345] on this, this is by design and we shouldn't want to change this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming examples failed to execute due to error in packaging,FLINK-32821,13546694,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,09/Aug/23 16:28,16/Jan/24 05:46,04/Jun/24 20:40,31/Aug/23 10:42,1.18.0,,,,,1.18.0,,,,Examples,,,,,,0,pull-request-available,,,"6 out of the 8 streaming examples failed to run:
 * Iteration & SessionWindowing & SocketWindowWordCount & WindowJoin failed to run due to java.lang.NoClassDefFoundError: org/apache/flink/streaming/examples/utils/ParameterTool
 * MatrixVectorMul & TopSpeedWindowing failed to run due to: Caused by: java.lang.ClassNotFoundException: org.apache.flink.connector.datagen.source.GeneratorFunction

The NoClassDefFoundError with ParameterTool is introduced by FLINK-32558 Properly deprecate DataSet API - ASF JIRA (apache.org), and we'd better resolve FLINK-32820 ParameterTool is mistakenly marked as deprecated - ASF JIRA (apache.org) first before we come to a fix for this problem.",,,,,,,,,,,FLINK-32820,,,,,,,,,,,,,,FLINK-32702,,,FLINK-32558,,,FLINK-34052,,,,,FLINK-34103,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 05:44:24 UTC 2024,,,,,,,,,,"0|z1joq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 20:36;afedulov;[~Zhanghao Chen] I cannot reproduce the listed issues. How do you execute the examples? Do you face the issue also when running in IDE?;;;","11/Aug/23 00:42;Zhanghao Chen;Hi [~afedulov] , I execute them using the same procedure in [https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/standalone/overview/#starting-a-standalone-cluster-session-mode] using the latest master branch.;;;","11/Aug/23 10:18;huweihua;Thanks for reporting this. The problem of missing of GeneratorFunction is tracked by https://issues.apache.org/jira/browse/FLINK-32702;;;","11/Aug/23 10:38;Zhanghao Chen;Hi [~huweihua], thanks for notifying me of that, I missed it before. How about tracking the issues together here? ;;;","11/Aug/23 11:35;huweihua;Fair enough, We cannot verify these examples unless these two issues are addressed.;;;","17/Aug/23 12:46;afedulov;[~Zhanghao Chen] I am still not sure why you faced this issue. Bundling datagen was part of the original PR: [https://github.com/apache/flink/pull/23079/files#diff-c7da2a9d0258717dbd97328195d580aedc3fe51240aeb4fc0f292d946dcdb4af]

Maybe there was some unlucky timing because Leonard might have  merged the commits separately and maybe you pulled right before this one got integrated [https://github.com/apache/flink/commit/dfb9cb851dc1f0908ea6c3ce1230dd8ca2b48733]
Anyhow, I believe we can close this issue.;;;","17/Aug/23 13:14;huweihua;Hi, [~afedulov] Thanks for you attention. 
the change in ""[https://github.com/apache/flink/pull/23079/files#diff-c7da2a9d0258717dbd97328195d580aedc3fe51240aeb4fc0f292d946dcdb4af]"" only bundle datagen connector to flink-example-streaming.jar, but it is not the final jar we used. You can check the final jar in target/TopSpeedWindowing.jar, there is no datagen related classes in it.

to reproduce this problem, we need a standalone cluster, and use './bin/flink run examples/streaming/TopSpeedWindowing.jar' to run this example

This issue could not reproduce by IDE, because IDE will add the dependency to classpathes automaticlly;;;","21/Aug/23 13:24;afedulov;[~huweihua] Oh, I see, good catch!

Ideally we should have some tests to get this covered.;;;","26/Aug/23 11:02;Zhanghao Chen;Hi [~afedulov], there's already a test on StateMachine example, I'll add tests for the rest.;;;","30/Aug/23 02:45;huweihua;master: ad6aa56004deb70ef65fded2d2972dfc936a77eb, 79f7bdbe6be973623e418a771588533bdef321b6, 3a44c1fbff1acf429f010d3f0fb06a6457727e16

release-1.18: eccae4bb563262f4b90c383fe2b3d92d4e4f3af4, e21e3629bccaf90081af17ff6a7ccc9b8557a8d6 534279ee388077513d4d7f3396e79fa094da3841, ;;;","10/Jan/24 09:35;qinjunjerry;Just tried out with docker-compose, the AsyncIO example does not run:
{code:java}
root@73186f600374:/opt/flink# bin/flink run /volume/flink-examples-streaming-1.18.0-AsyncIO.jar
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
java.lang.NoClassDefFoundError: org/apache/flink/connector/datagen/source/DataGeneratorSource
	at org.apache.flink.streaming.examples.async.AsyncIOExample.main(AsyncIOExample.java:82)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.connector.datagen.source.DataGeneratorSource
	at java.base/java.net.URLClassLoader.findClass(Unknown Source)
	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67)
	at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:65)
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51)
	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
	... 15 more
{code}
 

 ;;;","11/Jan/24 04:43;Zhanghao Chen;[~qinjunjerry] Thanks for reporting this. I'll take a look;;;","11/Jan/24 21:45;qinjunjerry;Thanks [~Zhanghao Chen] !;;;","16/Jan/24 05:44;Zhanghao Chen;FYI [~qinjunjerry] , I've created [FLINK-34103] AsyncIO example failed to run as DataGen Connector is not bundled - ASF JIRA (apache.org) to track this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ParameterTool is mistakenly marked as deprecated,FLINK-32820,13546693,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,09/Aug/23 16:19,13/Aug/23 03:25,04/Jun/24 20:40,13/Aug/23 03:25,1.18.0,,,,,1.18.0,,,,API / DataSet,API / DataStream,,,,,0,pull-request-available,,,"ParameterTool and AbstractParameterTool in pacakge flink-java is mistakenly marked as deprecated in [FLINK-32558] Properly deprecate DataSet API - ASF JIRA (apache.org). They are widely used for handling application parameters and is also listed in the Flink user doc: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/application_parameters/.|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/application_parameters/] Also, they are not directly related to Dataset API.",,,,,,,,,,,,,,FLINK-32821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 03:25:20 UTC 2023,,,,,,,,,,"0|z1jops:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 16:33;Zhanghao Chen;This is also related to the fix of [FLINK-32821] Streaming examples failed to execute due to error in packaging - ASF JIRA (apache.org). [~xtsong] if you think it valid, could you assign the two issues to me? I'd be willing to fix the two issues.;;;","10/Aug/23 01:38;xtsong;[~Zhanghao Chen], thanks for reporting this. I wasn't aware that ParameterTool is mentioned in documentation.

ParameterTool is currently in the flink-java module, which is likely removed together with DataSet API in future. That's why we tried to deprecate it. And because many DataStream examples and end-to-ends tests are still depending on it, we introduced the other ParameterTool in flink-exmaples-streaming, in a different package to avoid conflicts.

If should not be removed, it needs to be moved to another module (maybe flink-streaming-java / flink-core), probably in the same package so users need not to re-import it as long as the module is included as dependency. However, moving it to another module may introduce breaking changes to DataSet users, thus should only happen in the major version bump.

So I guess for now reverting the ParameterTool related changes from FLINK-32558 should be good enough.;;;","10/Aug/23 01:39;xtsong;And thanks for volunteering. You are assigned. Please go ahead.;;;","10/Aug/23 11:04;Zhanghao Chen;Thanks for the comment, I'll prepare a PR that reverts the ParameterTool related changes from FLINK-32558 this week.;;;","13/Aug/23 03:25;xtsong;master (1.18): fc2b5d8f53a41695117f6eaf4c798cc183cf1e36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink can not parse the param `#` correctly in k8s application mode,FLINK-32819,13546683,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhangjun,zhangjun,09/Aug/23 13:52,10/Aug/23 02:41,04/Jun/24 20:40,10/Aug/23 02:41,1.17.1,,,,,,,,,Runtime / Configuration,,,,,,0,,,,"when I submit a flink job in k8s application mode, and has a param contains `#` ,for example mysql password , the flink can not parse the param correctly.  the content after the `#` will lost.
{code:java}

/mnt/flink/flink-1.17.0/bin/flink run-application \
-Dexecution.target=kubernetes-application \
-Dkubernetes.container.image=xxxxx \
local:///opt/flink/usrlib/my.jar  \
--mysql-conf hostname=localhost \
--mysql-conf username=root \
--mysql-conf password=%&^GGJI#$jh665$ﬁ^% \
--mysql-conf port=3306 

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 02:37:31 UTC 2023,,,,,,,,,,"0|z1jonk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 02:37;JunRuiLi;This issue duplicates FLINK-27299 and also should be subsumed by FLINK-23620.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support region failover for adaptive scheduler,FLINK-32818,13546666,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,09/Aug/23 11:12,12/Sep/23 17:04,04/Jun/24 20:40,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,"The region failover strategy is useful for fast failover, and reduce the impact for business side. However, the adaptive scheduler doesn't support it so far.


https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#restart-pipelined-region-failover-strategy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 17:04:38 UTC 2023,,,,,,,,,,"0|z1jojs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 16:23;lsy;Interesting, +1;;;","09/Sep/23 19:30;talat;Hey [~fanrui] At Palo Alto Networks We also use Adaptive Scheduler. And we plan to work on this feature to enhance our jobs. Is there any plan or FLIP about this ? We can also provide resource for development for this task. ;;;","10/Sep/23 02:35;fanrui;Hi, [~talat] , thanks for your interest in Adaptive Scheduler.

 

I'm working in this Jira, I try to finish the improvement at Flink 1.19. Welcome to help review it in your free time, thanks~

 

As I understand, this improvement doesn't introduce any new public option and interface, so the FLIP isn't necessary, right? If the FLIP is necessary, please let me know, I'm happy to start a FLIP for this improvement.;;;","12/Sep/23 17:04;talat;Thank you [~fanrui] I look forward your PR :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports running jar file names with Spaces,FLINK-32817,13546665,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yesorno,yesorno,yesorno,09/Aug/23 11:12,31/Aug/23 10:13,04/Jun/24 20:40,31/Aug/23 10:13,1.14.0,,,,,1.19.0,,,,Deployment / YARN,,,,,,0,pull-request-available,,,"When submitting a flink jar to a yarn cluster, if the jar filename has spaces in it, the task will not be able to successfully parse the file path in `YarnLocalResourceDescriptor`, and the following exception will occur in JobManager.

The Flink jar file name is: StreamSQLExample 2.jar
{code:java}
bin/flink run -d -m yarn-cluster -p 1 -c org.apache.flink.table.examples.java.basics.StreamSQLExample StreamSQLExample\ 2.jar {code}
{code:java}
2023-08-09 18:54:31,787 WARN  org.apache.flink.runtime.extension.resourcemanager.NeActiveResourceManager [] - Failed requesting worker with resource spec WorkerResourceSpec {cpuCores=1.0, taskHeapSize=220.160mb (230854450 bytes), taskOffHeapSize=0 bytes, networkMemSize=158.720mb (166429984 bytes), managedMemSize=952.320mb (998579934 bytes), numSlots=1}, current pending count: 0
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Error to parse YarnLocalResourceDescriptor from YarnLocalResourceDescriptor{key=StreamSQLExample 2.jar, path=hdfs://***/.flink/application_1586413220781_33151/StreamSQLExample 2.jar, size=7937, modificationTime=1691578403748, visibility=APPLICATION, type=FILE}
    at org.apache.flink.util.concurrent.FutureUtils.lambda$supplyAsync$21(FutureUtils.java:1052) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) ~[?:1.8.0_152]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_152]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_152]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_152]
Caused by: org.apache.flink.util.FlinkException: Error to parse YarnLocalResourceDescriptor from YarnLocalResourceDescriptor{key=StreamSQLExample 2.jar, path=hdfs://sloth-jd-pub/user/sloth/.flink/application_1586413220781_33151/StreamSQLExample 2.jar, size=7937, modificationTime=1691578403748, visibility=APPLICATION, type=FILE}
    at org.apache.flink.yarn.YarnLocalResourceDescriptor.fromString(YarnLocalResourceDescriptor.java:112) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    at org.apache.flink.yarn.Utils.decodeYarnLocalResourceDescriptorListFromString(Utils.java:600) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    at org.apache.flink.yarn.Utils.createTaskExecutorContext(Utils.java:491) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    at org.apache.flink.yarn.YarnResourceManagerDriver.createTaskExecutorLaunchContext(YarnResourceManagerDriver.java:452) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    at org.apache.flink.yarn.YarnResourceManagerDriver.lambda$startTaskExecutorInContainerAsync$1(YarnResourceManagerDriver.java:383) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    at org.apache.flink.util.concurrent.FutureUtils.lambda$supplyAsync$21(FutureUtils.java:1050) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
    ... 4 more{code}
From what I understand, the HDFS cluster allows for file names with spaces, as well as S3.

 

I think we could replace the `LOCAL_RESOURCE_DESC_FORMAT` with 
{code:java}
// code placeholder
private static final Pattern LOCAL_RESOURCE_DESC_FORMAT =
        Pattern.compile(
                ""YarnLocalResourceDescriptor\\{""
                        + ""key=([\\S\\x20]+), path=([\\S\\x20]+), size=([\\d]+), modificationTime=([\\d]+), visibility=(\\S+), type=(\\S+)}""); {code}
add '\x20' to only match the spaces",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 10:13:40 UTC 2023,,,,,,,,,,"0|z1jojk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 01:21;yesorno;[~xtsong] WDYT? If it is reasonable, I want to handle it. ;;;","22/Aug/23 10:12;xtsong;Thanks for reporting and volunteering to fix this, [~yesorno].

I think this is a valid problem. Actually, I think it's more serious than not supporting filenames with spaces.

IIUC, what we need here is serialization and deserialization of `YarnLocalResourceDescriptor` so that we can pass it via environment variables. However, the pattern-matching based approach is problematic when there are user provided strings. E.g., what if the jar name contains ""key=(""? I know the chance is very little, but still this is unsafe.

I think it might worth to switch to a proper json serializer. Flink uses Jackson for json serialization and deserialization. You may take a look at {{JacksonMapperFactory}} and places where it is used.

WDYT?;;;","22/Aug/23 10:24;xtsong;cc [~wangyang0918] as the original contributor;;;","25/Aug/23 03:32;yesorno;[~xtsong] 

Thanks for your suggestion. I agree with you.

I'll serialize/deserialize the `YarnLocalResourceDescriptor` by Jackson;;;","28/Aug/23 05:02;xtsong;Thanks, [~yesorno]. You are assigned.

I notice you already opened a PR, but there are some CI failures. Please let me know when they are resolved and you are ready for a review.;;;","31/Aug/23 10:13;xtsong;master (1.19): 1d1247d4ae6d4313f7d952c4b2d66351314c9432;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the local recovery limitation for adaptive scheduler,FLINK-32816,13546653,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,09/Aug/23 09:42,16/Oct/23 11:52,04/Jun/24 20:40,10/Aug/23 04:50,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"FLINK-21450 has supported local recovery for adaptive scheduler, so this limitation can be removed.

 !image-2023-08-09-17-42-55-213.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21450,,,,,,,,,,,"09/Aug/23 09:42;fanrui;image-2023-08-09-17-42-55-213.png;https://issues.apache.org/jira/secure/attachment/13062000/image-2023-08-09-17-42-55-213.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 04:49:45 UTC 2023,,,,,,,,,,"0|z1jogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 04:49;fanrui;Merged via <master:1.18> dbaab1056f10442de6ca3fd44c61fc10c2892ef9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add HASHCODE support in Table API,FLINK-32815,13546577,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hanyuzheng,hanyuzheng,hanyuzheng,08/Aug/23 19:11,23/Feb/24 13:04,04/Jun/24 20:40,15/Jan/24 11:48,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,stale-major,,"*This is an implementation of HASHCODE internal function*

The {{hashcode}} function generates a hash code for a given input value, including support for computing hash values of binary data types. It creates a unique integer that represents the value passed to the function.

*Brief change log*
 * {{HASHCODE}} for Table API 

*Syntax:*
{code:java}
HASHCODE(value){code}
*Arguments:*
 * value: the value to be hashed.

*Returns:* The function returns a unique integer representing the hash code of the value. If the input argument is NULL, the function returns NULL.

Because it is an internal function, so it will not support sql anymore.

*see also:*

Java: [https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#hashCode--]

Python: [https://docs.python.org/3/library/functions.html#hash]

C#: [https://docs.microsoft.com/en-us/dotnet/api/system.object.gethashcode]

SQL Server: [https://docs.microsoft.com/en-us/sql/t-sql/functions/checksum-transact-sql]

MySQL: [https://dev.mysql.com/doc/refman/8.0/en/encryption-functions.html#function_md5]

PostgreSQL: [https://www.postgresql.org/docs/current/pgcrypto-hash.html]

Oracle: [https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/ORA-HASH.html]

Google Cloud BigQuery: [https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#farm_fingerprint]

AWS Redshift: [https://docs.aws.amazon.com/redshift/latest/dg/MD5.html]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 11:48:18 UTC 2024,,,,,,,,,,"0|z1jo00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 08:20;twalthr;From all links in the description, only the Oracle one (https://docs.oracle.com/cd/B14117_01/server.101/b10759/functions097.htm) is similar to the hashcode function proposed here. MD5 or FARM are standadized algorithms that are save to expose publicly. However, the proposed hashcode function uses internal hash logic for our data structures. And we might change the hash logic at any point in time. Therefore, I would make this function an internal function. The sole purpose for now should be to speed up other built-in functions.;;;","09/Aug/23 13:56;hanyuzheng;[~twalthr] Ok, I will change it to internal function.;;;","08/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","15/Jan/24 11:48;dwysakowicz;Implemented in 36542c134b1d4482b37850033dc66a209fd42331..4499553ce7d35b9782c54b66245a4ccb0627cbfc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support disabling failed JM shutdown in the operator,FLINK-32814,13546571,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,sap1ens,sap1ens,08/Aug/23 18:22,14/Aug/23 15:53,04/Jun/24 20:40,14/Aug/23 15:53,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"Currently, it's not possible to disable the shutdown of the jobmanager pods in a terminal state, e.g. {*}FAILED{*}. I may want to keep failed jobmanager pods around until they should be deleted explicitly, e.g. after an investigation.

The TTL for the shutdown is controlled by the *kubernetes.operator.jm-deployment.shutdown-ttl* option.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 11:29:44 UTC 2023,,,,,,,,,,"0|z1jnyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/23 18:56;gyfora;A large ttl would effectively disable it right?;;;","08/Aug/23 20:16;sap1ens;[~gyfora] true! I had to set it to 1 year.

Still, it feels hacky. There are many "".enabled"" config options to control other features, so I think adding one for terminal JM shutdown is justifiable. I submitted a PR for this.;;;","08/Aug/23 20:20;gyfora;I am not sure why anyone would want to keep the JM there forever. The only reason would be investigation , but how much time do you really need ? The logs are probably in splunk or somewhere else already. So even a month is more than enough.

We should only add configs when it makes sense. Here I don’t really see the value to be honest . 

 ;;;","09/Aug/23 07:32;gyfora;In any case, this is indeed a very minor change so if you think that it provides a reasonable value (and more than one user will actually use it :)) then we can add it.

Overall it's good to try limiting the number of configs available as they anyways grow over time with new features. At the end we will have too much;;;","09/Aug/23 18:08;sap1ens;It's your call :)

It is a niche feature indeed and maybe setting a large TTL is good enough.;;;","10/Aug/23 12:18;gyfora;I am leaning towards not having a new config option in this case. Let's ask some independent third parties :) 
[~mxm] [~thw] ?;;;","14/Aug/23 11:29;mxm;Good suggestion [~sap1ens]! However, I agree with Gyula that we don't need another configuration option because setting the existing config to something like {{""100 years""}} will effectively disable the shutdown. Is there any reason why this wouldn't work for you?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split autoscaler ConfigMap,FLINK-32813,13546547,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gyfora,mxm,mxm,08/Aug/23 14:09,06/Sep/23 05:18,04/Jun/24 20:40,,kubernetes-operator-1.6.0,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,"The autoscaler configmap stores the metrics of the current observation window, the past scaling decisions, and the parallelism overrides. We are already using compression and an eviction algorithm to avoid running over the 1MB limit.

We should think about breaking the ConfigMap apart and storing each of the three in its own ConfigMap.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-08 14:09:49.0,,,,,,,,,,"0|z1jntc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseRowDataLookupFunction HTable instantiation of thread safety problems,FLINK-32812,13546537,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,sunshine,sunshine,08/Aug/23 13:07,19/Oct/23 22:06,04/Jun/24 20:40,,1.17.1,,,,,1.17.1,,,,Connectors / HBase,,,,,,0,pull-request-available,security,,"HBaseRowDataLookupFunction HTable instantiation of thread safety problems in the actual development environment, the program has been performed, the close () method of the large probability can't perform, result in Ttable cannot be shut down, all use the same Ttable subsequent applications, multithreading safety hazard, Data errors occur.","{color:#4c9aff}Flink 1.17.1{color}

{color:#4c9aff}Hbase 2.4.11{color}

@Override

public void open(FunctionContext context) {
LOG.info(""start open ..."");
Configuration config = prepareRuntimeConfiguration();
try

{ hConnection = ConnectionFactory.createConnection(config);

{color:#de350b}table = (HTable) hConnection.getTable(TableName.valueOf(hTableName));{color} }

catch (TableNotFoundException tnfe) {
LOG.error(""Table '{}' not found "", hTableName, tnfe);
throw new RuntimeException(""HBase table '"" + hTableName + ""' not found."", tnfe);
} catch (IOException ioe)

{ LOG.error(""Exception while creating connection to HBase."", ioe); throw new RuntimeException(""Cannot create connection to HBase."", ioe); }

this.serde = new HBaseSerde(hbaseTableSchema, nullStringLiteral);
LOG.info(""end open."");
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"HBaseRowDataLookupFunction HTable instantiation thread safety problems, in the open () method of initialization HTable, but in the actual development environment, the program has been performed, the close () method of the large probability can't perform, result in Ttable cannot be shut down, Subsequent programs all use the same Ttable, which poses a multi-thread security risk, resulting in data errors.",false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,2023-08-08 13:07:26.0,,,,,,,,,,"0|z1jnr4:",9223372036854775807,HBaseRowDataLookupFunction HTable instantiation of thread safety problems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add port range support for taskmanager.data.bind-port,FLINK-32811,13546531,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,08/Aug/23 12:19,29/Aug/23 08:41,04/Jun/24 20:40,29/Aug/23 08:41,,,,,,1.19.0,,,,Runtime / Configuration,Runtime / Coordination,,,,,0,pull-request-available,,,"Adding this feature could be helpful for installation in a restrictive network setup. The ""port range"" support is already available for some other port config options anyway.

Right now, it is possible to specify a {{taskmanager.data.port}} and {{taskmanager.data.bind-port}} to be able to support NAT-like setups, although {{taskmanager.data.port}} is not bound to anything itself, so supporting a port range there is not an option according to my understanding.

Although, supporting a port range only for {{taskmanager.data.bind-port}} can be still helpful for anyone who does not require a NAT capability, because if {{taskmanager.data.bind-port}} is set and {{taskmanager.data.port}} is set to *0*, then the bound port will be used everywhere.

This change should keep the already possible setups working as is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19519,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 08:41:25 UTC 2023,,,,,,,,,,"0|z1jnps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/23 08:41;mbalassi;de01f02 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve managed memory usage in ListStateWithCache,FLINK-32810,13546519,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,08/Aug/23 11:18,05/Sep/23 03:25,04/Jun/24 20:40,05/Sep/23 03:25,,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,"Right now, by default, an instance of `ListStateWithCache` uses up all the managed memory allocated for`ManagedMemoryUseCase.OPERATOR`.

It could bring some bugs in some situations, for example, when there exist more than one `ListStateWithCache`s in a single operator, or there are other places using managed memory of `ManagedMemoryUseCase.OPERATOR`.

 

An approach to resolve such cases is to let the developer be aware about the usage of managed memory of `ManagedMemoryUseCase.OPERATOR`, instead of implicitly use up all of it.  Therefore, I think it is better to add a parameters `fraction` to specify how much memory is used in the `ListStateWithCache`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-08 11:18:25.0,,,,,,,,,,"0|z1jnn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnClusterDescriptor.isarchiveonlyincludedinShipArchiveFiles dose not work as expected,FLINK-32809,13546518,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,easonqin,easonqin,easonqin,08/Aug/23 11:13,11/Aug/23 04:52,04/Jun/24 20:40,11/Aug/23 04:52,1.18.0,,,,,1.18.0,,,,Deployment / YARN,,,,,,0,pull-request-available,,,"YarnClusterDescriptor.isarchiveonlyincludedinShipArchiveFiles(List<File> shipFiles) check wether the shipFiles are all archive files, but it dose not work as expected.
{code:java}
public static boolean isArchiveOnlyIncludedInShipArchiveFiles(List<File> shipFiles) {
    return shipFiles.stream()
            .filter(File::isFile)
            .map(File::getName)
            .map(String::toLowerCase)
            .allMatch(
                    name ->
                            name.endsWith("".tar.gz"")
                                    || name.endsWith("".tar"")
                                    || name.endsWith("".tgz"")
                                    || name.endsWith("".dst"")
                                    || name.endsWith("".jar"")
                                    || name.endsWith("".zip""));
} {code}
When we pass a directory and an archive file it should return false but it returns true. 
{code:java}
// dir1 is a directory and archive.zip is an archive file
List<File> files = Arrays.asList(new File(""/tmp/dir1""), new File(""/tmp/archive.zip"")); 
boolean result = isArchiveOnlyIncludedInShipArchiveFiles(files);
System.out.println(result); // Print true but is should print false{code}
If flink user want to add directory as ship file, they should use configuration: yarn.ship-files .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 04:52:23 UTC 2023,,,,,,,,,,"0|z1jnmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/23 11:18;fanrui;Thanks [~easonqin] report this bug!

`isArchiveOnlyIncludedInShipArchiveFiles` should return false once the file isn't ArchiveFile, so it should return false if ShipArchiveFile isn't file.

{quote}If flink user wants to add directory as ship file, they should use configuration: yarn.ship-files .{quote}

+1;;;","11/Aug/23 04:52;fanrui;Thanks for the contribution! :)

Merged via <master:1.18> ac85945947e11b2278c9fcdb24a6a6d695621ac4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
eventTimeWindow follow ContinuousProcessingTimeTrigger sink Infinite number of data when eventTime < processTime,FLINK-32808,13546512,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luca.yang,luca.yang,08/Aug/23 09:50,08/Aug/23 09:50,04/Jun/24 20:40,,1.17.1,,,,,,,,,API / Core,,,,,,0,,,,"事件时间窗口后接处理时间trigger(ContinuousProcessingTimeTrigger), 如图事件时间<处理时间,就会无限输出数据,这个bug在flink1.17.1发现. 在flink1.9没有这个问题.

输入输出:

!image-2023-08-08-17-50-32-763.png!

 

复现bug可用附件中:  flinkEventWindowAndProcessTriggerBUGLearn 

 ","<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
<modelVersion>4.0.0</modelVersion>

<groupId>org.example</groupId>
<artifactId>FlinkLocalDemo</artifactId>
<version>1.0-SNAPSHOT</version>
<packaging>jar</packaging>

<name>FlinkLocalDemo</name>
<url>http://maven.apache.org</url>

<properties>
<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
<flink.version>1.17.1</flink.version>
<scala.binary.version>2.12</scala.binary.version>
<scala.version>2.12.8</scala.version>
</properties>



<dependencies>
<!-- https://mvnrepository.com/artifact/com.chuusai/shapeless -->
<dependency>
<groupId>com.chuusai</groupId>
<artifactId>shapeless_${scala.binary.version}</artifactId>
<version>2.3.10</version>
</dependency>

<!-- https://mvnrepository.com/artifact/joda-time/joda-time -->
<dependency>
<groupId>joda-time</groupId>
<artifactId>joda-time</artifactId>
<version>2.12.5</version>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-avro</artifactId>
<version>${flink.version}</version>
</dependency>


<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-runtime-web</artifactId>
<version>${flink.version}</version>
</dependency>

<!-- https://mvnrepository.com/artifact/com.alibaba.fastjson2/fastjson2 -->
<dependency>
<groupId>com.alibaba.fastjson2</groupId>
<artifactId>fastjson2</artifactId>
<version>2.0.33</version>
</dependency>
<!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
<dependency>
<groupId>com.alibaba</groupId>
<artifactId>fastjson</artifactId>
<version>1.2.83</version>
<!-- <version>1.2.17</version>-->
</dependency>


<dependency>
<groupId>junit</groupId>
<artifactId>junit</artifactId>
<version>3.8.1</version>
<scope>test</scope>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-table-common -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-common</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- 引入flink1.13.0 scala2.12.12 -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-kafka</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-json</artifactId>
<version>${flink.version}</version>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-scala_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-csv</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- Either... -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-api-java-bridge</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- or... -->

<!-- 下面几个是代码中写sql需要的包 四个中一个都不能少 -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-api-scala-bridge_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-table-planner-loader -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-planner-loader</artifactId>
<version>${flink.version}</version>
<!-- <scope>test</scope>-->
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-table-runtime -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-runtime</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-files -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-files</artifactId>
<version>${flink.version}</version>
</dependency>


<!-- 注意: flink-table-planner-loader 不能和 flink-table-planner_${scala.binary.version} 共存-->
<!-- <dependency>-->
<!-- <groupId>org.apache.flink</groupId>-->
<!-- <artifactId>flink-table-planner_${scala.binary.version}</artifactId>-->
<!-- <version>${flink.version}</version>-->
<!-- <scope>provided</scope>-->
<!-- </dependency>-->


<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-clients</artifactId>
<version>${flink.version}</version>
</dependency>

<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-jdbc</artifactId>
<version>3.1.0-1.17</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>8.0.11</version>
</dependency>


</dependencies>
<build>
<plugins>
<!-- 打jar插件 -->
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-shade-plugin</artifactId>
<version>2.4.3</version>
<executions>
<execution>
<phase>package</phase>
<goals>
<goal>shade</goal>
</goals>
<configuration>
<filters>
<filter>
<artifact>*:*</artifact>
<excludes>
<exclude>META-INF/*.SF</exclude>
<exclude>META-INF/*.DSA</exclude>
<exclude>META-INF/*.RSA</exclude>
</excludes>
</filter>
</filters>
</configuration>
</execution>
</executions>
</plugin>
<plugin>
<groupId>org.scala-tools</groupId>
<artifactId>maven-scala-plugin</artifactId>
<version>2.15.2</version>
<executions>
<execution>
<goals>
<goal>compile</goal>
<goal>testCompile</goal>
</goals>
</execution>
</executions>
</plugin>
<plugin>
<groupId>net.alchim31.maven</groupId>
<artifactId>scala-maven-plugin</artifactId>
<version>3.2.2</version>
<executions>
<execution>
<id>scala-compile-first</id>
<phase>process-resources</phase>
<goals>
<goal>add-source</goal>
<goal>compile</goal>
</goals>
</execution>
</executions>
<configuration>
<scalaVersion>${scala.version}</scalaVersion>
</configuration>
</plugin>
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-assembly-plugin</artifactId>
<version>2.5.5</version>
<configuration>
<!--这部分可有可无,加上的话则直接生成可运行jar包-->
<!--<archive>-->
<!--<manifest>-->
<!--<mainClass>${exec.mainClass}</mainClass>-->
<!--</manifest>-->
<!--</archive>-->
<descriptorRefs>
<descriptorRef>jar-with-dependencies</descriptorRef>
</descriptorRefs>
</configuration>
</plugin>
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-compiler-plugin</artifactId>
<version>3.1</version>
<configuration>
<source>11</source>
<target>11</target>
</configuration>
</plugin>
</plugins>
</build>
</project>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 09:47;luca.yang;flinkEventWindowAndProcessTriggerBUGLearn.scala;https://issues.apache.org/jira/secure/attachment/13061978/flinkEventWindowAndProcessTriggerBUGLearn.scala","08/Aug/23 09:48;luca.yang;flinkWindowTest.scala;https://issues.apache.org/jira/secure/attachment/13061977/flinkWindowTest.scala","08/Aug/23 09:50;luca.yang;image-2023-08-08-17-50-32-763.png;https://issues.apache.org/jira/secure/attachment/13061976/image-2023-08-08-17-50-32-763.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-08 09:50:51.0,,,,,,,,,,"0|z1jnlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when i use emitUpdateWithRetract of udtagg,bug error",FLINK-32807,13546510,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,luca.yang,luca.yang,08/Aug/23 09:31,10/Nov/23 10:09,04/Jun/24 20:40,10/Nov/23 10:09,1.17.1,,,,,,,,,API / Scala,Table SQL / Planner,,,,,0,,,,"参考: [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/functions/udfs/#retraction-example]

我的代码:

[^Top2WithRetract.scala]

 

bug show error:
{code:java}
//代码占位符
/Users/thomas990p/Library/Java/JavaVirtualMachines/corretto-11.0.20/Contents/Home/bin/java -javaagent:/Users/thomas990p/Library/Application Support/JetBrains/Toolbox/apps/IDEA-U/ch-0/231.9161.38/IntelliJ IDEA.app/Contents/lib/idea_rt.jar=56941:/Users/thomas990p/Library/Application Support/JetBrains/Toolbox/apps/IDEA-U/ch-0/231.9161.38/IntelliJ IDEA.app/Contents/bin -Dfile.encoding=UTF-8 -classpath /Users/thomas990p/IdeaProjects/FlinkLocalDemo/target/classes:/Users/thomas990p/.m2/repository/com/chuusai/shapeless_2.12/2.3.10/shapeless_2.12-2.3.10.jar:/Users/thomas990p/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/Users/thomas990p/.m2/repository/joda-time/joda-time/2.12.5/joda-time-2.12.5.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-avro/1.17.1/flink-avro-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar:/Users/thomas990p/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.7/jackson-core-2.12.7.jar:/Users/thomas990p/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.7/jackson-databind-2.12.7.jar:/Users/thomas990p/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.7/jackson-annotations-2.12.7.jar:/Users/thomas990p/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/Users/thomas990p/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-runtime-web/1.17.1/flink-runtime-web-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-runtime/1.17.1/flink-runtime-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-rpc-core/1.17.1/flink-rpc-core-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-rpc-akka-loader/1.17.1/flink-rpc-akka-loader-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.17.1/flink-queryable-state-client-java-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-hadoop-fs/1.17.1/flink-hadoop-fs-1.17.1.jar:/Users/thomas990p/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.7.1-16.1/flink-shaded-zookeeper-3-3.7.1-16.1.jar:/Users/thomas990p/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/thomas990p/.m2/repository/org/apache/commons/commons-text/1.10.0/commons-text-1.10.0.jar:/Users/thomas990p/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/thomas990p/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.3/snappy-java-1.1.8.3.jar:/Users/thomas990p/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.82.Final-16.1/flink-shaded-netty-4.1.82.Final-16.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-shaded-guava/30.1.1-jre-16.1/flink-shaded-guava-30.1.1-jre-16.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-shaded-jackson/2.13.4-16.1/flink-shaded-jackson-2.13.4-16.1.jar:/Users/thomas990p/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/Users/thomas990p/.m2/repository/com/alibaba/fastjson2/fastjson2/2.0.33/fastjson2-2.0.33.jar:/Users/thomas990p/.m2/repository/com/alibaba/fastjson/1.2.83/fastjson-1.2.83.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-common/1.17.1/flink-table-common-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-core/1.17.1/flink-core-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-annotations/1.17.1/flink-annotations-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-metrics-core/1.17.1/flink-metrics-core-1.17.1.jar:/Users/thomas990p/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/thomas990p/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/thomas990p/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/thomas990p/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-shaded-asm-9/9.3-16.1/flink-shaded-asm-9-9.3-16.1.jar:/Users/thomas990p/.m2/repository/com/ibm/icu/icu4j/67.1/icu4j-67.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-connector-kafka/1.17.1/flink-connector-kafka-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-connector-base/1.17.1/flink-connector-base-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/kafka/kafka-clients/3.2.3/kafka-clients-3.2.3.jar:/Users/thomas990p/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-json/1.17.1/flink-json-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-shaded-force-shading/16.1/flink-shaded-force-shading-16.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-scala_2.12/1.17.1/flink-scala_2.12-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-java/1.17.1/flink-java-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/Users/thomas990p/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/thomas990p/.m2/repository/org/scala-lang/scala-reflect/2.12.7/scala-reflect-2.12.7.jar:/Users/thomas990p/.m2/repository/org/scala-lang/scala-compiler/2.12.7/scala-compiler-2.12.7.jar:/Users/thomas990p/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/Users/thomas990p/.m2/repository/com/twitter/chill_2.12/0.7.6/chill_2.12-0.7.6.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-streaming-scala_2.12/1.17.1/flink-streaming-scala_2.12-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-streaming-java/1.17.1/flink-streaming-java-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-csv/1.17.1/flink-csv-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-api-java-bridge/1.17.1/flink-table-api-java-bridge-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-api-java/1.17.1/flink-table-api-java-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-api-bridge-base/1.17.1/flink-table-api-bridge-base-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-api-scala-bridge_2.12/1.17.1/flink-table-api-scala-bridge_2.12-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-api-scala_2.12/1.17.1/flink-table-api-scala_2.12-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-runtime/1.17.1/flink-table-runtime-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-cep/1.17.1/flink-cep-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-connector-files/1.17.1/flink-connector-files-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-file-sink-common/1.17.1/flink-file-sink-common-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-table-planner_2.12/1.17.1/flink-table-planner_2.12-1.17.1.jar:/Users/thomas990p/.m2/repository/org/immutables/value/2.8.8/value-2.8.8.jar:/Users/thomas990p/.m2/repository/org/immutables/value-annotations/2.8.8/value-annotations-2.8.8.jar:/Users/thomas990p/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/Users/thomas990p/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/Users/thomas990p/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-clients/1.17.1/flink-clients-1.17.1.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-optimizer/1.17.1/flink-optimizer-1.17.1.jar:/Users/thomas990p/.m2/repository/commons-cli/commons-cli/1.5.0/commons-cli-1.5.0.jar:/Users/thomas990p/.m2/repository/org/apache/flink/flink-connector-jdbc/3.1.0-1.17/flink-connector-jdbc-3.1.0-1.17.jar:/Users/thomas990p/.m2/repository/mysql/mysql-connector-java/8.0.11/mysql-connector-java-8.0.11.jar:/Users/thomas990p/.m2/repository/com/google/protobuf/protobuf-java/2.6.0/protobuf-java-2.6.0.jar com.yy.udtaf.UdtaggDemo3
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Could not find an implementation method 'emitValue' in class 'com.yy.udtaf.Top2WithRetract' for function 'com.yy.udtaf.Top2WithRetract' that matches the following signature:
void emitValue(com.yy.udtaf.Top2WithRetractAccumulator, org.apache.flink.util.Collector)
at org.apache.flink.table.functions.UserDefinedFunctionHelper.validateClassForRuntime(UserDefinedFunctionHelper.java:323)
at org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.checkNeededMethods(ImperativeAggCodeGen.scala:496)
at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.$anonfun$checkNeededMethods$1(AggsHandlerCodeGenerator.scala:1234)
at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.$anonfun$checkNeededMethods$1$adapted(AggsHandlerCodeGenerator.scala:1234)
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.checkNeededMethods(AggsHandlerCodeGenerator.scala:1234)
at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.genEmitValue(AggsHandlerCodeGenerator.scala:1170)
at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.generateTableAggsHandler(AggsHandlerCodeGenerator.scala:443)
at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupTableAggregate.translateToPlanInternal(StreamExecGroupTableAggregate.java:146)
at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161)
at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161)
at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:145)
at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161)
at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
at scala.collection.Iterator.foreach(Iterator.scala:943)
at scala.collection.Iterator.foreach$(Iterator.scala:943)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
at scala.collection.IterableLike.foreach(IterableLike.scala:74)
at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
at scala.collection.TraversableLike.map(TraversableLike.scala:286)
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
at scala.collection.AbstractTraversable.map(Traversable.scala:108)
at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84)
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1803)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:945)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)
at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
at com.yy.udtaf.UdtaggDemo3$.main(UdtaggDemo3.scala:36)
at com.yy.udtaf.UdtaggDemo3.main(UdtaggDemo3.scala)
 
Process finished with exit code 1
 
  {code}
 ","<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
<modelVersion>4.0.0</modelVersion>

<groupId>org.example</groupId>
<artifactId>FlinkLocalDemo</artifactId>
<version>1.0-SNAPSHOT</version>
<packaging>jar</packaging>

<name>FlinkLocalDemo</name>
<url>http://maven.apache.org</url>

<properties>
<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
<flink.version>1.17.1</flink.version>
<scala.binary.version>2.12</scala.binary.version>
<scala.version>2.12.8</scala.version>
</properties>



<dependencies>
<!-- https://mvnrepository.com/artifact/com.chuusai/shapeless -->
<dependency>
<groupId>com.chuusai</groupId>
<artifactId>shapeless_${scala.binary.version}</artifactId>
<version>2.3.10</version>
</dependency>

<!-- https://mvnrepository.com/artifact/joda-time/joda-time -->
<dependency>
<groupId>joda-time</groupId>
<artifactId>joda-time</artifactId>
<version>2.12.5</version>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-avro</artifactId>
<version>${flink.version}</version>
</dependency>


<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-runtime-web</artifactId>
<version>${flink.version}</version>
</dependency>

<!-- https://mvnrepository.com/artifact/com.alibaba.fastjson2/fastjson2 -->
<dependency>
<groupId>com.alibaba.fastjson2</groupId>
<artifactId>fastjson2</artifactId>
<version>2.0.33</version>
</dependency>
<!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
<dependency>
<groupId>com.alibaba</groupId>
<artifactId>fastjson</artifactId>
<version>1.2.83</version>
<!-- <version>1.2.17</version>-->
</dependency>


<dependency>
<groupId>junit</groupId>
<artifactId>junit</artifactId>
<version>3.8.1</version>
<scope>test</scope>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-table-common -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-common</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- 引入flink1.13.0 scala2.12.12 -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-kafka</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-json</artifactId>
<version>${flink.version}</version>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-scala_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
</dependency>
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-csv</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- Either... -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-api-java-bridge</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- or... -->

<!-- 下面几个是代码中写sql需要的包 四个中一个都不能少 -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-api-scala-bridge_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-table-planner-loader -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-planner-loader</artifactId>
<version>${flink.version}</version>
<!-- <scope>test</scope>-->
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-table-runtime -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-runtime</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-files -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-files</artifactId>
<version>${flink.version}</version>
</dependency>


<!-- 注意: flink-table-planner-loader 不能和 flink-table-planner_${scala.binary.version} 共存-->
<!-- <dependency>-->
<!-- <groupId>org.apache.flink</groupId>-->
<!-- <artifactId>flink-table-planner_${scala.binary.version}</artifactId>-->
<!-- <version>${flink.version}</version>-->
<!-- <scope>provided</scope>-->
<!-- </dependency>-->


<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-clients</artifactId>
<version>${flink.version}</version>
</dependency>

<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-jdbc</artifactId>
<version>3.1.0-1.17</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>8.0.11</version>
</dependency>


</dependencies>
<build>
<plugins>
<!-- 打jar插件 -->
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-shade-plugin</artifactId>
<version>2.4.3</version>
<executions>
<execution>
<phase>package</phase>
<goals>
<goal>shade</goal>
</goals>
<configuration>
<filters>
<filter>
<artifact>*:*</artifact>
<excludes>
<exclude>META-INF/*.SF</exclude>
<exclude>META-INF/*.DSA</exclude>
<exclude>META-INF/*.RSA</exclude>
</excludes>
</filter>
</filters>
</configuration>
</execution>
</executions>
</plugin>
<plugin>
<groupId>org.scala-tools</groupId>
<artifactId>maven-scala-plugin</artifactId>
<version>2.15.2</version>
<executions>
<execution>
<goals>
<goal>compile</goal>
<goal>testCompile</goal>
</goals>
</execution>
</executions>
</plugin>
<plugin>
<groupId>net.alchim31.maven</groupId>
<artifactId>scala-maven-plugin</artifactId>
<version>3.2.2</version>
<executions>
<execution>
<id>scala-compile-first</id>
<phase>process-resources</phase>
<goals>
<goal>add-source</goal>
<goal>compile</goal>
</goals>
</execution>
</executions>
<configuration>
<scalaVersion>${scala.version}</scalaVersion>
</configuration>
</plugin>
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-assembly-plugin</artifactId>
<version>2.5.5</version>
<configuration>
<!--这部分可有可无,加上的话则直接生成可运行jar包-->
<!--<archive>-->
<!--<manifest>-->
<!--<mainClass>${exec.mainClass}</mainClass>-->
<!--</manifest>-->
<!--</archive>-->
<descriptorRefs>
<descriptorRef>jar-with-dependencies</descriptorRef>
</descriptorRefs>
</configuration>
</plugin>
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-compiler-plugin</artifactId>
<version>3.1</version>
<configuration>
<source>11</source>
<target>11</target>
</configuration>
</plugin>
</plugins>
</build>
</project>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 09:30;luca.yang;Top2WithRetract.scala;https://issues.apache.org/jira/secure/attachment/13061975/Top2WithRetract.scala","08/Aug/23 09:30;luca.yang;UdtaggDemo3.scala;https://issues.apache.org/jira/secure/attachment/13061974/UdtaggDemo3.scala",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 09 07:47:59 UTC 2023,,,,,,,,,,"0|z1jnl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 07:06;xuyangzhong;Hi, [~luca.yang] . I think this issue is duplicated with https://issues.apache.org/jira/browse/FLINK-31788 ?;;;","09/Nov/23 07:47;luca.yang;yes it is duplicated;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedJobResultStore keeps the non-dirty job entries forever,FLINK-32806,13546496,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hk__lrzy,mapohl,mapohl,08/Aug/23 08:03,27/Oct/23 18:29,04/Jun/24 20:40,,1.17.1,1.18.0,1.19.0,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,stale-assigned,starter,The {{EmbeddedJobResultStore}} keeps the entries of cleaned-up jobs in-memory forever. We might want to add a TTL to have those entries be removed after a certain amount of time to allow maintaining the memory footprint of the {{EmbeddedJobResultStore}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 18 12:12:58 UTC 2023,,,,,,,,,,"0|z1jnhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 04:02;hk__lrzy;[~mapohl]  I am interesting in this issue, can you assign it to me ;;;","18/Aug/23 13:48;mapohl;Thanks for picking it up. I assigned the issue to you.;;;","17/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Oct/23 08:20;samrat007;Hi [~mapohl] , 

I have a couple of questions regarding the proposed change:
 # Could you please clarify what the default time-to-live (TTL) for the entries will be? Additionally, are we considering exposing any configuration options for TTL, or is it best to avoid adding more configuration complexity?

 # Are we exploring any other conditions for removing an entry, apart from time-based removal? For example, should we also consider removing entries when the map size exceeds a certain threshold to prevent excessive memory consumption?

I'd appreciate your insights on these points.;;;","10/Oct/23 17:14;samrat007;[~hk__lrzy] are you working on the patch ? ;;;","18/Oct/23 08:32;mapohl;[~samrat007] thanks for your ideas. You raise some reasonable points.

{quote}
Could you please clarify what the default time-to-live (TTL) for the entries will be? Additionally, are we considering exposing any configuration options for TTL, or is it best to avoid adding more configuration complexity?
{quote}
I think we should keep the default time-to-live at infinity/max to align with the current behavior. Adding a configuration parameter to make this configurable sounds reasonable in that case, especially when considering optimizations for OLAP usecases.

{quote}
Are we exploring any other conditions for removing an entry, apart from time-based removal? For example, should we also consider removing entries when the map size exceeds a certain threshold to prevent excessive memory consumption?
{quote}
The memory footprint of the entry is still quite small. I'm wondering whether we should keep this in the backlog for the sake of not letting the configuration grow even more. WDYT?;;;","18/Oct/23 12:12;samrat007;Thank you [~mapohl] for your view. 
I am alligned with keeping default time-to-live at infinity/max to align with the current behavior. I will add configuration param to make this configurable. 



> The memory footprint of the entry is still quite small. I'm wondering whether we should keep this in the backlog for the sake of not letting the configuration grow even more. WDYT?



In our EMR cluster, we've observed this scenario occurring just once in the past year, and it was specific to a long-running cluster for certain Cx. Given this, it seems reasonable to keep it in the backlog for the time being. If the need for this feature arises in the future, we can certainly revisit it and incorporate it.

I'll proceed with working on the patch and will request your review within the next couple of days. :) 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify Support Java 17 (LTS) ,FLINK-32805,13546493,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,renqs,renqs,08/Aug/23 07:51,29/Aug/23 06:09,04/Jun/24 20:40,29/Aug/23 06:09,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 08:24:34 UTC 2023,,,,,,,,,,"0|z1jnh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 09:30;renqs;* Tested compiling with Java 17 and -Pjava17 -Pjava17-target
 * Tested starting a standalone cluster locally with Java 17
 * Tested running WordCount and StateMachineExample
 * Tested submitting a SQL job reading from Kafka and writing to STDOUT with SQL client

When compiling Kafka connector with Java 17, I found an issue in flink-connector-parent. See FLINK-32894. (This is independent to the release cycle of Flink main repo);;;","21/Aug/23 08:24;renqs;[~chesnay] Is there any other key points to test for Java 17 support? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-304: Pluggable failure handling for Apache Flink,FLINK-32804,13546492,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangm92,renqs,renqs,08/Aug/23 07:51,12/Sep/23 15:38,04/Jun/24 20:40,12/Sep/23 15:36,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31895,FLINK-33022,FLINK-33051,,,,"04/Sep/23 08:58;wangm92;image-2023-09-04-16-58-29-329.png;https://issues.apache.org/jira/secure/attachment/13062687/image-2023-09-04-16-58-29-329.png","04/Sep/23 09:00;wangm92;image-2023-09-04-17-00-16-413.png;https://issues.apache.org/jira/secure/attachment/13062688/image-2023-09-04-17-00-16-413.png","04/Sep/23 09:02;wangm92;image-2023-09-04-17-02-41-760.png;https://issues.apache.org/jira/secure/attachment/13062689/image-2023-09-04-17-02-41-760.png","04/Sep/23 09:03;wangm92;image-2023-09-04-17-03-25-452.png;https://issues.apache.org/jira/secure/attachment/13062690/image-2023-09-04-17-03-25-452.png","04/Sep/23 09:04;wangm92;image-2023-09-04-17-04-21-682.png;https://issues.apache.org/jira/secure/attachment/13062691/image-2023-09-04-17-04-21-682.png",,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 15:35:48 UTC 2023,,,,,,,,,,"0|z1jngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:57;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:57;renqs;[~pgaref] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","23/Aug/23 03:52;wangm92;[~renqs] can i task this, I'm interested in this;;;","23/Aug/23 06:21;renqs;[~wangm92] Thanks for volunteering! Assigned to you just now. ;;;","24/Aug/23 23:06;pgaref;Hey [~renqs]  – original documentation is part of https://issues.apache.org/jira/browse/FLINK-31889 – not merged yet but provides instructions how to implement customer FailureEnrichers.
Regarding testing, JMs rest endpoint can be used, have a failed job and make sure the appropriate failureLabels are exposed as part of https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-exceptions

Happy to help with more details and/or the task itself – [~wangm92] feel free to ping me;;;","31/Aug/23 09:05;renqs;[~wangm92] Any update on this one?;;;","31/Aug/23 13:29;wangm92;[~renqs] I just started doing this verification today. I will update the results of the verification tomorrow.;;;","04/Sep/23 09:04;wangm92;[~renqs] [~pgaref] hi, I have tested the functions of LIFP-304 here. Overall, the functions are OK, but I have also encountered some minor problems:
1. When FailureEnricherUtils loads FailureEnricher, if `jobmanager.failure-enrichers` is configured but not loaded, I think it should throw an exception or print some ERROR logs. I created Jira(https://issues.apache.org/jira/browse/FLINK-33022) a to follow up;
2. FLIP-304 introduces the `LabeledGlobalFailureHandler` interface. I think the `GlobalFailureHandler` interface can be removed in the future to avoid the existence of interfaces with duplicate functions. This can be promoted in future versions;
3. Regarding the user documentation, I left some comments. The user documentation is very helpful, and the code integration needs to be promoted as soon as possible; [~pgaref] 
4. https://issues.apache.org/jira/browse/FLINK-31895  about E2E test Here I think it needs to be promoted as soon as possible. If there is a Demo example, it will be more helpful to users.

here is my test:
First, I implemented a custom FailureEnricher and FailureEnricherFactory, and placed it in the plugins directory
!image-2023-09-04-16-58-29-329.png|width=898,height=330!
!image-2023-09-04-17-00-16-413.png|width=871,height=238!

Secondly, I build a test job. The test job will throw an exception when running. Ideally, it will hit the CustomEnricher.
!image-2023-09-04-17-02-41-760.png|width=867,height=433!

Finally, through restful api, you can see that corresponding exceptions will have corresponding label information.

!image-2023-09-04-17-03-25-452.png|width=875,height=20!
!image-2023-09-04-17-04-21-682.png|width=886,height=589!;;;","06/Sep/23 16:14;pgaref;Hey [~wangm92] – thanks for the detailed update! Answers inline
{quote}1. When FailureEnricherUtils loads FailureEnricher, if `jobmanager.failure-enrichers` is configured but not loaded, I think it should throw an exception or print some ERROR logs. I created Jira(https://issues.apache.org/jira/browse/FLINK-33022) a to follow up;
{quote}
Happy to take care of this
{quote}2. FLIP-304 introduces the `LabeledGlobalFailureHandler` interface. I think the `GlobalFailureHandler` interface can be removed in the future to avoid the existence of interfaces with duplicate functions. This can be promoted in future versions;
{quote}
Opened FLINK-33051 to work on this
{quote}3. Regarding the user documentation, I left some comments. The user documentation is very helpful, and the code integration needs to be promoted as soon as possible; [~pgaref] 
{quote}
Updated documentation PR – let me know what you think
{quote}4. https://issues.apache.org/jira/browse/FLINK-31895  about E2E test Here I think it needs to be promoted as soon as possible. If there is a Demo example, it will be more helpful to users.
{quote}
Lets split this to 2 tickets, one about e2e tests and one adding a demo;;;","07/Sep/23 02:16;wangm92;[~pgaref] LGTM, we can follow up in these Jiras, we can push this FLIP forward together
[~renqs] hi, This FLIP also has some TODOs that are followed up in FLINK-31508. Do we need to keep this test in the in process state?;;;","12/Sep/23 15:35;pgaref;Hey [~wangm92] closing this ticket at the main functionality was validated as discussed above. Linking the follow-up tickets as discussed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-32165 - Improve observability of fine-grained resource management,FLINK-32803,13546491,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huweihua,renqs,renqs,08/Aug/23 07:50,12/Sep/23 06:48,04/Jun/24 20:40,12/Sep/23 06:48,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32165,,,,,,,,"30/Aug/23 02:56;huweihua;image-2023-08-30-10-56-09-174.png;https://issues.apache.org/jira/secure/attachment/13062574/image-2023-08-30-10-56-09-174.png","30/Aug/23 02:57;huweihua;image-2023-08-30-10-57-11-049.png;https://issues.apache.org/jira/secure/attachment/13062575/image-2023-08-30-10-57-11-049.png","30/Aug/23 02:58;huweihua;image-2023-08-30-10-58-34-939.png;https://issues.apache.org/jira/secure/attachment/13062576/image-2023-08-30-10-58-34-939.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 06:47:54 UTC 2023,,,,,,,,,,"0|z1jngo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:56;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:56;renqs;[~chesnay] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","23/Aug/23 03:47;huweihua;I would like to take this issue. [~chesnay] [~renqs] ;;;","23/Aug/23 06:22;renqs;[~huweihua] Thanks for taking this! Assigned to you just now.;;;","30/Aug/23 03:00;huweihua;Hi, all

I have tested this feature and it looks good.
----
Logs the missing requested resources in RM side

!image-2023-08-30-10-56-09-174.png!

 
----
Logs the allocated slot resource info in the TM side

!image-2023-08-30-10-57-11-049.png!
----
TaskManager Page show the unassigned resources and allocated slot info

!image-2023-08-30-10-58-34-939.png!;;;","04/Sep/23 09:19;huweihua;[~chesnay], Hi, is there any other scenarios you would like to testing?;;;","12/Sep/23 06:47;renqs;Mark as resolved as the test result looks good and no further response from original author. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-291: Externalized Declarative Resource Management,FLINK-32802,13546490,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,ConradJam,renqs,renqs,08/Aug/23 07:50,14/Sep/23 02:27,04/Jun/24 20:40,14/Sep/23 02:27,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 08:18:34 UTC 2023,,,,,,,,,,"0|z1jngg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:56;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:56;renqs;[~dmvk] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","22/Aug/23 09:01;dmvk;sure, I'll write something down;;;","23/Aug/23 06:22;ConradJam;I want to try this task what should I do
Before that, I carefully read the relevant content of FLIP-291
At the same time I wrote related documentation for review FLINK-32671

cc [~renqs] [~dmvk] ;;;","23/Aug/23 06:26;renqs;[~ConradJam] Thanks for volunteering! Assigned to you just now. ;;;","31/Aug/23 09:04;renqs;[~ConradJam] Any update on this one?;;;","04/Sep/23 03:32;ConradJam;I want to synchronize it. Currently, my local test results are as follows:

(1) Ability to correctly add and reduce operator parallelism in the WebUI interface and rest api
(2) AdaptiveScheduler upper and lower parallelism restrictions take effect


One thing that is confusing is that after the discussion of FLINK-31608, a cooldown mechanism should be added. This mechanism was discussed in FLIP-322. At present, this part is still in the process of code review. I don’t know if the two are related. , if someone tells me that this part of the test will be added later


https://issues.apache.org/jira/browse/FLINK-31608

[https://github.com/apache/flink/pull/22985]

[https://cwiki.apache.org/confluence/display/FLINK/FLIP-322+Cooldown+period+for+adaptive+scheduler]

 

cc [~renqs] 

 ;;;","06/Sep/23 08:23;Sergey Nuyanzin;[~ConradJam]thanks for checking this

[~dmvk] could you please provide more details regarding comment above?;;;","12/Sep/23 08:12;chesnay;FLINK-31608 will not land in 1.18 (because it will likely need a separate FLIP), nor will FLIP-322 (feature freeze).;;;","12/Sep/23 08:18;ConradJam;Thanks Chesnay Report, I think my side of the test is done

cc [~dmvk] [~renqs] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-32524 - Improve the watermark aggregation performance when enabling the watermark alignment,FLINK-32801,13546489,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,renqs,renqs,08/Aug/23 07:49,18/Aug/23 14:29,04/Jun/24 20:40,18/Aug/23 14:29,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 14:29:28 UTC 2023,,,,,,,,,,"0|z1jng8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 14:29;fanrui;FLINK-32524 did a performance improvement about watermark alignment, and I have tested on my Local. It works well.:);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-32548 - Make watermark alignment ready for production use,FLINK-32800,13546488,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,renqs,renqs,08/Aug/23 07:48,18/Aug/23 14:29,04/Jun/24 20:40,18/Aug/23 14:29,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 14:28:32 UTC 2023,,,,,,,,,,"0|z1jng0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 14:28;fanrui;FLINK-32548 fixed a series of bugs related to watermark alignment, and I have tested on my Local. It works well.:);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-26603 -Decouple Hive with Flink planner,FLINK-32799,13546487,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,renqs,renqs,08/Aug/23 07:47,23/Aug/23 06:08,04/Jun/24 20:40,23/Aug/23 06:08,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 12:25;ruanhang1993;hive.png;https://issues.apache.org/jira/secure/attachment/13062321/hive.png","21/Aug/23 12:25;ruanhang1993;lib.png;https://issues.apache.org/jira/secure/attachment/13062322/lib.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 06:08:50 UTC 2023,,,,,,,,,,"0|z1jnfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 02:14;luoyuxia;After FLINK-26603, FLINK-31575, to use hive dialect, we then won't need to swap the flink-table-planner jar in FLINK_HOME/opt with flink-table-planner-loader in FLINK_HOME/lib.

To use hive dialect, please refer to the doc [hive dialect|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/hive-compatibility/hive-dialect/overview/#use-hive-dialect], but please remember don't swap the planner jar.;;;","21/Aug/23 12:25;ruanhang1993;Hi, all.

I test this with the hive alter sql. It seems good.

!hive.png!

My dependencies are as follows.

!lib.png!;;;","23/Aug/23 03:37;ruanhang1993;I have also tested one insert and select. It looks good.;;;","23/Aug/23 06:08;luoyuxia;[~ruanhang1993] Thanks for verifying... Close it now...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-294: Support Customized Catalog Modification Listener,FLINK-32798,13546486,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,ruanhang1993,renqs,renqs,08/Aug/23 07:47,31/Aug/23 02:43,04/Jun/24 20:40,31/Aug/23 02:43,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,The document about catalog modification listener is: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/catalogs/#catalog-modification-listener,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32968,,,,,,,,"24/Aug/23 03:53;ruanhang1993;result.png;https://issues.apache.org/jira/secure/attachment/13062403/result.png","24/Aug/23 03:53;ruanhang1993;sqls.png;https://issues.apache.org/jira/secure/attachment/13062402/sqls.png","23/Aug/23 12:22;ruanhang1993;test.png;https://issues.apache.org/jira/secure/attachment/13062381/test.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 02:12:55 UTC 2023,,,,,,,,,,"0|z1jnfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:55;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:55;renqs;[~zjureel]  Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","23/Aug/23 02:30;zjureel;[~renqs] I have add the document link in the ""Description"";;;","23/Aug/23 12:23;ruanhang1993;[~zjureel] Hi, where should users put their jar? I do not find the content about it in docs. 

I try this with the sql client and use SET sql.

It seems like I can set it with any random string and sql client will not check it. Is it an expected behavior?

!test.png!

 ;;;","23/Aug/23 13:07;zjureel;[~ruanhang1993] I checked the implementation in sql-gateway and there's an issue in the document  https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/catalogs/#catalog-modification-listener that users can not set the listener with `SET` for sql-gateway. Users should add the option `table.catalog-modification.listeners` in flink-conf.yaml or use it as dynamic parameter for sql-gateway. How can I create a pr for this? cc [~renqs];;;","24/Aug/23 01:57;renqs;[~zjureel] Thanks for the work. Please create another ticket for fixing the doc, and link it with this release testing issue. The patch should be merged on both master and release-1.18 branch now. ;;;","24/Aug/23 03:55;ruanhang1993;Hi, all.[~renqs] [~zjureel] 

I have done some tests about this feature.

1.Create a mysql table to store the changes.
{code:java}
CREATE TABLE `listener_test` (  `id` bigint NOT NULL AUTO_INCREMENT,  `catalog` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,  `identifier` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,  `type` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,  `detail` text CHARACTER SET utf8 COLLATE utf8_general_ci,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb3 ROW_FORMAT=DYNAMIC; {code}
2.Develop the Factory and Listener.
{code:java}
package org.self.listener;

import org.apache.flink.table.catalog.listener.CatalogModificationListener;
import org.apache.flink.table.catalog.listener.CatalogModificationListenerFactory;

public class MyCatalogListenerFactory implements CatalogModificationListenerFactory {
    @Override
    public CatalogModificationListener createListener(Context context) {
        return new MyCatalogListener(""jdbc:mysql://hostname:3306/db?useSSL=false&connectTimeout=30000"", ""username"", ""password"");
    }

    @Override
    public String factoryIdentifier() {
        return ""test"";
    }
} {code}
{code:java}
package org.self.listener;

import org.apache.flink.table.catalog.listener.AlterDatabaseEvent;
import org.apache.flink.table.catalog.listener.AlterTableEvent;
import org.apache.flink.table.catalog.listener.CatalogModificationEvent;
import org.apache.flink.table.catalog.listener.CatalogModificationListener;
import org.apache.flink.table.catalog.listener.CreateDatabaseEvent;
import org.apache.flink.table.catalog.listener.CreateTableEvent;
import org.apache.flink.table.catalog.listener.DropDatabaseEvent;
import org.apache.flink.table.catalog.listener.DropTableEvent;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.util.stream.Collectors;

public class MyCatalogListener implements CatalogModificationListener {
    private final String jdbcUrl;
    private final String username;
    private final String password;

    public MyCatalogListener(String jdbcUrl, String username, String password) {
        this.jdbcUrl = jdbcUrl;
        this.username = username;
        this.password = password;
    }

    @Override
    public void onEvent(CatalogModificationEvent event) {
        try {
            Class.forName(getDriverClassName());

            try (Connection connection = DriverManager.getConnection(jdbcUrl, username, password);
                 PreparedStatement statement = connection.prepareStatement(""INSERT INTO `listener_test` (`catalog`,`identifier`,`type`,`detail`) VALUES (?,?,?,?);"")) {
                String identifier;
                String type;
                String detail;
                String catalog = event.context().getCatalogName();
                if (event instanceof CreateDatabaseEvent) {
                    CreateDatabaseEvent cde = (CreateDatabaseEvent) event;
                    identifier = ""DB:"" + cde.databaseName();
                    detail = cde.database().getProperties().entrySet().stream().map(e -> e.getKey() + "":"" + e.getValue()).collect(Collectors.joining("", ""));
                    type = ""CREATE DB"";
                } else if (event instanceof AlterDatabaseEvent) {
                    AlterDatabaseEvent ade = (AlterDatabaseEvent) event;
                    identifier = ""DB:"" + ade.databaseName();
                    detail = ade.newDatabase().getProperties().entrySet().stream().map(e -> e.getKey() + "":"" + e.getValue()).collect(Collectors.joining("", ""));
                    type = ""ALTER DB"";
                } else if (event instanceof DropDatabaseEvent) {
                    DropDatabaseEvent dde = (DropDatabaseEvent) event;
                    identifier = ""DB:"" + dde.databaseName();
                    detail = ""null"";
                    type = ""DELETE DB"";
                } else if (event instanceof CreateTableEvent) {
                    CreateTableEvent cte = (CreateTableEvent) event;
                    identifier = ""TBL:"" + cte.identifier().toString();
                    detail = cte.table().toString();
                    type = ""CREATE TBL"";
                } else if (event instanceof AlterTableEvent) {
                    AlterTableEvent ate = (AlterTableEvent) event;
                    identifier = ""TBL:"" + ate.identifier().toString();
                    detail = ate.newTable().toString();
                    type = ""ALTER TBL"";
                } else if (event instanceof DropTableEvent) {
                    DropTableEvent dte = (DropTableEvent) event;
                    identifier = ""TBL:"" + dte.identifier().toString();
                    detail = dte.table().toString();
                    type = ""DELETE TBL"";
                } else {
                    throw new IllegalArgumentException(""Unknown event type."");
                }
                statement.setObject(1, catalog);
                statement.setObject(2, identifier);
                statement.setObject(3, type);
                statement.setObject(4, detail);
                statement.execute();
            }
        } catch (Exception e) {
            throw new IllegalArgumentException(e);
        }
    }

    private String getDriverClassName() {
        try {
            Class.forName(""com.mysql.cj.jdbc.Driver"");
            return ""com.mysql.cj.jdbc.Driver"";
        } catch (ClassNotFoundException e) {
            return ""com.mysql.jdbc.Driver"";
        }
    }
} {code}
3.Add file `org.apache.flink.table.factories.Factory` to the resources.

4.Package the code and put it in `lib`.

5.Add `table.catalog-modification.listeners: test` to `flink-conf.yaml`.

6.Start sql client and test. The test result as follows.

!sqls.png!

!result.png!

It seems good when using by the sql client. And I think we should add the Step 3 and 4 to docs.

Does FLIP-294 support to provide parameters when creating listeners? If it is supported, I think we should describe how to provide parameters for the listener in docs.;;;","25/Aug/23 03:20;ruanhang1993;Hi, all. [~renqs] [~zjureel] 

I have modify the code to test using the configuration.
{code:java}
package org.self.listener;

import org.apache.flink.configuration.ConfigOption;
import org.apache.flink.configuration.ConfigOptions;
import org.apache.flink.configuration.ReadableConfig;
import org.apache.flink.table.catalog.listener.CatalogModificationListener;
import org.apache.flink.table.catalog.listener.CatalogModificationListenerFactory;

import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

public class MyCatalogListenerFactory implements CatalogModificationListenerFactory {

    public static final ConfigOption<String> URL =
            ConfigOptions.key(""my.catalog.listener.url"")
                    .stringType()
                    .noDefaultValue()
                    .withDescription(
                            ""JDBC url of the MySQL database to use when connecting to the MySQL database server."");

    public static final ConfigOption<String> USERNAME =
            ConfigOptions.key(""my.catalog.listener.username"")
                    .stringType()
                    .noDefaultValue()
                    .withDescription(
                            ""Username of the MySQL database to use when connecting to the MySQL database server."");

    public static final ConfigOption<String> PASSWORD =
            ConfigOptions.key(""my.catalog.listener.password"")
                    .stringType()
                    .noDefaultValue()
                    .withDescription(
                            ""Password to use when connecting to the MySQL database server."");
    @Override
    public CatalogModificationListener createListener(Context context) {
        ReadableConfig config = context.getConfiguration();
        return new MyCatalogListener(config.get(URL), config.get(USERNAME), config.get(PASSWORD));
    }

    @Override
    public Set<ConfigOption<?>> requiredOptions() {
        Set<ConfigOption<?>> requiredOptions = new HashSet<>();
        requiredOptions.add(URL);
        requiredOptions.add(USERNAME);
        requiredOptions.add(PASSWORD);
        return requiredOptions;
    }

    @Override
    public String factoryIdentifier() {
        return ""test"";
    }
} {code}
And add the parameters in `flink-conf.yaml`.
{code:java}
table.catalog-modification.listeners: test
my.catalog.listener.url: jdbc:mysql://hostname:3306/db?useSSL=false&connectTimeout=30000
my.catalog.listener.username: username
my.catalog.listener.password: password{code}
It works well.;;;","25/Aug/23 14:53;zjureel;[~ruanhang1993] Thanks for your suggestions and I will create an issue for the docs cc [~renqs];;;","31/Aug/23 02:12;ruanhang1993;[~renqs] [~zjureel] , I think we could complete this testing task. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-288: Enable Dynamic Partition Discovery by Default in Kafka Source,FLINK-32797,13546485,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,loserwang1024,renqs,renqs,08/Aug/23 07:46,18/Aug/23 04:11,04/Jun/24 20:40,18/Aug/23 04:11,1.18.0,,,,,,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 04:10:31 UTC 2023,,,,,,,,,,"0|z1jnfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 02:19;loserwang1024;It has been already done;;;","18/Aug/23 04:10;renqs;Closing as FLIP-288 is a Kafka connector related proposal, which has been externalized from Flink release cycle. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-295: Support lazy initialization of catalogs and persistence of catalog configurations,FLINK-32796,13546484,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,ferenc-csaky,renqs,renqs,08/Aug/23 07:46,10/Sep/23 21:37,04/Jun/24 20:40,10/Sep/23 21:37,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 18:44;ferenc-csaky;check_after_restart.png;https://issues.apache.org/jira/secure/attachment/13062429/check_after_restart.png","24/Aug/23 18:42;ferenc-csaky;create_catalog.png;https://issues.apache.org/jira/secure/attachment/13062428/create_catalog.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 10 21:37:42 UTC 2023,,,,,,,,,,"0|z1jnf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:54;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:54;renqs;[~hackergin]  Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","22/Aug/23 08:56;hackergin;[~renqs]  Sure, I'll add a documentation soon. ;;;","23/Aug/23 11:31;ferenc-csaky;[~renqs] I volunteer to test this feature, feel free to assign it to me.

Relevant docs: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/catalogs/#catalog-store;;;","24/Aug/23 01:58;renqs;[~ferenc-csaky] Thanks for volunteering! Assigned to you.;;;","24/Aug/23 18:46;ferenc-csaky;Added config to {{{}flink-conf.yaml{}}}:
{code:yaml}
table.catalog-store.kind: file
table.catalog-store.file.path: file:///tmp/test_catalog_store
{code}
Dummy catalog create via SQL client:
!create_catalog.png|width=642,height=1108!

Catalog store folder check:
{code:sh}
/tmp/test_catalog_store
% cat test.yaml
default-database: ""dummy""
type: ""generic_in_memory""
{code}
Verify persisted catalog in a new SQL client session:
!check_after_restart.png|width=574,height=1083!;;;","24/Aug/23 18:59;ferenc-csaky;I think the documentation can be improved, I plan to file a PR with suggestions tomorrow.

Another point worth to note IMO (plan to add this to the doc as well) is that, the current FILE catalog store impl expects the given catalog store path folder to exist and it does not create it on its own, hence if a non-existing (or inaccessible) path is passed, SQL client will fail to start and the SQL gateway will return an arror for session create. Reason for that is the external FS support, so the file catalog store can be placed to any supported distributed FS (hdfs, s3, etc.) and in those cases creating the catalog store root might fail.

Maybe it makes sense to not fail in all cases, but at least try to create the dirs to the given path if it does not exists at the catalog store open and only fail if it cannot be done. I added the exernal file system support, so I can open a PR with this improvement tomorrow.;;;","25/Aug/23 02:31;hackergin;[~ferenc-csaky] Thank you for verifying this feature. I also believe that a reminder should be added regarding the default behavior of FileCatalogStore. Additionally, I think it is reasonable to throw an exception if the path does not exist and fails to be created.;;;","25/Aug/23 15:44;ferenc-csaky;[~hackergin] I created 2 PRs linked to this issue, if you have some time can you check them and see if you think it is okay? Thanks!;;;","06/Sep/23 08:21;Sergey Nuyanzin;[~ferenc-csaky] thanks for checking. One more question to double check: am I right that it works ok and there is no more findings except 2 mentioned?

[~hackergin] could you please have a look at the PRs submitted by [~ferenc-csaky]

;;;","06/Sep/23 08:52;ferenc-csaky;[~Sergey Nuyanzin] Correct! [~hackergin] I missed your last response on the doc PR, so I just addressed that, can you pls. check it again? Thanks!;;;","06/Sep/23 09:28;ferenc-csaky;Added PRs against master: [https://github.com/apache/flink/pull/23366,] [https://github.com/apache/flink/pull/23367] ;;;","10/Sep/23 21:36;Sergey Nuyanzin;Merged to master [034a60a60897c0cf9a06abb3dfc271f8ab21f374|https://github.com/apache/flink/commit/034a60a60897c0cf9a06abb3dfc271f8ab21f374]
and [9a94891156f2cd8070df48d75a4f481dc0a8b962|https://github.com/apache/flink/commit/9a94891156f2cd8070df48d75a4f481dc0a8b962]

merged to 1.18: [97d116645e3048af21475280b46ae54bd3efc731|https://github.com/apache/flink/commit/97d116645e3048af21475280b46ae54bd3efc731]
and [184cfcdb0caa08ad683ee9ac3c315e80a192b93d|https://github.com/apache/flink/commit/184cfcdb0caa08ad683ee9ac3c315e80a192b93d];;;","10/Sep/23 21:37;Sergey Nuyanzin;Since mentioned PRs are merged and it was confirmed that everything else was finished I'm going to close this issue.
Feel free to reopen if I miss anything;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-279 Unified the max display column width for SqlClient and Table APi in both Streaming and Batch execMode,FLINK-32795,13546483,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,jingge,renqs,renqs,08/Aug/23 07:44,22/Aug/23 07:07,04/Jun/24 20:40,22/Aug/23 07:07,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 22 07:07:35 UTC 2023,,,,,,,,,,"0|z1jnew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 07:07;jingge;duplicated with https://issues.apache.org/jira/browse/FLINK-32906;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-293: Introduce Flink Jdbc Driver For Sql Gateway,FLINK-32794,13546482,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiangyu0xf,renqs,renqs,08/Aug/23 07:43,06/Sep/23 09:22,04/Jun/24 20:40,06/Sep/23 09:22,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,Document for jdbc driver: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/jdbcdriver/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33016,,,,,,,,"04/Sep/23 07:33;xiangyu0xf;image-2023-09-04-15-33-33-074.png;https://issues.apache.org/jira/secure/attachment/13062683/image-2023-09-04-15-33-33-074.png","04/Sep/23 07:48;xiangyu0xf;image-2023-09-04-15-48-19-665.png;https://issues.apache.org/jira/secure/attachment/13062684/image-2023-09-04-15-48-19-665.png","04/Sep/23 07:51;xiangyu0xf;image-2023-09-04-15-51-17-161.png;https://issues.apache.org/jira/secure/attachment/13062685/image-2023-09-04-15-51-17-161.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 09:22:32 UTC 2023,,,,,,,,,,"0|z1jneo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:53;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:54;renqs;[~zjureel] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","23/Aug/23 02:31;zjureel;Thanks [~renqs], DONE;;;","23/Aug/23 06:41;xiangyu0xf;[~renqs] Hello, I would like to take this issue.;;;","23/Aug/23 06:44;renqs;[~xiangyu0xf] Thanks for taking the issue! Assigned to you just now;;;","31/Aug/23 09:03;renqs;[~xiangyu0xf] Any updates on this one? Thanks;;;","31/Aug/23 09:42;xiangyu0xf;[~renqs] Hi Qingsheng, I'm working on this now.;;;","04/Sep/23 07:33;xiangyu0xf;Verified by using with SqlLine

!image-2023-09-04-15-33-33-074.png|width=726,height=615!;;;","04/Sep/23 07:48;xiangyu0xf;Verified by using with Java application
!image-2023-09-04-15-48-19-665.png|width=818,height=403!;;;","04/Sep/23 07:51;xiangyu0xf;Verified using the Datasource Connection

!image-2023-09-04-15-51-17-161.png|width=1017,height=540!;;;","04/Sep/23 07:55;xiangyu0xf;Hi [~renqs], I have verified this feature by using with both jdbc tool(SqlLine) and java application.;;;","06/Sep/23 08:19;Sergey Nuyanzin;Thanks for checking [~xiangyu0xf]

[~xiangyu0xf], [~renqs] could you please confirm that it was tested and there is no open issues
I'm asking since if that's true I'm going to close it as done;;;","06/Sep/23 09:18;xiangyu0xf;[~Sergey Nuyanzin] Hi Sergey, IMHO it's ok to confirm that this feature was tested.;;;","06/Sep/23 09:22;Sergey Nuyanzin;thanks for the response then let's close it;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-31535 - Extend watermark-related features for SQL,FLINK-32793,13546481,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catyee,renqs,renqs,08/Aug/23 07:43,22/Aug/23 06:17,04/Jun/24 20:40,22/Aug/23 06:17,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 07:56;catyee;image-2023-08-21-15-56-38-669.png;https://issues.apache.org/jira/secure/attachment/13062307/image-2023-08-21-15-56-38-669.png","21/Aug/23 07:58;catyee;image-2023-08-21-15-58-16-215.png;https://issues.apache.org/jira/secure/attachment/13062308/image-2023-08-21-15-58-16-215.png","21/Aug/23 08:01;catyee;image-2023-08-21-16-01-15-515.png;https://issues.apache.org/jira/secure/attachment/13062310/image-2023-08-21-16-01-15-515.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 08:02:05 UTC 2023,,,,,,,,,,"0|z1jneg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 08:02;catyee;Hi, all.

I have tested this feature and it looks good.

 

the fast_source options:

'scan.watermark.emit.strategy'='on-event',
'scan.watermark.alignment.group'='alignment-group-1', 
'scan.watermark.alignment.max-drift'='1min'

the slow_source options:

'scan.watermark.emit.strategy'='on-periodic',
'scan.watermark.alignment.group'='alignment-group-1', 
'scan.watermark.alignment.max-drift'='1min'

 

The Execution Plan(with table options):

!image-2023-08-21-15-56-38-669.png|width=1021,height=195!

The Execution Plan(with OPTIONS hint):

!image-2023-08-21-15-58-16-215.png|width=1002,height=210!

The Metrics:

!image-2023-08-21-16-01-15-515.png|width=1010,height=164!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-27237 - Partitioned table statement enhancement,FLINK-32792,13546480,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,renqs,renqs,08/Aug/23 07:41,11/Aug/23 12:17,04/Jun/24 20:40,11/Aug/23 12:17,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/23 09:40;ruanhang1993;add.png;https://issues.apache.org/jira/secure/attachment/13062056/add.png","11/Aug/23 09:44;ruanhang1993;drop.png;https://issues.apache.org/jira/secure/attachment/13062058/drop.png","11/Aug/23 09:40;ruanhang1993;show.png;https://issues.apache.org/jira/secure/attachment/13062057/show.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 12:16:45 UTC 2023,,,,,,,,,,"0|z1jne8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 01:46;luoyuxia;Verify add partitions/show partitions/drop partitions. Please see doc:

[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/alter/#add]

[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/alter/#drop]

[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/alter/#rename]

https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/show/#show-partitions;;;","11/Aug/23 09:42;ruanhang1993;`show partitions` succeed.

!show.png|width=642,height=344!

'alter add partition' succeed.

!add.png|width=635,height=226!;;;","11/Aug/23 09:48;ruanhang1993;`drop partition` :
 * drop single partition succeed.
 * drop two partitions failed. There are two places to consider:
 ** The first partition is actually dropped.
 ** These partitions can be dropped one by one. Not sure whether there are some problems in my tests.

!drop.png|width=879,height=500!;;;","11/Aug/23 12:16;luoyuxia;[~ruanhang1993] Thanks for verfying. I'd like to say it seems an issue of HiveCatalog, instead of the the framework. So, I'd like to mark it as verified.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-308: Support Time Travel,FLINK-32791,13546479,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,renqs,renqs,08/Aug/23 07:41,18/Aug/23 07:02,04/Jun/24 20:40,18/Aug/23 07:02,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 06:55;tartarus;image-2023-08-18-14-55-18-883.png;https://issues.apache.org/jira/secure/attachment/13062262/image-2023-08-18-14-55-18-883.png","18/Aug/23 06:55;tartarus;image-2023-08-18-14-55-55-817.png;https://issues.apache.org/jira/secure/attachment/13062263/image-2023-08-18-14-55-55-817.png","18/Aug/23 06:56;tartarus;image-2023-08-18-14-56-19-198.png;https://issues.apache.org/jira/secure/attachment/13062264/image-2023-08-18-14-56-19-198.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 07:02:00 UTC 2023,,,,,,,,,,"0|z1jne0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 17:55;hackergin;A test process that can be used as a reference. Test time travel with apache-paimon   

https://docs.google.com/document/d/1Z8gz5Gc4dTNkKtbCrWk-isDReBQvPu0kJ8yQ-wU1uE8/;;;","18/Aug/23 06:59;tartarus;Verified the Time Travel with paimon catalog;

Write the data with the following SQL:

 
{code:java}
insert into word_count select 'hello1', 1;
insert into word_count select 'hello2', 1;
insert into word_count select 'hello3', 1;
insert into word_count select 'hello4', 1;
{code}
 

After a while insert the data again:
{code:java}
insert into word_count values ('hello1', 2), ('hello2', 3), ('hello3', 4), ('hello4', 2); {code}
 

!image-2023-08-18-14-55-18-883.png!

 

Flink SQL> select * from word_count FOR SYSTEM_TIME AS OF TIMESTAMP '2023-08-18 14:40:00';

!image-2023-08-18-14-55-55-817.png!

 

Flink SQL> select * from word_count FOR SYSTEM_TIME AS OF TIMESTAMP '2023-08-18 14:50:00';

!image-2023-08-18-14-56-19-198.png!;;;","18/Aug/23 07:02;luoyuxia;[~tartarus] Thanks for verifying.. Closing...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-311: Support Call Stored Procedure,FLINK-32790,13546478,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,renqs,renqs,08/Aug/23 07:39,16/Aug/23 03:00,04/Jun/24 20:40,16/Aug/23 03:00,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 07:23;hackergin;截屏2023-08-15 15.23.15.png;https://issues.apache.org/jira/secure/attachment/13062201/%E6%88%AA%E5%B1%8F2023-08-15+15.23.15.png","15/Aug/23 07:23;hackergin;截屏2023-08-15 15.23.37.png;https://issues.apache.org/jira/secure/attachment/13062202/%E6%88%AA%E5%B1%8F2023-08-15+15.23.37.png","15/Aug/23 07:23;hackergin;截屏2023-08-15 15.23.54.png;https://issues.apache.org/jira/secure/attachment/13062203/%E6%88%AA%E5%B1%8F2023-08-15+15.23.54.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 03:00:58 UTC 2023,,,,,,,,,,"0|z1jnds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 07:26;hackergin;Test call procedures with paimon catalog. 

I implementd a compact prcedure for paimon catalog. 

https://github.com/apache/incubator-paimon/commit/f28469bd10a03186b922c4d77224448b5a9bf1b5

 !截屏2023-08-15 15.23.15.png! 


 !截屏2023-08-15 15.23.37.png! 


 !截屏2023-08-15 15.23.54.png! ;;;","16/Aug/23 03:00;luoyuxia;[~hackergin] Thanks for helping. Closing...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-303: Support REPLACE TABLE AS SELECT statement,FLINK-32789,13546477,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,renqs,renqs,08/Aug/23 07:39,15/Aug/23 03:16,04/Jun/24 20:40,15/Aug/23 03:16,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 02:59;hackergin;截屏2023-08-15 10.59.35.png;https://issues.apache.org/jira/secure/attachment/13062196/%E6%88%AA%E5%B1%8F2023-08-15+10.59.35.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 03:16:42 UTC 2023,,,,,,,,,,"0|z1jndk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 03:01;hackergin;Verified the RTAS with paimon catalog;


 !截屏2023-08-15 10.59.35.png! ;;;","15/Aug/23 03:16;luoyuxia;[~hackergin] Thanks for help. Closing...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpeculativeScheduler do not handle allocate task errors when schedule speculative tasks may causes resource leakage.,FLINK-32788,13546476,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,08/Aug/23 07:38,18/Aug/23 08:59,04/Jun/24 20:40,18/Aug/23 08:59,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"When the SpeculativeScheduler allocates slots for speculative tasks, exceptions may occur, but there is currently no exception handling mechanism in place. This can lead to resource leakage (such as FLINK-32768) when errors occur. In such cases, a fatalError should be triggered.",,,,,,,,,,,FLINK-32876,,,,,,,,,,,,,,,,,,,,,,FLINK-32768,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 08:59:23 UTC 2023,,,,,,,,,,"0|z1jndc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 08:59;zhuzh;master/1.18: b93216f785526605e61c280273196aca717ac7c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-305: Support atomic for CREATE TABLE AS SELECT(CTAS) statement,FLINK-32787,13546475,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,renqs,renqs,08/Aug/23 07:38,07/Sep/23 01:13,04/Jun/24 20:40,07/Sep/23 01:13,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/23 11:20;hackergin;image-2023-08-22-19-20-23-490.png;https://issues.apache.org/jira/secure/attachment/13062353/image-2023-08-22-19-20-23-490.png","30/Aug/23 11:28;hackergin;image-2023-08-30-19-28-28-420.png;https://issues.apache.org/jira/secure/attachment/13062599/image-2023-08-30-19-28-28-420.png","30/Aug/23 11:28;hackergin;image-2023-08-30-19-28-36-709.png;https://issues.apache.org/jira/secure/attachment/13062600/image-2023-08-30-19-28-36-709.png","30/Aug/23 11:28;hackergin;image-2023-08-30-19-28-44-721.png;https://issues.apache.org/jira/secure/attachment/13062601/image-2023-08-30-19-28-44-721.png","06/Sep/23 10:56;hackergin;image-2023-09-06-18-55-59-095.png;https://issues.apache.org/jira/secure/attachment/13062722/image-2023-09-06-18-55-59-095.png","06/Sep/23 10:56;hackergin;image-2023-09-06-18-56-15-596.png;https://issues.apache.org/jira/secure/attachment/13062723/image-2023-09-06-18-56-15-596.png","30/Aug/23 11:25;hackergin;截屏2023-08-30 19.24.57.png;https://issues.apache.org/jira/secure/attachment/13062598/%E6%88%AA%E5%B1%8F2023-08-30+19.24.57.png",,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 13:09:34 UTC 2023,,,,,,,,,,"0|z1jnd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 11:20;hackergin;The error message ""CTAS + paimon"" occurs when performing atomicity testing. The logic behind getSupportsStagingDynamicTableSink is based on creating a DynamicTableSink using the options specified in the CREATE TABLE statement. However, paimon has a special case where options are not required in the CREATE TABLE statement. But when calling getTable, it automatically adds the path parameter, which causes an error because the DynamicTableSink is created without this path parameter.

 

!image-2023-08-22-19-20-23-490.png!;;;","22/Aug/23 11:57;hackergin;Here, an exception is thrown directly, and the user may not be clear about the reason. We can catch the exception and use non-atomic CTAS, or we can prompt the user to disable atomicity.;;;","30/Aug/23 11:27;hackergin;I have implemented CTAS in the filesystem connector, and it meets expectations overall.

[https://github.com/hackergin/flink/commit/6068433e714ced69b18c0f09d49ad9ca3dbd7b80]

 

!截屏2023-08-30 19.24.57.png!

 

The relevant files can also be written normally.

 

 ;;;","30/Aug/23 11:29;hackergin;Also， I found a problem when use CTAS with Filesystem connector. 

 

When executing CTAS, the original file is not overwritten.  I think this doesn't meet expectations, and we should fix this issue.

!image-2023-08-30-19-28-28-420.png!

!image-2023-08-30-19-28-36-709.png!

 

!image-2023-08-30-19-28-44-721.png!;;;","06/Sep/23 02:56;luoyuxia;[~hackergin] Thanks for verifying...And sorry for late reply.

If that's the atmoic case, I'd like to suggest to you to clear the paths in commit method firstly and then commit the tmp files to the path. So that we can make sure it won't consider the old files.

If that's not, I think it may be the case that after drop the table, the files actually  is not be deleted and with the existing old files, the rtas just create a new table in the existing directory and it won't overwrite the old files. I'd like to suggest to  delete the old files before execute the \{{REPLACE TABLE }} statement to see what will happen.

cc [~tartarus] ;;;","06/Sep/23 10:56;hackergin;[~luoyuxia] 

 

Currently, this issue is encountered in non-atomic scenarios. From the code, it can be seen that the table will first be dropped. Since my test involves a temporary table, dropping the table does not delete the actual file.

!image-2023-09-06-18-55-59-095.png!

 

When constructing DataStreamSink, overwrite is not enabled by default, so the original files are not cleared.

In RTAS scenarios, it is reasonable to not enable overwrite by default, so this behavior should meet expectations.

!image-2023-09-06-18-56-15-596.png!

 

However, to avoid user confusion, I suggest documenting this behavior in the documentation.

In the future, I think we can also add relevant configurations for users to control whether RTAS should use overwrite.

 

cc [~tartarus] 

 ;;;","06/Sep/23 13:09;tartarus;[~hackergin]  thanks for verifying~

I think there are two issues that need to be addressed:
 * When atomicity is enabled, an exception may occur when creating a DynamicTableSink, and we need to prompt the user to disable atomicity.
 * When we use InMemoryCatalog, RTAS drop table will only delete the metadata, not clean up the underlying data files, RTAS write data does not use overwrite semantics by default, so it looks like the data is duplicated, this problem needs to be clarified in the documentation.

I'll fix it with FLINK-33050.

 

cc [~luoyuxia] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-302: Support TRUNCATE TABLE statement in batch mode,FLINK-32786,13546474,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,renqs,renqs,08/Aug/23 07:37,14/Aug/23 03:06,04/Jun/24 20:40,14/Aug/23 03:06,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 03:03;ruanhang1993;after.png;https://issues.apache.org/jira/secure/attachment/13062167/after.png","14/Aug/23 03:03;ruanhang1993;before.png;https://issues.apache.org/jira/secure/attachment/13062168/before.png","14/Aug/23 03:03;ruanhang1993;truncate.png;https://issues.apache.org/jira/secure/attachment/13062169/truncate.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 03:06:47 UTC 2023,,,,,,,,,,"0|z1jncw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 01:54;luoyuxia;First try to implement for a table sink  {{{}[SupportsTruncate|https://github.com/apache/flink/blob/dfb9cb851dc1f0908ea6c3ce1230dd8ca2b48733/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/sink/abilities/SupportsTruncate.java#L33]{}}},   We can implement a simple sink table or reuse some other sink table.

Then use truncate statement  to truncate the table.

please see doc: 

 ;;;","14/Aug/23 03:02;ruanhang1993;I have implemented the interface in JDBC connector for MySQL. See my commit([https://github.com/ruanhang1993/flink-connector-jdbc/commit/06470ce4f0c899b9369bb40ee9bb8f410f8b4db2]).

Before truncate:

!before.png!

After truncate:

!after.png!

The test sql:

!truncate.png!

[~luoyuxia] All tests passed.;;;","14/Aug/23 03:06;luoyuxia;[~ruanhang1993] Thanks for helping verifing. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-292: Enhance COMPILED PLAN to support operator-level state TTL configuration,FLINK-32785,13546473,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,renqs,renqs,08/Aug/23 07:36,06/Sep/23 09:21,04/Jun/24 20:40,06/Sep/23 09:21,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 09:21:19 UTC 2023,,,,,,,,,,"0|z1jnco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 09:03;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 09:03;renqs;[~qingyue]  Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","23/Aug/23 02:49;qingyue;This ticket aims to verify FLINK-31791: Enhance COMPILED PLAN to support operator-level state TTL configuration.

More details about this feature and how to use it can be found in this [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/overview/#configure-operator-level-state-ttl]. The verification steps are as follows.
h3. Part I: Functionality Verification

1. Start the standalone session cluster and sql client.

2. Execute the following DDL statements.
{code:sql}
CREATE TABLE `default_catalog`.`default_database`.`Orders` (
  `order_id` INT,
  `line_order_id` INT
) WITH (
  'connector' = 'datagen', 
  'rows-per-second' = '5'

); 

CREATE TABLE `default_catalog`.`default_database`.`LineOrders` (
  `line_order_id` INT,
  `ship_mode` STRING
) WITH (
  'connector' = 'datagen',
  'rows-per-second' = '5'
);

CREATE TABLE `default_catalog`.`default_database`.`OrdersShipInfo` (
  `order_id` INT,
  `line_order_id` INT,
  `ship_mode` STRING ) WITH (
  'connector' = 'print'
);
{code}
 
3. Generate Compiled Plan
{code:sql}
COMPILE PLAN '/path/to/plan.json' FOR
INSERT INTO OrdersShipInfo 
SELECT a.order_id, a.line_order_id, b.ship_mode 
FROM Orders a JOIN LineOrders b ON a.line_order_id = b.line_order_id;
{code}
 

4. Verify JSON plan content
The generated JSON file should contain the following ""state"" JSON array for StreamJoin ExecNode.
{code:json}
{
    ""id"" : 5,
    ""type"" : ""stream-exec-join_1"",
    ""joinSpec"" : {
      ...
    },
    ""state"" : [ {
      ""index"" : 0,
      ""ttl"" : ""0 ms"",
      ""name"" : ""leftState""
    }, {
      ""index"" : 1,
      ""ttl"" : ""0 ms"",
      ""name"" : ""rightState""
    } ],
    ""inputProperties"": [...],
    ""outputType"": ...,
    ""description"": ...
}
{code}
h3. Part II: Compatibility Verification

Repeat the previously described steps using the flink-1.17 release, and then execute the generated plan using 1.18 via
{code:sql}
EXECUTE PLAN '/path/to/plan-generated-by-old-flink-version.json'
{code}
 ;;;","24/Aug/23 09:21;Sergey Nuyanzin;[~renqs], [~qingyue] 
I can take it if there is no objections
;;;","24/Aug/23 10:00;renqs;[~snuyanzin] Thanks for volunteering! Assigned to you just now.;;;","27/Aug/23 16:24;Sergey Nuyanzin;Checked plan in 1.18: it contains {{TTL}} as mentioned above.
Compiled in 1.17 and then executed in 1.18: the job was successfully running in 1.18

However there is an issue that after about 1 min 30 sec the job fails with 
{noformat}
Caused by: java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id localhost:45987-ac10bb timed out.
	... 30 more
{noformat}

I checked it with 1.17.1, 1.16 and it is reproduced there as well, so probably it is not related to current change

UPD: probably the reason is not enough resources for tasks
in 1.16 it fails as
{noformat}
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:227)
	... 39 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	... 37 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
{noformat};;;","29/Aug/23 09:49;qingyue;Thanks, [~snuyanzin], for the thorough test.

From the stacktrace, the issue might not be relevant to the change, but anyway, I'll look at the problem ASAP.;;;","31/Aug/23 03:08;qingyue;Hi, [~snuyanzin], 
This issue is because the test step description is not accurate enough. The datagen connector, by default, emits 10,000 records per second. If the default state backend type is hashmap and the table.exec.state.ttl is not set, it quickly leads to an OOM. I've updated the test procedure description; please let me know if there are any other issues.
 
 
 ;;;","31/Aug/23 22:47;Sergey Nuyanzin;Hi [~qingyue]
thanks for your response
I gave another iteration with your renewed test description
Now I can confirm that it works without issues;;;","01/Sep/23 08:12;Sergey Nuyanzin;Please let me know if anything else is required or we can close it;;;","06/Sep/23 09:17;qingyue;Hi [~Sergey Nuyanzin], Thanks for your work, and we can close it.;;;","06/Sep/23 09:21;Sergey Nuyanzin;great, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-300 Add targetColumns to DynamicTableSink#Context,FLINK-32784,13546472,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,renqs,renqs,08/Aug/23 07:35,18/Aug/23 09:25,04/Jun/24 20:40,18/Aug/23 09:25,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 09:24:44 UTC 2023,,,,,,,,,,"0|z1jncg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 09:24;lincoln.86xy;This api change is for connector developers, and been verified by internal test values table sink on partial insertion scenario.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Improve parallel download of RocksDB incremental state,FLINK-32783,13546471,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,renqs,renqs,08/Aug/23 07:34,06/Sep/23 08:31,04/Jun/24 20:40,06/Sep/23 08:31,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,"This feature is automatically used whenever we download state during a restart from a RocksDB incremental checkpoint. This should be tested with and without task-local recovery.

Will be covered by the nightly tests:

*     run_test ""Resuming Externalized Checkpoint (rocks, incremental, no parallelism change) end-to-end test"" ""$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 rocks true true"" ""skip_check_exceptions""
*     run_test ""Resuming Externalized Checkpoint (rocks, incremental, scale up) end-to-end test"" ""$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 4 rocks true true"" ""skip_check_exceptions""
*     run_test ""Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test"" ""$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 4 2 rocks true true"" ""skip_check_exceptions""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 08:31:16 UTC 2023,,,,,,,,,,"0|z1jnc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:50;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:50;renqs;[~srichter] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","04/Sep/23 04:07;yunta;Why do we need to test this internal improvement manually? From my understanding, this is not a FLIP and not so visible to users.

If we stick to adding such a release testing, I could help to verify this.;;;","05/Sep/23 07:45;knaufk;I don't think we need to test it after talking to [~srichter]. The correctness is verified by our nightly tests. If someone wants to verify the performance improvements, we could leave it open, but I don't think this is strictly necessary.;;;","05/Sep/23 09:14;yunta;+1 for [~knaufk]'s advice, let's close this ticket [~renqs].;;;","06/Sep/23 08:31;Sergey Nuyanzin;Thanks for the advice [~knaufk]
I'm going to close it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing:  Disable WAL in RocksDBWriteBatchWrapper by default,FLINK-32782,13546470,13545663,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,renqs,renqs,08/Aug/23 07:33,06/Sep/23 08:30,04/Jun/24 20:40,06/Sep/23 08:30,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,"Covered by nightly tests, for example 

- run_test ""Resuming Externalized Checkpoint (rocks, incremental, no parallelism change) end-to-end test"" ""$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 rocks true true"" ""skip_check_exceptions""
- run_test ""Resuming Externalized Checkpoint (rocks, incremental, scale up) end-to-end test"" ""$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 4 rocks true true"" ""skip_check_exceptions""
- run_test ""Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test"" ""$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 4 2 rocks true true"" ""skip_check_exceptions""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 08:29:51 UTC 2023,,,,,,,,,,"0|z1jnc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:49;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:49;renqs;[~srichter] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","31/Aug/23 09:13;renqs;[~srichter] IIUC this feature is not visible to end users, so we don't need to test it manually right? ;;;","04/Sep/23 04:08;yunta;Same as I raised in FLINK-32783, why do we need to test this internal improvement manually?;;;","05/Sep/23 07:45;knaufk;I don't think we need to test it after talking to [~srichter]. The correctness is verified by our nightly tests. If someone wants to verify the performance improvements, we could leave it open, but I don't think this is strictly necessary.;;;","05/Sep/23 09:14;yunta;+1 for [~knaufk]'s advice, let's close this ticket [~renqs].;;;","06/Sep/23 08:29;Sergey Nuyanzin;Thanks for the advice [~knaufk]
I'm going to close it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Add a metric for back-pressure from the ChangelogStateBackend,FLINK-32781,13546469,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,renqs,renqs,08/Aug/23 07:33,31/Aug/23 03:11,04/Jun/24 20:40,31/Aug/23 03:11,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,"The back-pressure from ChangelogStateBackend is reported as [`changelogBusyTimeMsPerSecond`,|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/#io]  its value should be 0 if the changelog is not enabled by default, otherwise it should be a non-negative value. This metric can be seen in the metric tab in flink web ui of any job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 09:38;Yanfei Lei;image-2023-08-21-17-38-56-927.png;https://issues.apache.org/jira/secure/attachment/13062312/image-2023-08-21-17-38-56-927.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 03:11:14 UTC 2023,,,,,,,,,,"0|z1jnbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 09:43;Yanfei Lei;Test `changelogBusyTimeMsPerSecond` metric with standalone cluster, `changelogBusyTimeMsPerSecond`  is shown under Metrics tab as expected.

!image-2023-08-21-17-38-56-927.png!;;;","22/Aug/23 09:02;renqs;[~Yanfei Lei] Thanks for verifying. Could you find someone else to make a x-verification as you are the original author of the feature? Also it'll be nice to provide an instruction about how to verify the feature in the ""Description"" field. Thanks! ;;;","22/Aug/23 09:25;Yanfei Lei;[~masteryhx] Could you help with a x-verification?;;;","22/Aug/23 09:32;masteryhx;[~Yanfei Lei] Sure, I will help to verify this.;;;","31/Aug/23 03:11;masteryhx;Verified the metric in UI, just as [~Yanfei Lei] shows.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-324: Introduce Runtime Filter for Flink Batch Jobs,FLINK-32780,13546468,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,renqs,renqs,08/Aug/23 07:31,29/Aug/23 02:40,04/Jun/24 20:40,28/Aug/23 12:51,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,"This issue aims to verify FLIP-324: https://cwiki.apache.org/confluence/display/FLINK/FLIP-324%3A+Introduce+Runtime+Filter+for+Flink+Batch+Jobs

We can enable runtime filter by set: table.optimizer.runtime-filter.enabled: true

1. Create two tables, one small table (small amount of data), one large table (large amount of data), and then run join query on these two tables(such as the example in FLIP doc: SELECT * FROM fact, dim WHERE x = a AND z = 2). The Flink table planner should be able to obtain the statistical information of these two tables (for example, Hive table), and the data volume of the small table should be less than ""table.optimizer.runtime-filter.max-build-data-size"", and the data volume of the large table should be larger than ""table.optimizer.runtime-filter.min-probe-data-size"".

2. Show the plan of the join query. The plan should include nodes such as LocalRuntimeFilterBuilder, GlobalRuntimeFilterBuilder and RuntimeFilter. We can also verify plan for the various variants of above query.

3. Execute the above plan, and: 
* Check whether the data in the large table has been successfully filtered  
* Verify the execution result, the execution result should be same with the execution plan which disable runtime filter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/23 12:50;lsy;image-2023-08-28-20-50-26-687.png;https://issues.apache.org/jira/secure/attachment/13062521/image-2023-08-28-20-50-26-687.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 12:49:33 UTC 2023,,,,,,,,,,"0|z1jnbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:48;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:49;renqs;[~wanglijie] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","28/Aug/23 12:49;lsy;RuntimeFilter was validated against the TPC-DS test set. Enabling the table.optimizer.runtime-filter.enabled option. The following validations were done separately:
1. verified the plan of the query, and confirmed that many queries were inserted into the RuntimeFilter through the plan.
2. For the whole TPC-DS dataset, before enabling runtime, total time is 5141s, after is 4883s, the gain of RuntimeFilter is 5%, the queries with significant gain are q88(140s -> 62s),q93(117s -> 101s), q95(218s -> 70s), other queries with limited gain.
3. q24, q72 showed performance regression, especially q72. By checking the plan of q72, compared with not turning on RuntimeFilter, we found that there is a dependency between the upstream and downstream operators, which leads to the source node not being able to be executed in parallel, thus leading to the performance regression. I don't think we should insert the RuntimeFilter operator for this pattern because it doesn't filter the amount of data that the Join operator needs to process by itself.

!image-2023-08-28-20-50-26-687.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-301: Hybrid Shuffle supports Remote Storage,FLINK-32779,13546467,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Weijie Guo,renqs,renqs,08/Aug/23 07:31,29/Aug/23 12:08,04/Jun/24 20:40,29/Aug/23 12:08,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,,,,"This ticket aims to verify https://issues.apache.org/jira/browse/FLINK-31634.

This verification mainly contains two parts.

Part 1. Run without remote storage. 
This part mainly is to verify the new mode can use the Memory tier and Disk tier dynamically when shuffling.
Set the mode to new hybrid shuffle mode(execution.batch-shuffle-mode: ALL_EXCHANGES_HYBRID_SELECTIVE), and run a simple job. For example(tpcds q1.sql). When the resource is enough, then the upstream and the downstream can run at the same time.

Part2. Run with remote storage.
This part mainly is to verify the new mode can use the Memory tier, Disk tier, Remote tier dynamically when shuffling.
   2.1 Set the mode to new hybrid shuffle mode(execution.batch-shuffle-mode: ALL_EXCHANGES_HYBRID_SELECTIVE)
   2.2 set the remote storage path with the option(taskmanager.network.hybrid-shuffle.remote.path: oss://flink-runtime/runtime/shuffle, note that the path oss://flink-runtime/runtime/shuffle in oss should be exist).
   2.3 Modify the 
 option TieredStorageConfiguration#DEFAULT_MIN_RESERVE_DISK_SPACE_FRACTION to 1, compile the package, then run a simple job. For example(tpcds q1.sql).  Check the shuffle data is written to the remote storage in the path oss://flink-runtime/runtime/shuffle.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 10:42:42 UTC 2023,,,,,,,,,,"0|z1jnbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 08:47;renqs;Unassigning the issue from the original author of the new feature. Please re-assign the ticket to the actual person doing the test.;;;","22/Aug/23 08:48;renqs;[~tanyuxin] Could you provide some instructions or link to the documentation in the ""Description"" field about how to verify the feature? Thanks!;;;","22/Aug/23 09:25;tanyuxin;[~renqs] Thanks for adding the tests. I will add some instructions and drive the tests.;;;","29/Aug/23 10:42;Weijie Guo;Thank you for the detailed testing instructions.
I took the following steps:

1. Modifying the option {{TieredStorageConfiguration#DEFAULT_MIN_RESERVE_DISK_SPACE_FRACTION}} to 1 and re-compile the flink dist. 
2. Filling in OSS related configuration options in {{flink-conf.yaml}}.
3.Submitting tpc-ds q1 in yarn cluster.

Everything seems to be doing well, and in line with my expectations.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream join data output sequence is inconsistent with input sequence,FLINK-32778,13546453,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,simen,simen,08/Aug/23 04:26,04/Dec/23 21:04,04/Jun/24 20:40,,1.16.1,,,,,1.7.3,,,,Table SQL / Runtime,,,,,,0,,,,"{code:sql}

-- flink version:1.16.1
-- parallelism.default: 1

CREATE TABLE s1(
    id string,
    gk bigint,
    price int
 )WITH(
    'connector' = 'kafka'
    ,'properties.bootstrap.servers' = 'xx:9092'
    ,'properties.group.id' = 'xx-xx'
    ,'scan.startup.mode' = 'earliest-offset'
    ,'value.format' = 'json'
    ,'topic' = 'topic1'
 );

CREATE TABLE s2(
    id string,
    name string
 )WITH(
    'connector' = 'kafka'
    ,'properties.bootstrap.servers' = 'xx:9092'
    ,'properties.group.id' = 'xx-xx'
    ,'scan.startup.mode' = 'earliest-offset'
    ,'value.format' = 'json'
    ,'topic' = 'topic2'
 );

create table sink(
    id string,
    name string,
    gk bigint,
    price int
 )with(
    'connector'='print'
 );

create view v1 as select
    id,
    gk,
    last_value(price) price
from s1
group by id,gk;

insert into sink
select
    v1.id,
    s2.name,
    v1.gk,
    v1.price
from v1
left join s2 on v1.id=s2.id;

-- 1.Enter two pieces of data into the topic1 topic:
-- {""id"":""1"",""gk"":758,""price"":100}
-- {""id"":""1"",""gk"":1818,""price"":200}

-- The output is as follows:
-- +I[1, null, 758, 100]
-- +I[1, null, 1818, 200]

-- 2.Enter two pieces of data into the topic2 topic:
-- {""id"":1,""name"":""z3""}

-- The output is as follows:
-- -D[1, null, 1818, 200]
-- -D[1, null, 758, 100]
-- +I[1, z3, 1818, 200]
-- +I[1, z3, 758, 100]

-- My doubt is that the output should be in the order of input , like below:
-- -D[1, null, 758, 100]
-- -D[1, null, 1818, 200]
-- +I[1, z3, 758, 100]
-- +I[1, z3, 1818, 200]

-- 3.When I re-run the above sql, the results are output in the order of input
-- +I[1, z3, 758, 100]
-- +I[1, z3, 1818, 200]

-- Is there a way to control this uncertainty？

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31729,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-08 04:26:13.0,,,,,,,,,,"0|z1jn88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Okhttp version to support IPV6,FLINK-32777,13546447,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,08/Aug/23 03:21,12/Sep/23 15:32,04/Jun/24 20:40,12/Sep/23 15:32,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"It is reported by user:


I was testing flink-kubernetes-operator in an IPv6 cluster and found out the below issues:

Caused by: javax.net.ssl.SSLPeerUnverifiedException: Hostname fd70:e66a:970d::1 not verified:
    certificate: sha256/EmX0EhNn75iJO353Pi+1rClwZyVLe55HN3l5goaneKQ=
    DN: CN=kube-apiserver
    subjectAltNames: [fd70:e66a:970d:0:0:0:0:1, 2406:da14:2:770b:3b82:51d7:9e89:76ce, 10.0.170.248, c0c813eaff4f9d66084de428125f0b9c.yl4.ap-northeast-1.eks.amazonaws.com, ip-10-0-170-248.ap-northeast-1.compute.internal, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local]

Which seemed to be related to a known issue of okhttp.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 15:32:34 UTC 2023,,,,,,,,,,"0|z1jn74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/23 03:38;ZhenqiuHuang;From the discussion in OKHttp community. The issue is due to IPv6 hostnames Verification logic. It is resolved by this PR 

https://github.com/square/okhttp/pull/5889;;;","08/Aug/23 03:46;ZhenqiuHuang;Flink operator is already using the latest stable version 4.11. It means the fix has not been officially released to a stable version yet.

[~gyfora] What's u opinion on this?;;;","05/Sep/23 09:41;heywxl;FYI, adding environment variables of ` KUBERNETES_DISABLE_HOSTNAME_VERIFICATION=true` works for me.
 
This env variable needs to be added to both the Flink operator and the Flink job definition.;;;","05/Sep/23 15:25;ZhenqiuHuang;[~heywxl] Thanks for the confirmation. 

Given there is already a workaround, the PR will be revised to add IPV6 support suggestions in Doc. At the same time, upgrade the okhttp version when 5.0.0 is already is the ultimate goal. ;;;","12/Sep/23 15:32;gyfora;merged to main 8978dde3facd3094a2cbb1a7706588c3fa7ef5fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding HTTP basic authentication support to PrometheusPushGatewayReporter,FLINK-32776,13546446,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stonewu,stonewu,08/Aug/23 03:19,15/Aug/23 03:05,04/Jun/24 20:40,,1.17.1,,,,,,,,,Runtime / Metrics,,,,,,0,pull-request-available,,,"For security reasons, PushGateways generally enable HTTP basic authentication

but currently Flink does not support pushing metrics to a PushGateway that has HTTP basic authentication turned on",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Tue Aug 15 03:05:27 UTC 2023,,,,,,,,,,"0|z1jn6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 03:05;stonewu;hi~[~martijnvisser] can you please assign this ticket to me? I've already submitted a pull request to accomplish this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support yarn.provided.lib.dirs to add parent directory to classpath,FLINK-32775,13546427,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,argoyal,argoyal,argoyal,07/Aug/23 22:52,14/Sep/23 01:52,04/Jun/24 20:40,14/Sep/23 01:52,,,,,,1.19.0,,,,Deployment / YARN,,,,,,0,pull-request-available,,,"Currently with {*}yarn.provided.lib.dirs{*}, Flink libs can be copied to HDFS location in each cluster and when set Flink tries to reuse the same jars avoiding uploading it every time and YARN also caches it in the nodes.

 

This works fine with jars but if we try to add the xml file parent directory to path, Flink job submission fails. If I add the parent directory of the xml to the 
{noformat}
yarn.ship-files{noformat}
 Flink job is submitted successfully.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 14 01:52:32 UTC 2023,,,,,,,,,,"0|z1jn2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 22:58;argoyal;Rootcause : The parent directory is not being added when using {{yarn.provided.lib.dirs}} but when using {{{}yarn.ship-files{}}}, the parent dir is added.{{{}yarn.provided.lib.dirs{}}} ([Link|https://github.com/apache/flink/blob/255d83087c6a6432270e6886ffdcf85dae00c241/flink-yarn/src/main/java/org/apache/flink/yarn/YarnApplicationFileUploader.java#L343C1-L344C1]) should construct the classpath similar as {{yarn.ship-files}} ([Link|https://github.com/apache/flink/blob/255d83087c6a6432270e6886ffdcf85dae00c241/flink-yarn/src/main/java/org/apache/flink/yarn/YarnApplicationFileUploader.java#L233]) where we should add the parent dir also, followed by jars.

Will raise a PR with the fix.;;;","09/Aug/23 00:35;argoyal;Raised a PR and added test case.;;;","14/Sep/23 01:51;wangyang0918;Merged in master via 1f9621806451411af26ccbab5c5342ef3308e219.;;;","14/Sep/23 01:52;wangyang0918;Thanks [~argoyal]  for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reconciliation for autoscaling overrides gets stuck after cancel-with-savepoint,FLINK-32774,13546399,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,mxm,mxm,07/Aug/23 17:35,09/Aug/23 15:33,04/Jun/24 20:40,09/Aug/23 15:33,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,"Since https://issues.apache.org/jira/browse/FLINK-32589 the operator does not rely on the Flink configuration anymore to store the parallelism overrides. Instead, it stores them internally in the autoscaler config map. Upon scalings without the rescaling API, the spec is changed on the fly during reconciliation and the parallelism overrides are added.

Unfortunately, this yields to the cluster getting stuck with the job in FINISHED state after taking a savepoint for upgrade. The operator assumes that the new cluster got deployed successfully and goes into DEPLOYED state again.

Log flow (from oldest to newest):
 # Rescheduling new reconciliation immediately to execute scaling operation.
 # Upgrading/Restarting running job, suspending first...
 # Job is in running state, ready for upgrade with SAVEPOINT
 # Suspending existing deployment.
 # Suspending job with savepoint.
 # Job successfully suspended with savepoint
 # The resource is being upgraded
 # Pending upgrade is already deployed, updating status.
 # Observing JobManager deployment. Previous status: DEPLOYING
 # JobManager deployment port is ready, waiting for the Flink REST API...
 # DEPLOYED The resource is deployed/submitted to Kubernetes, but it’s not yet considered to be stable and might be rolled back in the future

It appears the issue might be in (8): [https://github.com/apache/flink-kubernetes-operator/blob/c09671c5c51277c266b8c45d493317d3be1324c0/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/AbstractFlinkDeploymentObserver.java#L260] because the generation id hasn't been changed by the mere parallelism override change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 15:33:36 UTC 2023,,,,,,,,,,"0|z1jmwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 15:33;gyfora;merged to main: 48d6703fa1f795e9849a3e690264cc5f6273349c
release-1.6: 575ea323f09a437cf9f483968588ea77c5a98835;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PEM certificates in the SQL Runner example,FLINK-32773,13546365,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mehdidb1993,mehdidb1993,07/Aug/23 13:18,14/Aug/23 11:23,04/Jun/24 20:40,14/Aug/23 11:23,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"When having a Kafka connector using SSL to connect the comment pattern used in the Flink SQL Runner example is considering 
{code:java}
-----BEGIN CERTIFICATE----- {code}
{code:java}
-----END CERTIFICATE----- {code}
as comments and is not setting correctly the certificates used in SSL.
h3. How to reproduce

This issue can be reproduced as follows (for Kafka) when having ssl enabled in Kafka.
 * In [https://github.com/apache/flink-kubernetes-operator/blob/main/examples/flink-sql-runner-example/sql-scripts/statement-set.sql] you should have the property {{properties.ssl.truststore.certificates}}

*Example*

 
{code:java}
WITH (
    'connector' = 'kafka',
    ...
    'properties.ssl.truststore.certificates' = '-----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    '
); {code}
 * Deploy and check that there is an error to create the Kafka producer/consumer

{code:java}
org.apache.kafka.common.KafkaException: Failed to construct kafka producer
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:439) ~[kafka-clients-3.2.3.jar:?]
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:289) ~[kafka-clients-3.2.3.jar:?]
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:316) ~[kafka-clients-3.2.3.jar:?]
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:301) ~[kafka-clients-3.2.3.jar:?]
	at org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:55) ~[flink-connector-kafka-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.kafka.sink.KafkaWriter.<init>(KafkaWriter.java:182) ~[flink-connector-kafka-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.kafka.sink.KafkaSink.createWriter(KafkaSink.java:111) ~[flink-connector-kafka-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.kafka.sink.KafkaSink.createWriter(KafkaSink.java:57) ~[flink-connector-kafka-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler.createWriter(StatefulSinkWriterStateHandler.java:117) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.initializeState(SinkWriterOperator.java:146) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:274) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: org.apache.kafka.common.errors.InvalidConfigurationException: No matching CERTIFICATE entries in PEM file {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/645,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 11:23:09 UTC 2023,,,,,,,,,,"0|z1jmow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 11:23;gyfora;merged to main 699d0e97e8827d56b231bc58eac56e4db388ed5d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support of SET statements in the SQL Runner example,FLINK-32772,13546363,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mehdidb1993,mehdidb1993,mehdidb1993,07/Aug/23 13:14,11/Aug/23 09:26,04/Jun/24 20:40,11/Aug/23 06:54,kubernetes-operator-1.5.0,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"When having a script containing `SET` statement the SQL Runner example execute it as an SQL query and is not setting the environment config instead.
h2. How to reproduce

This issue can be reproduced as follows:
 * Add {{SET 'parallelism.default' = '2';}} on top of the SQL statements in [https://github.com/apache/flink-kubernetes-operator/blob/main/examples/flink-sql-runner-example/sql-scripts/statement-set.sql]
 * Build and execute the example, and verify that the error {{The main method caused an error: Unsupported SQL query! executeSql() only accepts a single SQL statement of type [...]}} is happening in the pod log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/644,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 06:54:55 UTC 2023,,,,,,,,,,"0|z1jmog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 06:54;gyfora;merged to main a12b9431ac05965041d4c90d95592a40c3360692;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the invalid config option slotmanager.request-timeout,FLINK-32771,13546352,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,huweihua,guoyangze,guoyangze,07/Aug/23 12:05,16/Aug/23 09:53,04/Jun/24 20:40,16/Aug/23 02:45,1.17.0,,,,,1.18.0,,,,Runtime / Configuration,Runtime / Coordination,,,,,0,pull-request-available,,,"{{slotmanager.request-timeout}} has been deprecated since 1.6 and is not used after the removal of {{SlotManagerImpl}}. We can simply remove it now.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 02:47:09 UTC 2023,,,,,,,,,,"0|z1jmm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 02:47;huweihua;master: b3557cfdc0a2897356027c16d97161c11680c802;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the inaccurate backlog number of Hybrid Shuffle,FLINK-32770,13546348,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Wencong Liu,Wencong Liu,07/Aug/23 11:52,21/Aug/23 03:05,04/Jun/24 20:40,21/Aug/23 03:05,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,The backlog is inaccurate in both memory and disk tier. We should fix it to prevent redundant memory usage in reader side.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 03:05:20 UTC 2023,,,,,,,,,,"0|z1jml4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 03:05;Weijie Guo;master(1.18) via ed4937cbc0ee73e08856cc59bfbf788682c70a09.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PeriodicMaterializationManager pass descriptionFormat with invalid placeholder to MailboxExecutor#execute(),FLINK-32769,13546342,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,07/Aug/23 11:28,16/Sep/23 02:33,04/Jun/24 20:40,16/Sep/23 02:32,,,,,,1.19.0,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,"descriptionFormat in _MailboxExecutor#execute( ThrowingRunnable<? extends Exception> command, String descriptionFormat, Object... descriptionArgs)_ will be used in _String.format()_ which can't accept placeholder like ""{}"". But PeriodicMaterializationManager passed the descriptionFormat with invalid placeholder ‘{}’.

 

Hi [~ym] and [~Yanfei Lei] , PTAL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 16 02:32:35 UTC 2023,,,,,,,,,,"0|z1jmjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/23 02:32;Yanfei Lei;Merged via 13469792821ee1e9d2171a28a071cc986626316e ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource times out,FLINK-32768,13546324,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,JunRuiLi,mapohl,mapohl,07/Aug/23 09:45,10/Aug/23 02:55,04/Jun/24 20:40,10/Aug/23 02:55,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,Tests,,,,,0,pull-request-available,test-stability,,"SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource is timing out:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52009&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=9022

{code}
2023-08-06T05:34:27.1867230Z Aug 06 05:34:27 ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007fb7d4e82800 nid=0x6dde7 waiting on condition [0x00007fb7834a4000]
2023-08-06T05:34:27.1867541Z Aug 06 05:34:27    java.lang.Thread.State: WAITING (parking)
2023-08-06T05:34:27.1867777Z Aug 06 05:34:27 	at sun.misc.Unsafe.park(Native Method)
2023-08-06T05:34:27.1868191Z Aug 06 05:34:27 	- parking to wait for  <0x00000000a77360d8> (a java.util.concurrent.CompletableFuture$Signaller)
2023-08-06T05:34:27.1868571Z Aug 06 05:34:27 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2023-08-06T05:34:27.1868896Z Aug 06 05:34:27 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2023-08-06T05:34:27.1869240Z Aug 06 05:34:27 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-08-06T05:34:27.1869682Z Aug 06 05:34:27 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2023-08-06T05:34:27.1870022Z Aug 06 05:34:27 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2023-08-06T05:34:27.1870395Z Aug 06 05:34:27 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:229)
2023-08-06T05:34:27.1870858Z Aug 06 05:34:27 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:165)
2023-08-06T05:34:27.1871251Z Aug 06 05:34:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31843,,,,,FLINK-30846,,,FLINK-32788,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 02:55:53 UTC 2023,,,,,,,,,,"0|z1jmfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 09:46;mapohl;There was a similar issue in 1.17 (covered by FLINK-30846) which was fixed. [~JunRuiLi] can you have a look at this?;;;","07/Aug/23 12:50;JunRuiLi;Hi [~mapohl], thanks for reporting this issue. I tried to reproduce this issue locally but was not successful, so it will take some time to resolve it. If there are any debug logs or a heap dump available, they would be helpful in resolving the issue.;;;","07/Aug/23 12:52;mapohl;Thanks for looking into it. The only artifacts that are available are the ones provided by the build.;;;","08/Aug/23 02:45;JunRuiLi;This issue arises because createDeploymentHandles requires a correspondence between executionsToDeploy and executionSlotAssignments ([here|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultExecutionDeployer.java#L140]), but after FLINK-31843, SimpleExecutionSlotAllocator broke this assumption ([here|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SimpleExecutionSlotAllocator.java#L127]). The error only occurs during schedule slow tasks by SpeculativeScheduler because AdaptiveBatchScheduler will schedule task one-by-one, while SpeculativeScheduler may schedule multiple slow tasks at once. This issue renders speculative execution completely unusable, so I will raise its priority and prepare a PR to fix it. 

cc [~wanglijie] [~zhuzh] ;;;","10/Aug/23 02:55;zhuzh;Fixed via 6806f30fe234790f25b5ca83e6d2910286a76aeb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix en-doc SHOW CREATE TABLE usage,FLINK-32767,13546310,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,07/Aug/23 08:18,02/Nov/23 08:09,04/Jun/24 20:40,02/Nov/23 08:09,1.18.0,,,,,1.19.0,,,,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 02 08:09:04 UTC 2023,,,,,,,,,,"0|z1jmco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 08:09;leonard;fixed in master(1.19): d9342f50ff002638dc11fef2b72ffd551e38a2d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support kafka catalog,FLINK-32766,13546292,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,07/Aug/23 04:36,20/May/24 12:42,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"Support kafka catalog
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 12:42:12 UTC 2024,,,,,,,,,,"0|z1jm8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 12:42;chalixar;This is a great addition, However I would prefer to have a FLIP for this one! happy to coordinate about it or if you want to drive the FLIP
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create view should reuse calcite tree,FLINK-32765,13546290,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,07/Aug/23 03:48,07/Aug/23 08:17,04/Jun/24 20:40,,1.16.1,,,,,,,,,Table SQL / API,,,,,,0,,,,"{code:java}
// code placeholder

CREATE TABLE source (
    k2 bigint,
    o1 bigint
) 
COMMENT 'source'
WITH (
  'connector' = 'datagen'
);

CREATE TABLE print_table (
    k1 bigint
) WITH (
  'connector' = 'blackhole'
);

CREATE TABLE print_table2 (
    k1 bigint
) WITH (
  'connector' = 'blackhole'
);

create view v1 as 
  select k2*2 as k2 from source where o1 > 20;


insert into print_table select k2 from v1 where k2 >9;
insert into print_table2 select k2 from v1 where k2 <80; {code}
This SQL will cause view v1 logic being created for 2 times.  Why can't us create a table in executeInternal and keep a QueryOperationCatalogView in CatalogManager?
{code:java}
// code placeholder

public TableResult executeInternal(String statement) 
...
else if (operation instanceof CreateViewOperation) {
    CreateViewOperation createViewOperation = (CreateViewOperation) operation;
    Table table = sqlQuery(createViewOperation.getCatalogView().getOriginalQuery()); {code}
 

 

this could enable us to avoid codegen for multiple time if we could reuse the some part of the query

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-07 03:48:51.0,,,,,,,,,,"0|z1jm88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlotManager supports pulling up all TaskManagers at initialization ,FLINK-32764,13546286,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xiangyu0xf,xiangyu0xf,07/Aug/23 03:26,10/Aug/23 02:11,04/Jun/24 20:40,10/Aug/23 02:11,,,,,,,,,,,,,,,,0,,,,"For OLAP session clusters, It is better to pull all TM when the cluster starts, rather than waiting for the job to come and assign it later.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15959,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-07 03:26:02.0,,,,,,,,,,"0|z1jm7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exceptions cause retry not to take effect in SocketTextStreamFunction,FLINK-32763,13546251,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,miaojf,miaojf,06/Aug/23 04:39,06/Nov/23 06:01,04/Jun/24 20:40,,1.17.1,,,,,1.17.1,,,,API / DataStream,,,,,,0,pull-request-available,,,"Uncaught exceptions cause retry not to take effect in SocketTextStreamFunction

 

Caused by: java.net.ConnectException: Connection refused: connect
    at java.net.DualStackPlainSocketImpl.connect0(Native Method)
    at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:589)
    at org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.run(SocketTextStreamFunction.java:104)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323)",jdk 1.8.0_161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,Mon Nov 06 02:56:20 UTC 2023,,,,,,,,,,"0|z1jlzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/23 05:11;miaojf;Catch the exception to make the retry code take effect;;;","06/Nov/23 02:56;paul8263;Hi community,

I got the same issue and wanna work on it.

Could someone please assign this issue to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support concurrency control when submitting OLAP jobs to Dispatcher,FLINK-32762,13546238,13417633,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,05/Aug/23 16:00,05/Aug/23 16:00,04/Jun/24 20:40,,,,,,,,,,,Client / Job Submission,,,,,,0,,,,"In the OLAP scenario, we need to support concurrency control of submitted jobs. When QPS surges, this could avoid performance degradation due to excessive load .

 

Currently, we support multiple concurrency control options including max-running-jobs and max-running-tasks-per-taskmanager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-05 16:00:33.0,,,,,,,,,,"0|z1jlwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use UUID based on PhysicalStateHandleID as SharedStateRegistryKey ChangelogStateHandleStreamImpl,FLINK-32761,13546225,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Feifan Wang,Feifan Wang,05/Aug/23 08:55,28/Aug/23 07:00,04/Jun/24 20:40,28/Aug/23 06:59,1.18.0,,,,,1.19.0,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,"_ChangelogStateHandleStreamImpl#getKey()_ use _System.identityHashCode(stateHandle)_ as _SharedStateRegistryKey_ while stateHandle is not _FileStateHandle_ or {_}ByteStreamStateHandle{_}. That can easily lead to collision, although from the current code path, it only affects the test code.

In FLINK-29913 , we use UUID based on PhysicalStateHandleID as SharedStateRegistryKey in IncrementalRemoteKeyedStateHandle, we can reuse this method in ChangelogStateHandleStreamImpl.

 

WDYT [~roman]  ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 06:59:52 UTC 2023,,,,,,,,,,"0|z1jlts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/23 20:52;roman;Good point [~Feifan Wang], I think it makes sense.;;;","28/Aug/23 06:59;roman;Merged as 5f9de3eda3eed74472651aaecd2b3882fe76b61f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Version Conflict in flink-sql-connector-hive for shaded.parquet prefix packages,FLINK-32760,13546217,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dongwoo.kim,dongwoo.kim,05/Aug/23 06:04,25/Aug/23 01:06,04/Jun/24 20:40,25/Aug/23 01:06,1.17.1,,,,,1.16.3,1.17.2,1.18.0,,Connectors / Hive,,,,,,0,pull-request-available,,,"h2. Summary

In https://issues.apache.org/jira/browse/FLINK-23074 it seems like shading parquet dependency from *hive-exec* is done. 

But I think this is not enough and causing errors like below when I try to read parquet file using sql-gateway which requires both *flink-parquet* and *flink-sql-connector-hive* dependencies.

!image-2023-08-05-14-50-47-806.png|width=1392,height=909!

 
h2. {color:#172b4d}Cause{color}

{color:#172b4d}Parquet dependency not only includes *org.apache.parquet* but also *shaded.parquet* prefix dependencies. ([ref|https://github.com/apache/parquet-mr/blob/515734c373f69b5250e8b63eb3d1c973da893b63/pom.xml#L72]){color}

{color:#172b4d}So we need to shade both.{color}

{color:#172b4d}- flink-parquet depends on Parquet 1.12.3 with shaded Thrift 0.16.0 (prefix: {{{}shaded.parquet{}}}){color}

{color:#172b4d}- flink-sql-connector-hive depends on hive-exec 3.1.3 with Parquet 1.10.0 and shaded Thrift 0.9.3 (prefix: {{{}shaded.parquet{}}}){color}

{color:#172b4d}- Code compiled against Thrift 0.16.0 attempts to run against 0.9.3, causing the error.{color}
h2. {color:#172b4d}Proposed solution{color}

Adding new shading rule to flink-sql-connector-hive project.

I have confirmed that adding this rule could resolve the above error.
{code:xml}
<relocation>
     <pattern>shaded.parquet</pattern>
     <shadedPattern>shaded.parquet.flink.hive.shaded</shadedPattern>
</relocation>{code}
 

I would be happy to implement it if the proposal is accepted. Thanks

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/23 05:50;dongwoo.kim;image-2023-08-05-14-50-47-806.png;https://issues.apache.org/jira/secure/attachment/13061936/image-2023-08-05-14-50-47-806.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 14:16:53 UTC 2023,,,,,,,,,,"0|z1jls0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 08:43;luoyuxia;Thanks for raising it. Please go head to open a pr to us. I'll help review.;;;","07/Aug/23 15:32;dongwoo.kim;Thanks for your help. I'll soon share the pr;;;","10/Aug/23 02:05;dongwoo.kim;Hi [~luoyuxia], I have opened the pr. 
I'd greatly appreciate it if you could review the pr when you have some free time. Thank you!;;;","24/Aug/23 12:49;luoyuxia;master: 6bb5a12eb0fd106f14538bb716650417a46adf58

1.18: 69cae5e260d128f40656f146b9467f81c93b1e47

1.17: e0605dbff738ac7b8237aa3dacaff50e65d6195e

1.16: 19c8778a38436a2e9d9390e0547783cc4161e97f;;;","24/Aug/23 14:16;dongwoo.kim;Hi [~luoyuxia] , I have opened backport PRs. 
Please check in your free time. 
1.16: [https://github.com/apache/flink/pull/23286]
1.17: [https://github.com/apache/flink/pull/23287]
1.18: [https://github.com/apache/flink/pull/23288] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the removed config in the doc,FLINK-32759,13546212,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,05/Aug/23 03:54,07/Aug/23 05:00,04/Jun/24 20:40,07/Aug/23 05:00,,,,,,1.18.0,,,,Documentation,Runtime / Coordination,,,,,0,pull-request-available,,,"The cluster.declarative-resource-management.enabled was removed at FLINK-21095(https://github.com/apache/flink/pull/15838/files), so it doesn't work now.

However, the flink doc still have it.

 !image-2023-08-05-11-54-17-714.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/23 03:54;fanrui;image-2023-08-05-11-54-17-714.png;https://issues.apache.org/jira/secure/attachment/13061933/image-2023-08-05-11-54-17-714.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 04:59:53 UTC 2023,,,,,,,,,,"0|z1jlqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 04:59;fanrui;Merged via <master:1.18> e69f410baf93b7a172b2aeac01bb0e5b893c9a14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink bounds are overly restrictive and outdated,FLINK-32758,13546201,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,deepyaman,deepyaman,deepyaman,05/Aug/23 00:01,30/Aug/23 05:21,04/Jun/24 20:40,30/Aug/23 05:21,1.17.1,1.19.0,,,,1.17.2,1.18.0,,,API / Python,,,,,,0,pull-request-available,test-stability,,"Hi! I am part of a team building the Flink backend for Ibis ([https://github.com/ibis-project/ibis]). We would like to leverage PyFlink under the hood for execution; however, PyFlink's requirements are incompatible with several other Ibis requirements. Beyond Ibis, PyFlink's outdated and restrictive requirements prevent it from being used alongside most recent releases of Python data libraries.

Some of the major libraries we (and likely others in the Python community interested in using PyFlink alongside other libraries) need compatibility with:
 * PyArrow (at least >=10.0.0, but there's no reason not to be also be compatible with latest)
 * pandas (should be compatible with 2.x series, but also probably with 1.4.x, released January 2022, and 1.5.x)
 * numpy (1.22 was released in December 2022)
 * Newer releases of Apache Beam
 * Newer releases of cython

Furthermore, uncapped dependencies could be more generally preferable, as they avoid the need for frequent PyFlink releases as newer versions of libraries are released. A common (and great) argument for not upper-bounding dependencies, especially for libraries: [https://iscinumpy.dev/post/bound-version-constraints/]

I am currently testing removing upper bounds in [https://github.com/apache/flink/pull/23141]; so far, builds pass without issue in [b65c072|https://github.com/apache/flink/pull/23141/commits/b65c0723ed66e01e83d718f770aa916f41f34581], and I'm currently waiting on [c8eb15c|https://github.com/apache/flink/pull/23141/commits/c8eb15cbc371dc259fb4fda5395f0f55e08ea9c6] to see if I can get PyArrow to resolve >=10.0.0. Solving the proposed dependencies results in:



{{#}}
{{# This file is autogenerated by pip-compile with Python 3.8}}
{{# by the following command:}}
{{#}}
{{#    pip-compile --config=pyproject.toml --output-file=dev/compiled-requirements.txt dev/dev-requirements.txt}}
{{#}}
{{apache-beam==2.49.0}}
{{    # via -r dev/dev-requirements.txt}}
{{avro-python3==1.10.2}}
{{    # via -r dev/dev-requirements.txt}}
{{certifi==2023.7.22}}
{{    # via requests}}
{{charset-normalizer==3.2.0}}
{{    # via requests}}
{{cloudpickle==2.2.1}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{crcmod==1.7}}
{{    # via apache-beam}}
{{cython==3.0.0}}
{{    # via -r dev/dev-requirements.txt}}
{{dill==0.3.1.1}}
{{    # via apache-beam}}
{{dnspython==2.4.1}}
{{    # via pymongo}}
{{docopt==0.6.2}}
{{    # via hdfs}}
{{exceptiongroup==1.1.2}}
{{    # via pytest}}
{{fastavro==1.8.2}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{fasteners==0.18}}
{{    # via apache-beam}}
{{find-libpython==0.3.0}}
{{    # via pemja}}
{{grpcio==1.56.2}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{    #   grpcio-tools}}
{{grpcio-tools==1.56.2}}
{{    # via -r dev/dev-requirements.txt}}
{{hdfs==2.7.0}}
{{    # via apache-beam}}
{{httplib2==0.22.0}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{idna==3.4}}
{{    # via requests}}
{{iniconfig==2.0.0}}
{{    # via pytest}}
{{numpy==1.24.4}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{    #   pandas}}
{{    #   pyarrow}}
{{objsize==0.6.1}}
{{    # via apache-beam}}
{{orjson==3.9.2}}
{{    # via apache-beam}}
{{packaging==23.1}}
{{    # via pytest}}
{{pandas==2.0.3}}
{{    # via -r dev/dev-requirements.txt}}
{{pemja==0.3.0 ; platform_system != ""Windows""}}
{{    # via -r dev/dev-requirements.txt}}
{{pluggy==1.2.0}}
{{    # via pytest}}
{{proto-plus==1.22.3}}
{{    # via apache-beam}}
{{protobuf==4.23.4}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{    #   grpcio-tools}}
{{    #   proto-plus}}
{{py4j==0.10.9.7}}
{{    # via -r dev/dev-requirements.txt}}
{{pyarrow==11.0.0}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{pydot==1.4.2}}
{{    # via apache-beam}}
{{pymongo==4.4.1}}
{{    # via apache-beam}}
{{pyparsing==3.1.1}}
{{    # via}}
{{    #   httplib2}}
{{    #   pydot}}
{{pytest==7.4.0}}
{{    # via -r dev/dev-requirements.txt}}
{{python-dateutil==2.8.2}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{    #   pandas}}
{{pytz==2023.3}}
{{    # via}}
{{    #   -r dev/dev-requirements.txt}}
{{    #   apache-beam}}
{{    #   pandas}}
{{regex==2023.6.3}}
{{    # via apache-beam}}
{{requests==2.31.0}}
{{    # via}}
{{    #   apache-beam}}
{{    #   hdfs}}
{{six==1.16.0}}
{{    # via}}
{{    #   hdfs}}
{{    #   python-dateutil}}
{{tomli==2.0.1}}
{{    # via pytest}}
{{typing-extensions==4.7.1}}
{{    # via apache-beam}}
{{tzdata==2023.3}}
{{    # via pandas}}
{{urllib3==2.0.4}}
{{    # via requests}}
{{wheel==0.41.0}}
{{    # via -r dev/dev-requirements.txt}}
{{zstandard==0.21.0}}
{{    # via apache-beam}}
{{# The following packages are considered to be unsafe in a requirements file:}}
{{# pip}}
{{# setuptools}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/23 02:19;dianfu;image-2023-08-29-10-19-37-977.png;https://issues.apache.org/jira/secure/attachment/13062534/image-2023-08-29-10-19-37-977.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 05:21:28 UTC 2023,,,,,,,,,,"0|z1jlog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 02:06;dianfu;Merged to master via 0dd6f9745b8df005e3aef286ae73092696ca2799;;;","25/Aug/23 02:09;dianfu;Will backport to release-1.17 and release-1.8 branch after one official nightly test of the master branch.;;;","28/Aug/23 11:55;Sergey Nuyanzin;[~deepyaman] , [~dianfu] could you please have a look?

it seems it is the reason of nightly build failure on macos 

now after merging to master it is failing every night
26.08.2023: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52666&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=104] 
27.08.2023: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52680&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=104] 
28.08.2023: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52692&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102]

in logs
{noformat}
2023-08-28T00:18:56.3513860Z       Building wheels for collected packages: fastavro, crcmod, dill, hdfs, pymongo, docopt
2023-08-28T00:18:56.3567790Z         Building wheel for fastavro (pyproject.toml): started
2023-08-28T00:18:56.3600990Z         Building wheel for fastavro (pyproject.toml): finished with status 'error'
2023-08-28T00:18:56.3649060Z         error: subprocess-exited-with-error
2023-08-28T00:18:56.3678250Z       
2023-08-28T00:18:56.3715590Z         × Building wheel for fastavro (pyproject.toml) did not run successfully.
2023-08-28T00:18:56.3743070Z         │ exit code: 1
2023-08-28T00:18:56.3776690Z         ╰─> [150 lines of output]
2023-08-28T00:18:56.3804780Z             running bdist_wheel
2023-08-28T00:18:56.3853290Z             running build
2023-08-28T00:18:56.3878030Z             running build_py
2023-08-28T00:18:56.3898470Z             creating build
2023-08-28T00:18:56.3925210Z             creating build/lib.macosx-10.9-x86_64-cpython-37
2023-08-28T00:18:56.3963550Z             creating build/lib.macosx-10.9-x86_64-cpython-37/fastavro
{noformat};;;","28/Aug/23 11:55;Sergey Nuyanzin;also switch to blocker, since every nightly is failing with this;;;","28/Aug/23 13:32;deepyaman;[~Sergey Nuyanzin] This looks to be related to [https://github.com/fastavro/fastavro/issues/701]; while we pin `cython<3` for PyFlink, `fastavro` is getting built separately with Cython 3. One possible solution is to do something like [https://stackoverflow.com/a/76837035/1093967,] where `cython<3` is installed globally in the environment and used for building all of the libraries (I think). I'm not sure how you all feel about that, but I try to raise a PR with that, if helpful. It seems the failing test is on nightly build that runs a lot more checks; I'm not sure how I can verify that a potential fix would work, if I try? Can I trigger these tests manually?

The other possibility is to check why `fastavro>=1.8.1` isn't getting picked, and it's using `fastavro==1.8.0`. The newer versions have the Cython pin in their build requirements, and we wouldn't need to do a `pip wheel --no-build-isolation`. I can try to check this later today.;;;","28/Aug/23 18:49;Sergey Nuyanzin;> I'm not sure how I can verify that a potential fix would work, if I try? Can I trigger these tests manually?
if you have set your own CI then this could help (just movement of the job to normal CI from nightly) https://github.com/apache/flink/pull/23045/commits/16b65720306ca820dfd83f1ccaacb4b0aed850ac

if you haven't set your own CI then also rename of job is required. Or once a fix ready I can help scheduling it on my own CI;;;","29/Aug/23 02:12;dianfu;[~Sergey Nuyanzin] [~deepyaman]  I have submitted a hotfix to temporary limiting fastavro < 1.8 to make the CI green (verified it on my CI): [https://github.com/apache/flink/commit/345dece9a8fd58d6ea1c829052fb2f3c68516b48];;;","29/Aug/23 02:23;dianfu;[~deepyaman] Actually the tests have passed on the CI:
!image-2023-08-29-10-19-37-977.png!

It's failing when building wheel package for MacOS.

It uses third-party platform **cibuildwheel** to build wheel packages, see [https://github.com/apache/flink/blob/master/tools/azure-pipelines/build-python-wheels.yml#L41] for more details. Currently it's using version 2.8.0. I will try if the latest version (2.15.0) works on my CI.

 ;;;","29/Aug/23 07:01;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52740&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=108;;;","29/Aug/23 14:08;dianfu;Have verified that upgrade *{*}cibuildwheel{*}* doesn't work and exclude fastavro 1.8.0 works: fastavro>=1.1.0,!=1.8.0.

[~deepyaman] What's your thought?;;;","29/Aug/23 15:03;deepyaman;[~dianfu] I'm happy with the `\!=1.8.0` constraint!;;;","30/Aug/23 05:13;dianfu;Fixed in master via 5b5a0af15d57ed4424cf8dd744808433e397ebc4;;;","30/Aug/23 05:21;dianfu;Merged to:
- release-1.18 via 1f7796ee50cfbea4fb633692e6be01070ed45c6f and 8551a39ee46054d3ec05f3d31758f7ad39b69a39
- release-1.17 via 30eeb91c3d2048b88e0a9903d9c973085df2c2ea and 870ac98dcdb92774fed783254a3bf4d8ddc317aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update the copyright year in NOTICE files,FLINK-32757,13546173,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,04/Aug/23 16:49,04/Aug/23 22:42,04/Jun/24 20:40,04/Aug/23 22:42,mongodb-1.0.0,mongodb-1.0.1,,,,mongodb-1.0.2,,,,Connectors / MongoDB,,,,,,0,pull-request-available,,,The current copyright year is 2014-2022 in NOTICE files. We should change it to 2014-2023.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 22:32:31 UTC 2023,,,,,,,,,,"0|z1jli8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/23 22:32;liangtl;Merged commit [{{fc656c4}}|https://github.com/apache/flink-connector-mongodb/commit/fc656c420e9b20676bf5e67c0c1c059a5ad44216] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse ClientHighAvailabilityServices when creating RestClusterClient,FLINK-32756,13546138,13559834,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiangyu0xf,xiangyu0xf,xiangyu0xf,04/Aug/23 11:15,14/Dec/23 04:29,04/Jun/24 20:40,,,,,,,,,,,Client / Job Submission,,,,,,0,pull-request-available,,,"Currently, every newly built RestClusterClient will create a new ClientHighAvailabilityServices which is both unnecessary and resource consuming. For example, each ZooKeeperClientHAServices contains a ZKClient which holds a connection to ZK server and several related threads.

By reusing ClientHighAvailabilityServices across multiple RestClusterClient instances, we can save system resources(threads, connections), connection establish time and leader retrieval time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 02:09:24 UTC 2023,,,,,,,,,,"0|z1jlag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 12:34;xiangyu0xf;[~guoyangze] Hi, would u kindly assign this Jira to me.;;;","29/Nov/23 02:09;guoyangze;[~xiangyu0xf] Done, go ahead!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add quick start guide for Flink OLAP,FLINK-32755,13546136,13548014,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangyu0xf,xiangyu0xf,xiangyu0xf,04/Aug/23 11:04,09/Oct/23 02:09,04/Jun/24 20:40,04/Sep/23 01:34,,,,,,1.19.0,,,,Documentation,,,,,,0,pull-request-available,,,"I propose to add a new {{QUICKSTART.md}} guide that provides instructions for beginner to build a production ready Flink OLAP Service by using flink-jdbc-driver, flink-sql-gateway and flink session cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33209,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 01:34:03 UTC 2023,,,,,,,,,,"0|z1jla0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/23 11:35;libenchao;+1;;;","04/Aug/23 12:06;lsy;Big +1;;;","04/Sep/23 01:34;zjureel;Fixed by fbef3c22757a2352145599487beb84e02aaeb389;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using SplitEnumeratorContext.metricGroup() in restoreEnumerator causes NPE,FLINK-32754,13546131,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,Yu Chen,Yu Chen,04/Aug/23 10:53,09/Aug/23 04:03,04/Jun/24 20:40,09/Aug/23 04:03,1.17.0,1.17.1,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,"We registered some metrics in the `enumerator` of the flip-27 source via `SplitEnumerator.metricGroup()`, but found that the task prints NPE logs in JM when restoring, suggesting that `SplitEnumerator. metricGroup()` is null.
{*}Meanwhile, the task does not experience failover, and the Checkpoints cannot be successfully created even after the task is in running state{*}.

We found that the implementation class of `SplitEnumerator` is `LazyInitializedCoordinatorContext`, however, the metricGroup() is initialized after calling lazyInitialize(). By reviewing the code, we found that at the time of SourceCoordinator.resetToCheckpoint(), lazyInitialize() has not been called yet, so NPE is thrown.

*Q: Why does this bug prevent the task from creating the Checkpoint?*
`SourceCoordinator.resetToCheckpoint()` throws an NPE which results in the member variable `enumerator` in `SourceCoordinator` being null. Unfortunately, all Checkpoint-related calls in `SourceCoordinator` are called via `runInEventLoop()`.
In `runInEventLoop()`, if the enumerator is null, it will return directly.

*Q: Why this bug doesn't trigger a task failover?*
In `RecreateOnResetOperatorCoordinator.resetAndStart()`, if `internalCoordinator.resetToCheckpoint` throws an exception, then it will catch the exception and call `cleanAndFailJob ` to try to fail the job.
However, `globalFailureHandler` is also initialized in `lazyInitialize()`, while `schedulerExecutor.execute` will ignore the NPE triggered by `globalFailureHandler.handleGlobalFailure(e)`.
Thus it appears that the task did not failover.
!image-2023-08-04-18-28-05-897.png|width=963,height=443!",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31268,,,,,,,,,,,,,,,,,,,"04/Aug/23 10:28;Yu Chen;image-2023-08-04-18-28-05-897.png;https://issues.apache.org/jira/secure/attachment/13061925/image-2023-08-04-18-28-05-897.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 03:15:44 UTC 2023,,,,,,,,,,"0|z1jl8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/23 11:56;yunta;I think this problem still existed, and [~ruanhang1993] could you please take a look?;;;","07/Aug/23 03:15;ruanhang1993;[~Yu Chen] [~yunta] ,Thanks for the issue.

I have opened a duplicate issue FLINK-31268 about it. And I have raised a [PR|https://github.com/apache/flink/pull/22048] to fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print JVM flags on AZP,FLINK-32753,13546118,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,zakelly,zakelly,zakelly,04/Aug/23 09:16,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,1.20.0,,,,Build System / Azure Pipelines,,,,,,0,pull-request-available,stale-assigned,,"I suggest printing JVM flags before the tests run, which could help investigate the test failures (especially memory or GC related issue). An example of pipeline output [here|https://dev.azure.com/lzq82555906/flink-for-Zakelly/_build/results?buildId=122&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=165]. You may search 'JVM information' in this log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 13 22:35:04 UTC 2023,,,,,,,,,,"0|z1jl60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-24909 SQL syntax highlighting in SQL Client,FLINK-32752,13546115,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,Sergey Nuyanzin,Sergey Nuyanzin,04/Aug/23 09:01,15/Aug/23 09:23,04/Jun/24 20:40,15/Aug/23 09:23,,,,,,1.18.0,,,,Tests,,,,,,0,release-testing,,,"After FLINK-24909 it is possible to specify syntax highlight color schema as mentioned in doc via 
{{sql-client.display.color-schema}} config option
{code:sql}
SET 'sql-client.display.color-schema' = ...
{code}
Possible values are  {{chester}}, {{dracula}}, {{solarized}}, {{vs2010}}, {{obsidian}}, {{geshi}}, {{default}}.
It allows to highlight keywords, quoted text, sql identifiers quoted text (ticks for default dialect and double quotes for Hive), comments (both one-line and block comments), hints",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 03:35;ruanhang1993;part1.png;https://issues.apache.org/jira/secure/attachment/13062174/part1.png","14/Aug/23 03:35;ruanhang1993;part2.png;https://issues.apache.org/jira/secure/attachment/13062175/part2.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 09:19:43 UTC 2023,,,,,,,,,,"0|z1jl5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 03:36;ruanhang1993;Hi, all.

I have tested this feature and it looks good.

!part1.png!!part2.png!;;;","14/Aug/23 03:50;ruanhang1993;Hi, [~Sergey Nuyanzin] .

I have raise a PR(https://github.com/apache/flink/pull/23205) to fix some missing parts for this feature in docs. Please take a look at it. Thanks.

 ;;;","15/Aug/23 09:19;Sergey Nuyanzin;good catch, thanks for the testing and submitting PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DistinctAggregateITCaseBase.testMultiDistinctAggOnDifferentColumn got stuck on AZP,FLINK-32751,13546105,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,04/Aug/23 07:11,30/Aug/23 05:33,04/Jun/24 20:40,30/Aug/23 05:33,1.16.3,1.17.2,1.18.0,1.19.0,,1.16.3,1.17.2,1.18.0,1.19.0,Table SQL / API,,,,,,0,pull-request-available,test-stability,,"This build hangs https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51955&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=14399

{noformat}
Aug 04 03:03:47 ""ForkJoinPool-1-worker-51"" #28 daemon prio=5 os_prio=0 cpu=49342.66ms elapsed=3079.49s tid=0x00007f67ccdd0000 nid=0x5234 waiting on condition  [0x00007f6791a19000]
Aug 04 03:03:47    java.lang.Thread.State: WAITING (parking)
Aug 04 03:03:47 	at jdk.internal.misc.Unsafe.park(java.base@11.0.19/Native Method)
Aug 04 03:03:47 	- parking to wait for  <0x00000000ad3b1fb8> (a java.util.concurrent.CompletableFuture$Signaller)
Aug 04 03:03:47 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.19/LockSupport.java:194)
Aug 04 03:03:47 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.19/CompletableFuture.java:1796)
Aug 04 03:03:47 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.19/ForkJoinPool.java:3118)
Aug 04 03:03:47 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.19/CompletableFuture.java:1823)
Aug 04 03:03:47 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.19/CompletableFuture.java:1998)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:171)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 04 03:03:47 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Aug 04 03:03:47 	at java.util.Iterator.forEachRemaining(java.base@11.0.19/Iterator.java:132)
Aug 04 03:03:47 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:309)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:109)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testMultiDistinctAggOnDifferentColumn(DistinctAggregateITCaseBase.scala:97)
~~
{noformat}
it is very likely that it is an old issue
the similar case was mentioned for 1.11.0 and closed because of lack of occurrences 
FLINK-16923

and another similar one FLINK-22100 which was marked as a duplicate of FLINK-21996",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29642,,,,,,,,FLINK-16923,FLINK-21996,FLINK-22100,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 05:33:49 UTC 2023,,,,,,,,,,"0|z1jl34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/23 12:27;mapohl;The actual test failure happened in {{SortDistinctAggregateITCase#testMultiDistinctAggOnDifferentColumn}} which derives the test from {{DistinctAggregateITCaseBase}}.

FYI: This can be determined by looking at the surefire reporting which prints that {{HashDistinctAggregateITCase}} completed but {{SortDistinctAggregateITCase}} didn't.
{code}
[...]
Aug 04 02:12:29 02:12:29.073 [INFO] Running org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase
[...]
Aug 04 02:19:04 02:19:04.720 [INFO] Running org.apache.flink.table.planner.runtime.batch.sql.agg.HashDistinctAggregateITCase
Aug 04 02:20:38 02:20:38.255 [INFO] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 93.527 s - in org.apache.flink.table.planner.runtime.batch.sql.agg.HashDistinctAggregateITCase
[...]
{code}

The issue we're seeing seems to be independent of the actual test, though.

The timeout happens when the {{CollectDynamicSink}} tries to request more data through the Dispatcher which forwards the request. Unfortunately, we don't have any logs from the Dispatcher side of that request. Therefore, we cannot reliably say where the request halted.

[~Sergey Nuyanzin] had a point when pointing out that there are multiple other past Jira issues (FLINK-20254, FLINK-22129, FLINK-22181, FLINK-22100) that had a similar stacktrace. These Jiras were handled as duplicates of FLINK-21996 which was a bug in RPC layer with messages being swallowed. In the end, it's strange that the request wasn't completed in some way due to the {{MiniCluster}} having been shut down.

There should be an error being thrown when trying to get the RPC endpoint for the dispatcher (through [LeaderGatewayRetriever#getFuture|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/webmonitor/retriever/LeaderGatewayRetriever.java#L49]) or for the JobMaster (through [Dispatcher#getJobMaster|https://github.com/apache/flink/blob/c6d58e17e8ce736a062234e1558ac8d7b65990ef/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1455]) which we don't see. That makes it more likely that the RPC request/response was swallowed.;;;","08/Aug/23 12:14;mapohl;[~chesnay] had another guess: {{CollectResultFetcher.sendRequest}} (which is the call that doesn't get back even while the cluster is shutdown) triggers the
 * Dispatcher#deliverCoordinationRequestToCoordinator
 ** JobMaster#deliverCoordinationRequestToCoordinator
 *** JobMaster#sendRequestToCoordinator
 **** SchedulerNG#deliverCoordinationRequestToCoordinator
 ***** OperatorCoordinatorHandler#deliverCoordinationRequestToCoordinator
 ****** CollectSinkOperatorCoordinator#handleCoordinationRequest

The latter one submits the request handling to an {{executorService}} which is owned by the coordinator. This {{executorService}} is shut down as part of the coordinator's close mechanism using the {{shutdown}} call. Any already submitted task is not stopped but continues to operator while the MiniCluster shutdown is happening (where the RPC system is shut down as well in the end). There is some chance that the task got executed but the RPC system is shut down in the mean time. Any completion of future within {{CollectSinkOperatorCoordinator#handleRequestImpl}} might be lost when the RPC system is shut down.

I'm gonna go ahead and try to come up with a test scenario for this issue. If we conclude that this reasoning makes sense, it would mean that Jira issue isn't blocking the 1.18 release. That problem exists also in older versions of Flink.;;;","09/Aug/23 10:54;mapohl;I played around a bit more with the test and I'm not convinced that the issue is the shutdown procedure. AFAIU, {{CollectSinkOperatorCoordinator#handleRequestImpl}} would immediately fail if the {{CollectSinkOperatorCoordinator#close}} is triggered because it closes the socket connection which would cause an error while reading from the socket's inputstream. The future should be completed in the catch block of {{CollectSinkOperatorCoordinator#handleRequestImpl}} (even if the cluster is shut down in the mean time).

Enabling more logs in the local test execution also reveals that the response through the {{CollectSinkOperatorCoordinator}} is rarely filled with data. But the test itself succeeds anyway. -That seems odd: I'm wondering whether there's another path that sends out the data to the client.- The JobClient does, indeed, collect the results after the job terminated in a separated branch in [CollectResultFetcher#next()|https://github.com/apache/flink/blob/8119411addd9c82c15bab8480e7b35b8e6394d43/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultFetcher.java#L118]. The corresponding data is coming from the CollectSinkFunction where it collects all the outstanding data while closing the function (see [CollectSinkFunction#accululateFinalResults|https://github.com/apache/flink/blob/5ae8cb0503449b07f76d0ab621c3e81734496b26/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunction.java#L304]). 

[~renqs] do you have someone who can look into that issue from the runtime perspective?;;;","09/Aug/23 11:50;mapohl;Some other findings:
* The thread dump doesn't contain the {{collect-sink-operator-coordinator-executor-thread-pool}} thread pool which gives the indication that the {{CollectSinkOperatorCoordinator}} executorService was actually shut down. 
* The Socket is never established (i.e. ""Sink connection established"" doesn't show up in the logs; see [CollectSinkOperatorCoordinator:136|https://github.com/apache/flink/blob/bedcc3f7b5c0fc184953d3c1a969f03887db2cae/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectSinkOperatorCoordinator.java#L136]) which might indicate that the connect call could have also been blocked. The {{connect}} itself is called without a timeout (value: 0) which makes the connect possibly block forever. But that contradicts the item above where we claimed that the corresponding thread {{collect-sink-operator-coordinator-executor-thread-pool}} isn't listed in the thread dump anymore.

I'm gonna go ahead and provide the PR that hopefully improves the logging of this scenario a bit more.;;;","09/Aug/23 15:12;mapohl;I checked whether {{Socket#connect}} (see [CollectSinkOperatorCoordinator:165|https://github.com/apache/flink/blob/e110b0318d66c741ca0f609ec661ddaea599379c/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectSinkOperatorCoordinator.java#L165]) might block the thread execution if there's no {{ServerSocket}} waiting for a connection. But the call still fails with a {{ConnectException}} if there's no {{ServerSocket}} listening on the given port. That would have been the only guess why the request didn't complete.;;;","14/Aug/23 12:41;mapohl;There was another test error with FLINK-29642 reported that has the same stacktrace: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24&l=15625]
{code:java}
[...]
Oct 14 02:18:03 ""main"" #1 prio=5 os_prio=0 tid=0x00007fa9bc00b800 nid=0x3528 waiting on condition [0x00007fa9c54e3000]
Oct 14 02:18:03    java.lang.Thread.State: WAITING (parking)
Oct 14 02:18:03 	at sun.misc.Unsafe.park(Native Method)
Oct 14 02:18:03 	- parking to wait for  <0x00000000aaae6cd0> (a java.util.concurrent.CompletableFuture$Signaller)
Oct 14 02:18:03 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Oct 14 02:18:03 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Oct 14 02:18:03 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Oct 14 02:18:03 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Oct 14 02:18:03 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Oct 14 02:18:03 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Oct 14 02:18:03 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Oct 14 02:18:03 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Oct 14 02:18:03 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
Oct 14 02:18:03 	at org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.testWithoutCatalogDB(MySqlCatalogITCase.java:309)
[...] {code};;;","21/Aug/23 07:23;mapohl;I merged the changes into the branches:

master (1.18): b5b1340f91d6cd2deb563eb67d8cbb3d1852544d
1.17: 8da9d50235776e41901b19c0f226f2826f03650e
1.16: ee6fbd0c4a9e3ee1b30872389829a284b1c6e215

It's not a fix but rather a change in how issues are logged in production for the {{CollectSinkOperatorCoordinator}}. I'm gonna keep this issue open in case we run into this problem again.;;;","24/Aug/23 09:20;mapohl;It happened again: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52570&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15116]

This time it happened in the {{{}FunctionITCase{}}}:
{code:java}
Aug 24 06:21:16 ""main"" #1 prio=5 os_prio=0 tid=0x00007f94d000b800 nid=0x78b2 waiting on condition [0x00007f94d8e99000]
Aug 24 06:21:16    java.lang.Thread.State: WAITING (parking)
Aug 24 06:21:16         at sun.misc.Unsafe.park(Native Method)
Aug 24 06:21:16         - parking to wait for  <0x00000000a6078ec0> (a java.util.concurrent.CompletableFuture$Signaller)
Aug 24 06:21:16         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Aug 24 06:21:16         at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Aug 24 06:21:16         at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Aug 24 06:21:16         at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Aug 24 06:21:16         at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Aug 24 06:21:16         at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:171)
Aug 24 06:21:16         at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Aug 24 06:21:16         at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 24 06:21:16         at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 24 06:21:16         at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Aug 24 06:21:16         at java.util.Iterator.forEachRemaining(Iterator.java:115)
Aug 24 06:21:16         at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)
Aug 24 06:21:16         at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.testSpecializedFunctionWithExpressionEvaluation(FunctionITCase.java:1257)
[...]{code};;;","24/Aug/23 09:53;Sergey Nuyanzin;also happened for 1.17
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52566&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12876

{noformat}
Aug 24 08:47:25 ""ForkJoinPool-1-worker-0"" #34 daemon prio=5 os_prio=0 tid=0x00007efde5632000 nid=0x72747 waiting on condition [0x00007efdd88c9000]
Aug 24 08:47:25    java.lang.Thread.State: WAITING (parking)
Aug 24 08:47:25 	at sun.misc.Unsafe.park(Native Method)
Aug 24 08:47:25 	- parking to wait for  <0x00000000a7385b78> (a java.util.concurrent.CompletableFuture$Signaller)
Aug 24 08:47:25 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Aug 24 08:47:25 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Aug 24 08:47:25 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Aug 24 08:47:25 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Aug 24 08:47:25 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Aug 24 08:47:25 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170)
Aug 24 08:47:25 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Aug 24 08:47:25 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 24 08:47:25 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 24 08:47:25 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Aug 24 08:47:25 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Aug 24 08:47:25 	at org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.materializeResult(BuiltInAggregateFunctionTestBase.java:121)

{noformat};;;","24/Aug/23 10:00;Sergey Nuyanzin;and for 1.16:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52565&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=15322
{noformat}
Aug 24 03:18:51 ""main"" #1 prio=5 os_prio=0 tid=0x00007f9da400b800 nid=0x72f5 waiting on condition [0x00007f9dad338000]
Aug 24 03:18:51    java.lang.Thread.State: WAITING (parking)
Aug 24 03:18:51 	at sun.misc.Unsafe.park(Native Method)
Aug 24 03:18:51 	- parking to wait for  <0x00000000c2cc4b60> (a java.util.concurrent.CompletableFuture$Signaller)
Aug 24 03:18:51 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Aug 24 03:18:51 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Aug 24 03:18:51 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Aug 24 03:18:51 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Aug 24 03:18:51 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Aug 24 03:18:51 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170)
Aug 24 03:18:51 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Aug 24 03:18:51 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 24 03:18:51 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 24 03:18:51 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Aug 24 03:18:51 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Aug 24 03:18:51 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
Aug 24 03:18:51 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
Aug 24 03:18:51 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
Aug 24 03:18:51 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
Aug 24 03:18:51 	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testAggWithCube(AggregateReduceGroupingITCase.scala:523)
Aug 24 03:18:51 	
{noformat};;;","24/Aug/23 10:06;mapohl;The {{BuiltInAggregateFunctionTestBase}} failure in 1.17 mentioned in [Sergey's comment|https://issues.apache.org/jira/browse/FLINK-32751?focusedCommentId=17758482&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17758482] and the {{MySqlCatalogITCase}} failure that happened in Oct 2022 (mentioned in [this comment|https://issues.apache.org/jira/browse/FLINK-32751?focusedCommentId=17754085&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17754085]) -might indicate that it's at least not a blocker for 1.18.-

Even though it is concerning that the error appears three times in a short period now. And the fix that was applied should cancel all requests while shutting down the {{executorService}}. I'm gonna have another look.;;;","24/Aug/23 10:13;mapohl;Ok, I'm raising this issue to become a blocker again. The amount of time it appears now is worrying. It might be related to our fix that we put in all the branches.;;;","24/Aug/23 12:34;Sergey Nuyanzin;another from today's master https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52591&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","24/Aug/23 12:35;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52543&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12436;;;","24/Aug/23 12:37;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52514&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=13893;;;","24/Aug/23 12:39;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52479&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14155;;;","24/Aug/23 12:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52512&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf;;;","24/Aug/23 12:45;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52513&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14143;;;","25/Aug/23 06:32;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52582&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14446;;;","25/Aug/23 08:23;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52623&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13839;;;","25/Aug/23 09:35;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52634&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15261;;;","25/Aug/23 09:56;mapohl;There's a race condition between closing the coordinator (which closes the socket and set the corresponding field to {{null}}) and handling a request where a socket is opened again if the corresponding socket field is null. If the close call happens after a new request is submitted but before the null check in the request handling happens, a new Socket will be created that's never closed but doesn't get any data, anymore. That results in the response never returning.

This race condition was also able to happen prior to the changes we introduced in the merges documented in [the comment above|https://issues.apache.org/jira/browse/FLINK-32751?focusedCommentId=17756703&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17756703]. I'm still puzzled why this issue starts to appear more frequently now. The main difference (besides the log output refactoring) was to change from {{shutdown}} to {{shutdownNow}} for the executor service and when the {{closeConnection}} call happened (before or after the shutdown). I would have expected that we run into this issue more often with the {{closeConnection}} call happening before the shutdown of the {{ExecutorService}} (which is how it was implemented before the recent changes) because there's a higher chance for the socket to be recreated. I still have to investigate that one.

But I'm working on a fix now with more elaborate unit testing of the class.;;;","25/Aug/23 15:53;mapohl;I provided a PR with the fix (see https://github.com/apache/flink/pull/23296);;;","28/Aug/23 08:22;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52644&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14472;;;","28/Aug/23 08:23;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52667&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28282;;;","28/Aug/23 08:25;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52666&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12115;;;","28/Aug/23 08:57;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52680&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=28413;;;","28/Aug/23 08:58;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52681&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14351;;;","28/Aug/23 09:00;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52682&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12892;;;","28/Aug/23 09:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52692&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12799;;;","28/Aug/23 09:03;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52693&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=16951;;;","28/Aug/23 09:05;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52694&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=13268;;;","29/Aug/23 07:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52740&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=29248;;;","29/Aug/23 07:07;mapohl;All from the same CI run:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52741&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=13816
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52741&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=15359
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52741&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14788;;;","30/Aug/23 05:33;mapohl;master (1.19):
* 5b8c4a74521410ca0391d63614c7a7da0b7f365d (actual fix)
* d343a0bc79b0c85c845d0011d29404567f28dd0a (improved testability)
1.18:
* 750311755b55ea2abf3313798453dcfefc36675a (actual fix)
* 848183c6dcd16c0c6a9766a2a4ea64c4a923040e (improved testability)
1.17:
* b6e4cc58adceb2de78d6cad0a794a3e24b43f896 (actual fix)
* abb5d7583232d72375019fb673334e72890f84e0 (improved testability)
1.16:
* 68af78571df6d4f0512bc2dda8b84f80f09adc3c (actual fix)
* ebcd8654f75d1eaabe139cc5779b670737d1bbe7 (improved testability);;;",,,,
fix resouces not fix in testcase,FLINK-32750,13546098,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,xuzifu,xuzifu,04/Aug/23 06:08,10/Aug/23 12:55,04/Jun/24 20:40,10/Aug/23 12:55,,,,,,,,,,Tests,,,,,,0,pull-request-available,,,fix resouces not fix in testcase， in some test case did not close resource in right way，this can cause connection leak in extreme scenarios. so need make a pr to fix it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 12:55:07 UTC 2023,,,,,,,,,,"0|z1jl1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 08:26;mapohl;[~xuzifu] please be more specific in this Jira issue. The title and description isn't clear. Based on the (closed) PR it is about adding the close mechanism to certain resources in various tests? Why is the PR closed again? The PR's description is also not filled out.

I'm going to remove the {{fixVersion}} in the mean time: That field is usually only set after the fix ended up in the corresponding branch. This issue could be closed in the future if it doesn't become more specific.;;;","07/Aug/23 08:31;xuzifu;[~mapohl]  ok， i would describe more details for it，and you can remove the {{fixVersion}} in the mean time，thanks;;;","07/Aug/23 09:59;xuzifu;[~mapohl]   pr had open again;;;","10/Aug/23 12:55;mapohl;I'm closing the issue because the corresponding PR was closed as well (the change wasn't reasonable enough to be merged).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql gateway supports default catalog loaded by CatalogStore,FLINK-32749,13546094,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,04/Aug/23 05:19,03/Sep/23 10:57,04/Jun/24 20:40,01/Sep/23 06:20,1.19.0,,,,,1.19.0,,,,Table SQL / Gateway,,,,,,0,pull-request-available,,,"Currently sql gateway will create memory catalog as default catalog, it should support default catalog loaded by catalog store",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 03 10:57:55 UTC 2023,,,,,,,,,,"0|z1jl0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/23 07:49;yangs;Can assign it to me?;;;","01/Sep/23 06:20;zjureel;Fixed by 1797a70c0074b9e0cbfd20abc2c33050a7906578;;;","03/Sep/23 10:57;xuzifu;[~zjureel]  can i get your wechar id，some problem want to talk with you， best wishes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WriteSinkFunction::cleanFile need close write automaticly,FLINK-32748,13546075,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Incomplete,,xuzifu,xuzifu,04/Aug/23 02:40,04/Aug/23 04:14,04/Jun/24 20:40,04/Aug/23 04:13,,,,,,1.9.4,,,,API / Core,,,,,,0,pull-request-available,,,WriteSinkFunction::cleanFile need close write automaticly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-04 02:40:21.0,,,,,,,,,,"0|z1jkwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ddl operations for catalog from CatalogStore,FLINK-32747,13546074,13541314,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,04/Aug/23 02:35,05/Feb/24 05:31,04/Jun/24 20:40,10/Aug/23 03:59,1.19.0,,,,,1.18.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,Fix ddl operations for catalog which loaded by CatalogStore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 03:59:35 UTC 2023,,,,,,,,,,"0|z1jkw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 03:59;zjureel;Fixed with 76554d186ad198384ff783e3e3f040dd738b7571 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using ZGC in JDK17 to solve long time class unloading STW,FLINK-32746,13546069,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,xiangyu0xf,xiangyu0xf,04/Aug/23 02:16,12/Oct/23 11:11,04/Jun/24 20:40,12/Oct/23 11:11,,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,"In a OLAP session cluster, a TM need to frequently create new classloaders and  generate new classes. These classes will be accumulated in metaspace. When metaspace data usage reaches a threshold, a FullGC with a long time Stop-the-World will be triggered. Currently, both SerialGC, ParallelGC and G1GC are doing Stop-the-World class unloading. Only ZGC supports concurrent class unload, see more in [https://bugs.openjdk.org/browse/JDK-8218905|https://bugs.openjdk.org/browse/JDK-8218905).].

 

In our scenario, a class unloading for a 2GB metaspace with 5million classes will stop the application more than 40 seconds. After switch to ZGC, the maximum STW of the application has been reduced to less than 10ms.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 11:11:01 UTC 2023,,,,,,,,,,"0|z1jkv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 11:11;xiangyu0xf;JDK17 is already supported in https://issues.apache.org/jira/browse/FLINK-15736, user can enable ZGC by configuring by customizing the `env.java.opts.all` parameter. There already have a example in OLAP QuickStart docs：https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/olap_quickstart/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a flag to skip InputSelectable preValidate step,FLINK-32745,13546011,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,03/Aug/23 14:42,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,1.20.0,,,,API / DataStream,Runtime / Configuration,,,,,0,,,,"{{StreamingJobGraphGenerator#preValidate}} has a step where it checks that no operator implements {{InputSelectable}} if checkpointing is enabled, because these features aren't compatible.

This step can be extremely expensive when the {{CodeGenOperatorFactory}} is used, because it requires all generated operator classes to actually be compiled (which usually only happens on the task manager).

If you know what jobs you're running this step can be pure overhead.
It would be nice if we'd have a flag to skip this validation step.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 07:58:33 UTC 2023,,,,,,,,,,"0|z1jki8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/23 07:58;yangs;can assign this issue to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveDialectQueryITCase.testWithOverWindow fails with timeout,FLINK-32744,13546007,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,03/Aug/23 14:18,21/Nov/23 15:11,04/Jun/24 20:40,,1.17.1,1.19.0,,,,,,,,Connectors / Hive,Tests,,,,,0,auto-deprioritized-critical,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51922&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=22287

{code}
Aug 03 03:36:53 [ERROR] org.apache.flink.connectors.hive.HiveDialectQueryITCase.testWithOverWindow  Time elapsed: 38.534 s  <<< ERROR!
Aug 03 03:36:53 org.apache.flink.table.api.TableException: Failed to execute sql
Aug 03 03:36:53 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:976)
Aug 03 03:36:53 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1424)
Aug 03 03:36:53 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:765)
Aug 03 03:36:53 	at org.apache.flink.connectors.hive.HiveDialectQueryITCase.testWithOverWindow(HiveDialectQueryITCase.java:690)
Aug 03 03:36:53 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 03 03:36:53 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 03 03:36:53 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 03 03:36:53 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 03 03:36:53 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 03 03:36:53 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 03 03:36:53 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 03 03:36:53 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 03 03:36:53 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 03 03:36:53 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 03 03:36:53 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 03 03:36:53 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 03 03:36:53 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 03 03:36:53 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 03 03:36:53 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 03 03:36:53 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 03 03:36:53 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 03 03:36:53 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Aug 03 03:36:53 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Aug 03 03:36:53 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Aug 03 03:36:53 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Aug 03 03:36:53 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Aug 03 03:36:53 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Aug 03 03:36:53 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
Aug 03 03:36:53 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Aug 03 03:36:53 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Aug 03 03:36:53 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Aug 03 03:36:53 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Aug 03 03:36:53 Caused by: java.lang.Exception: Could not create actor system
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startLocalActorSystem(AkkaBootstrapTools.java:238)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:349)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:327)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:247)
Aug 03 03:36:53 	at org.apache.flink.runtime.minicluster.MiniCluster.createLocalRpcService(MiniCluster.java:1179)
Aug 03 03:36:53 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:355)
Aug 03 03:36:53 	at org.apache.flink.client.program.PerJobMiniClusterFactory.submitJob(PerJobMiniClusterFactory.java:77)
Aug 03 03:36:53 	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:85)
Aug 03 03:36:53 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2206)
Aug 03 03:36:53 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
Aug 03 03:36:53 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:957)
Aug 03 03:36:53 	... 48 more
Aug 03 03:36:53 Caused by: java.util.concurrent.TimeoutException: Futures timed out after [20 seconds]
Aug 03 03:36:53 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
Aug 03 03:36:53 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
Aug 03 03:36:53 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:223)
Aug 03 03:36:53 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57)
Aug 03 03:36:53 	at scala.concurrent.Await$.result(package.scala:146)
Aug 03 03:36:53 	at akka.stream.SystemMaterializer.<init>(SystemMaterializer.scala:90)
Aug 03 03:36:53 	at akka.stream.SystemMaterializer$.createExtension(SystemMaterializer.scala:39)
Aug 03 03:36:53 	at akka.stream.SystemMaterializer$.createExtension(SystemMaterializer.scala:32)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl.registerExtension(ActorSystem.scala:1165)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl.$anonfun$loadExtensions$1(ActorSystem.scala:1208)
Aug 03 03:36:53 	at scala.collection.Iterator.foreach(Iterator.scala:943)
Aug 03 03:36:53 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
Aug 03 03:36:53 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
Aug 03 03:36:53 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
Aug 03 03:36:53 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
Aug 03 03:36:53 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl.loadExtensions$1(ActorSystem.scala:1202)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl.loadExtensions(ActorSystem.scala:1221)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:1042)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:1029)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:1029)
Aug 03 03:36:53 	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:1052)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:102)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:62)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:54)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaUtils.createActorSystem(AkkaUtils.java:421)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startActorSystem(AkkaBootstrapTools.java:253)
Aug 03 03:36:53 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startLocalActorSystem(AkkaBootstrapTools.java:236)
Aug 03 03:36:53 	... 58 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 15:11:16 UTC 2023,,,,,,,,,,"0|z1jkhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Nov/23 15:11;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54741&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=22152;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kafka connector source can directly parse data collected from kafka-connect,FLINK-32743,13545992,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sunxiaojian,sunxiaojian,03/Aug/23 12:47,16/Feb/24 06:14,04/Jun/24 20:40,,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,stale-major,,"If the data in Kafka is collected through the Kafka connect system, it is hoped that the data collected through Flink has already excluded the schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 03 22:35:10 UTC 2023,,,,,,,,,,"0|z1jke0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-examples-batch module,FLINK-32742,13545972,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Wencong Liu,Wencong Liu,03/Aug/23 09:31,08/Aug/23 03:15,04/Jun/24 20:40,,2.0.0,,,,,2.0.0,,,,Examples,,,,,,0,2.0-related,,,"All DataSet APIs will be deprecated in [FLINK-32558], and the examples in the flink-examples-batch module should no longer be included in flink-dist. This change aims to prevent guiding users to continue to use the DataSet API. 

However, it is important to note that for testing purposes, the module is still utilized by many end-to-end tests. Therefore, we should explore options to remove the examples from the flink-dist before removing the DataSet API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 03:15:11 UTC 2023,,,,,,,,,,"0|z1jk9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 16:13;jiabao.sun;Hi [~Wencong Liu], can I take this ticket?;;;","08/Aug/23 02:25;Wencong Liu;Hi [~jiabao.sun], currently flink-examples-batch module should be reserved to ensure the correctness of DataSet API(Although it's deprecated). The removal will be started in Flink 1.20 or Flink 2.0. This issue is just a reminder.;;;","08/Aug/23 02:34;jiabao.sun;Thanks [~Wencong Liu] to remind me of this.
By the way, I found some test cases depends on the classes of flink-examples-batch module such as JsonJobGraphGenerationTest, HDFSTest.
Maybe we can decouple these tests first.;;;","08/Aug/23 02:46;Wencong Liu;Good idea [~jiabao.sun]. Maybe you could find all classes that depend on flink-examples-batch (these classes are not testing or designed for DataSet API, otherwise they can be deleted directly in the future). Then decouple them in a seperated jira. I'll help you assign that ticket, WDYT?;;;","08/Aug/23 03:15;jiabao.sun;[~Wencong Liu] That's good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove DataSet related descriptions in doc,FLINK-32741,13545966,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,pegasas,Wencong Liu,Wencong Liu,03/Aug/23 09:12,08/Aug/23 04:32,04/Jun/24 20:40,08/Aug/23 04:30,2.0.0,,,,,1.18.0,,,,Documentation,,,,,,0,2.0-related,pull-request-available,,"Since All DataSet APIs will be deprecated in [FLINK-32558] and we don't recommend developers to use the DataSet, the descriptions of DataSet should be removed in the doc after [FLINK-32558].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 04:30:58 UTC 2023,,,,,,,,,,"0|z1jk88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 17:45;pegasas;Hi, [~Wencong Liu] ,

May I work on this issue?;;;","04/Aug/23 01:42;xtsong;Thanks for volunteering, [~pegasas]. You are assigned. Please go ahead.

FYI, the DataSet documentation should be removed before we shipping the release 1.18, which means we only have ~2-3 weeks woking on this (including PR review).;;;","04/Aug/23 02:32;pegasas;Thanks! will start immediately. will link PR to this issue.;;;","08/Aug/23 04:30;xtsong;master (1.18): dcdca1e460369d43391a7632717a85ea59d556b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce literal null value for flink-conf.yaml,FLINK-32740,13545964,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,paul8263,paul8263,03/Aug/23 08:17,10/Aug/23 03:34,04/Jun/24 20:40,,1.15.4,,,,,,,,,API / Core,,,,,,0,pull-request-available,,,"Hi community,

Currently in flink-conf.yaml we only consider simplified YAML syntax like key value pairs. And it might not be the right timing to indroduce YAML parser. As [FLINK-23620 |https://issues.apache.org/jira/browse/FLINK-23620] has been stated, there might be some keys that violate the YAML naming conventions.

The current situation is, if we want to unset the value (or set the value as its default), what we could do is to remove the key value pair completely or leave the value blank (e.g. `rest.port: `). It might be inconvenient or less readable for conf file that controlled by other applications.

So is it necessary to add some special literal values that will be translated to the real null value? For YAML we usually see ~, null or empty value as null value. If necessary I would like to do this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 03:34:32 UTC 2023,,,,,,,,,,"0|z1jk7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 09:49;JunRuiLi;[~paul8263] 

Could you provide more details about the problem you are trying to solve?

IIUC, after using the standard yaml parser, it naturally supports reading empty value or null value, and parses the value into null. Then the value will not be put into the Configuration object. It can achieve the effect of using the default value of the configuration item.;;;","03/Aug/23 10:48;paul8263;Hi [~JunRuiLi],

I did not mean to use the standard YAML parser. We just need to compare the extracted values with the predefined null literals. If the value should be treated as null, we won't add it to the Configuration object. It's the same as how we deal with the empty value now. ;;;","03/Aug/23 12:26;JunRuiLi;Hi, [~paul8263] 

I understand that your idea is to predefine a field as null. If the extracted value is equal to this field, the configuration statement will be skipped. I have a little concern about this: how can we let users avoid using predefined fields as value? This will lead to behavior changes that the user is not aware of. For example, the user configured ""key: null"" in the past, but this configuration will no longer take effect after this change. 
Do you have any good way to solve this problem?;;;","04/Aug/23 01:16;paul8263;Hi [~JunRuiLi],

Good question. It is also my concern.

I searched values in [Configuration|http://https//nightlies.apache.org/flink/flink-docs-release-1.17/zh/docs/deployment/config/] and there are no valid values named null(but there are some valid values named NONE). Some values may contain null (e.g. /dev/null) but they will not be translated to null. If the user intends to set the value as the string ""null"", a slight change is needed by adding double quotes (e.g. some.key: ""null"").

I also investigate the null literals defined in YAML standard:
 * <empty string>
 * ~
 * null (case sensitive)

Currently we may follow all the standards or only part of them, as the YAML parser is not used.;;;","10/Aug/23 03:34;paul8263;After further investigation Configuration only provides one-way conversion (`ConfigurationUtils::convertToString`). In order to support configuration serialization and deserialization for literal null values, we have to introduce two-way string conversion, by adding double quotes when writing to files and removing quotes when read from files. It may increase the code complexity.

To introduce tow-way conversion, we may implement a string literal to Java string conversion method in ""ConfigurationUtils::convertValue"" and change the ""Configuration::getString(String key, String defaultValue)"" to invoke getString(ConfigOption<String> configOption, String overrideDefault) method.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
restarting not work,FLINK-32739,13545958,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,SpongebobZ,SpongebobZ,03/Aug/23 07:33,04/Aug/23 05:30,04/Jun/24 20:40,04/Aug/23 05:30,1.14.5,,,,,,,,,API / Core,,,,,,0,,,,"In my flink standalone cluster, I config 2 taskmanagers. Then I test with following steps:
 # submit streaming job which was configed to fixed restart strategy to flink session environment
 # this job was running on taskmanager1. Then I killed the taskmanager1.
 # this job turned to be failed after restarting attemps.

this job could not be transported to taskmanager2 which had enough slots as expected.

Here's the exception trace:
{code:java}
2023-08-03 15:13:56
org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=10000)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
    at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
    at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1473)
    at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1133)
    at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1073)
    at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:776)
    at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195)
    at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182)
    at org.apache.flink.runtime.scheduler.SharedSlot.lambda$release$4(SharedSlot.java:271)
    at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
    at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683)
    at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010)
    at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:271)
    at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152)
    at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:419)
    at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeAndReleaseSlots(DefaultDeclarativeSlotPool.java:411)
    at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:382)
    at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.internalReleaseTaskManager(DeclarativeSlotPoolService.java:249)
    at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.releaseTaskManager(DeclarativeSlotPoolService.java:230)
    at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:506)
    at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.handleTaskManagerConnectionLoss(JobMaster.java:1348)
    at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyTargetUnreachable(JobMaster.java:1359)
    at org.apache.flink.runtime.heartbeat.HeartbeatMonitorImpl.reportHeartbeatRpcFailure(HeartbeatMonitorImpl.java:123)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.runIfHeartbeatMonitorExists(HeartbeatManagerImpl.java:275)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.reportHeartbeatTargetUnreachable(HeartbeatManagerImpl.java:267)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.handleHeartbeatRpcFailure(HeartbeatManagerImpl.java:262)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.lambda$handleHeartbeatRpc$0(HeartbeatManagerImpl.java:248)
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: org.apache.flink.runtime.jobmaster.JobMasterException: TaskManager with id 192.168.0.10:46626-007128 is no longer reachable.
    ... 35 more
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 09:04;SpongebobZ;jobmanager.log;https://issues.apache.org/jira/secure/attachment/13061892/jobmanager.log",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 03:43:41 UTC 2023,,,,,,,,,,"0|z1jk6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 08:41;JunRuiLi;[~SpongebobZ] would you share more details of the failed job?
 - Error stack
 - JM logs

In fact, release-1.14 is no longer maintained, can you use the release-1.17.1 to try again?;;;","03/Aug/23 09:06;SpongebobZ;[~JunRuiLi] Hi Junrui, I had uploaded the JM partitial logs relative to the failed job which jobid is `15ffcdad572bd98fd74ffd45848e84cb`. The Error stack was in the issue description.

And I found some relative logs in the other taskmanger:
{code:java}
2023-08-03 17:21:41,618 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 109179dd9cefa1f4b109b8f726d0e0f7 for job 15ffcdad572bd98fd74ffd45848e84cb from resource manager with leader id 00000000000000000000000000000000.
2023-08-03 17:21:41,618 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 109179dd9cefa1f4b109b8f726d0e0f7.
2023-08-03 17:21:41,618 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 15ffcdad572bd98fd74ffd45848e84cb for job leader monitoring.
2023-08-03 17:21:41,618 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@192.168.0.11:6123/user/rpc/jobmanager_3 with leader id 00000000-0000-0000-0000-000000000000.
2023-08-03 17:21:41,621 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2023-08-03 17:21:41,625 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@192.168.0.11:6123/user/rpc/jobmanager_3 for job 15ffcdad572bd98fd74ffd45848e84cb.
2023-08-03 17:21:41,625 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 15ffcdad572bd98fd74ffd45848e84cb.
2023-08-03 17:21:41,626 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 15ffcdad572bd98fd74ffd45848e84cb.
2023-08-03 17:21:41,627 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:2, state:ALLOCATED, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=256.000mb (268435451 bytes), taskOffHeapMemory=0 bytes, managedMemory=245.760mb (257698041 bytes), networkMemory=61.440mb (64424510 bytes)}, allocationId: 109179dd9cefa1f4b109b8f726d0e0f7, jobId: 15ffcdad572bd98fd74ffd45848e84cb).
2023-08-03 17:21:41,629 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 15ffcdad572bd98fd74ffd45848e84cb from job leader monitoring.
2023-08-03 17:21:41,629 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 15ffcdad572bd98fd74ffd45848e84cb.{code};;;","03/Aug/23 12:13;JunRuiLi;[~SpongebobZ] From the JM log, the job ""15ffcdad572bd98fd74ffd45848e84cb"" has 5 regions. When taskmanager1 is killed, all 5 regions will record a restart, the maximum number of restarts is exceeded in the end. So you can increase maxNumberRestartAttempts or configure ""jobmanager.execution.failover-strategy: full"" to enable job global restart.;;;","04/Aug/23 03:10;SpongebobZ;Thanks [~JunRuiLi] , this option helps. When one of regions restart, it will be restarted in the original job or new job ?;;;","04/Aug/23 03:43;JunRuiLi;[~SpongebobZ] It will just restart tasks in the original jobs. And more details about failover-strategy can see [here|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#restart-pipelined-region-failover-strategy]. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PROTOBUF format supports projection push down,FLINK-32738,13545932,13543785,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhoujira86,zhoujira86,zhoujira86,03/Aug/23 02:20,29/Mar/24 02:53,04/Jun/24 20:40,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,support projection push down for protobuf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 03:01:13 UTC 2023,,,,,,,,,,"0|z1jk0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 02:25;zhoujira86;[~libenchao] created, please help assign to me master, thanks a lot;;;","03/Aug/23 02:26;libenchao;[~zhoujira86] Assigned to you. I've changed the fixVersion to '1.19.0' since 1.18.0 is in feature freeze stage now.;;;","04/Aug/23 03:01;lsy;[~zhoujira86] Looks good to me if we can support this feature. Kindly remind you should also consider the case of nested fields pushdown.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Snowflake sink connector,FLINK-32737,13545920,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,morezaei00,morezaei00,02/Aug/23 21:18,09/Nov/23 12:22,04/Jun/24 20:40,09/Nov/23 12:22,,,,,,,,,,,,,,,,1,,,,"Implement a sink connector to allow for loading pre-processed data into Snowflake. In the streaming world, this can make use of the [Snowpipe Streaming API|https://javadoc.io/doc/net.snowflake/snowflake-ingest-sdk/2.0.2/net/snowflake/ingest/streaming/package-summary.html] and potentially in a future version support the exactly-once semantics.

This sink would use the latest unified sink interfaces to implement at-least-once as a first step, providing {{DeliveryGuarantee.NONE}} and {{DeliveryGuarantee.AT_LEAST_ONCE}} semantics. A user would be able to provide a custom serializer for going from {{InputT}} incoming type to {{Map<String, Object>}} as documented by Snowflake, and configure the sink using a builder pattern.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 09 12:21:50 UTC 2023,,,,,,,,,,"0|z1jjy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/23 03:51;morezaei00;This connector has been open-sourced and the first version, {{1.0.7-1.17}}, is available:
https://github.com/deltastreaminc/flink-connector-snowflake;;;","09/Nov/23 12:21;martijnvisser;I'm not sure why this is registered in the ASF Flink Jira; since it's not part of the ASF Flink Project, it should not be listed/used here. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a post to announce new connectors and connector externalisation,FLINK-32736,13545892,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,elphastori,elphastori,elphastori,02/Aug/23 17:02,04/Aug/23 15:19,04/Jun/24 20:40,04/Aug/23 15:19,,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,"Announce the availability of the DynamoDB, MongoDB and OpenSearch connectors as well the externalisation of Flink connectors.

*References*
 * [FLIP-252: Amazon DynamoDB Sink Connector|https://cwiki.apache.org/confluence/display/FLINK/FLIP-252%3A+Amazon+DynamoDB+Sink+Connector]
 * [FLIP-262: Introduce MongoDB connector|https://cwiki.apache.org/confluence/display/FLINK/FLIP-262%3A+Introduce+MongoDB+connector]
 * [FLIP-243: Dedicated OpenSearch connectors|https://cwiki.apache.org/confluence/display/FLINK/FLIP-243%3A+Dedicated+Opensearch+connectors]
 * [Externalized Connector development|https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 15:18:56 UTC 2023,,,,,,,,,,"0|z1jjrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 17:03;elphastori;[~danny.cranmer] Could you please assign this to me?;;;","02/Aug/23 22:17;elphastori;https://github.com/apache/flink-web/pull/668;;;","04/Aug/23 15:18;dannycranmer;Merged commit [{{aca5d7a}}|https://github.com/apache/flink-web/commit/aca5d7a38f0d30590679e039d7b34f73592ac40f] into apache:asf-site ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Gateway for Native Kubernetes Application Mode ,FLINK-32735,13545882,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,pegasas,pegasas,02/Aug/23 15:24,04/Aug/23 05:31,04/Jun/24 20:40,04/Aug/23 05:31,1.19.0,,,,,,,,,Deployment / Kubernetes,Table SQL / Gateway,,,,,0,,,,"Hi, Flink Community,

I have visited our doc on these pages:
 # Flink Native Kubernetes: [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/]
 # Flink SQL Gateway: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql-gateway/overview/]
 # Flink Kubernetes Operator Roadmap: [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/development/roadmap/]

Will we support Flink SQL on Native Kubernetes in our next version?

Personally, I think it is an important feature, especially in this cloud-native trend era. 

Since we have support per job on YARN,

I do not know if there is non-technical thinking for blocking. such as commerce.

Have we supported Flink SQL Gateway with native k8s now?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Fri Aug 04 05:31:38 UTC 2023,,,,,,,,,,"0|z1jjpk:",9223372036854775807,tracked by FLIP-316.,,,,,,,,,,,,,,,,,,,"04/Aug/23 04:35;lsy;[~pegasas] 

As stated in the documentation you listed, Native K8S is a session cluster, similar to yarn session clusters, and Flink SQL gateway supports native k8s. You can specify the context for the K8S clusters when you start the Gateway, and then the Gateway will be able to submit the jobs to the corresponding clusters.

Based on your description, I guess you may be asking if Flink SQL Gateway supports application mode on native k8s, which is currently not supported, but the community is already discussing this issue with FLIP-316, and we may be able to provide support for it in version 1.19. You can see FLIP-316 for more cdetail.

Finally, this is a question about Flink SQL gateway and K8S integration support, not to solve a specific implementation details of Flink, opening a Jira issue is not appropriate, you can directly in Flink's user mailing list to start an discussion, there will be experts in the community to answer your questions.

 

[1]: https://cwiki.apache.org/confluence/display/FLINK/FLIP-316%3A+Introduce+SQL+Driver;;;","04/Aug/23 05:30;pegasas;Thanks [~lsy]! 
Your answer exactly fits my question!
Will follow FLIP-316 for further steps and close this issue.;;;","04/Aug/23 05:31;pegasas;Tracked by FLIP-316.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add .mvn/maven.config to .gitignore,FLINK-32734,13545853,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,02/Aug/23 13:05,03/Aug/23 09:52,04/Jun/24 20:40,03/Aug/23 09:52,1.18.0,1.19.0,,,,1.18.0,,,,Build System,,,,,,0,pull-request-available,,,"FLINK-28016 allowed Flink to switch to newer (i.e. 3.2.5+) Maven versions. Maven 3.3.1 introduced certain local configuration files that are automatically injected (e.g. {{.mvn/jvm.options}} and {{.mvn/maven.config}}).

The former one is actively used (and checked into the repository) to define certain Java-related requirements. {{.mvn/maven.config}} instead, is not utilized right now. It can be used, though, to customize the Maven execution to once needs (e.g. always skipping {{flink-runtime-web}} through {{-Pskip-webui-build}}).

To be fair, that would be also possible through the {{${user.home}/.m2/settings.xml}} through profiles. There are certain configuration parameters, though, that are not configurable easily through profiles.

Configuring a checkout-specific local repository isn't possible. Checkout-specific local repositories are handy, though when working in multiple Flink checkouts (e.g. one checkout for reviews and one for contributions). Using a single local repository would cause problems when working on a branch locally while reviewing other PRs (and checking them out) that touch the same files.

A workaround for such a scenario would be to have a checkout-specific settings.xml that specifies a checkout-specific local repository.

To support this, I'm suggesting to add {{.mvn/maven.config}} to {{.gitignore}} for now. That would allow the developer to enable specific Maven parameters by default that match his/her needs. We can revert this change as soon as we identify the need to have this file included in the repository.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28016,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 09:52:35 UTC 2023,,,,,,,,,,"0|z1jjj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 09:52;mapohl;master: 93da795b3b33fa028962ee00e1a13bb60c51b809;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AlgoOperator for FpGrowth,FLINK-32733,13545833,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,indyw,indyw,02/Aug/23 11:02,02/Oct/23 22:35,04/Jun/24 20:40,,ml-2.4.0,,,,,,,,,Library / Machine Learning,,,,,,0,pull-request-available,stale-major,,"_Frequent Pattern Growth_ Algorithm is the method of finding frequent patterns. 
The distributed version of FpGrowth algorithm is described in Li et al., PFP: Parallel FP-Growth for Query Recommendation. This algorithm will be an addition to the 
_org.apache.flink.ml.recommendation_ package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 02 22:35:12 UTC 2023,,,,,,,,,,"0|z1jjeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto offset reset should be exposed to user,FLINK-32732,13545816,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zhoujira86,zhoujira86,02/Aug/23 08:07,15/Apr/24 09:28,04/Jun/24 20:40,15/Apr/24 09:28,1.16.1,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,stale-major,,"{code:java}
// code placeholder
maybeOverride(
        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,
        startingOffsetsInitializer.getAutoOffsetResetStrategy().name().toLowerCase(),
        true); {code}
now flink override the auto.offset.reset with the scan.startup.mode config, and user's explicit config does not take effect. I think maybe we should expose this to customer?

 

I think after consuming kafka records from earliest to latest, the scan.startup.mode should no longer influence the kafka scan behave. So I suggest change the override to false.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 06 22:35:02 UTC 2023,,,,,,,,,,"0|z1jjaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 08:22;zhoujira86;[~renqs]  let me know your thinking please;;;","03/Aug/23 02:33;zhoujira86;[~renqs] The main defect of the current implement is that if the earliest records are timed out very quickly, the task can always be restarting when it found the offset not found when it startup;;;","06/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayE2ECase.testHiveServer2ExecuteStatement failed due to MetaException,FLINK-32731,13545797,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,mapohl,mapohl,02/Aug/23 06:51,02/Nov/23 13:30,04/Jun/24 20:40,12/Sep/23 07:23,1.18.0,,,,,1.18.0,1.19.0,,,Table SQL / Gateway,,,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=10987

{code}
Aug 02 02:14:04 02:14:04.957 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 198.658 s <<< FAILURE! - in org.apache.flink.table.gateway.SqlGatewayE2ECase
Aug 02 02:14:04 02:14:04.966 [ERROR] org.apache.flink.table.gateway.SqlGatewayE2ECase.testHiveServer2ExecuteStatement  Time elapsed: 31.437 s  <<< ERROR!
Aug 02 02:14:04 java.util.concurrent.ExecutionException: 
Aug 02 02:14:04 java.sql.SQLException: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation d440e6e7-0fed-49c9-933e-c7be5bbae50d.
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
Aug 02 02:14:04 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 02 02:14:04 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 02 02:14:04 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Aug 02 02:14:04 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Aug 02 02:14:04 	at java.lang.Thread.run(Thread.java:750)
Aug 02 02:14:04 Caused by: org.apache.flink.table.api.TableException: Could not execute CreateTable in path `hive`.`default`.`CsvTable`
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1289)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:939)
Aug 02 02:14:04 	at org.apache.flink.table.operations.ddl.CreateTableOperation.execute(CreateTableOperation.java:84)
Aug 02 02:14:04 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1080)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:570)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:458)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:210)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
Aug 02 02:14:04 	... 7 more
Aug 02 02:14:04 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create table default.CsvTable
Aug 02 02:14:04 	at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:547)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.lambda$createTable$16(CatalogManager.java:950)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1283)
Aug 02 02:14:04 	... 16 more
Aug 02 02:14:04 Caused by: MetaException(message:Got exception: java.net.ConnectException Call From 70d5c7217fe8/172.17.0.2 to hadoop-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)
Aug 02 02:14:04 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 02 02:14:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 02 02:14:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)
Aug 02 02:14:04 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 02 02:14:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 02 02:14:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)
Aug 02 02:14:04 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createTable(HiveMetastoreClientWrapper.java:174)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:539)
Aug 02 02:14:04 	... 18 more
Aug 02 02:14:04 
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
Aug 02 02:14:04 	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQL(FlinkDistribution.java:341)
Aug 02 02:14:04 	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:281)
Aug 02 02:14:04 	at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$GatewayClusterControllerImpl.submitSQLJob(LocalStandaloneFlinkResource.java:220)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.SqlGatewayE2ECase.executeStatement(SqlGatewayE2ECase.java:133)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.SqlGatewayE2ECase.testHiveServer2ExecuteStatement(SqlGatewayE2ECase.java:107)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 02 02:14:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 02 02:14:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 02 02:14:04 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 02 02:14:04 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 02 02:14:04 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 02 02:14:04 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 02 02:14:04 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
Aug 02 02:14:04 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 02 02:14:04 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 02 02:14:04 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 02 02:14:04 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 02 02:14:04 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 02 02:14:04 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 02 02:14:04 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 02 02:14:04 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 02 02:14:04 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
Aug 02 02:14:04 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 02 02:14:04 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 02 02:14:04 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 02 02:14:04 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 02 02:14:04 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Aug 02 02:14:04 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Aug 02 02:14:04 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Aug 02 02:14:04 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Aug 02 02:14:04 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Aug 02 02:14:04 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Aug 02 02:14:04 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
Aug 02 02:14:04 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Aug 02 02:14:04 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Aug 02 02:14:04 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Aug 02 02:14:04 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Aug 02 02:14:04 Caused by: java.sql.SQLException: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation d440e6e7-0fed-49c9-933e-c7be5bbae50d.
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
Aug 02 02:14:04 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 02 02:14:04 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 02 02:14:04 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Aug 02 02:14:04 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Aug 02 02:14:04 	at java.lang.Thread.run(Thread.java:750)
Aug 02 02:14:04 Caused by: org.apache.flink.table.api.TableException: Could not execute CreateTable in path `hive`.`default`.`CsvTable`
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1289)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:939)
Aug 02 02:14:04 	at org.apache.flink.table.operations.ddl.CreateTableOperation.execute(CreateTableOperation.java:84)
Aug 02 02:14:04 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1080)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:570)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:458)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:210)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
Aug 02 02:14:04 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
Aug 02 02:14:04 	... 7 more
Aug 02 02:14:04 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create table default.CsvTable
Aug 02 02:14:04 	at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:547)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.lambda$createTable$16(CatalogManager.java:950)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1283)
Aug 02 02:14:04 	... 16 more
Aug 02 02:14:04 Caused by: MetaException(message:Got exception: java.net.ConnectException Call From 70d5c7217fe8/172.17.0.2 to hadoop-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)
Aug 02 02:14:04 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 02 02:14:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 02 02:14:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)
Aug 02 02:14:04 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 02 02:14:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 02 02:14:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 02 02:14:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 02 02:14:04 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)
Aug 02 02:14:04 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createTable(HiveMetastoreClientWrapper.java:174)
Aug 02 02:14:04 	at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:539)
Aug 02 02:14:04 	... 18 more
Aug 02 02:14:04 
Aug 02 02:14:04 	at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:385)
Aug 02 02:14:04 	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254)
Aug 02 02:14:04 	at org.apache.flink.tests.util.flink.FlinkDistribution.lambda$submitSQLJob$6(FlinkDistribution.java:293)
Aug 02 02:14:04 	at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:126)
Aug 02 02:14:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 02 02:14:04 	at java.lang.Thread.run(Thread.java:750)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32988,FLINK-33418,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 07:23:16 UTC 2023,,,,,,,,,,"0|z1jj6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 06:53;mapohl;[~fsk119] can you look at this issue or delegate it to the right person. This seems to be a new problem. Feel free to lower the priority if you think it shouldn't block the release.;;;","07/Aug/23 13:47;fsk119;[~mapohl] hi. I think the failed test is because the test failed to start the hive container. We can find the container has the following message:

 
{code:java}
02:13:03,693 [docker-java-stream--148018928] INFO  org.apache.flink.table.gateway.containers.HiveContainer      [] - STDOUT: 2023-08-02 07:58:03,693 INFO gave up: hdfs-namenode entered FATAL state, too many start retries too quickly
{code}

Because it's just a test issue and the current information doesn't help us to understand why namenode fails to start up, I think can reduce the priority. I will open a PR to back up the namenode and metastore logs when test fails again.

;;;","09/Aug/23 07:46;fsk119;Merged into master: 5654eb798c744c924aff93d68ec3c4e413e75232 

Waiting for more details.;;;","14/Aug/23 08:58;Sergey Nuyanzin;Hi [~fsk119]
it seems there is another reproduction on AZP
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52200&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d
{noformat}
Aug 11 12:11:05 Caused by: MetaException(message:Got exception: java.net.ConnectException Call From 2c2d0325a890/172.17.0.2 to hadoop-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)
Aug 11 12:11:05 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 11 12:11:05 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 11 12:11:05 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 11 12:11:05 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)
Aug 11 12:11:05 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
Aug 11 12:11:05 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 11 12:11:05 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 11 12:11:05 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 11 12:11:05 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 11 12:11:05 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)
Aug 11 12:11:05 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
Aug 11 12:11:05 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createTable(HiveMetastoreClientWrapper.java:174)
Aug 11 12:11:05 	at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:539)
Aug 11 12:11:05 	... 18 more
Aug 11 12:11:05 
Aug 11 12:11:05 	at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:385)
Aug 11 12:11:05 	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254)
Aug 11 12:11:05 	at org.apache.flink.tests.util.flink.FlinkDistribution.lambda$submitSQLJob$6(FlinkDistribution.java:293)
Aug 11 12:11:05 	at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:126)
Aug 11 12:11:05 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 11 12:11:05 	at java.lang.Thread.run(Thread.java:750)

{noformat};;;","23/Aug/23 03:01;fsk119;Thanks for the sharing. I think we should add some retry mechanism to restart the container when namenode fails. I will open a PR to fix this soon.;;;","28/Aug/23 01:42;fsk119;Merged into master: 4b84b6cd5983ae8f058fae731eb0f4af6214b738

Waiting to check whether it works...;;;","30/Aug/23 14:09;mapohl;Thanks for looking into it, [~fsk119]. How do you find out whether ""it works""? Should we provide backports for this test instability as well? It affects 1.18 but the change you documented was only merged to {{master}}.;;;","31/Aug/23 03:43;fsk119;> How do you find out whether ""it works""?

I just try to observe whether the test fails again in the daily run tests. But I find I can not find the flink-ci.flink-master-mirror pipeline anymore..

>  It affects 1.18 but the change you documented was only merged to master.

Sure. I will cherry-pick this to release-1.18
;;;","31/Aug/23 06:45;mapohl;Yeah, the Pipeline overview went away. We have to check what's wrong there. In the meantime, you can use [this overview|https://dev.azure.com/apache-flink/apache-flink/_build?view=runs] which includes not only the {{master}} and release branches builds but also the PR builds. That's a bit annoying.;;;","06/Sep/23 08:30;jingge;[~fsk119] do you have time to move forward? Thanks!;;;","07/Sep/23 09:59;fsk119;The current fix doesn't work in the hive3 envrionment. I have opened a PR to fix this. But the fix works in the hive2 environment.;;;","12/Sep/23 07:23;fsk119;Merged into release-1.18: 
3dcdc7f29384bc399e65ce46253975570e93481f
9e5659ea65278b2b699ab0c0f0eafc918a0107bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchExecMultipleInput may contain unsupported SourceTransformation,FLINK-32730,13545790,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lsy,Weijie Guo,Weijie Guo,02/Aug/23 04:52,16/Aug/23 07:06,04/Jun/24 20:40,16/Aug/23 07:06,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"When testing 10T TPC-DS on cluster, Q6 always reports an error as shown in the figure during compile time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32608,,,,,,,,,,,,,,,,"02/Aug/23 04:51;Weijie Guo;bug.png;https://issues.apache.org/jira/secure/attachment/13061842/bug.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 07:06:07 UTC 2023,,,,,,,,,,"0|z1jj54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 04:53;Weijie Guo;Marked as blocker as this can be stably reproduced.;;;","02/Aug/23 04:54;Weijie Guo;By offline discuss with [~lsy], this is caused by FLINK-32608. He will help fix this.;;;","04/Aug/23 06:02;lsy;[~Weijie Guo] Thanks for report, I will fix it.;;;","15/Aug/23 07:59;Sergey Nuyanzin;[~Weijie Guo], [~lsy] could you please let us know if there is any progress/blockers here?;;;","15/Aug/23 08:28;lsy;[~Sergey Nuyanzin] Hi, we need to wait for [~lincoln.86xy] to review the code. He's on holiday and will be back tomorrow.;;;","15/Aug/23 09:05;Sergey Nuyanzin;thanks for the update;;;","16/Aug/23 07:06;lincoln.86xy;Fixed in master: 079cd60510df03d66068af4d2f9128d401f8e728;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow create an initial deployment with suspend state,FLINK-32729,13545767,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,01/Aug/23 23:14,15/Aug/23 15:31,04/Jun/24 20:40,09/Aug/23 07:24,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"With this feature, Users could create an application in suspend status as a backup for the other running application to improve the failure recovery time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 07:24:24 UTC 2023,,,,,,,,,,"0|z1jj00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 07:24;gyfora;merged to main 8777b307c97e876f4c31615ab81f664353f89a9a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metrics are not reported in Python UDF (used inside FlinkSQL) when exception is raised,FLINK-32728,13545746,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elkhand,elkhand,01/Aug/23 18:07,01/Aug/23 18:29,04/Jun/24 20:40,,1.16.2,1.17.1,,,,,,,,API / Python,Table SQL / Runtime,,,,,0,,,,"When Python UDF (which is used inside FlinkSQL) raises an exception, then metrics get lost and not reported.  Facing this issue both in Flink 1.16.2 and Flink 1.17.1 (Python 3.9).
If an exception is not raised, then metrics show up.
 
It is not mentioned on Flink documentation that UDFs should not throw an exception.
=======================
FlinkSQL script content:
=======================

CREATE TABLE input_table (
price DOUBLE
) WITH (
'connector' = 'datagen',
'rows-per-second' = '1'
);

CREATE TABLE output_table WITH ('connector' = 'print')
LIKE input_table (EXCLUDING ALL);

CREATE FUNCTION myDivide AS 'custom_udf.divide_udf'
LANGUAGE PYTHON;

-- Fail scenario: ZeroDivisionError: division by zero
INSERT into output_table (select myDivide(value, 0)  from input_table);

=======================
Python UDF content:
=======================

from pyflink.table.udf import ScalarFunction, udf
from pyflink.table import DataTypes

import logging

class DivideUDF(ScalarFunction):
    def __init__(self):
        self.success_counter = None
        self.fail_counter = None
    def open(self, function_context):
        self.success_counter = function_context.get_metric_group().counter(""flinksql_custom_udf_success_metric"")
        self.fail_counter = function_context.get_metric_group().counter(""flinksql_custom_udf_fail_metric"")
    def eval(self, x, y):
        [logging.info|http://logging.info/]('executing custom udf with logging and metric example...')
        try:
            result = x/y
            self.success_counter.inc()
            return result
        except Exception as e:
            self.fail_counter.inc()
            raise e

divide_udf = udf(DivideUDF(), result_type=DataTypes.DOUBLE())
 
=======================
Exception stack trace:
=======================
 
2023-07-26 18:17:20
org.apache.flink.runtime.taskmanager.AsynchronousException: Caught exception while processing timer.
at org.apache.flink.streaming.runtime.tasks.StreamTask$StreamTaskAsyncExceptionHandler.handleAsyncException(StreamTask.java:1575)
at org.apache.flink.streaming.runtime.tasks.StreamTask.handleAsyncException(StreamTask.java:1550)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1704)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1693)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:838)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:787)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: TimerException\{java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush}
... 15 more
Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.invokeFinishBundle(AbstractExternalPythonFunctionOperator.java:107)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByTime(AbstractPythonFunctionOperator.java:300)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$open$0(AbstractPythonFunctionOperator.java:118)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1702)
... 14 more
Caused by: java.lang.RuntimeException: Failed to close remote bundle
at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:423)
at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:407)
at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.lambda$invokeFinishBundle$0(AbstractExternalPythonFunctionOperator.java:86)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
... 1 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 3: Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 267, in _execute
    response = task()
  File ""/usr/local/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 340, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/usr/local/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 580, in do_instruction
    return getattr(self, request_type)(
  File ""/usr/local/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 618, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/usr/local/lib/python3.9/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 995, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File ""/usr/local/lib/python3.9/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 221, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 346, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 348, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 169, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 196, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""/usr/local/lib/python3.9/site-packages/pyflink/fn_execution/table/operations.py"", line 101, in process_element
    return self.func(value)
  File ""<string>"", line 1, in <lambda>
  File ""/tmp/python-dist-9b10e17c-99f8-4a8a-adb0-90faca859ca5/python-files/blob_p-e59b6f256efca9ae3fc37635498b82f316f8ac65-34f486b7351b4e2c73282ab4069c075e/custom_udf.py"", line 27, in eval
    raise e
  File ""/tmp/python-dist-9b10e17c-99f8-4a8a-adb0-90faca859ca5/python-files/blob_p-e59b6f256efca9ae3fc37635498b82f316f8ac65-34f486b7351b4e2c73282ab4069c075e/custom_udf.py"", line 22, in eval
    result = x/y
ZeroDivisionError: float division by zero
 
 at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:318)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:301)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:834)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
at org.apache.beam.vendor.grpc.v1p43p2.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
... 3 more",Flink 1.16.2 and Flink 1.17.1 (Python 3.9),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-01 18:07:06.0,,,,,,,,,,"0|z1jivc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding capability of setting Attributes in Google Pub/Sub sink,FLINK-32727,13545729,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yinghai,yinghai,01/Aug/23 15:07,18/Sep/23 08:21,04/Jun/24 20:40,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,,,1,,,,"Currently the Google PubSub Sink can only publish messages with data in body. To take full advantage of Google PubSub, having the ability to set attributes is important, so subscriptions can filter on attributes and consume part of the event stream.

Google Doc on [message|https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage] and [subscription.|https://cloud.google.com/pubsub/docs/subscription-message-filter]

 

I have a basic idea on this and had a patch working locally. Wondering how I can incorporate this back to the official connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 08:21;vetle@roeim.net;pubsub-attributes-0.patch;https://issues.apache.org/jira/secure/attachment/13062982/pubsub-attributes-0.patch",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 18 08:18:27 UTC 2023,,,,,,,,,,"0|z1jirk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 08:17;vetle@roeim.net;Came here to open a discussion about this, we also have this need ... and I also have something working for this! :D

 ;;;","18/Sep/23 08:18;vetle@roeim.net;[~yinghai] can you attach a patch against the current main branch?

I've attached mine, please check it out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Test Flink Release 1.18,FLINK-32726,13545663,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,renqs,renqs,renqs,01/Aug/23 09:06,24/Oct/23 12:42,04/Jun/24 20:40,14/Sep/23 02:27,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,release-testing,,,,,,,,,,,,,,,,,FLINK-32932,FLINK-33271,FLINK-33347,FLINK-32921,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-01 09:06:55.0,,,,,,,,,,"0|z1jicw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to control writing of timestamp to Kafka topic in KafkaRecordSerializationSchema.builder,FLINK-32725,13545661,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiechenling,xiechenling,01/Aug/23 08:42,01/Aug/23 08:42,04/Jun/24 20:40,,1.14.0,,,,,,,,,Connectors / Kafka,,,,,,0,,,,"In the older versions of Kafka sink for Flink, it was possible to configure whether the message timestamp should be written to Kafka. This was achievable using the method `FlinkKafkaProducer.setWriteTimestampToKafka(true)`.

However, in the newer versions of Kafka sink, when using `KafkaRecordSerializationSchema.builder()`, the message timestamp is automatically written to the Kafka topic using the context's timestamp.

{code:scala}
KafkaSink
...
.setRecordSerializer(KafkaRecordSerializationSchema.builder()
...
.build()
{code}


If a user wishes to exclude the timestamp from being written to Kafka, they currently need to create a custom `KafkaRecordSerializationSchema` by extending it and overriding the `serialize` method.

{code:scala}
KafkaSink.builder[(String, String)]()
.setBootstrapServers(kafkaAddress)
.setRecordSerializer((element: (String, String), context: KafkaRecordSerializationSchema.KafkaSinkContext, timestamp: lang.Long) => {
new ProducerRecord(sinkTopic, element._1.getBytes, element._2.getBytes)
})
{code}

I propose adding a new method, similar to `setWriteTimestampToKafka`, to `KafkaRecordSerializationSchema.builder()`, which allows users to control whether the timestamp should be included in the output to the Kafka topic. This would provide a more straightforward and consistent approach for users who do not want the timestamp to be written to Kafka.

Thank you for considering this enhancement.",flink 1.16.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-01 08:42:35.0,,,,,,,,,,"0|z1jicg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mark CEP API classes as Public / PublicEvolving,FLINK-32724,13545660,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,01/Aug/23 08:31,01/Aug/23 08:31,04/Jun/24 20:40,,,,,,,,,,,Library / CEP,,,,,,0,,,,"Currently most CEP API classes, e.g. Pattern, PatternSelectFunction etc are not annotated as Public / PublicEvolving. We should improve this to make it clear which classes are public.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-08-01 08:31:03.0,,,,,,,,,,"0|z1jic8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-334 : Decoupling autoscaler and kubernetes and support the Standalone Autoscaler,FLINK-32723,13545642,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,01/Aug/23 07:21,11/Nov/23 15:13,04/Jun/24 20:40,10/Nov/23 08:17,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,"This is an umbrella jira for decoupling autoscaler and kubernetes.

https://cwiki.apache.org/confluence/x/x4qzDw",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 11 15:13:25 UTC 2023,,,,,,,,,,"0|z1ji88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 14:40;fanrui;FLIP-334 (FLINK-32723) wants to decouple autoscalers and kubernetes operators and support autoscaler standalone. FLINK-33099 is supporting the autoscaler standalone, it works as a separate process and support a single flink cluster. Flink users can play autoscaler and in-place rescale with flink-1.18 directly without kubernetes.

Based on it, I think once FLINK-33099 is merged, we almost met our initial expectations of FLIP-334.

In addition, kubernetes operator 1.7 will be released soon, and the rest improvements related to autoscaler or autoscaler-standalone can be done in the kubernetes operator 1.8 or later. So I create FLINK-33452 to follow them. Hopefully, autoscaler standalone will be production-ready in a future release.;;;","10/Nov/23 08:16;fanrui;Hi [~gyfora]  [~mxm] , I resolve this Jira first, it means the FLIP-334(decoupling and support autoscaler standalone) has already done in kubernetes-operator 1.7.0. Thank you very much for your professional suggestions and review on FLIP-334. :)

I create FLINK-33452 to follow the a series of improvements related to Autoscaler Standalone, and do them gradually in the future versions.;;;","11/Nov/23 15:13;mxm;Thanks for following through with this [~fanrui]! Great work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
239 exit code in flink-runtime,FLINK-32722,13545638,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,01/Aug/23 07:08,28/Nov/23 08:07,04/Jun/24 20:40,,1.16.2,,,,,,,,,Test Infrastructure,,,,,,0,auto-deprioritized-major,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51852&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=8418
{code:java}
Aug 01 01:03:49 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-runtime: There are test failures.
Aug 01 01:03:49 [ERROR] 
Aug 01 01:03:49 [ERROR] Please refer to /__w/2/s/flink-runtime/target/surefire-reports for the individual test results.
Aug 01 01:03:49 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Aug 01 01:03:49 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 01 01:03:49 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/2/s/flink-runtime/target/surefire/surefirebooter1803120121827605294.jar /__w/2/s/flink-runtime/target/surefire 2023-08-01T00-58-17_520-jvmRun1 surefire9107652818401825168tmp surefire_261701267003130520249tmp
Aug 01 01:03:49 [ERROR] Error occurred in starting fork, check output in log
Aug 01 01:03:49 [ERROR] Process Exit Code: 239
Aug 01 01:03:49 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 01 01:03:49 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/2/s/flink-runtime/target/surefire/surefirebooter1803120121827605294.jar /__w/2/s/flink-runtime/target/surefire 2023-08-01T00-58-17_520-jvmRun1 surefire9107652818401825168tmp surefire_261701267003130520249tmp
Aug 01 01:03:49 [ERROR] Error occurred in starting fork, check output in log
Aug 01 01:03:49 [ERROR] Process Exit Code: 239
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Aug 01 01:03:49 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Aug 01 01:03:49 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Aug 01 01:03:49 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Aug 01 01:03:49 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Aug 01 01:03:49 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
Aug 01 01:03:49 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Aug 01 01:03:49 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 01 01:03:49 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 01 01:03:49 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 01 01:03:49 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
Aug 01 01:03:49 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Aug 01 01:03:49 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Aug 01 01:03:49 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Aug 01 01:03:49 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Aug 01 01:03:49 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 01 01:03:49 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/2/s/flink-runtime/target/surefire/surefirebooter1803120121827605294.jar /__w/2/s/flink-runtime/target/surefire 2023-08-01T00-58-17_520-jvmRun1 surefire9107652818401825168tmp surefire_261701267003130520249tmp
Aug 01 01:03:49 [ERROR] Error occurred in starting fork, check output in log
Aug 01 01:03:49 [ERROR] Process Exit Code: 239
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Aug 01 01:03:49 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Aug 01 01:03:49 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 01 01:03:49 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Aug 01 01:03:49 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Aug 01 01:03:49 [ERROR] at java.lang.Thread.run(Thread.java:748)
Aug 01 01:03:49 [ERROR] -> [Help 1]
Aug 01 01:03:49 [ERROR] 
Aug 01 01:03:49 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Aug 01 01:03:49 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Aug 01 01:03:49 [ERROR] 
Aug 01 01:03:49 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Aug 01 01:03:49 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
Aug 01 01:03:49 [ERROR] 
Aug 01 01:03:49 [ERROR] After correcting the problems, you can resume the build with the command
Aug 01 01:03:49 [ERROR]   mvn <goals> -rf :flink-runtime {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33671,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 08 22:35:15 UTC 2023,,,,,,,,,,"0|z1ji7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/23 07:10;mapohl;I did a log analysis on unfinished tests without success. Any test class that was executed had a corresponding log message stating that the test finished:
{code:java}
$ cat watchdog | grep -e ""Running "" -e ""Tests run"" | grep -o ""[^ ]*$"" | sort | uniq -c | grep -v '     2'
      9 0
      1 1
      1 26
      1 34
      1 as
      4 org.apache.flink.api.common.operators.base.InnerJoinOperatorBaseTest
      4 org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtilsTest
      4 org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest
      4 org.apache.flink.runtime.rest.messages.AggregatedTaskDetailsInfoTest
      4 org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest {code};;;","01/Aug/23 14:50;plucas;Possibly related: FLINK-18290;;;","30/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Oct/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
agg max/min supports char type,FLINK-32721,13545619,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,jackylau,jackylau,jackylau,01/Aug/23 02:19,21/Dec/23 09:46,04/Jun/24 20:40,19/Dec/23 12:39,1.18.0,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,stale-assigned,,"{code:java}
// flink
Flink SQL> CREATE TABLE Orders (
>     name char(10),
>     price        DECIMAL(32,2),
>     buyer        ROW<first_name STRING, last_name STRING>,
>     order_time   TIMESTAMP(3)
> ) WITH (
>   'connector' = 'datagen'
> );
[INFO] Execute statement succeed.


Flink SQL> select max(name) from Orders;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Max aggregate function does not support type: ''CHAR''.
Please re-check the data type. {code}
{code:java}
// mysql

CREATE TABLE IF NOT EXISTS `docs` (
  `id` int(6) unsigned NOT NULL,
  `rev` int(3) unsigned NOT NULL,
  `content` char(200) NOT NULL,
  PRIMARY KEY (`id`,`rev`)
) DEFAULT CHARSET=utf8;
INSERT INTO `docs` (`id`, `rev`, `content`) VALUES
  ('1', '1', 'The earth is flat'),
  ('2', '1', 'One hundred angels can dance on the head of a pin'),
  ('1', '2', 'The earth is flat and rests on a bull\'s horn'),
  ('1', '3', 'The earth is like a ball.');

select max(content) from docs;

// result 
|max(content)|
The earth is like a ball.{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 12:26:20 UTC 2023,,,,,,,,,,"0|z1ji34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 07:20;paul8263;Hi community:

It seems that Flink does not support max/min functions do not work with char value. However those functions work smoothly with varchar.

We can update the min/max aggregation functions, by making them treat char values the same as varchar values.

I world like to fix this. Could someone assign this to me? Thanks.;;;","02/Aug/23 07:45;jackylau;hi [~libenchao]  do you have time to review it [https://github.com/apache/flink/pull/23121] ?;;;","02/Aug/23 12:26;libenchao;[~paul8263] Seems Jacky has proposed a PR already, feel free to help review the PR.

[~jackylau] I won't have much time recently due to the upcoming Apache Community Over Code Asia 2023, but feel free to ping me if no other reviewers take care of it.;;;","03/Aug/23 07:35;paul8263;Hi [~libenchao] and [~jackylau],

Thank you for your contribution.

In this PR you introduced CharMaxAggFunction and CharMinAggFunction especially for char fields. They have overwritten the getResultType method. I am confused about the way this method works. Does it affect the downstream external system, by providing the correct field types? It seems that legacy Flink does not distinguish varchar and char. I tested it by using the following sql with Flink 1.15.4 (without this PR min function works correctly with char field type):

{code:java}
tEnv.executeSql(""create table test(name char(10)) with ('connector' = 'datagen')"");
tEnv.executeSql(""select typeof(min(name)) from test limit 10"").print();
{code}

I tested char(10), varchar(10) and string types. For all of them we can get the correct field types using typeof function.
;;;","04/Aug/23 10:26;lsy;[~jackylau] After tracking down and finding the history issue https://issues.apache.org/jira/browse/FLINK-12834, we have char type support for the Min function in it. After discussing with [~lzljs3620320]  offline, the reason why we don't support char type for Max function is because of the omission. So I think we should reuse the existing code as we did for the Min function, instead of having to extract a new code implementation for char alone.;;;","03/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Oct/23 01:06;paul8263;Hi [~jackylau],

Are you still working on this? Based on the discussion it seems that we can reuse createMaxAggFunction.;;;","06/Nov/23 07:52;paul8263; Hi [~jackylau], [~libenchao] and [~lsy],

It seems that the solution considering reusing the MaxAggFunction to deal with char data type is the following:
[Flink-32721|https://github.com/apache/flink/pull/23671]

Correct me if I am wrong.

[~jackylau] If you think it is applicable, feel free to reference mine. Thanks.
;;;","08/Nov/23 07:46;paul8263;Hi Community,

 

It seems that there is a contradiction in what CHAR means in Flink. Char type in SQL means a fixed-length string but in org.apache.flink.api.common.typeinfo.Types.CHAR is a single character with the type of java.lang.Character in Java.

Based on this in order to test max aggretation function with type 'char'  in unit test org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.scala, we should not define another filed with ""Types.{color:#c77dbb}CHAR"" {color}{color:#172b4d}as it stands for a single character. The only {color:#172b4d}representation{color} of ""char"" type in SQL in org.apache.flink.api.common.typeinfo.Types is STRING. 
{color};;;","18/Dec/23 01:53;jackylau;hi [~lsy] sorry for late response, i will modify it;;;","19/Dec/23 12:19;lsy;Sorry for responding later, I found this issue is duplicated with FLINK-25476;;;","19/Dec/23 12:26;lsy;[~xuyangzhong] opened the PR Jan 17, 2022 for issue FLINK-25476,  so I'll review the first PR by time, and any other duplicate issues will be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add GENERATE_SERIES support in SQL & Table API,FLINK-32720,13545600,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hanyuzheng,hanyuzheng,01/Aug/23 00:05,01/Dec/23 14:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,stale-major,,"GENERATE_SERIES Function

Description

Constructs an array of values between {{start}} and {{{}end{}}}, inclusive.

Parameters {{start}} and {{end}} can be an {{INT}} or {{{}BIGINT{}}}.

{{{}step{}}}, if supplied, specifies the step size. The step can be positive or negative. If not supplied, {{step}} defaults to {{{}1{}}}. Parameter {{step}} must be an {{{}INT{}}}.

Syntax
The syntax for the GENERATE_SERIES function is:
{code:java}
GENERATE_SERIES(start, end)
GENERATE_SERIES(start, end, step){code}
 

Example
Let's look at some  function examples and explore how to use the SPLIT function.

For example:

 
{code:java}
SELECT GENERATE_SERISE(1, 5);
Result: [1,2,3,4,5]

SELECT GENERATE_SERISE(0, 10, 2); 
Result: [0, 2, 4, 6, 8, 10] {code}
see also:

1.PostgreSQL: PostgreSQL offers a function called {{generate_series}} which generates a set of contiguous integers from a start to an end value. An optional 'step' parameter is available to specify the increment between each integer.

https://www.postgresql.org/docs/current/functions-srf.html

2.ksqlDB: As you mentioned, ksqlDB provides a function called {{GENERATE_SERIES}} that generates a series of numbers, starting from a given start value, incrementing each time by a step value, until it reaches or exceeds a given end value.

https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/scalar-functions/#generate_series

3.BigQuery: BigQuery has a function called {{GENERATE_ARRAY}} that generates an array consisting of integers from the start value to the end value, with each integer incremented by the step value. You can find more details in the https://cloud.google.com/bigquery/docs/reference/standard-sql/array_functions#generate_array

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 09 22:35:07 UTC 2023,,,,,,,,,,"0|z1jhyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/23 00:37;hanyuzheng;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","09/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Block upgrades during checkpoints when LATEST_STATE recovery is configured,FLINK-32719,13545572,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,31/Jul/23 18:37,31/Jul/23 18:37,04/Jun/24 20:40,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,"Savepoints are allowed to finish if LATEST_STATE recovery is configured. The same optimization should be added with regards to checkpoints (introduced in FLINK-29634)

Can be tested [similar to savepoints.|https://github.com/apache/flink-kubernetes-operator/blob/8d307086122bdf4f74aaa9a69b5b8cc9025afdc8/flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/sessionjob/SessionJobReconcilerTest.java#L388]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-31 18:37:28.0,,,,,,,,,,"0|z1jhso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 'UNION ALL' with a 'GROUP BY' condition in Flink is causing the checkpoint to become unbounded,FLINK-32718,13545500,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,isugimpy,agg_neha07,agg_neha07,31/Jul/23 09:09,24/Sep/23 05:55,04/Jun/24 20:40,,1.16.1,,,,,,,,,Table SQL / API,Table SQL / Planner,,,,,0,,,,"I am having trouble understanding why adding a 'UNION ALL' with a 'GROUP BY' condition in Flink is causing the checkpoint to become unbounded. I've attached two Flink SQL queries where the only difference is the addition of one more 'UNION ALL'. Surprisingly, as soon as we add the 'UNION ALL' part to the query, the checkpoint size keeps increasing, which is not expected. We anticipated it would follow the same pattern of increasing and decreasing based on the throughput, as it does without the 'UNION ALL'.
When I analyze the breakdown of the checkpoint size, I find that the 'Interval Join' operator has the largest size. I suspect there might be a bug causing this difference in checkpoint behavior due to the addition of the 'GROUP BY' in the query.
We are using Flink version 1.16.1 with RocksDB as the backend, and incremental checkpointing is enabled.
PS: Interestingly, when I take periodic savepoints for both jobs, their patterns are similar, and the savepoint size is smaller than the checkpoint in the job with the 'UNION ALL'. Attaching the queries in the comment below.

With Union ALL:

select
  *
from
  (
    with p2d as (
      select
        delivered.job_id,
        delivered.store_id,
        substring(
          GetGeoHash(
            CAST(delivered.metadata_json.address.lat as double),
            CAST(delivered.metadata_json.address.lng as double)
          )
          from
            1 for 6
        ) AS cx_gh6,
        (
          (
            cast(
              delivered.metadata_json.lastMileDistance as double
            ) / cast(1000 as double)
          ) /(
            cast(
              delivered.updated_at - pickedup.updated_at as double
            ) / cast(60 * 60 * 1000 as double)
          )
        ) as lm_speed_kmph,
        delivered.proctime as proctime
      from
        awz_s3_OrderLogsEvent pickedup
        inner join awz_s3_OrderLogsEvent delivered on delivered.job_id = pickedup.job_id
        and delivered.status = 'DELIVERY_DELIVERED'
        and delivered.type = 'INSTA'
        and pickedup.proctime between delivered.proctime - interval '95' minute
        and delivered.proctime + interval '5' minute
      where
        pickedup.status = 'DELIVERY_PICKEDUP'
        and pickedup.type = 'INSTA'
    )
    select
      'lmSpeedKmph_avg_storeId_30m#' || cast(store_id as varchar) as key,
      round(Avg(lm_speed_kmph), 4) AS `value`
    from
      p2d
    group by
      HOP(
        proctime,
        interval '5' minute,
        interval '30' minute
      ),
      store_id
    union all
    select
      'lmSpeedKmph_avg_cxGh6_30m#' || cast(cx_gh6 as varchar) as key,
      round(Avg(lm_speed_kmph), 4) AS `value`
    from
      p2d
    group by
      HOP(
        proctime,
        interval '5' minute,
        interval '30' minute
      ),
      cx_gh6
  )

 

Without Union all which is showing expected behaviour:

select
  *
from
  (
    with p2d as (
      select
        delivered.job_id,
        delivered.store_id,
        substring(
          GetGeoHash(
            CAST(delivered.metadata_json.address.lat as double),
            CAST(delivered.metadata_json.address.lng as double)
          )
          from
            1 for 6
        ) AS cx_gh6,
        (
          (
            cast(
              delivered.metadata_json.lastMileDistance as double
            ) / cast(1000 as double)
          ) /(
            cast(
              delivered.updated_at - pickedup.updated_at as double
            ) / cast(60 * 60 * 1000 as double)
          )
        ) as lm_speed_kmph,
        delivered.proctime as proctime
      from
        awz_s3_OrderLogsEvent pickedup
        inner join awz_s3_OrderLogsEvent delivered on delivered.job_id = pickedup.job_id
        and delivered.status = 'DELIVERY_DELIVERED'
        and delivered.type = 'INSTA'
        and pickedup.proctime between delivered.proctime - interval '95' minute
        and delivered.proctime + interval '5' minute
      where
        pickedup.status = 'DELIVERY_PICKEDUP'
        and pickedup.type = 'INSTA'
    )
    select
      'lmSpeedKmph_avg_cxGh6_30m#' || cast(cx_gh6 as varchar) as key,
      round(Avg(lm_speed_kmph), 4) AS `value`
    from
      p2d
    group by
      HOP(
        proctime,
        interval '5' minute,
        interval '30' minute
      ),
      cx_gh6
  )

 

 

!withoutHeader.png!",Apache Flink 1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/23 09:10;agg_neha07;Screenshot 2023-07-31 at 10.40.30 AM.png;https://issues.apache.org/jira/secure/attachment/13061788/Screenshot+2023-07-31+at+10.40.30+AM.png","31/Jul/23 09:10;agg_neha07;Screenshot 2023-07-31 at 10.40.38 AM.png;https://issues.apache.org/jira/secure/attachment/13061789/Screenshot+2023-07-31+at+10.40.38+AM.png","31/Jul/23 09:10;agg_neha07;Screenshot 2023-07-31 at 10.41.07 AM.png;https://issues.apache.org/jira/secure/attachment/13061790/Screenshot+2023-07-31+at+10.41.07+AM.png","31/Jul/23 09:10;agg_neha07;Screenshot 2023-07-31 at 10.41.26 AM.png;https://issues.apache.org/jira/secure/attachment/13061791/Screenshot+2023-07-31+at+10.41.26+AM.png","31/Jul/23 09:10;agg_neha07;Screenshot 2023-07-31 at 10.41.35 AM.png;https://issues.apache.org/jira/secure/attachment/13061792/Screenshot+2023-07-31+at+10.41.35+AM.png","31/Jul/23 09:10;agg_neha07;Screenshot 2023-07-31 at 10.41.48 AM.png;https://issues.apache.org/jira/secure/attachment/13061793/Screenshot+2023-07-31+at+10.41.48+AM.png","24/Sep/23 05:54;agg_neha07;withoutHeader.png;https://issues.apache.org/jira/secure/attachment/13063126/withoutHeader.png",,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 19 10:09:40 UTC 2023,,,,,,,,,,"0|z1jhco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 12:57;qingyue;Can you provide the physical plans (by the ""EXPLAIN"" statement) for the two queries to better identify the issue?;;;","19/Sep/23 02:18;agg_neha07;Hello Jane Chan,

I am attaching the Physical Plan. Let me know if you want the execution plan as well.

== Optimized Physical Plan ==
Union(all=[true], union=[rill_server_timestamp, key, value])
:- Calc(select=[CAST(PROCTIME_MATERIALIZE(PROCTIME()) AS TIMESTAMP(3) NOT NULL) AS rill_server_timestamp, ||(_UTF-16LE'lmSpeedKmph_avg_storeId_30m#', CAST(store_id0 AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")) AS key, ROUND($f1, 4) AS value])
:  +- GroupWindowAggregate(groupBy=[store_id0], window=[SlidingGroupWindow('w$, proctime0, 1800000, 300000)], select=[store_id0, AVG($f2) AS $f1])
:     +- Exchange(distribution=[hash[store_id0]])
:        +- Calc(select=[proctime0, store_id AS store_id0, /(/(CAST(metadata_json.lastMileDistance AS DOUBLE), 1000:DOUBLE), /(CAST(-(updated_at0, updated_at) AS DOUBLE), 3.6E6:DOUBLE)) AS $f2])
:           +- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=false, leftLowerBound=-5700000, leftUpperBound=300000, leftTimeIndex=2, rightTimeIndex=4], where=[AND(=(job_id0, job_id),>=(proctime, -(proctime0, 5700000:INTERVAL MINUTE)), <=(proctime, +(proctime0, 300000:INTERVAL MINUTE)))], select=[job_id, updated_at, proctime, job_id0, updated_at0, store_id, metadata_json, proctime0])
:              :- Exchange(distribution=[hash[job_id]])
:              :  +- Calc(select=[job_id, updated_at, PROCTIME() AS proctime], where=[AND(=(status, _UTF-16LE'DELIVERY_PICKEDUP':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), =(type, _UTF-16LE'INSTA':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""))])
:              :     +- TableSourceScan(table=[[*anonymous_datastream_source$10*]], fields=[event_id, uuid, time_stamp, schema_version, _server_time_stamp, id, job_id, event_type, update_time, update_json, status, updated_at, type, structured, store_id, deTriggered, metadata_json, proctime])
:              +- Exchange(distribution=[hash[job_id]])
:                 +- Calc(select=[job_id, updated_at, store_id, metadata_json, PROCTIME() AS proctime], where=[AND(=(status, _UTF-16LE'DELIVERY_DELIVERED':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), =(type, _UTF-16LE'INSTA':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""))])
:                    +- TableSourceScan(table=[[*anonymous_datastream_source$10*]], fields=[event_id, uuid, time_stamp, schema_version, _server_time_stamp, id, job_id, event_type, update_time, update_json, status, updated_at, type, structured, store_id, deTriggered, metadata_json, proctime])
+- Calc(select=[CAST(PROCTIME_MATERIALIZE(PROCTIME()) AS TIMESTAMP(3) NOT NULL) AS rill_server_timestamp, ||(_UTF-16LE'lmSpeedKmph_avg_cxGh6_30m#', $f1) AS key, ROUND($f1_0, 4) AS value])
   +- GroupWindowAggregate(groupBy=[$f1], window=[SlidingGroupWindow('w$, proctime0, 1800000, 300000)], select=[$f1, AVG($f2) AS $f1_0])
      +- Exchange(distribution=[hash[$f1]])
         +- Calc(select=[proctime0, SUBSTRING(GetGeoHash(metadata_json.address.lat, metadata_json.address.lng), 1, 6) AS $f1, /(/(CAST(metadata_json.lastMileDistance AS DOUBLE), 1000:DOUBLE), /(CAST(-(updated_at0, updated_at) AS DOUBLE), 3.6E6:DOUBLE)) AS $f2])
            +- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=false, leftLowerBound=-5700000, leftUpperBound=300000, leftTimeIndex=2, rightTimeIndex=3], where=[AND(=(job_id0, job_id),>=(proctime, -(proctime0, 5700000:INTERVAL MINUTE)), <=(proctime, +(proctime0, 300000:INTERVAL MINUTE)))], select=[job_id, updated_at, proctime, job_id0, updated_at0, metadata_json, proctime0])
               :- Exchange(distribution=[hash[job_id]])
               :  +- Calc(select=[job_id, updated_at, PROCTIME() AS proctime], where=[AND(=(status, _UTF-16LE'DELIVERY_PICKEDUP':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), =(type, _UTF-16LE'INSTA':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""))])
               :     +- TableSourceScan(table=[[*anonymous_datastream_source$10*]], fields=[event_id, uuid, time_stamp, schema_version, _server_time_stamp, id, job_id, event_type, update_time, update_json, status, updated_at, type, structured, store_id, deTriggered, metadata_json, proctime])
               +- Exchange(distribution=[hash[job_id]])
                  +- Calc(select=[job_id, updated_at, metadata_json, PROCTIME() AS proctime], where=[AND(=(status, _UTF-16LE'DELIVERY_DELIVERED':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), =(type, _UTF-16LE'INSTA':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""))])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$10*]], fields=[event_id, uuid, time_stamp, schema_version, _server_time_stamp, id, job_id, event_type, update_time, update_json, status, updated_at, type, structured, store_id, deTriggered, metadata_json, proctime])

without union all query:

== Optimized Physical Plan ==
Calc(select=[CAST(PROCTIME_MATERIALIZE(PROCTIME()) AS TIMESTAMP(3) NOT NULL) AS rill_server_timestamp, ||(_UTF-16LE'lmSpeedKmph_avg_cxGh6_30m#', $f1) AS key, ROUND($f1_0, 4) AS value])
+- GroupWindowAggregate(groupBy=[$f1], window=[SlidingGroupWindow('w$, proctime0, 1800000, 300000)], select=[$f1, AVG($f2) AS $f1_0])
   +- Exchange(distribution=[hash[$f1]])
      +- Calc(select=[proctime0, SUBSTRING(GetGeoHash(metadata_json.address.lat, metadata_json.address.lng), 1, 6) AS $f1, /(/(CAST(metadata_json.lastMileDistance AS DOUBLE), 1000:DOUBLE), /(CAST(-(updated_at0, updated_at) AS DOUBLE), 3.6E6:DOUBLE)) AS $f2])
         +- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=false, leftLowerBound=-5700000, leftUpperBound=300000, leftTimeIndex=2, rightTimeIndex=3], where=[AND(=(job_id0, job_id), >=(proctime, -(proctime0, 5700000:INTERVAL MINUTE)), <=(proctime, +(proctime0, 300000:INTERVAL MINUTE)))], select=[job_id, updated_at, proctime, job_id0, updated_at0, metadata_json, proctime0])
            :- Exchange(distribution=[hash[job_id]])
            :  +- Calc(select=[job_id, updated_at, PROCTIME() AS proctime], where=[AND(=(status, _UTF-16LE'DELIVERY_PICKEDUP':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), =(type, _UTF-16LE'INSTA':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""))])
            :     +- TableSourceScan(table=[[*anonymous_datastream_source$11*]], fields=[event_id, uuid, time_stamp, schema_version, _server_time_stamp, id, job_id, event_type, update_time, update_json, status, updated_at, type, structured, store_id, deTriggered, metadata_json, proctime])
            +- Exchange(distribution=[hash[job_id]])
               +- Calc(select=[job_id, updated_at, metadata_json, PROCTIME() AS proctime], where=[AND(=(status, _UTF-16LE'DELIVERY_DELIVERED':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), =(type, _UTF-16LE'INSTA':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""))])
                  +- TableSourceScan(table=[[*anonymous_datastream_source$11*]], fields=[event_id, uuid, time_stamp, schema_version, _server_time_stamp, id, job_id, event_type, update_time, update_json, status, updated_at, type, structured, store_id, deTriggered, metadata_json, proctime]);;;","19/Sep/23 09:44;qingyue;[~agg_neha07] Thanks for providing the both plans.

From the plan which derived from ""union all"" query, it shows there are two interval join rel nodes, and two group window aggregate rel nodes (which means there will be two extra stateful operators for the runtime), so it's expected that the total checkpoint size should be larger than the query without ""union all"". Since there's no further info about the pipeline, it's hard to draw conclusions that there is a bug.;;;","19/Sep/23 10:09;agg_neha07;[~qingyue]  I understand that the checkpoint size will be larger for the job where there is ""union all"" but it should not keep on increasing and should show the same pattern as the individual jobs do. For the individual queries, the checkpoint size remains bounded and as soon as i put union all, checkpoint size keeps on increasing.

Please look at the screenshots attached for more information.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change of topic list from KafkaSource not works,FLINK-32717,13545485,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Information Provided,,kination,kination,31/Jul/23 07:51,02/Aug/23 11:55,04/Jun/24 20:40,31/Jul/23 09:42,,,,,,,,,,,,,,,,0,,,,"I've setup 'KafkaSource' as following:

 
{code:java}
KafkaSource.<T>builder().setTopics(""A"", ""B"")...{code}
 

and later, removed 1 topic from list as following and restart application.

 
{code:java}
KafkaSource.<T>builder().setTopics(""B"")...{code}
 

But application still ingest data from topic 'A'.
 * Is this expected?
 * If it is, how can I stop ingesting data from removed topic?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 11:55:29 UTC 2023,,,,,,,,,,"0|z1jh9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/23 09:42;martijnvisser;If you're restarting from a checkpoint/savepoint, that is expected. The `setTopics` is only followed when starting from a clean job. If you don't want this, you currently have to start with a clean state. The situation is expected to be improved with https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217389320;;;","02/Aug/23 11:55;kination;[~martijnvisser] got it. Thanks for following-up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Give 'Default'(or maybe 'None') option for 'scheduler-mode',FLINK-32716,13545481,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kination,kination,31/Jul/23 07:43,23/Oct/23 01:05,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"By setting-up scheduler-mode as 'REACTIVE', it scales-up/down by computing status.

[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#advanced-scheduling-options]

But currently it only allows 'REACTIVE', and when I want to de-activate with such value as 'None', it causes exception.

(For now, it causes exception if I setup any other value instead of 'REACTIVE')

 

To make configuration bit more flexible, how about give 'None' (or 'Default') as an option, to run in default mode?

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 23 01:05:34 UTC 2023,,,,,,,,,,"0|z1jh8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 01:04;paul8263;Hi [~kination],

I got the same problem. Currently I am working on automatic Flink configuration and got the same annoying issue. The only legal value is 'reactive'. If we want to remain the default value, we have to complelely remove its configuration key.

For each configuration key there might be 3 statuses:
1. Not presented.
2. Set as the default value.
3. Set as other value.

Now for some keys we cannot use status 2 to set the defaul value. It may cause inconsistency in configuration.

I think for all flink configurations, there should be a way to remaining their default values while keeping their keys listed in the configuration file.

Are you currently working on it? If not I would like to take this issue.

Thanks.;;;","03/Aug/23 01:13;kination;[~paul8263] thanks for comment!

I'm currently working on it. If it's okay please review the work when ready  :)

But seems I need to wait to be assigned...;;;","21/Aug/23 12:07;kination;I've add following PR for this issue.

[https://github.com/apache/flink/pull/23248]

 

Could someone help review?;;;","20/Oct/23 01:21;paul8263;Hi [~kination] ,

The issue ID in your commit message is incorrect. Please correct it and thus it would be attached to this ticket.;;;","20/Oct/23 11:50;kination;[~paul8263]  thanks for comment.

> The issue ID in your commit message is incorrect

-> Could you let me know how to fix it? I'm not sure what 'issue ID in commit message' is...;;;","23/Oct/23 01:05;paul8263;Hi [~kination],

The title of your PR now is [FLINK-32176] create default ..., the issue ID FLINK-32176 is incorrect.

Please update your commit message with the pattern like '[FLINK-32716] [core] your PR description'. Before pushing you need to regenerate the docs by following the instruction as the CI report said in flink-docs/README.md.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessFailureCancelingITCase.testCancelingOnProcessFailure,FLINK-32715,13545478,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,31/Jul/23 07:21,02/Aug/23 11:22,04/Jun/24 20:40,02/Aug/23 11:22,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51798&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=7610

{code}
Jul 29 01:03:36 01:03:36.670 [ERROR] org.apache.flink.test.recovery.ProcessFailureCancelingITCase.testCancelingOnProcessFailure  Time elapsed: 9.138 s  <<< ERROR!
Jul 29 01:03:36 java.lang.IllegalStateException: The DefaultLeaderElectionService should have been stopped before closing the instance.
Jul 29 01:03:36 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jul 29 01:03:36 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.close(DefaultLeaderElectionService.java:291)
Jul 29 01:03:36 	at org.apache.flink.runtime.highavailability.AbstractHaServices.closeAndCleanupAllData(AbstractHaServices.java:201)
Jul 29 01:03:36 	at org.apache.flink.test.recovery.ProcessFailureCancelingITCase.testCancelingOnProcessFailure(ProcessFailureCancelingITCase.java:249)
Jul 29 01:03:36 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24304,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 11:22:09 UTC 2023,,,,,,,,,,"0|z1jh7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/23 07:23;mapohl;I'm linking an old issue where the same test was failing but with a different stacktrace. I closed FLINK-24304 because we don't have any logs of the old run, anymore.

I'm linking these two because the issue can be still the same and we're just experiencing a different stacktrace because of the leader election changes that happened after 1.12 ;;;","31/Jul/23 09:57;mapohl;I'm lowering the priority again: It looks like a testing issue. The test stops the application but doesn't wait for it to be over (see [ProcessFailureCancelingITCase:241|https://github.com/apache/flink/blob/603181da811edb47c0d573492639a381fbbedc28/flink-tests/src/test/java/org/apache/flink/test/recovery/ProcessFailureCancelingITCase.java#L241]) before closing the {{HaServices}} (see [ProcessFailureCancelingITCase:249|https://github.com/apache/flink/blob/603181da811edb47c0d573492639a381fbbedc28/flink-tests/src/test/java/org/apache/flink/test/recovery/ProcessFailureCancelingITCase.java#L241]).

Older versions of the test didn't fail here because the Precondition was only recently introduced with the leader election changes around FLINK-26522.;;;","02/Aug/23 11:22;mapohl;master: 99b98338266e32c08284ccac3ac49229ec29bfa9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC: Add dialect for OceanBase database,FLINK-32714,13545442,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wanghe,wanghe,wanghe,31/Jul/23 02:58,01/Mar/24 08:36,04/Jun/24 20:40,01/Mar/24 08:36,,,,,,jdbc-3.2.0,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,"OceanBase is a distributed relational database, the community edition of OceanBase is open sourced at [https://github.com/oceanbase/oceanbase.]

The enterprise edition of OceanBase is compatible with MySql and Oracle, which means we can reuse almost all the dialect rules. 

The difference from other databases is that we must provide the compatibility mode firstly, then the connector can determine which dialect to use, so a startup option like 'compatible-mode'  is needed.

A dialect implementation for OceanBase is like below: 
{code:java}
package org.apache.flink.connector.jdbc.databases.oceanbase;


import org.apache.flink.connector.jdbc.converter.JdbcRowConverter;
import org.apache.flink.connector.jdbc.databases.mysql.dialect.MySqlDialect;
import org.apache.flink.connector.jdbc.databases.oracle.dialect.OracleDialect;
import org.apache.flink.connector.jdbc.dialect.AbstractDialect;
import org.apache.flink.table.types.logical.LogicalTypeRoot;
import org.apache.flink.table.types.logical.RowType;

import javax.annotation.Nonnull;

import java.util.Optional;
import java.util.Set;

/** JDBC dialect for OceanBase. */
public class OceanBaseDialect extends AbstractDialect {

    private static final long serialVersionUID = 1L;

    private final AbstractDialect dialect;

    public OceanBaseDialect(@Nonnull String compatibleMode) {
        switch (compatibleMode.toLowerCase()) {
            case ""mysql"":
                this.dialect = new MySqlDialect();
                break;
            case ""oracle"":
                this.dialect = new OracleDialect();
                break;
            default:
                throw new IllegalArgumentException(
                        ""Unsupported compatible mode: "" + compatibleMode);
        }
    }

    @Override
    public String dialectName() {
        return ""OceanBase"";
    }

    @Override
    public Optional<String> defaultDriverName() {
        return Optional.of(""com.oceanbase.jdbc.Driver"");
    }

    @Override
    public Set<LogicalTypeRoot> supportedTypes() {
        return dialect.supportedTypes();
    }

    @Override
    public JdbcRowConverter getRowConverter(RowType rowType) {
        return dialect.getRowConverter(rowType);
    }

    @Override
    public String getLimitClause(long limit) {
        return dialect.getLimitClause(limit);
    }

    @Override
    public String quoteIdentifier(String identifier) {
        return dialect.quoteIdentifier(identifier);
    }

    @Override
    public Optional<String> getUpsertStatement(
            String tableName, String[] fieldNames, String[] conditionFields) {
        return dialect.getUpsertStatement(tableName, fieldNames, conditionFields);
    }

    @Override
    public Optional<Range> timestampPrecisionRange() {
        return dialect.timestampPrecisionRange();
    }

    @Override
    public Optional<Range> decimalPrecisionRange() {
        return dialect.decimalPrecisionRange();
    }

    @Override
    public String appendDefaultUrlProperties(String url) {
        return dialect.appendDefaultUrlProperties(url);
    }
}
 {code}
 ",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 08:36:44 UTC 2024,,,,,,,,,,"0|z1jgzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/23 03:37;yangs;Can assgin the issue to me?;;;","31/Jul/23 05:44;wanghe;Hi  [~yangs] , sorry for the late reply. I have submitted a pr for this ticket, if you have any ideas, please comment to let me know.;;;","01/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Oct/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","01/Mar/24 08:36;martijnvisser;Fixed in apache/flink-connector-jdbc:main 786bd15951887a98f3a4962ba95e4e99b7214d7a

fixVersion set to 3.2.0 since this introduces new functionality for the connector, so next release should be a new minor release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cascade deprecation to non-private methods that reference SourceFunction,FLINK-32713,13545434,13449984,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,30/Jul/23 17:00,09/Aug/23 03:17,04/Jun/24 20:40,09/Aug/23 03:17,1.18.0,,,,,1.18.0,,,,Connectors / Common,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 03:17:34 UTC 2023,,,,,,,,,,"0|z1jgy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/23 03:17;leonard;Resolved in master: 3be078179d07cf3bf9fda43400c3b88afead1662;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Record with RowKind of type -U should not be sent downstream when the before field is empty in ogg-json which op_type is U .,FLINK-32712,13545383,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yuanoOo,yuanoOo,29/Jul/23 11:43,22/Dec/23 18:39,04/Jun/24 20:40,,1.17.1,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,"The BEFORE field in ogg-json can be configured to contain no table fields, as in the ogg-json example below.
For this case, the current version of ogg-json format, sends downstream a Record of type UPDATE_BEFORE with null fields, eg. `Record(-U, null, null, null,...)`, which obviously makes no sense.
{code:java}
{
  ""table"": ""ZBZZZ"",
  ""op_type"": ""U"",
  ""op_ts"": ""2023-07-20 21:45:34.860817"",
  ""current_ts"": ""2023-07-21T05:45:36.615000"",
  ""pos"": ""00002564940142073691"",
  ""before"": {},
  ""after"": {
      ""ID"": 1461242,
      ""PROPERTY_01"": ""tc"",
      ""PROPERTY_02"": null,
      ""PROPERTY_03"": null,
      ""PROPERTY_04"": ""K"",
      ""PROPERTY_05"": ""5"",
      ""PROPERTY_06"": null,
      ""PROPERTY_07"": null,
      ""PROPERTY_08"": null,
      ""PROPERTY_09"": null,
      ""PROPERTY_10"": null
  }
}{code}
For this case, ogg-json format should not send a Record of type UPDATE_BEFORE downstream。 Worse, it can sometimes cause the flink job to crash out.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 03:11:42 UTC 2023,,,,,,,,,,"0|z1jgmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/23 12:42;yuanoOo;I raised a PR in response to this question, has anyone review this PR?

https://github.com/apache/flink/pull/23102;;;","31/Jul/23 07:13;lsy;[~yuanoOo] It looks like this optimization makes sense, can you give a clear title about this issue? And give more description of the problem in the description part.;;;","11/Aug/23 03:11;yuanoOo;[~lsy] Hi, thanks for the advice.

I have added a more detailed description for this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type mismatch when proctime function used as parameter,FLINK-32711,13545308,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,28/Jul/23 14:02,28/Mar/24 12:10,04/Jun/24 20:40,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,stale-major,,"reproduce case:

{code:sql}
SELECT TYPEOF(PROCTIME())
{code}

this query will fail with 

org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of function's argument data type 'TIMESTAMP_LTZ(3) NOT NULL' and actual argument type 'TIMESTAMP_LTZ(3)'.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 08 22:35:16 UTC 2023,,,,,,,,,,"0|z1jg60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/23 14:11;aitozi;It's caused by the hard coded nullable result type of PROCTIME_MATERIALIZE codegen.;;;","01/Aug/23 01:48;aitozi;I pushed a fix for this, anyone can help review ?;;;","08/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The LeaderElection component IDs for running is only the JobID which might be confusing in the log output,FLINK-32710,13545263,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bytesmith,mapohl,mapohl,28/Jul/23 09:29,23/Aug/23 13:48,04/Jun/24 20:40,23/Aug/23 13:48,1.18.0,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,starter,,"I noticed that the leader log messages for the jobs use the plain job ID as the component ID. That might be confusing when reading the logs since it's a UUID with no additional context.

We might want to add a prefix (e.g. {{job-}} to these component IDs.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:48:38 UTC 2023,,,,,,,,,,"0|z1jfw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/23 14:56;bytesmith;Hi, [~mapohl] . If i understand correctly , this ticket is supposed to add an id prefix that could represent the component, like zk-\{JobId} ? if so , I would like to fix it. please assign it to me.;;;","29/Jul/23 15:00;mapohl;Thanks for volunteering. You're right: it's about adding a prefix describing the component. Probably, a prefix like {{job-}} would do it.;;;","23/Aug/23 13:48;mapohl;master (1.19): dddc91fec72fe61d4c2bcae68a7c02f6db656c1c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of low memory utilization for Hybrid Shuffle,FLINK-32709,13545209,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,28/Jul/23 02:22,11/Aug/23 02:40,04/Jun/24 20:40,11/Aug/23 02:40,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,"Currently, each subpartition in Disk/Remote has a segment size of 8M. When writing segments to the Disk tier with a parallelism of 1000, only shuffle data exceeding 1000 * 8M can be written to the Memory tier again. However, for most shuffles, the data volume size falls below this limit, significantly impacting Memory tier utilization. 
For better performance, it is necessary to address this issue to improve the memory tier utilization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 02:39:14 UTC 2023,,,,,,,,,,"0|z1jfk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 02:39;Weijie Guo;Master(1.18) via 0f841624a388be9ad551ec1ccbc15abb31044d6e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the write logic in remote tier of Hybrid Shuffle,FLINK-32708,13545208,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Wencong Liu,Wencong Liu,28/Jul/23 02:18,07/Aug/23 06:27,04/Jun/24 20:40,07/Aug/23 06:27,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,"Currently, on the writer side in the remote tier, the flag file indicating the latest segment id is updated first, followed by the creation of the data file. This results in an incorrect order of file creation and we should fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 06:27:28 UTC 2023,,,,,,,,,,"0|z1jfk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/23 06:27;Weijie Guo;master(1.18) via 6e138f1a4832bd4379fa6289f0445f1ede9b5a54.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The timestamp field precision should be set to 0 instead of 1,FLINK-32707,13545207,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liangjj,liangjj,28/Jul/23 02:04,28/Jul/23 02:04,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,Connectors / JDBC,,,,,,0,,,,"When I tried to create a pg jdbc source from the jdbc catalog, I received a prompt that the precision of the timestamp field cannot be set to 0

!image-2023-07-28-09-58-18-037.png!

 

I found the description on the [pg|[https://www.postgresql.org/docs/12/datatype-datetime.html]|https://www.postgresql.org/docs/12/datatype-datetime.html],]  , i modify the minial precision to 0, and it does work. !image-2023-07-28-09-58-26-013.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/23 01:48;liangjj;image-2023-07-28-09-48-22-509.png;https://issues.apache.org/jira/secure/attachment/13061718/image-2023-07-28-09-48-22-509.png","28/Jul/23 01:58;liangjj;image-2023-07-28-09-58-18-037.png;https://issues.apache.org/jira/secure/attachment/13061717/image-2023-07-28-09-58-18-037.png","28/Jul/23 01:58;liangjj;image-2023-07-28-09-58-26-013.png;https://issues.apache.org/jira/secure/attachment/13061716/image-2023-07-28-09-58-26-013.png",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-28 02:04:11.0,,,,,,,,,,"0|z1jfjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SPLIT(STRING) support in SQL & Table API,FLINK-32706,13545190,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hanyuzheng,hanyuzheng,hanyuzheng,27/Jul/23 20:17,22/May/24 14:02,04/Jun/24 20:40,22/May/24 14:02,,,,,,1.20.0,,,,,,,,,,0,pull-request-available,stale-assigned,,"SPLIT Function

Description
Splits a string into an array of substrings, based on a delimiter.

Syntax
The syntax for the SPLIT function is:
{code:java}
SPLIT(col1, delimiter){code}
Splits a string into an array of substrings based on a delimiter. If the delimiter is not found, then the original string is returned as the only element in the array. If the delimiter is empty, then all characters in the string are split. If either, string or delimiter, are NULL, then a NULL value is returned.

If the delimiter is found at the beginning or end of the string, or there are contiguous delimiters, then an empty space is added to the array.

Example
Let's look at some  function examples and explore how to use the SPLIT function.

For example:

 
{code:java}
SELECT SPLIT('abcdefg', 'c');
Result: ['ab', 'defg']

{code}
see also:

1. ksqlDB Split function

ksqlDB provides a scalar function named {{SPLIT}} which splits a string into an array of substrings based on a delimiter.

Syntax: {{SPLIT(string, delimiter)}}

For example: {{SPLIT('a,b,c', ',')}} will return {{{}['a', 'b', 'c']{}}}.

[https://docs.ksqldb.io/en/0.8.1-ksqldb/developer-guide/ksqldb-reference/scalar-functions/#split]

2. Apache Hive Split function

Hive offers a function named {{split}} which splits a string around a specified delimiter and returns an array of strings.

Syntax: {{array<string> split(string str, string pat)}}

For example: {{split('a,b,c', ',')}} will return {{{}[""a"", ""b"", ""c""]{}}}.

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF

3. Spark SQL Split function

Spark SQL also offers a function named {{{}split{}}}, similar to the one in Hive.

Syntax: {{split(str, pattern[, limit])}}

Here, {{limit}} is an optional parameter to specify the maximum length of the returned array.

For example: {{split('oneAtwoBthreeC', '[ABC]', 2)}} will return {{{}[""one"", ""twoBthreeC""]{}}}.

[https://spark.apache.org/docs/latest/api/sql/index.html#split]

4. Presto Split function

Presto offers a function named {{split}} which splits a string around a regular expression and returns an array of strings.

Syntax: {{array<varchar> split(string str, string regex)}}

For example: {{split('a.b.c', '\.')}} will return {{{}[""a"", ""b"", ""c""]{}}}.

[https://prestodb.io/docs/current/functions/string.html]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 14:02:25 UTC 2024,,,,,,,,,,"0|z1jfg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 20:18;hanyuzheng;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","08/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","22/May/24 14:02;dwysakowicz;Implemented in 4c6571d075b1d1ff5e7b9d7ec3bf625329155fbf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the beta for watermark alignment,FLINK-32705,13545146,13542635,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,27/Jul/23 14:08,02/Aug/23 02:05,04/Jun/24 20:40,02/Aug/23 02:05,1.18.0,,,,,1.18.0,,,,Connectors / Common,,,,,,0,pull-request-available,,,"Remove the beta for watermark alignment.

https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/#watermark-alignment-_beta_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 02:05:35 UTC 2023,,,,,,,,,,"0|z1jf68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 02:05;fanrui;Thanks [~pnowojski] help review, merged via:
<master: 1.18> 796555ecd30e9eb4aa5b9245d928add337ad41b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports spilling to disk when feedback channel memory buffer is full,FLINK-32704,13545117,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,27/Jul/23 11:04,13/Sep/23 12:42,04/Jun/24 20:40,13/Sep/23 12:42,,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,"Currently, the Flink ML Iteration cache feedback data in memory, which would cause OOM in some cases. We need to support spilling to disk when feedback channel memory buffer is full.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 13 12:31:16 UTC 2023,,,,,,,,,,"0|z1jf00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 12:31;lindong;Merged to apache/flink-ml master branch 865404910caf53259df5cea1fc25ca29f96ae9bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[hotfix] flink-python POM has a typo for protobuf-java in shading config,FLINK-32703,13545110,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,27/Jul/23 10:49,27/Jul/23 19:30,04/Jun/24 20:40,27/Jul/23 19:30,1.16.2,1.17.1,1.18.0,,,1.18.0,,,,API / Python,,,,,,0,pull-request-available,,,"Fix typo. `inculde` -> `include`

 

 
{code:java}
                                <includes combine.children=""append"">
                                    <include>net.razorvine:*</include>
                                    <include>net.sf.py4j:*</include>
                                    <include>org.apache.beam:*</include>
                                    <include>com.fasterxml.jackson.core:*</include>
                                    <include>joda-time:*</include>
                                    <inculde>com.google.protobuf:*</inculde>
                                    <include>org.apache.arrow:*</include>
                                    <include>io.netty:*</include>
                                    <include>com.google.flatbuffers:*</include>
                                    <include>com.alibaba:pemja</include>
                                </includes> {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 19:30:46 UTC 2023,,,,,,,,,,"0|z1jeyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 19:30;dannycranmer;Merged commit [{{c8a3979}}|https://github.com/apache/flink/commit/c8a39797d36dc03c28ea665f25455943cbc20b74] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming examples could not run,FLINK-32702,13545081,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,huweihua,huwh,huwh,27/Jul/23 09:02,16/Aug/23 02:44,04/Jun/24 20:40,11/Aug/23 11:16,1.18.0,,,,,,,,,Examples,,,,,,0,pull-request-available,,,"There are some streaming examples that depend on flink-connector-datagen, but didn't package the datagen connector to the final jar. As a result, these examples couldn't run.

* Iteration.jar
* TopSpeedWindowing.jar
* SessionWindowing.jar

I would like to use the Maven Shade Plugin to package the dependencies in a single jar.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32821,FLINK-28227,,,,,,,,,,,,,,,,"27/Jul/23 08:54;huwh;image-2023-07-27-16-54-20-070.png;https://issues.apache.org/jira/secure/attachment/13061690/image-2023-07-27-16-54-20-070.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-27 09:02:48.0,,,,,,,,,,"0|z1jes0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential Memory Leak in Flink CEP due to Persistent Starting States in NFAState,FLINK-32701,13545061,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pd17,pd17,pd17,27/Jul/23 07:30,21/Mar/24 08:33,04/Jun/24 20:40,21/Mar/24 08:33,1.16.1,1.16.2,1.17.0,1.17.1,,1.20.0,,,,Library / CEP,,,,,,0,auto-deprioritized-critical,cep,CEP,"Our team has encountered a potential memory leak issue while working with the Complex Event Processing (CEP) library in Flink v1.17.
h2. Context

The CEP Operator maintains a keyed state called NFAState, which holds two queues: one for partial matches and one for completed matches. When a key is first encountered, the CEP creates a starting computation state and stores it in the partial matches queue. As more events occur that match the defined conditions (e.g., a TAKE condition), additional computation states get added to the queue, with their specific type (normal, pending, end) depending on the pattern sequence.
However, I have noticed that the starting computation state remains in the partial matches queue even after the pattern sequence has been completely matched. This is also the case for keys that have already timed out. As a result, the state gets stored for all keys that the CEP ever encounters, leading to a continual increase in the checkpoint size.
h2.  How to reproduce this
 # Pattern Sequence - A not_followed_by B within 5 mins
 # Time Characteristic - EventTime
 # StateBackend - HashMapStateBackend

On my local machine, I started this pipeline and started sending events at the rate of 10 events per second (only A) and as expected after 5 mins, CEP started sending pattern matched output with the same rate. But the issue was that after every 2 mins (checkpoint interval), checkpoint size kept on increasing. Expectation was that after 5 mins (2-3 checkpoints), checkpoint size will remain constant since any window of 5 mins will consist of the same number of unique keys (older ones will get matched or timed out hence removed from state). But as you can see below attached images, checkpoint size kept on increasing till 40 checkpoints (around 1.5hrs).
P.S. - After 3 checkpoints (6 mins), the checkpoint size was around 1.78MB. Hence assumption is that ideal checkpoint size for a 5 min window should be less than 1.78MB.

As you can see after 39 checkpoints, I triggered a savepoint for this pipeline. After that I used a savepoint reader to investigate what all is getting stored in CEP states. Below code investigates NFAState of CEPOperator for potential memory leak.
{code:java}
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.cep.nfa.NFAState;
import org.apache.flink.cep.nfa.NFAStateSerializer;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.runtime.state.filesystem.FsStateBackend;
import org.apache.flink.state.api.OperatorIdentifier;
import org.apache.flink.state.api.SavepointReader;
import org.apache.flink.state.api.functions.KeyedStateReaderFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;
import org.junit.jupiter.api.Test;

import java.io.Serializable;
import java.util.Objects;

public class NFAStateReaderTest {

    private static final String NFA_STATE_NAME = ""nfaStateName"";

    @Test
    public void testNfaStateReader() throws Exception {
        StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
        SavepointReader savepointReader =
                SavepointReader.read(environment, ""file:///opt/flink/savepoints/savepoint-093404-9bc0a38654df"", new FsStateBackend(""file:///abc""));
        DataStream<NFAStateOutput> stream = savepointReader.readKeyedState(OperatorIdentifier.forUid(""select_pattern_events""), new NFAStateReaderTest.NFAStateReaderFunction());
        stream.print();
        environment.execute();
    }

    static class NFAStateReaderFunction extends KeyedStateReaderFunction<DynamicTuple, NFAStateOutput> {

        private ValueState<NFAState> computationStates;
        private static Long danglingNfaCount = 0L;
        private static Long newNfaCount = 0L;
        private static Long minTimestamp = Long.MAX_VALUE;
        private static Long minKeyForCurrentNfa = Long.MAX_VALUE;
        private static Long minKeyForDanglingNfa = Long.MAX_VALUE;
        private static Long maxKeyForDanglingNfa = Long.MIN_VALUE;
        private static Long maxKeyForCurrentNfa = Long.MIN_VALUE;

        @Override
        public void open(Configuration parameters) {
            computationStates = getRuntimeContext().getState(new ValueStateDescriptor<>(NFA_STATE_NAME, new NFAStateSerializer()));
        }

        @Override
        public void readKey(DynamicTuple key, Context ctx, Collector<NFAStateOutput> out) throws Exception {
            NFAState nfaState = computationStates.value();
            if (Objects.requireNonNull(nfaState.getPartialMatches().peek()).getStartTimestamp() != -1) {
                minTimestamp = Math.min(minTimestamp, nfaState.getPartialMatches().peek().getStartTimestamp());
                minKeyForCurrentNfa = Math.min(minKeyForCurrentNfa, Long.parseLong(key.getTuple().getField(0)));
                maxKeyForCurrentNfa = Math.max(maxKeyForCurrentNfa, Long.parseLong(key.getTuple().getField(0)));
                newNfaCount++;
            } else {
                danglingNfaCount++;
                minKeyForDanglingNfa = Math.min(minKeyForDanglingNfa, Long.parseLong(key.getTuple().getField(0)));
                maxKeyForDanglingNfa = Math.max(maxKeyForDanglingNfa, Long.parseLong(key.getTuple().getField(0)));
            }
            NFAStateOutput nfaStateOutput =
                    new NFAStateOutput(
                            danglingNfaCount,
                            minTimestamp,
                            newNfaCount,
                            minKeyForCurrentNfa,
                            maxKeyForCurrentNfa,
                            minKeyForDanglingNfa,
                            maxKeyForDanglingNfa);
            out.collect(nfaStateOutput);
        }
    }

    @Data
    @NoArgsConstructor
    @AllArgsConstructor
    static class NFAStateOutput implements Serializable {
        private Long danglingNfaCount;
        private Long minTimestamp;
        private Long newNfaCount;
        private Long minKeyForCurrentNfa;
        private Long maxKeyForCurrentNfa;
        private Long minKeyForDanglingNfa;
        private Long maxKeyForDanglingNfa;
    }
}
{code}
 
As an output it printed nfaStateOutput for each key but since all the attributes in nfaStateOutput are aggregates, hence finalOutput printed was
{code:java}
NFAStateReaderTest.NFAStateOutput(danglingNfaCount=34391, minTimestamp=1690359951958, newNfaCount=3000, minKeyForCurrentNfa=6244230, maxKeyForCurrentNfa=6247229, minKeyForDanglingNfa=629818, maxKeyForDanglingNfa=6244229){code}
 
As we can see, checkpoint is storing approximately 34391 dangling states (for keys which have expired (matched or timed out) ) whereas there are only 3000 active keys (for which there are partial matches which are eligible for further pattern sequence matching) which is expected since throughput is 10 events per second which amounts to 3000 unique keys in 5 mins.
h2.  Questions

Hence, I am curious about the reasoning behind this design choice, specifically why the starting state remains in the partial matches queue for all keys, even those that have either timed out or completed their matches.
Additionally, I am wondering what the implications would be if we were to delete this starting state assuming that
 # it is the only state left in the partial match queue.
 # The completed match queue in nfaState is empty.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/23 06:25;pd17;Screenshot 2023-07-26 at 11.45.06 AM.png;https://issues.apache.org/jira/secure/attachment/13061677/Screenshot+2023-07-26+at+11.45.06+AM.png","27/Jul/23 06:25;pd17;Screenshot 2023-07-26 at 11.50.28 AM.png;https://issues.apache.org/jira/secure/attachment/13061676/Screenshot+2023-07-26+at+11.50.28+AM.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,JAVA,Thu Mar 21 08:33:45 UTC 2024,,,,,,,,,,"0|z1jenk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 07:34;martijnvisser;[~Juntao Hu] [~nicholasjiang] Any thoughts on this one?;;;","04/Aug/23 11:01;pd17;[~martijnvisser] [~Juntao Hu] [~nicholasjiang] During the execution of a job, two primary sources of memory leak have been identified in the CEP Operator:
 # {{NFAState}}
 # {{{}SharedBuffer.eventsCount{}}}.

Implementations to resolve these memory leaks are as follows:

*NFAState Leak Resolution*

The {{NFAState}} keyed state in CEP Operator contains two states:
 * {{Queue<ComputationState> partialMatches}}
 * {{Queue<ComputationState> completedMatches}}

Despite all events for a key being processed (either matches or timed out), the {{partialMatches}} still retains the starting state for that key. This occurs for every key encountered by CEP throughout the job execution, leading to a memory leak. To mitigate this, a check has been introduced: once all matches have been processed and the time for all states advances based on the watermark, the {{NFAState}} is cleared if {{completedMatches}} is empty and {{partialMatches}} only contains a single state (the starting state).

 
{code:java}
// STEP 4
updateNFA(nfaState);

 // In order to remove dangling partial matches
if (nfaState.getPartialMatches().size() == 1 && nfaState.getCompletedMatches().isEmpty()) {
    computationStates.clear();
}
{code}
 

The applied fix has been tested with the existing set of Flink unit test cases, all of which have passed. The fix has also been verified against our specific use case scenarios, and it functions as expected.

 

*SharedBuffer.EventsCount Leak Resolution*

The {{eventsCount}} in the shared buffer is responsible for maintaining the mapping of timestamp and {{eventId}} for each event for a key. As the watermark surpasses the timestamp of an event, CEP continues to remove mappings from {{{}eventsCount{}}}. However, an empty map state for a key still consumes memory, resulting in a memory leak. To rectify this, a check has been added: if the {{eventsCount}} map state is empty after the CEP Operator advances time (removing events and matches with a timestamp earlier than the watermark), it is cleared.

This fix, upon testing, resulted in the failure of two unit test cases. These failures occurred because the tests assert a fixed number of total state writes in the CEP Operator when evaluating a pattern sequence. As expected, this number has increased because we are clearing the {{eventsCount}} map. However, when tested against our specific use case scenarios, the fix functioned correctly.

 
{code:java}
void advanceTime(long timestamp) throws Exception {
        Iterator<Long> iterator = eventsCount.keys().iterator();
        while (iterator.hasNext()) {
            Long next = iterator.next();
            if (next < timestamp) {
                iterator.remove();
            }
        }
        
        //memory leak resolution
        if (eventsCount.isEmpty()) {
            eventsCount.clear();
        }
}
{code}
Please let me know if there are any concerns or questions.;;;","05/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Mar/24 08:33;dwysakowicz;Fixed in a9cde49118bab4b32b2d1ae1f97beb94eb967f9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support job drain for Savepoint upgrade mode jobs in Flink Operator,FLINK-32700,13545051,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mmangal,mmangal,mmangal,27/Jul/23 06:46,31/Aug/23 08:53,04/Jun/24 20:40,31/Aug/23 08:53,kubernetes-operator-1.5.0,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"During cancel job with savepoint upgrade mode, jobs can be allowed to drain by advancing the watermark to the end, before they are stopped, so that the in-flight data is not lost. 

If the job fails to drain and hits timeout or any other error, it can be cancelled without taking a savepoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 08:53:36 UTC 2023,,,,,,,,,,"0|z1jelc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 07:19;gyfora;We could introduce a simple config for this;;;","27/Jul/23 08:01;mmangal;Good point! We can add a new configuration to allow users to determine if they want to drain their job. The flink REST endpoint supports this drain functionality:

[https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-stop]

The 'advanceToEndOfTime' parameter in the stopWithSavepoint method can be set based on the configured value.

 

Related issue that can be addressed is that sometimes the job may hit timeout while creating savepoint during cancel, which causes the job to be blocked at the deletion stage until manually intervened. For this case, we can call the cancel method in the TimeoutException catch block, so that we proceed with stopping the job in the case of timeout error.

 

If this solution looks correct, I can work on a PR for these changes.;;;","27/Jul/23 11:18;gyfora;Sounds good, we have to be careful with simply calling cancel in a savepoint timeout, that may cause state loss. Users need to switch to a stateless upgrade mode to allow dropping state which would call cancel anyways.;;;","27/Jul/23 11:18;gyfora;We should not silently fall back to cancel;;;","27/Jul/23 19:12;mmangal;We can create a configuration for this well, so that users can decide whether they want the job to be cancelled on timeout.;;;","27/Jul/23 19:16;gyfora;No I don't think we should have a config for that. We want to promote stateful upgrades, if losing the state is not a problem users can switch to stateless upgrade. Or use last-state which works even without taking a savepoint. Adding further configs would just complicate things without practical benefits I believe.;;;","28/Jul/23 22:40;talat;[~gyfora] Currently there is an issue on Operator's delete. We want to drain job because when we call
{code:java}
kubectl delete flinkdeployment{code}
Operator delete immediately for stateless/stateful jobs. So we lose in flight data. I believe by default Operator should delete jobs by emitting max Watermark. However emitting max watermark also has an issue, If the sink is in stuck state we can not delete the job and we created deadlock situation. 

Does not matter Flinkdeployment upgrade mode we use but if we use last-state/savepoint state we should drain in flight data to prevent unnecessary data duplication. We are not silently cancel the jobs. Actually we wait until savepoint/checkpoint timeout to when user delete their flinkdeployment. Current situation even Operator does not wait for timeout, delete immediately. 

We would like to follow your suggestion. But please keep in your mind We have 20K+ stateful/stateless job those are triggered by programmatically. There is no way to change their deployment manually for us. 

 

cc [~mmangal] ;;;","29/Jul/23 05:25;gyfora;Don’t get me wrong I think adding support for draining makes perfect sense and we should start with that:)

In a separate ticket we can discuss the savepoint timeout behavior . It sounds a little strange that you want to drain pipelines to avoid losing data but you would be ok with losing data on a savepoint timeout . I think if the job started to fail we should simply cancel/delete I agree there is a related ticket somewhere I will try to dig it out after I come back from vacation on Wednesday.;;;","29/Jul/23 05:29;gyfora;Here it is:) https://issues.apache.org/jira/browse/FLINK-30529;;;","30/Jul/23 09:08;talat;Our customers are ok to lose data if The job is not able to recoverable. Because stop job is triggered by customer and most of time they want to roll out new code or change a settings. If there are not able drain job we dont want to wait 1 day to apply fix. Because there is no way to recover if sink is down. And also why we want to drain the job because we want to reduce data duplication as much as possible. We trigger drain so job stop reading from kafka and if savepoint is successful we will commit offset and when we start that job again it will start where it is exactly left. We dont see much cancel job action much on our current production. 

For Google Dataflow we implement auto cancel for the jobs. Without any human interaction we cancel job after a certain timeout it is usually less than 1 hour. For Flink we want to do similar thing. 

We are ok to handle savepoint cancel issue under different ticket. This fix will introduce Job stuck issue if we merge only drain part. But nothing different than current master. 

 

Have a good vacation. I will be off also too for next week :) 

[~mmangal] Could you update your mr according [~gyfora] 's suggestion ?;;;","31/Aug/23 08:53;gyfora;merged to main a728ba768e20236184e2b9e9e45163304b8b196c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"select typeof(proctime()); throw exception in sql-client",FLINK-32699,13545033,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,27/Jul/23 05:11,11/Mar/24 12:43,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,,,,"{code:java}
Flink SQL> select typeof(proctime()); 
 
[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of function's argument data type 'TIMESTAMP_LTZ(3) NOT NULL' and actual argument type 'TIMESTAMP_LTZ(3)'
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 02:30:15 UTC 2023,,,,,,,,,,"0|z1jehc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 02:30;aitozi;Hi [~jackylau], sorry for not notice this ticket, I have opened another ticket https://issues.apache.org/jira/projects/FLINK/issues/FLINK-32711 about this, could you help also take a review for that ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add getCheckpointOptions interface in ManagedSnapshotContext,FLINK-32698,13545027,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Ming Li,Ming Li,27/Jul/23 03:40,27/Jul/23 06:20,04/Jun/24 20:40,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,"Currently only {{checkpointID}} and {{checkpointTimestamp}} information are provided in {{{}ManagedSnapshotContext{}}}. We hope to provide more information about {{{}CheckpointOptions{}}}, so that operators can adopt different logics when performing {{{}SnapshotState{}}}.

 

An example is to adopt different behaviors according to the type of checkpoint. For example, in {{{}Paimon{}}}, we hope that the paimon‘s snapshot written by {{checkpoint}} can expire automatically, while the paimon‘s snapshot written by {{savepoint}} can be persisted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 06:20:55 UTC 2023,,,,,,,,,,"0|z1jeg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 03:55;zjureel;[~Ming Li] `ManagedSnapshotContext` is a `public` interface and there should be a flip if you want to add something in it. 

On the other hand, I found the sink operator in `Paimon` is a `OneInputStreamOperator` which is also `public` in flink and it has `snapshotState` method which has `snapshotType` in `CheckpointOptions`, maybe we can consider it first.;;;","27/Jul/23 03:56;zhuzh;Thanks for the proposal. [~Ming Li]
Flink tends to avoid expose unnecessary information to users. So It's better to describe the specific needed information and the detailed reason for it.
Besides that, a FLIP and public discussion is needed before we can make this kind of changes to the public interfaces.;;;","27/Jul/23 05:55;fanrui;bq. I found the sink operator in `Paimon` is a `OneInputStreamOperator` which is also `public` in flink and it has `snapshotState` method which has `snapshotType` in `CheckpointOptions`, maybe we can consider it first.

Hi [~Ming Li], if the `OneInputStreamOperator` is enough, do you think this jira is necessary?;;;","27/Jul/23 06:20;Ming Li;[~zjureel] [~zhuzh] [~fanrui]  Thanks for your suggestions, I will try to extend {{OneInputStreamOperator}} to verify whether it meets the needs. I'm going to create a community discussion email later, detailing the background and needs. Looking forward to your reply.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using JDBC to insert data into the oracle database, an error will be reported if the target field is lowercase,",FLINK-32697,13545020,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sunyanyong,sunyanyong,27/Jul/23 02:39,31/Jul/23 01:18,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,Connectors / JDBC,,,,,,0,,,,"When using JDBC to insert data into the oracle database, if the target field is lowercase, an error will be reported.

An example input format is ""insert into tableName(ID, `""name""`) values(1, 'test')"".

The reason for the error is that there is a problem with the judgment logic in the 
FieldNamedPreparedStatementImpl.java.  The judgment logic will treat double quotes as delimiters.
!image-2023-07-27-10-39-31-884.png|width=876,height=540!
 
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/23 02:39;sunyanyong;image-2023-07-27-10-39-31-884.png;https://issues.apache.org/jira/secure/attachment/13061669/image-2023-07-27-10-39-31-884.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 31 01:18:00 UTC 2023,,,,,,,,,,"0|z1jeeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/23 11:03;martijnvisser;[~sunyanyong] Do you want to fix it?;;;","31/Jul/23 01:18;sunyanyong;[~martijnvisser] I am very happy to fix this bug, but I am not sure about the rules for fixing bugs. Can you give me a guide of the process?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote Kinesis connector support to PublicEvolving,FLINK-32696,13545005,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,26/Jul/23 21:27,27/Jul/23 08:52,04/Jun/24 20:40,,,,,,,,,,,Connectors / Kinesis,,,,,,0,,,,,,,,,,,,,,,FLINK-24438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 21:27:43.0,,,,,,,,,,"0|z1jeb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate SourceFunction in Tests to Source V2 API,FLINK-32695,13545000,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,26/Jul/23 20:31,27/Jul/23 19:29,04/Jun/24 20:40,,,,,,,,,,,Connectors / Common,,,,,,0,,,,"* ProcessingTimeServiceSource in StreamSourceOperatorLatencyMetricsTest (org.apache.flink.streaming.runtime.operators)
 * TestSourceFunction in CommonExecSinkITCase (org.apache.flink.table.planner.plan.nodes.exec.common)
 * CheckpointingNonParallelSourceWithListState in MigrationTestUtils (org.apache.flink.test.checkpointing.utils)
 * NonSerializableTupleSource in StreamingOperatorsITCase (org.apache.flink.test.streaming.api)
 * ImmediatelyFinishingSource in StreamTaskFinalCheckpointsTest (org.apache.flink.streaming.runtime.tasks)
 * SimpleStringGenerator in CheckpointedStreamingProgram (org.apache.flink.test.classloading.jar)
 * TupleSource in StreamingOperatorsITCase (org.apache.flink.test.streaming.api)
 * SystemExitSourceFunction in StreamTaskSystemExitTest (org.apache.flink.streaming.runtime.tasks)
 * EmptySource in SourceStreamTaskTest (org.apache.flink.streaming.runtime.tasks)
 * NumberSource in IgnoreInFlightDataITCase (org.apache.flink.test.checkpointing)
 * MyTimestampSource in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * InfiniteTestSource in StreamTaskTimerITCase (org.apache.flink.test.streaming.runtime)
 * SavepointSource in SavepointReaderITTestBase (org.apache.flink.state.api)
 * Source in TimersSavepointITCase (org.apache.flink.test.checkpointing)
 * MyTimestampSourceInfinite in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * TestSource in WithMasterCheckpointHookConfigTest (org.apache.flink.streaming.graph)
 * MockSourceFunction in StreamTaskTest (org.apache.flink.streaming.runtime.tasks)
 * MyCustomSourceFunction (org.apache.flink.python.util)
 * RowSourceFunction in StreamExecutionEnvironmentTest (org.apache.flink.streaming.api)
 * FromRowDataSourceFunction in TestValuesTableFactory (org.apache.flink.table.planner.factories)
 * CancelLockingSource in SourceStreamTaskTest (org.apache.flink.streaming.runtime.tasks)
 * LockStepSourceWithOneWmPerElement in SourceTaskTerminationTest (org.apache.flink.streaming.runtime.tasks)
 * NonStoppingSource in SourceStreamTaskTest (org.apache.flink.streaming.runtime.tasks)
 * FromElementSourceFunctionWithWatermark in TestValuesRuntimeFunctions (org.apache.flink.table.planner.factories)
 * TestSource in InterruptSensitiveRestoreTest (org.apache.flink.streaming.runtime.tasks)
 * SavepointSource in DataSetSavepointReaderITTestBase (org.apache.flink.state.api)
 * FailingCollectionSource (org.apache.flink.table.planner.runtime.utils)
 * SessionEventGeneratorDataSource in SessionWindowITCase (org.apache.flink.test.windowing.sessionwindows)
 * Source in BuiltInAggregateFunctionTestBase (org.apache.flink.table.planner.functions)
 * InterruptedSource in SourceStreamTaskTest (org.apache.flink.streaming.runtime.tasks)
 * MyNonWatermarkingSource in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * NormalSource in NotifyCheckpointAbortedITCase (org.apache.flink.test.checkpointing)
 * MaxWatermarkSource (org.apache.flink.state.api.utils)
 * TestSource in RestoreUpgradedJobITCase (org.apache.flink.test.checkpointing)
 * InfiniteSource in StreamSourceOperatorWatermarksTest (org.apache.flink.streaming.runtime.operators)
 * InfiniteLongSourceFunction in JobCancelingITCase (org.apache.flink.test.cancelling)
 * TestSource in TypeFillTest (org.apache.flink.streaming.api)
 * ManuallyClosedSourceFunction in CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase (org.apache.flink.streaming.runtime.tasks)
 * GenericSourceFunction in YarnTestCacheJob (org.apache.flink.yarn.testjob)
 * MockSource in SourceStreamTaskTest (org.apache.flink.streaming.runtime.tasks)
 * DummySource in ReactiveModeITCase (org.apache.flink.test.scheduling)
 * —
 * Anonymous in testDisablingBufferTimeout() in BufferTimeoutITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testWatermarkForwarding() in SideOutputITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testCoGroup() in CoGroupJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testCoGroup() in CoGroupJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testJoin() in CoGroupJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testJoin() in CoGroupJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testSelfJoin() in CoGroupJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testTimestampExtractorWithAutoInterval() in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testTimestampExtractorWithCustomWatermarkEmit() in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testTimestampExtractorWithDecreasingCustomWatermarkEmit() in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testTimestampExtractorWithLongMaxWatermarkFromSource() in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testTimestampExtractorWithLongMaxWatermarkFromSource2() in TimestampITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in buildSourceStream() in IntervalJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testBoundedUnorderedStreamsStillJoinCorrectly() in IntervalJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in testBoundedUnorderedStreamsStillJoinCorrectly() in IntervalJoinITCase (org.apache.flink.test.streaming.runtime)
 * Anonymous in emitUntil() in CheckpointStoreITCase (org.apache.flink.test.checkpointing)
 * Anonymous in go() in Anonymous in setUp() in JobManagerMetricsITCase (org.apache.flink.runtime.metrics)
 * Anonymous in testSources() in StreamExecutionEnvironmentTest (org.apache.flink.streaming.api)
 * Anonymous in testParallelismBounds() in StreamExecutionEnvironmentTest (org.apache.flink.streaming.api)
 * Anonymous in getEnvironment() in ChangelogStateBackendLoadingTest (org.apache.flink.state.changelog)
 * Anonymous in getScanRuntimeProvider() in ScanSourceBase in TableFactoryHarness (org.apache.flink.table.planner.factories)
 * Anonymous in doTestPropagationFromCheckpointConfig() in CheckpointExceptionHandlerConfigurationTest (org.apache.flink.streaming.runtime.tasks)
 * CheckpointedSource in StatefulJobSavepointMigrationITCase (org.apache.flink.api.scala.migration)
 * FiniteTestSource (org.apache.flink.table.planner.runtime.stream)
 * CheckpointedSource (org.apache.flink.api.scala.migration)
 * EventTimeSourceFunction in TimeTestUtil$ (org.apache.flink.table.planner.runtime.utils)
 * in testReduceWindow() in WindowReduceITCase (org.apache.flink.streaming.api.scala)
 * in testReduceWithWindowFunction() in WindowReduceITCase (org.apache.flink.streaming.api.scala)
 * in testReduceWithProcessWindowFunction() in WindowReduceITCase (org.apache.flink.streaming.api.scala)
 * in testReduceAllWindow() in WindowReduceITCase (org.apache.flink.streaming.api.scala)
 * in testReduceAllWithWindowFunction() in WindowReduceITCase (org.apache.flink.streaming.api.scala)
 * in testReduceAllWithProcessWindowFunction() in WindowReduceITCase (org.apache.flink.streaming.api.scala)
 * in testRichWindowFunction() in WindowFunctionITCase (org.apache.flink.streaming.api.scala)
 * in testRichProcessWindowFunction() in WindowFunctionITCase (org.apache.flink.streaming.api.scala)
 * in testRichAllWindowFunction() in WindowFunctionITCase (org.apache.flink.streaming.api.scala)
 * in testRichProcessAllWindowFunction() in WindowFunctionITCase (org.apache.flink.streaming.api.scala)
 * in testCoGroup() in CoGroupJoinITCase (org.apache.flink.streaming.api.scala)
 * in testCoGroup() in CoGroupJoinITCase (org.apache.flink.streaming.api.scala)
 * in testJoin() in CoGroupJoinITCase (org.apache.flink.streaming.api.scala)
 * in testJoin() in CoGroupJoinITCase (org.apache.flink.streaming.api.scala)
 * in testSelfJoin() in CoGroupJoinITCase (org.apache.flink.streaming.api.scala)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 20:31:14.0,,,,,,,,,,"0|z1jea0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate ParallelSourceFunction classes to Source V2 API or prepare for removal,FLINK-32694,13544999,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,26/Jul/23 20:26,28/Oct/23 13:13,04/Jun/24 20:40,,,,,,,,,,,Connectors / Common,,,,,,0,,,," 
 * SimpleEndlessSourceWithBloatedState in HeavyDeploymentStressTestProgram (org.apache.flink.deployment)
 * -StatefulSequenceSource (org.apache.flink.streaming.api.functions.source)-   Migration not needed: https://github.com/apache/flink/pull/23611
 * ArrowSourceFunction (org.apache.flink.table.runtime.arrow.sources)
 * InputFormatSourceFunction (org.apache.flink.streaming.api.functions.source)
 * -EventsGeneratorSource (org.apache.flink.streaming.examples.statemachine.generator)-  resolved in FLINK-32670
 * FromSplittableIteratorFunction (org.apache.flink.streaming.api.functions.source)
 * DataGeneratorSource (org.apache.flink.streaming.api.functions.source.datagen)
 * – Tests:
 * MySource in StatefulStreamingJob (org.apache.flink.test)
 * RandomLongSource in StickyAllocationAndLocalRecoveryTestJob (org.apache.flink.streaming.tests)
 * SequenceGeneratorSource (org.apache.flink.streaming.tests)
 * TtlStateUpdateSource (org.apache.flink.streaming.tests)
 * StringSourceFunction in NettyShuffleMemoryControlTestProgram (org.apache.flink.streaming.tests)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 20:26:09.0,,,,,,,,,,"0|z1je9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cascade deprecation to classes that directly implement SourceFunction,FLINK-32693,13544998,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,afedulov,afedulov,afedulov,26/Jul/23 20:23,31/Jul/23 18:56,04/Jun/24 20:40,31/Jul/23 18:55,,,,,,,,,,Connectors / Common,,,,,,0,,,,"* SocketTextStreamFunction (org.apache.flink.streaming.api.functions.source)
 * FiniteTestSource (org.apache.flink.streaming.util)
 * FromIteratorFunction (org.apache.flink.streaming.api.functions.source)
 * Generator in FileSinkProgram (org.apache.flink.connector.file.sink)
 * RichSourceFunction (org.apache.flink.streaming.api.functions.source)
 * FileMonitoringFunction (org.apache.flink.streaming.api.functions.source)
 * FromElementsFunction (org.apache.flink.streaming.api.functions.source)
 * — Tests
 * PeriodicSourceGenerator in PeriodicStreamingJob (org.apache.flink.streaming.tests)
 * Generator in StreamSQLTestProgram (org.apache.flink.sql.tests)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 20:23:22.0,,,,,,,,,,"0|z1je9k:",9223372036854775807,Subsumed by FLINK-32670,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[umbrella] Nice-to-haves for SourceFunction API removal,FLINK-32692,13544997,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,26/Jul/23 20:10,26/Jul/23 20:43,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,See also FLINK-28045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 20:10:09.0,,,,,,,,,,"0|z1je9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make it possible to use builtin functions without catalog/db set,FLINK-32691,13544992,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jhughes,jhughes,jhughes,26/Jul/23 18:12,30/Jul/23 08:08,04/Jun/24 20:40,28/Jul/23 11:28,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"# Relative to https://issues.apache.org/jira/browse/FLINK-32584, function lookup fails without the catalog and database set.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32584,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 28 11:28:55 UTC 2023,,,,,,,,,,"0|z1je88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/23 11:28;dwysakowicz;Fixed in 3f63e03e83144e9857834f8db1895637d2aa218a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report Double.NAN instead of null for missing autoscaler metrics,FLINK-32690,13544987,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,morhidi,morhidi,26/Jul/23 16:48,27/Jul/23 18:30,04/Jun/24 20:40,27/Jul/23 18:30,,,,,,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,Kubernetes Operator,,,,,,0,pull-request-available,,,Change null values to Double.NAN for autoscaler metrics during blackout periods when no data is gathered. This appears to be a more common practice then null. Also consistent with other metrics we have.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 16:48:44.0,,,,,,,,,,"0|z1je74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insufficient validation for table.local-time-zone,FLINK-32689,13544976,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,26/Jul/23 14:41,07/Aug/23 14:41,04/Jun/24 20:40,07/Aug/23 14:41,,,,,,1.18.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"There are still cases where timezone information is lost silently due to the interaction between {{java.util.TimeZone}} and {{java.time.ZoneId}}.

This might be theoretical problem, but I would feel safer if we change the check to:
{code}
if (!java.util.TimeZone.getTimeZone(zoneId).toZoneId().equals(ZoneId.of(zoneId))) {
   throw new ValidationException(errorMessage);
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 14:41:22 UTC 2023,,,,,,,,,,"0|z1je4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/23 14:46;twalthr;[~leonard] Could you give more context why FLINK-22349 was necessary? The check above would fail for {{UTC-1}} and {{UT-1}} (which is also officially supported). But current master accepts {{UT-1}}, is this a bug?;;;","26/Jul/23 14:50;twalthr;Update: I could confirm it with this program
{code}
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        DataStream<Row> row = env.fromElements(Row.of(Instant.now().toEpochMilli()));
        tableEnv.createTemporaryView(""t"", row);

        tableEnv.getConfig().set(TableConfigOptions.LOCAL_TIME_ZONE, ""UT-10"");

        tableEnv.executeSql(""SELECT TO_TIMESTAMP_LTZ(f0, 3) FROM t"").print();
{code}

Will prepare a fix.;;;","27/Jul/23 08:26;twalthr;Also this is incorrect:

{code}
        tableEnv.getConfig().setLocalTimeZone(ZoneOffset.ofHours(2));

        tableEnv.executeSql(""SELECT TO_TIMESTAMP_LTZ(f0, 3) FROM t"").print();
{code}

The zone offset is ignored.;;;","07/Aug/23 14:41;twalthr;Fixed in master: 8c9b1767e08e3fdb3b60a0cc7f09a92647181d64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated exception history fields,FLINK-32688,13544965,13540585,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,26/Jul/23 13:01,05/Dec/23 06:42,04/Jun/24 20:40,,1.18.0,,,,,2.0.0,,,,Runtime / REST,,,,,,0,2.0-related,,,"The fields were already marked as deprecated (see [JobExceptionInfo|https://github.com/apache/flink/blob/a49f1aaec6239401cc8b1dac731d290e95290caf/flink-runtime/src/main/java/org/apache/flink/runtime/rest/messages/JobExceptionsInfo.java#L35]) but were not discussed as part of a FLIP. Working on this issue would require creating a FLIP to cover the REST API change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-26 13:01:13.0,,,,,,,,,,"0|z1je28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on handleGlobalFailureAndRestartAllTasks.BATCH_EVENLY since 2023-07-23,FLINK-32687,13544964,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Not A Problem,,martijnvisser,martijnvisser,26/Jul/23 12:56,26/Jul/23 14:04,04/Jun/24 20:40,26/Jul/23 14:04,1.18.0,,,,,,,,,,,,,,,0,,,,http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=handleGlobalFailureAndRestartAllTasks.BATCH_EVENLY&extr=on&quarts=on&equid=off&env=2&revs=200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 14:04:53 UTC 2023,,,,,,,,,,"0|z1je20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/23 13:59;huwh;[~martijnvisser] Thanks for reporting this. This regression is caused by https://github.com/apache/flink/pull/22913. This PR improves the Failover performance, The time cost for the STREAMING scene is reduced by 80%, and reducing the BATCH and STREAMING_EVENLY scenes by 20%.

But there is a certain performance regression in the BatchEvenly scenario, I think this is acceptable. Because: 1) Batch Evenly is a strategy that is unlikely to be used in production, batch tasks run for a short time, and resources can be released when they are finished; 2) This only affects the failover process, and part of batch tasks (with block shuffling) will not trigger the global Failover.;;;","26/Jul/23 14:04;martijnvisser;[~huwh] Thanks for the explainer, let's close it then :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on startScheduling.BATCH and startScheduling.STREAMING since 2023-07-24 ,FLINK-32686,13544962,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Wencong Liu,martijnvisser,martijnvisser,26/Jul/23 12:55,01/Aug/23 12:14,04/Jun/24 20:40,01/Aug/23 12:13,1.18.0,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=startScheduling.STREAMING&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=startScheduling.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 12:14:23 UTC 2023,,,,,,,,,,"0|z1je1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 03:33;Wencong Liu;Thanks for reporting this. The second regression since 07.24 is caused by commit id: c9e1833642650e0b1ea162371dd7c6d35f2e21b7. The commit disables the repair in FLINK-32094 unexpectedly and causes the regression in 07.24. I'll open a pull request to fix this. The first regression in 07.09 is not related with FLINK-32094.;;;","01/Aug/23 12:13;Weijie Guo;master(1.18) via 8ed28b45109f11c62ded0d020dba46da74034e6b.;;;","01/Aug/23 12:14;Weijie Guo;Feel free to reopen this if it can still be reproduced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on sortedMultiInput and sortedTwoInput since 2023-07-18,FLINK-32685,13544961,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,martijnvisser,martijnvisser,26/Jul/23 12:54,03/Aug/23 09:11,04/Jun/24 20:40,03/Aug/23 09:11,1.18.0,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=sortedMultiInput&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=sortedTwoInput&extr=on&quarts=on&equid=off&env=2&revs=200",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 09:11:08 UTC 2023,,,,,,,,,,"0|z1je1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/23 12:10;Weijie Guo;Through some investigation, I have found this may caused by the branch related to changelog writer in the mailbox loop, and FLINK-19010 has exacerbated this issue. I have prepared a PR to improve this.;;;","03/Aug/23 09:11;Weijie Guo;master(1.18) via 5e3cad518058675f8719c42b1f80a157b6b447a0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Renaming AkkaOptions into RpcOptions,FLINK-32684,13544960,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,26/Jul/23 12:51,30/Jan/24 08:59,04/Jun/24 20:40,30/Jan/24 08:59,1.18.0,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,2.0-related,pull-request-available,,"FLINK-32468 introduced Apache Pekko as an replacement for Akka. This involved renaming classes (besides updating comments). {{AkkaOptions}} was the only occurrence that wasn't renamed as it's annotated as {{@PublicEvolving}}.

This issue is about renaming {{AkkaOptions}} into {{PekkoOptions}} (or a more general term considering FLINK-29281)",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 08:59:34 UTC 2024,,,,,,,,,,"0|z1je14:",9223372036854775807,AkkaOptions are deprecated and replaced with RpcOptions,,,,,,,,,,,,,,,,,,,"26/Jul/23 15:35;chesnay;Related to FLINK-14052.;;;","18/Aug/23 09:42;zhuzh;To support smooth migration, we can introduce {{PekkoOptions}} and deprecate {{AkkaOptions}} in ahead(in 1.19).
Given that {{AkkaOptions}} is {{@PublicEvolving}}, theoretically we can even remove it in Flink 1.20.;;;","05/Dec/23 06:40;Weijie Guo;I'm +1 for introducing {{PekkoOptions}} to deprecate old {{AkkaOptions}} in 1.19.;;;","24/Jan/24 10:38;mapohl;I'm working on that one now. I decided against renaming the class to {{PekkoOptions}} but rather something more generic like {{RpcOptions}}. That way we avoid running into the same issue again if we decide to merge FLINK-29281;;;","30/Jan/24 08:59;mapohl;master: [c678244a3890273145a786b9e1bf1a4f96f6dcfd|https://github.com/apache/flink/commit/c678244a3890273145a786b9e1bf1a4f96f6dcfd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Pekko from 1.0.0 to 1.0.1,FLINK-32683,13544934,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mdedetrich,mdedetrich,mdedetrich,26/Jul/23 09:16,27/Aug/23 08:56,04/Jun/24 20:40,27/Aug/23 08:56,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,stale-assigned,,Updates Pekko dependency to 1.0.1 which contains the following bugfix [https://github.com/apache/incubator-pekko/pull/492] . See [https://github.com/apache/incubator-pekko/issues/491] for more info,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 26 22:35:11 UTC 2023,,,,,,,,,,"0|z1jdvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce option for choosing time function evaluation methods,FLINK-32682,13544924,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dwysakowicz,dwysakowicz,dwysakowicz,26/Jul/23 08:21,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,1.20.0,,,,Table SQL / Planner,Table SQL / Runtime,,,,,0,pull-request-available,stale-assigned,,"In [FLIP-162|https://cwiki.apache.org/confluence/x/KAxRCg] as future plans it was discussed to introduce an option {{table.exec.time-function-evaluation}} to control evaluation method of time function.

We should add this option to be able to evaluate time functions with {{query-time}} method in streaming mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 22:35:07 UTC 2023,,,,,,,,,,"0|z1jdt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBStateDownloaderTest.testMultiThreadCleanupOnFailure unstablie,FLINK-32681,13544919,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,chesnay,chesnay,26/Jul/23 07:51,11/Dec/23 09:21,04/Jun/24 20:40,01/Aug/23 15:36,1.18.0,,,,,1.18.0,,,,Runtime / State Backends,Tests,,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51712&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef

Failed 3 times in yesterdays nightly run.

{code}
Jul 26 01:12:46 01:12:46.889 [ERROR] org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.testMultiThreadCleanupOnFailure  Time elapsed: 0.044 s  <<< FAILURE!
Jul 26 01:12:46 java.lang.AssertionError
Jul 26 01:12:46 	at org.junit.Assert.fail(Assert.java:87)
Jul 26 01:12:46 	at org.junit.Assert.assertTrue(Assert.java:42)
Jul 26 01:12:46 	at org.junit.Assert.assertFalse(Assert.java:65)
Jul 26 01:12:46 	at org.junit.Assert.assertFalse(Assert.java:75)
Jul 26 01:12:46 	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.testMultiThreadCleanupOnFailure(RocksDBStateDownloaderTest.java:151)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 15:36:21 UTC 2023,,,,,,,,,,"0|z1jds8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 09:13;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51712&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10308;;;","28/Jul/23 02:59;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51777&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10286;;;","28/Jul/23 13:05;mapohl;twice in the same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51777&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10286
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51777&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10241;;;","28/Jul/23 13:09;mapohl;I'm linking FLINK-32345 as a cause because that failing test was added in that issue. [~srichter] can you take this one?

I raised the priority to Critical as well.;;;","31/Jul/23 13:35;srichter;[~mapohl] I was trying to reproduce this locally without success, could it be an infra related problem? The code is calling `FileUtils::deleteDirectoryQuietly` to cleanup files and if something goes wrong during the deletion, it could still find the directories and fail the test. I could also just try to test if delete was called and not if the files where actually deleted to abstract away from such infra problems, wdyt?;;;","31/Jul/23 14:41;mapohl;The test failure is reproducible locally (with a Linux machine). I ran the test locally (utilizing Intellij's repeated execution feature) and it failed consistently after a few runs. Are you using MacOS or Windows? Adding sleep seems to lower the changes before of the failure appearing.

Working with callbacks here would be a way to test the code. But it would be still good to understand why we see this strange behavior.

It doesn't look like the {{deleteDirectoryQuietly}} call is the issue (even though I'd argue that exposing exceptions would be the better way to do it): When changing the code to {{deleteDirectory}}, the assertion was still the same with no {{IOException}} appearing instead.;;;","31/Jul/23 15:41;srichter;I'm running on Mac, but with a sleep I already managed to reproduce it. I'll take a look what's going on.;;;","01/Aug/23 07:53;Feifan Wang;Hi [~srichter] , I'm also very interested in this problem. I think it is caused by the directory being created again by the download task that was already started. I think it can be solved with a cleaner that is aware of task failure, and I submitted a draft [PR|https://github.com/apache/flink/pull/23111] explaining the idea (perhaps as a final fix if you think it is appropriate). You can take a look if you don't mind. :);;;","01/Aug/23 08:09;Feifan Wang;Sorry [~mapohl]  I didn't notice that the ticket has been assigned before submitting the PR.;;;","01/Aug/23 09:59;srichter;[~Feifan Wang]Thanks for the offer but I've already worked on a fix yesterday. But please feel free to review it later.;;;","01/Aug/23 15:36;srichter;merged into master d11cc32;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job vertex names get messed up once there is a source vertex chained with a MultipleInput vertex in job graph,FLINK-32680,13544918,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,wanglijie,wanglijie,26/Jul/23 07:34,01/Aug/23 14:08,04/Jun/24 20:40,01/Aug/23 14:08,1.16.2,1.17.1,1.18.0,,,1.16.3,1.17.2,1.18.0,,,,,,,,0,pull-request-available,,,"Take the following test(put it to {{MultipleInputITCase}}) as example:
{code:java}
    @Test
    public void testMultipleInputDoesNotChainedWithSource() throws Exception {
        testJobVertexName(false);
    }
    
    @Test
    public void testMultipleInputChainedWithSource() throws Exception {
        testJobVertexName(true);
    }

    public void testJobVertexName(boolean chain) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        TestListResultSink<Long> resultSink = new TestListResultSink<>();

        DataStream<Long> source1 = env.fromSequence(0L, 3L).name(""source1"");
        DataStream<Long> source2 = env.fromElements(4L, 6L).name(""source2"");
        DataStream<Long> source3 = env.fromElements(7L, 9L).name(""source3"");

        KeyedMultipleInputTransformation<Long> transform =
                new KeyedMultipleInputTransformation<>(
                        ""MultipleInput"",
                        new KeyedSumMultipleInputOperatorFactory(),
                        BasicTypeInfo.LONG_TYPE_INFO,
                        1,
                        BasicTypeInfo.LONG_TYPE_INFO);
        if (chain) {
            transform.setChainingStrategy(ChainingStrategy.HEAD_WITH_SOURCES);
        }
        KeySelector<Long, Long> keySelector = (KeySelector<Long, Long>) value -> value % 3;

        env.addOperator(
                transform
                        .addInput(source1.getTransformation(), keySelector)
                        .addInput(source2.getTransformation(), keySelector)
                        .addInput(source3.getTransformation(), keySelector));

        new MultipleConnectedStreams(env).transform(transform).rebalance().addSink(resultSink).name(""sink"");

        env.execute();
    }{code}
 

When we run {{testMultipleInputDoesNotChainedWithSource}} , all job vertex names are normal:

!image-2023-07-26-15-24-24-077.png|width=494,height=246!

When we run {{testMultipleInputChainedWithSource}} (the MultipleInput chained with source1), job vertex names get messed up (all job vertex names contain {{{}Source: source1{}}}):

!image-2023-07-26-15-23-29-551.png|width=515,height=182!

 

I think it's a bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/23 07:23;wanglijie;image-2023-07-26-15-23-29-551.png;https://issues.apache.org/jira/secure/attachment/13061637/image-2023-07-26-15-23-29-551.png","26/Jul/23 07:24;wanglijie;image-2023-07-26-15-24-24-077.png;https://issues.apache.org/jira/secure/attachment/13061636/image-2023-07-26-15-24-24-077.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 14:07:57 UTC 2023,,,,,,,,,,"0|z1jds0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/23 09:27;JunRuiLi;This bug is because the global ChainedSources are used when generating the JobVertex name ([here|https://github.com/apache/flink/blob/c8ae39d4ac73f81873e1d8ac37e17c29ae330b23/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGenerator.java#L903]). But in fact, it should be filtered according to the id of the current node. An example can refer to is [here.|https://github.com/apache/flink/blob/c8ae39d4ac73f81873e1d8ac37e17c29ae330b23/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGenerator.java#L1052]

If the idea is correct, I will prepare a PR to fix this issue.;;;","26/Jul/23 09:28;wanglijie;Thanks [~JunRuiLi] , assiged to you.;;;","01/Aug/23 14:07;wanglijie;Fixed via:
master(1.18): 9e6f8e8e127e8dd81ded4a2214fd710c8ff8180a
release-1.17: 5a87cfac875feabf29c0f001e5591ca48a9e5e94
release-1.16: 3c46ffb1e27d4a369c7016b631fb8d5f944c3e50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter conditions cannot be pushed to JOIN in some case,FLINK-32679,13544910,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,grandfisher,grandfisher,26/Jul/23 06:51,10/Mar/24 00:07,04/Jun/24 20:40,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,"There is a case
{code:java}
SELECT a.id, b.id, c.id, d.id, e.id
	, f.id
FROM `table-v1` a
	INNER JOIN `table-v2` b ON a.id = b.id
	INNER JOIN `table-v3` c ON b.id = c.id
	INNER JOIN `table-v4` d ON c.id = d.id
	INNER JOIN `table-v5` e ON d.id = e.id
	INNER JOIN `table-v6` f ON a.id = f.id
WHERE f.id = 0
{code}
In this sql, each table should have a condition {*}id=0{*}, but actually only table *f* and *a* has this condition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 10 00:07:13 UTC 2024,,,,,,,,,,"0|z1jdq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 03:56;libenchao;In FLINK-28753 (merged in 1.16.0), we have supported this, what's the version are you using? Does it reproduce on current master branch?;;;","27/Jul/23 05:36;grandfisher;It's not the same question. I think FLINK-28753 is fix question that push condition to join both side(I'm not sure,calcite may solve this question use another rule). 

Our question is ,the conditions can be chained by different join conditions.This problem seems push a filter that on left input of a join  to the right input of the join by join keys.;;;","27/Jul/23 15:59;libenchao;{{FilterJoinRule}} is supposed to push filters below {{Join}} for inner join, Flink extends Calcite's {{FilterJoinRule}} to {{FlinkFilterJoinRule}} to do more things. If there is anything happens in original {{FilterJoinRule}}, but not in {{FlinkFilterJoinRule}}, I think we should improve it.;;;","10/Mar/24 00:07;jeyhunkarimov;Hi [~grandfisher] the mentioned query does pushes the join conditions down the join operator (I verified below with the current master - d6a4eb966fbc47277e07b79e7c64939a62eb1d54). Or am I missing something? 

 
{code:java}
Calc(select=[CAST(0 AS INTEGER) AS id, b, CAST(0 AS INTEGER) AS id0, b0, CAST(0 AS INTEGER) AS id1, b1, CAST(0 AS INTEGER) AS id2, b2, CAST(0 AS INTEGER) AS id3, b3])
+- Join(joinType=[InnerJoin], where=[true], select=[b, b0, b1, b2, b3], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
   :- Exchange(distribution=[single])
   :  +- Join(joinType=[InnerJoin], where=[true], select=[b, b0, b1, b2], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
   :     :- Exchange(distribution=[single])
   :     :  +- Join(joinType=[InnerJoin], where=[true], select=[b, b0, b1], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
   :     :     :- Exchange(distribution=[single])
   :     :     :  +- Join(joinType=[InnerJoin], where=[true], select=[b, b0], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
   :     :     :     :- Exchange(distribution=[single])
   :     :     :     :  +- Calc(select=[b], where=[(id = 0)])
   :     :     :     :     +- TableSourceScan(table=[[default_catalog, default_database, v1]], fields=[id, b])
   :     :     :     +- Exchange(distribution=[single])
   :     :     :        +- Calc(select=[b], where=[(id = 0)])
   :     :     :           +- TableSourceScan(table=[[default_catalog, default_database, v2]], fields=[id, b])
   :     :     +- Exchange(distribution=[single])
   :     :        +- Calc(select=[b], where=[(0 = id)])
   :     :           +- TableSourceScan(table=[[default_catalog, default_database, v3]], fields=[id, b])
   :     +- Exchange(distribution=[single])
   :        +- Calc(select=[b], where=[(0 = id)])
   :           +- TableSourceScan(table=[[default_catalog, default_database, v4]], fields=[id, b])
   +- Exchange(distribution=[single])
      +- Calc(select=[b], where=[(0 = id)])
         +- TableSourceScan(table=[[default_catalog, default_database, v5]], fields=[id, b]){code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Stress-Test to cover multiple low-level changes in Flink,FLINK-32678,13544909,13545663,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,mapohl,mapohl,26/Jul/23 06:50,30/Aug/23 08:16,04/Jun/24 20:40,29/Aug/23 13:31,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,release-testing,,,"-We decided to do another round of testing for the LeaderElection refactoring which happened in [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+refactoring+leaderelection+to+make+flink+support+multi-component+leader+election+out-of-the-box].-

This release testing task is about running a bigger amount of jobs in a Flink environment to look for unusual behavior. This Jira issue shall cover the following 1.18 efforts:
 * Leader Election refactoring ([FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+refactoring+leaderelection+to+make+flink+support+multi-component+leader+election+out-of-the-box], FLINK-26522)
 * Akka to Pekko transition (FLINK-32468)
 * flink-shaded 17.0 updates (FLINK-32032)",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26522,FLINK-32032,FLINK-32468,,,,,,,,,,,,,FLINK-32994,"29/Aug/23 05:10;wangyang0918;qps-configmap-get-115.png;https://issues.apache.org/jira/secure/attachment/13062539/qps-configmap-get-115.png","29/Aug/23 05:10;wangyang0918;qps-configmap-get-117.jpg;https://issues.apache.org/jira/secure/attachment/13062541/qps-configmap-get-117.jpg","29/Aug/23 05:10;wangyang0918;qps-configmap-get-118.png;https://issues.apache.org/jira/secure/attachment/13062543/qps-configmap-get-118.png","29/Aug/23 05:10;wangyang0918;qps-configmap-patch-118.png;https://issues.apache.org/jira/secure/attachment/13062544/qps-configmap-patch-118.png","29/Aug/23 05:10;wangyang0918;qps-configmap-put-115.png;https://issues.apache.org/jira/secure/attachment/13062540/qps-configmap-put-115.png","29/Aug/23 05:10;wangyang0918;qps-configmap-put-117.jpg;https://issues.apache.org/jira/secure/attachment/13062542/qps-configmap-put-117.jpg",,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 08:16:03 UTC 2023,,,,,,,,,,"0|z1jdq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/23 07:19;mapohl;[~wangyang0918] I'm wondering whether we could use this test as an umbrella test for two other topics that ended up in the 1.18 release:
* FLINK-32032 (flink-shaded update including netty)
* FLINK-32468 (Migration from Akka to Pekko)

They are quite fundamental changes and hard to test in a manual way. The proposal would be to do a stress test on Flink 1.18. For FLINK-32032 and FLINK-32468, observing that nothing breaks might be good enough. WDYT?;;;","31/Jul/23 09:16;wangyang0918;I believe it is reasonable. I would also keep an eye on the netty and Pekko related logs during the stress test.;;;","23/Aug/23 09:44;mapohl;[~wangyang0918] do you have any updates on this issue?;;;","28/Aug/23 04:03;wangyang0918;*Functionality Test*
1. [SUCCEED] Build the docker image with release-1.18 branch
2. [SUCCEED] Use the flink-k8s-operator to start a Flink app with HA enabled, check the logs, UI
3. [SUCCEED] Check HA ConfigMaps, one for leader election and one for the job checkpoint
4. [SUCCEED] Check the thread dump of the JobManager and verify only one leader elector is running(the value is 4 before 1.15 with old HA)
5. [SUCCEED] Use the command {{kubectl exec flink-example-statemachine-897cb6d4f-bzdv5 – /bin/sh -c 'kill 1'}}  to kill the JobManager and verify no more TaskManager is created(Flink should reuse the existing TaskManager before idle timeout).
6. [SUCCEED] Verify the Flink job recover from the latest checkpoint and keep running
2023-08-28 03:40:29,167 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Restoring job 596bdc6b7ac5bcefb611c3df08d64520 from Checkpoint 101 @ 1693194000259 for 596bdc6b7ac5bcefb611c3df08d64520 located at oss://flink-test/flink-k8s-ha-stress-test/flink-cp/596bdc6b7ac5bcefb611c3df08d64520/chk-101.
 
 
All the things work well after refactoring of leader-election, akka, and flink-shaded. I just find a log that could be improved by replacing the object id with some more meaningful name.
2023-08-28 03:40:18,258 INFO org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - LeaderContender has been registered under component 'resourcemanager' for org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver@2a19a0fe.
 

 

I am still working on the stress test and will share the result later today.;;;","28/Aug/23 12:21;wangyang0918;*Stress Test*
Run 1000 Flink Jobs with 1 JM and 1 TM for each
1. Flink version 1.15.4 with {{high-availability.use-old-ha-services=true}}
Flink JobManager has 4 leader electors(RestServer, ResourceManager, Dispatcher, JobManager) to periodically update the K8s ConfigMap. So the QPS of {{PUT ConfigMap}}  for 1000 jobs will be roughly 800 req/s ≈ 4(leader elector) * 1000(Flink JobManager pods) / 5(renew interval). The the QPS of {{GET ConfigMap}} is twice as much as {{{}PUT{}}}.

2. Flink version 1.17.1(same as 1.15.4 with {{{}high-availability.use-old-ha-services=false{}}})
Flink will only have one shared leader elector. So the QPS of {{PUT ConfigMap}}  for 1000 jobs will be roughly 200 req/s ≈ 1(leader elector) * 1000(Flink JobManager pods) / 5(renew interval). The the QPS of {{GET ConfigMap}} is twice as much as {{{}PUT{}}}.
 
3. Flink version 1.18-snapshot
Flink will only have one shared leader elector. So the QPS of {{PATCH ConfigMap}}  for 1000 jobs will be roughly 200 req/s ≈ 1(leader elector) * 1000(Flink JobManager pods) / 5(renew interval). The the QPS of {{GET ConfigMap}} is same as {{{}PATCH{}}}.
 
!qps-configmap-put-115.png|width=694,height=176!
!qps-configmap-put-117.jpg|width=694,height=176!
!qps-configmap-patch-118.png|width=694,height=176!

From the above two pictures, we could verify that the new leader elector in 1.18 only sends a quarter of the write requests of the old one in 1.15 on the K8s APIServer. It will significantly reduce the stress on the K8s APIServer.

 

!qps-configmap-get-115.png|width=694,height=176!
!qps-configmap-get-117.jpg|width=694,height=176!
!qps-configmap-get-118.png|width=694,height=176!

We could also find that the read requests of 1.18 are half of the 1.17. The root cause is fabric8 6.6.2(FLINK-31997) has introduced the PATCH http method for updating the leader annotation. It will save a GET request for each update.

 
||Flink Version||PUT/PATCH QPS||GET QPS||
|1.15.4 with old HA|800|1600|
|1.17.1|200|400|
|1.18.0|200|200|

 

All in all, the Flink 1.18 puts less stress on the K8s APIServer while all the 1000 Flink jobs run normally as before.;;;","28/Aug/23 12:22;wangyang0918;[~mapohl] Please ping me if you believe we still need more other tests on this ticket.;;;","29/Aug/23 13:31;mapohl;Thanks for looking into it. I guess, we're good to go if nothing else came up. We didn't see any issues as part of the normal CI builds either.

{quote}
All the things work well after refactoring of leader-election, akka, and flink-shaded. I just find a log that could be improved by replacing the object id with some more meaningful name.
2023-08-28 03:40:18,258 INFO org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - LeaderContender has been registered under component 'resourcemanager' for org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver@2a19a0fe.
{quote}
I guess, you're referring to the {{KubernetesLeaderElectionDriver@2a19a0fe}} part of the log message. This was left on purpose to differentiate different driver instances. I couldn't think of a better way to indicate different instances other than relying on the instance reference.;;;","30/Aug/23 01:56;wangyang0918;I just remember we have a {{toString}} in the {{KubernetesLeaderElectionDriver}} which will print the leader ConfigMap name. Anyway, given that we only have one ConfigMap for the leader election, using the object id also makes sense to me.;;;","30/Aug/23 08:16;mapohl;That's actually a good point: I didn't notice that we missed migrating the {{toString}} method from the old {{LeaderElectionDriver}} to the new one (the {{MultipleComponentLeaderElectionDriver}} missed this method). I created FLINK-32994 as a follow-up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-benchmarks-regression-check failed to send slack messages since 2023.07.17,FLINK-32677,13544900,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,26/Jul/23 06:11,26/Jul/23 07:23,04/Jun/24 20:40,26/Jul/23 07:04,,,,,,,,,,Benchmarks,,,,,,0,,,,"{code:java}
</content></entry><entry><title>Response Code: 404</title><link rel=""alternate"" type=""text/html"" href=""http://codespeed.dak8s.net:8080/log""/><id>271780</id><published>2023-07-19T11:38:29Z</published><updated>2023-07-19T11:38:29Z</updated><content>Jul 19, 2023 11:38:29 AM jenkins.plugins.slack.StandardSlackService publish
WARNING: Response Code: 404
</content></entry><entry><title>Slack post may have failed. Response: null</title><link rel=""alternate"" type=""text/html"" href=""http://codespeed.dak8s.net:8080/log""/><id>271779</id><published>2023-07-19T11:38:29Z</published><updated>2023-07-19T11:38:29Z</updated><content>Jul 19, 2023 11:38:29 AM jenkins.plugins.slack.StandardSlackService publish
WARNING: Slack post may have failed. Response: null {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 07:21:35 UTC 2023,,,,,,,,,,"0|z1jdo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/23 06:14;Yanfei Lei;[~MartijnVisser] It seems that the Jenkins plugin has been reset since 2023.07.17, could you please help take a look?;;;","26/Jul/23 06:48;martijnvisser;[~Yanfei Lei] I think that was because the ASF Slack workspace had too many integrations active, so I removed some of them already on the 17th and re-added the integration for Jenkins. ;;;","26/Jul/23 07:02;Yanfei Lei;Thanks for the response, I got the reason, the [token|https://apache-flink.slack.com/services/B05J88JAD6C] of Jenkins integration has been changed after resetting. I updated the token in [Jenkins credentials|http://codespeed.dak8s.net:8080/credentials/store/system/domain/_/credential/2d52c4a5-ab95-42f5-b9b7-eb1a1a95b232] , now it works again. :);;;","26/Jul/23 07:21;martijnvisser;That makes sense [~Yanfei Lei] - Thanks for fixing it!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for catalog modification listener,FLINK-32676,13544885,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,26/Jul/23 02:21,02/Aug/23 04:36,04/Jun/24 20:40,02/Aug/23 04:36,1.18.0,,,,,1.18.0,,,,Documentation,Table SQL / Runtime,,,,,0,pull-request-available,,,Add doc for catalog modification listener,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32402,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 04:36:40 UTC 2023,,,,,,,,,,"0|z1jdko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 04:36;renqs;master: 7a5500e390a8ea7076fc1fb73eb248468a42032b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for the tiered storage of hybrid shuffle,FLINK-32675,13544882,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,26/Jul/23 01:40,02/Aug/23 07:03,04/Jun/24 20:40,02/Aug/23 07:03,1.18.0,,,,,1.18.0,,,,Documentation,Runtime / Network,,,,,0,pull-request-available,,,"The new Hybrid Shuffle mode supporting remote storage (https://issues.apache.org/jira/browse/FLINK-31634) has finished, we should also update the Flink doc of Hybrid Shuffle.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31634,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 07:03:24 UTC 2023,,,,,,,,,,"0|z1jdk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 07:03;xtsong;master (1.18): b18bde9bb44e94bc2fb7f84c49367c6988aa6dfe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for the new Context.getTargetColumns,FLINK-32674,13544880,13528793,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,26/Jul/23 01:21,27/Jul/23 05:38,04/Jun/24 20:40,27/Jul/23 05:38,1.18.0,,,,,1.18.0,,,,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 05:38:23 UTC 2023,,,,,,,,,,"0|z1jdjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 05:38;lincoln.86xy;fixed in master:65f200b819e47800ff31cad55d4fe5c4a31706c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrage Google PubSub connector to V2,FLINK-32673,13544822,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,25/Jul/23 13:40,01/Apr/24 16:59,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 16:59:20 UTC 2024,,,,,,,,,,"0|z1jd6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 16:39;rskraba;Hello!  There's been some work on this: https://github.com/apache/flink-connector-gcp-pubsub/pull/2  (adapted from previous work on FLINK-20625), but it's stalled due to lack of reviewers with Pub/Sub expertise :/  Any ideas?;;;","28/Jul/23 11:07;martijnvisser;There's two things imho:

1. I can restart the discussion I originally opened on this at https://lists.apache.org/thread/c1stxqmwdzxmpjp6pj2pxsvmpbtwhqnf to see if someone comes forward to help out
2. If no-one comes forward, then I would be inclined to open a vote to remove the connector from Flink given a lack of maintainers;;;","01/Aug/23 16:04;yinghai;We started using GCP Pubsub connectors with Flink but definitely not experts. 

How can we help to keeps this connector live and moving forward?;;;","01/Aug/23 18:02;rskraba;In the short term, it would be really helpful to have a second, neutral pair of eyes on the PR above!  https://github.com/apache/flink-connector-gcp-pubsub/pull/2  

In my experience, it's helpful for a committer to have a review from two parties to make the decision to merge.  In this case, this PR has gone through a couple of hands (the original author, reviewer, and then I took it up), so it shouldn't take much to get it into mergeable shape.  As a dev that uses this connector directly, your opinion is helpful!

In the longer term, to benefit from the community ""weight"" behind this connector, it just needs a bit of attention (maintenance and support, bug fixes, bumps).  If you're interested in being involved, that would make (at least) two of us :D  That would be encouraging!;;;","28/Mar/24 15:56;clmccart;Hey all! What's the status on this PR: [https://github.com/apache/flink-connector-gcp-pubsub/pull/2 |https://github.com/apache/flink-connector-gcp-pubsub/pull/2]?

I have a new implementation of the Google Pub/Sub source connector that is almost ready. The goal of the new implementation is three-fold:
 # Implement [FLIP-27|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95653748]
 # Address some users' performance concerns
 ## change the implementation from using Pu/Sub Pull API to Pub/Sub Streaming Pull API 
 ## implement automatic message leasing
 # Re-engage with this connector and start actively supporting it

I'm wondering how best to proceed considering the status of the currently outstanding PR. To me, there appears to be two options:
 # Get the outstanding PR merged in and then open a PR with the new implementation (which might overwrite a significant amount of what was just merged in)
 # We close the outstanding pull request and I open a fresh pull request with the new implementation

Let me know your thoughts; thanks!

 ;;;","30/Mar/24 20:15;dchristle;Hi Claire,

The code in the open PR does work, but there are some aspects of the design/interfaces that seem (to me) to need refactoring to support better throughput/concurrency, as well as the Streaming API.

I'd be in favor of seeing the new implementation in a new PR, and assuming it goes through, closing the existing one.;;;","01/Apr/24 16:59;clmccart;That sounds good to me. Thanks, David!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate RabbitMQ connector to Source V2 API,FLINK-32672,13544821,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,25/Jul/23 13:39,28/Jul/23 11:09,04/Jun/24 20:40,,,,,,,,,,,Connectors/ RabbitMQ,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 28 11:09:23 UTC 2023,,,,,,,,,,"0|z1jd6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/23 11:09;martijnvisser;There is already FLINK-20628 and a PR https://github.com/apache/flink-connector-rabbitmq/pull/1 - it just appears that the committer isn't responding to the review comments left by [~chesnay]

It's kind of a similar problem to the Google Pubsub connector: we need maintainers, else I don't see how we can keep this connector alive. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Externalized Declarative Resource Management,FLINK-32671,13544801,13527017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,knaufk,knaufk,25/Jul/23 11:56,25/Oct/23 14:52,04/Jun/24 20:40,23/Oct/23 22:09,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 24 02:03:36 UTC 2023,,,,,,,,,,"0|z1jd20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/23 06:54;ConradJam; I would like to add English and Chinese documents 

cc  [~knaufk] ;;;","05/Sep/23 22:39;shazeline;Will there also be guidance on the production-readiness of this feature in the docs? For example ""MVP"" vs ""Beta"" and limitations/tradeoffs users should keep in mind?;;;","11/Sep/23 13:23;ConradJam;I've added this piece of description because FLIP-291 still seems to have some work in progress and is not yet production-ready, which I noted as an MVP feature in the documentation.  cc [~shazeline] ;;;","23/Oct/23 22:03;dmvk;master: 8af765b4c9cd3519193b89dae40a8f8c2439c661

release-1.18: 1d17dc71cf98b6540a506c3c9670bbd0b47052a5;;;","23/Oct/23 22:08;dmvk;[~ConradJam] I've prepared a more extensive documentation of the feature, but it lacks Chinese translation. Would you be able to help there?;;;","24/Oct/23 02:03;ConradJam;[~dmvk] sure let me take it and finish it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cascade deprecation to classes that implement SourceFunction,FLINK-32670,13544788,13449984,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,afedulov,afedulov,afedulov,25/Jul/23 11:15,08/Aug/23 15:34,04/Jun/24 20:40,08/Aug/23 15:34,1.18.0,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 15:34:40 UTC 2023,,,,,,,,,,"0|z1jcz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/23 15:34;leonard;Resolved in master: dfb9cb851dc1f0908ea6c3ce1230dd8ca2b48733;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support range-port for taskmanager data port,FLINK-32669,13544766,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,25/Jul/23 09:26,25/Jul/23 10:33,04/Jun/24 20:40,,,,,,,,,,,Runtime / Network,,,,,,0,,,,"We can setup range-port for taskmanager rpc port to avoid occupying an unexpected port(such as the port of datanode service).
 
However, we can't setup range-port for taskmanager data port(config-key: taskmanager.data.port). In production env, it's unreasonable to setup a specify port, thus we usually not setup this configuration key. 
 
The problem is without setup taskmanager data port, it's possible to conflict with port of other services. Thus is it necessary to support  range-port  for  taskmanager data port ?

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 10:33:58 UTC 2023,,,,,,,,,,"0|z1jcu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 10:17;JunRuiLi;IIUC, the default value of the configuration item taskmanager.data.port is 0, which means that an idle port will be randomly selected, so generally speaking, no port conflicts will occur. Could you please explain in detail the scenario where port conflicts will occur if this configuration item is not set?;;;","25/Jul/23 10:33;stupid_pig;In our hadoop cluster, our datanode service would  restarts to update patche occasionally.  When we stop datanode service, and start it start after a while（sometime it would be longer, if datanode service crash in weekend) , the port conflict will happen.
 
 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix up watchdog timeout error msg  in common.sh(e2e test) ,FLINK-32668,13544754,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,loserwang1024,loserwang1024,loserwang1024,25/Jul/23 08:11,21/Aug/23 08:10,04/Jun/24 20:40,21/Aug/23 08:10,1.16.2,1.17.1,1.18.0,,,1.18.0,,,,Build System / CI,,,,,,0,pull-request-available,,,"When run e2e test, an error like this occrurs:

!image-2023-07-25-15-27-37-441.png|width=733,height=115!

 

The corresponding code:
{code:java}
kill_test_watchdog() {
    local watchdog_pid=$(cat $TEST_DATA_DIR/job_watchdog.pid)
    echo ""Stopping job timeout watchdog (with pid=$watchdog_pid)""
    kill $watchdog_pid
} 
internal_run_with_timeout() {
    local timeout_in_seconds=""$1""
    local on_failure=""$2""
    local command_label=""$3""
    local command=""${@:4}""

    on_exit kill_test_watchdog
   (
           command_pid=$BASHPID
           (sleep ""${timeout_in_seconds}"" # set a timeout for this command
            echo ""${command_label:-""The command '${command}'""} (pid: $command_pid) did not finish after $timeout_in_seconds seconds.""
eval ""${on_failure}""
           kill ""$command_pid"") & watchdog_pid=$!
           echo $watchdog_pid > $TEST_DATA_DIR/job_watchdog.pid
           # invoke
          $command
  )

}{code}
 

When {{$command}} completes before the timeout, the watchdog process is killed successfully. However, when {{$command}} times out, the watchdog process kills {{$command}} and then exits itself, leaving behind an error message when trying to kill its own process ID with {{{}kill $watchdog_pid{}}}.This error msg ""no such process"" is hard to understand.

 

So, I will modify like this with better error message:

 
{code:java}
kill_test_watchdog() {
      local watchdog_pid=$(cat $TEST_DATA_DIR/job_watchdog.pid)
      if kill -0 $watchdog_pid > /dev/null 2>&1; then
           echo ""Stopping job timeout watchdog (with pid=$watchdog_pid)""
           kill $watchdog_pid
      else
            echo ""[ERROR] Test is timeout""
            exit 1       
      fi
} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/23 07:27;loserwang1024;image-2023-07-25-15-27-37-441.png;https://issues.apache.org/jira/secure/attachment/13061593/image-2023-07-25-15-27-37-441.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 08:10:31 UTC 2023,,,,,,,,,,"0|z1jcrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 09:07;loserwang1024;[~mapohl] ,[~rmetzger] ,[~Leonard] ，CC. Would you like to assign this to me?;;;","25/Jul/23 11:28;mapohl;Thanks for raising this issue, [~loserwang1024]. I'm not sure about the error message, though: Isn't it more of a warning rather than an error message because the outcome of the function is still the desired one (i.e. the process is gone). But we can move that discussion into the corresponding PR. I'm gonna assign the issue to you and update the Jira metadata.;;;","26/Jul/23 02:52;loserwang1024;Thanks for your advise, [~mapohl] .

In fact, it's also hard for me to decide whether to throw an error, which depends on whether a timeout test case meets our requirements.
 *  If we think test case needs to meet the required time performance requirements, we need to throw an error here. 
 * If we think that even if the timeout is met, it's  still ok. The reason why kills this test is that {*}we just want to execute the next one quickly{*}, then there is no need to throw an error here.A warning is well enough.

What do you think?;;;","26/Jul/23 10:25;mapohl;As far as I see it, this failure can only occur if the watchdog thread exited on its own or the watchdog process was exited by some other process (which would be problematic if the corresponding test is still operating). The latter case is the problematic one because we might miss killing the test. We should throw an error in that case. We don't have to worry if both processes are already killed (in this situation, maybe even a warning is not needed but rather a informal output). WDYT?;;;","17/Aug/23 02:45;loserwang1024;[~mapohl] , thanks a lot. One more question:
{quote} We don't have to worry if both processes are already killed (in this situation, maybe even a warning is not needed but rather a informal output).
{quote}
When the watchdog thread exits on its own, it indicates that the test has timed out. If no errors are thrown, then this timed-out test is considered successful finally.Is this the expected behavior?

For example, the flink-streaming-kafka end-to-end test is set to run for 6 minutes. However, if the download of the Kafka distribution takes longer than 6 minutes, or if the Flink execution is too slow, the test may time out.

WDYT?;;;","17/Aug/23 08:23;mapohl;If the test times out, the failure handling should be triggered (see [common.sh:957|https://github.com/apache/flink/blob/c8ae39d4ac73f81873e1d8ac37e17c29ae330b23/flink-end-to-end-tests/test-scripts/common.sh#L957]) to print a meaningful output. Nonetheless, the following kill command will cause the actual test run to return with a non-zero exit code. As a consequence, the script will fail. So, your scenario should be already covered by the existing code.

The {{kill_test_watchdog}} covers a separate scenario: Killing the watchdog process if it's still around. Here, we shouldn't cover the corresponding test run anymore (and imply whether the test succeeded or not based on whether the watchdog process is still around). The error handling, as said in the previous paragraph, should be covered in the test execution itself.

Please correct me if I'm missing anything.;;;","17/Aug/23 12:34;loserwang1024;I get it.Thanks a lot,  [~mapohl] .I will modify this as soon as possible.;;;","21/Aug/23 08:10;mapohl;master (1.18): 404a7804188c520b83ef38c863150b8bc4da80a3

No backports were created because it's a minor issue without any effects on CI execution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable users to independently adjust the high availability strategies related to jobs through configuration,FLINK-32667,13544745,13561934,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,guoyangze,zjureel,zjureel,25/Jul/23 07:09,18/Dec/23 06:54,04/Jun/24 20:40,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,stale-assigned,,"In OLAP scenarios, we only require the leader election services for the Dispatcher / ResourceManager and RestEndpoint in the JobManager process. Leader election services and persistent services are redundant for jobs and may impact cluster performance.
To generate HA services suitable for OLAP scenarios, we introduce the high-availability.enable-job-recovery parameter. When users enable HA with Kubernetes or ZooKeeper and set this option to false, we will select the combination of DefaultLeaderServices and EmbeddedPersistentServices. Additionally, we will set the JobMaster's LeaderElectionService and LeaderRetrieverService to the Standalone version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33033,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 07:42:10 UTC 2023,,,,,,,,,,"0|z1jcpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/23 11:03;mapohl;[~zjureel] do we have performance tests that cover the use case(s) for short-lived OLAP queries (as discussed in FLIP-25318)?;;;","04/Aug/23 03:10;zjureel;[~mapohl] We have an internally e2e test for olap queries to statistical latency and QPS. However, e2e testing requires some resources. I'm considering how to build this test in the community, and whether each important issue can be tested in flink-benchmarks project. What do you think of it? Thanks

;;;","04/Aug/23 09:00;mapohl;It would be great to have these performance tests set up before merging the changes. That would give us some more assurance that working on those issues is reasonable. 

I'm also wondering whether there was a broader discussion on adding this short-living jobs to the scope of Apache Flink in the ML. Do you have a link to that discussion?;;;","04/Aug/23 11:50;zjureel;[~mapohl] I think performance testing is a good suggestion, we also discussed and created FLINK-25356 at the beginning. I will consider how to add e2e benchmark in project flink-benchmarks and I think we should to add micro benchmarks for primary issue.

Unfortunately, there is currently no ML about the ML for short-living jobs in flink. We only discussed this with [~xtsong] off-line when we created FLINK-25318, but I strongly agree with you that we need to initiate broader discussion in the community dev ML. I will collect our practical experiences and initiate a discussion in ML later, thank you very much for your valuable suggestion!;;;","04/Aug/23 12:37;mapohl;Thanks, [~zjureel]. I think it's a good idea to initiate the discussion in the mailing list. That way we might gather additional feedback on the scenarios and avoid redoing things because we haven't considered certain edge cases. That said, I'm confident that this use case makes sense to consider as part of the Flink roadmap. I'm looking forward to your ML post. ;;;","08/Aug/23 09:45;chesnay;This shouldn't be tied to the restart strategy. Persisting the jobs in HA ensures the job is run at all once it was submitted (aka, if the JM crashes before the job has run); which doesn't have anything to do with _restarts_.;;;","08/Aug/23 12:53;zjureel;[~chesnay] [~mapohl] Currently ha service consists of two parts

Part1: Flink cluster will register its dispatcher address, rest port to ha service such as zk or configmap for k8s, then the client can get these information and submit job to dispatcher via rest, this is needed in OLAP scenario

Part2: Dispatcher will validate, save and recover job from JobGraphStore and JobResultStore requires failover.

In OLAP scenario, Part2 stores the jobs to external systems (zk/s3) which may cause latency jitter in olap queries, so we only need Part1 not Par2. My original idea was to add an option for ha service and it can use embedded JobGraphStore and memory JobResultStore, it may be the more suitable solution. But it will cause the job will not recover even if it is configured with failover when JM crashes.

The second idea is that we do not store jobs without failover to ha service, but currently we can only check this according to restart strategy and also the issues mentioned by [~chesnay] above. 

What do you think of this, do you think the first idea (add option for ha service) is feasible? Looking forward to your feedback, thanks



;;;","09/Aug/23 00:41;zjureel;[~chesnay] In addition to the above solution, I think we can add a new failover strategy. Currently there are two failover strategy: full and region, we can add a new strategy such as `none` for the jobs and they don't need to store relevant information, WDYT?;;;","10/Aug/23 09:37;mapohl;I see [~chesnay]'s concern that the restart strategy is not really suited conceptually to be used for limiting the job's HA capabilities. The HA topic is more of a cluster configuration feature rather than a per-job feature. That's why it also feels odd to combine the cluster configuration with the job configuration when collecting the filter mechanism in the {{JobGraphStore}} implementation, you're proposing in the attached PR.

About your comment on the HA services:
{quote}
Part1: Flink cluster will register its dispatcher address, rest port to ha service such as zk or configmap for k8s, then the client can get these information and submit job to dispatcher via rest, this is needed in OLAP scenario

Part2: Dispatcher will validate, save and recover job from JobGraphStore and JobResultStore requires failover.
{quote}
Your observattion is correct here: The {{HighAvailabilityServices}} could be structured in a better way. LeaderElection (your part 1) is used to maintain the right state of the Flink cluster whereas job-related HA data (your part 2) is used to ensure that a job can be recovered. Essentially, those are two different features which should be reflected in separate interfaces rather than a single one (i.e. {{HighAvailabilityServices}}). 

I put some thoughts into reorganizing the {{HighAvailabilityServices}} interface as part of the recent leader election work (see [this FLINK-31816 comment|https://issues.apache.org/jira/browse/FLINK-31816?focusedCommentId=17741054&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17741054]). I didn't proceed with it because I didn't see much value other than slightly cleaner code at that time. But such a refactoring would allow us to separate the two topics for the HA backend.

But I have to point out: That approach would be different to what you had in mind with analyzing the restart strategy. It would work on a cluster level rather on a per-job level (in contrast to what you had in mind with analyzing the restart strategy).;;;","11/Aug/23 08:00;zjureel;[~mapohl] Thanks for your detailed explanation and I caught it. Currently Flink will create `FailoverStrategy` for jobs according to the cluster configuration and as I mentioned above there are only two strategies: full and region.

I think decoupling `LeaderElection` and `job-related HA data` from `HighAvailabilityServices` is a very good solution, that's what we want for OLAP queries.
As you mentioned in [FLINK-31816|https://issues.apache.org/jira/browse/FLINK-31816?focusedCommentId=17741054&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17741054]:
 
```
HighAvailabilityServices could get a single implementation that requires a factory method for creating JobGraphStore, JobResultStore, CheckpointRecoveryFactory, and BlobStore. Additionally, it would require a LeaderElectionService (which is essentially a factory for LeaderElection instances)
```

I think we can do it now and after that we can add a new failover strategy such as `none` for cluster and create embedding factory. What do you think? [~mapohl][~chesnay]

;;;","14/Aug/23 08:29;mapohl;Sounds good to me. But we should focus on the performance tests before proceeding here if the community decides that it's worth supporting the OLAP scenarios in one or another form.;;;","04/Sep/23 12:21;zjureel;[~mapohl] As we discussed in the threads https://lists.apache.org/thread/2wj1wfzcg162534v8olqt18y2x9x99od and https://lists.apache.org/thread/szdr4ngrfcmo7zko4917393zbqhgw0v5, we have came to an agreement about flink olap and [~jark] has already added the relevant content in flink roadmap https://flink.apache.org/roadmap/ , so I think we can continue to promote olap related issues.

Regarding this issue, I would like to synchronize our current progress. We do a simple e2e test internally and we found that the latency of the simplest query with HA is 6.5 times higher than that without HA. I think we can add this olap micro benchmarks in flink-benchmarks for HA service and then continue this issue. After that, we can add a micro benchmark for the new interfaces to compare to the previous one. What do you think? Thanks;;;","04/Sep/23 12:35;mapohl;Sounds reasonable. (y) fyi: I might not be that responsive the next 3 weeks;;;","05/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","27/Nov/23 07:42;guoyangze;FYI, since FLINK-33033 has been merged and proves the benefit we could gain from embedded meta store. I'd like to revive this work and file a FLIP in the near future.;;;",,,,,,,,,,,,,,,,,,,,,,,,
ASM rewrite class lead to package failed.,FLINK-32666,13544743,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,lizhiqiang,lizhiqiang,25/Jul/23 07:06,26/Jul/23 07:32,04/Jun/24 20:40,25/Jul/23 08:06,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,"{code:java}
[DEBUG] Processing JAR /Users/lzq/Desktop/Projects/Flink/flink/flink-master/flink-table/flink-table-planner/target/flink-table-planner_2.12-1.17-SNAPSHOT.jar
[DEBUG] Rewrote class bytecode: org/apache/calcite/interpreter/JaninoRexCompiler.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$Frame.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/ImmutableRelBuilder$Config.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$OverCallImpl.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$GroupKey.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/RelBuilder$OverCall.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$RelOptTableFinder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/ImmutableRelBuilder$Config$Builder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$Registrar.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/RelBuilder$OverCallImpl$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$AggCallImpl.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/ImmutableRelBuilder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$AggCall.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/ImmutableRelBuilder$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/RelBuilder$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$GroupKeyImpl.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/RelBuilder$Config.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/ImmutableRelBuilder$Config$InitShim.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$AggCallPlus.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$AggCallImpl2.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/RelBuilder$Shifter.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/tools/RelBuilder$Field.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/tools/RelBuilder$2.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/util/javac/JaninoCompiler$JaninoCompilerArgs.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/util/javac/JaninoCompiler.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/util/javac/JaninoCompiler$AccountingClassLoader.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/adapter/enumerable/EnumerableInterpretable$1$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/adapter/enumerable/EnumerableInterpretable$EnumerableNode.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/adapter/enumerable/EnumerableInterpretable$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/adapter/enumerable/EnumerableInterpretable$StaticFieldDetector.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/adapter/enumerable/EnumerableInterpretable.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/adapter/enumerable/EnumerableInterpretable$1$1$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/jdbc/CalciteSchemaBuilder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify$SafeRexVisitor.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify$SargCollector.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify$RexSargBuilder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify$IsPredicate.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify$Predicate.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify$Comparison.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rex/RexSimplify$CaseBranch.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rex/RexSimplify$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rex/RexSimplify.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/hint/NodeTypeHintPredicate$NodeType.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/hint/NodeTypeHintPredicate$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/hint/HintPredicates.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/hint/NodeTypeHintPredicate.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Window$Group.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Correlate.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/core/Window$2.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/core/Minus.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Snapshot.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/core/Correlate$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Window$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/core/Window$3.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Sort.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Window.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Intersect.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Filter.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/core/Union.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Values.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/SetOp.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Window$Group$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/core/Window$RexWinAggCall.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalCorrelate.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalTableScan.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalUnion.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalValues.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/logical/LogicalWindow$2.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalMinus.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalIntersect.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/logical/LogicalWindow$4.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/logical/LogicalWindow$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalWindow.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalSort.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/logical/LogicalWindow$3.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalFilter.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalWindow$WindowKey.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/logical/LogicalSnapshot.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/metadata/JaninoRelMetadataProvider.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/metadata/JaninoRelMetadataProvider$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/rel/metadata/JaninoRelMetadataProvider$NoHandler.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/rel/metadata/JaninoRelMetadataProvider$Key.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarProjectRuleConfig$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$CorelMapBuilder.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$DeferredLookup.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$RegisterArgs.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$CorRef.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$AdjustProjectForCountAggregateRule.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveSingleAggregateRuleConfig$InitShim.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveSingleAggregateRuleConfig.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveCorrelationForScalarAggregateRule$RemoveCorrelationForScalarAggregateRuleConfig.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/AuxiliaryConverter$Impl.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableSqlToRelConverter$Config$Builder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableAdjustProjectForCountAggregateRuleConfig$Builder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$CorelMapBuilder$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$AdjustProjectForCountAggregateRule$AdjustProjectForCountAggregateRuleConfig.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarAggregateRuleConfig$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$CorDef.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$SortExpressionConverter.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveSingleAggregateRuleConfig$Builder.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/AuxiliaryConverter.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableSqlToRelConverter$Config.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$Blackboard.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableSqlToRelConverter$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$CorelMap.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$HistogramShuttle.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableAdjustProjectForCountAggregateRuleConfig$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$RexAccessShuttle.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarAggregateRuleConfig.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$Frame.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$4.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveSingleAggregateRule.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableAdjustProjectForCountAggregateRuleConfig.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveSingleAggregateRule$RemoveSingleAggregateRuleConfig.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$CorrelationUse.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$DecorrelateRexShuttle.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$AggregateFinder.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarProjectRuleConfig$Builder.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarAggregateRuleConfig$InitShim.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveCorrelationRexShuttle.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$2.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableAdjustProjectForCountAggregateRuleConfig$InitShim.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$Blackboard$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$Config.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$AggConverter.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveCorrelationForScalarProjectRule.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarProjectRuleConfig$InitShim.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveSingleAggregateRuleConfig$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$5.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarProjectRuleConfig.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/ImmutableRemoveCorrelationForScalarAggregateRuleConfig$Builder.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$LookupContext.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$1.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$NoOpSubQueryConverter.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveCorrelationForScalarAggregateRule.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$3.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$Config.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$SubQuery.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableSqlToRelConverter.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/SqlToRelConverter$SqlIdentifierFinder.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/ImmutableSqlToRelConverter$Config$InitShim.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql2rel/RelDecorrelator$RemoveCorrelationForScalarProjectRule$RemoveCorrelationForScalarProjectRuleConfig.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$ValidationError.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$4.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/ProcedureNamespace.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$SelectExpander.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$2.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$InsertNamespace.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$IdInfo.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationModifier.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$1.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$PatternValidator.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$DmlNamespace.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$DeriveTypeVisitor.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$3.class
[DEBUG] Keeping original class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$Status.class
[DEBUG] Rewrote class bytecode: org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationReplacer.class
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:12 min (Wall Clock)
[INFO] Finished at: 2023-07-25T15:02:19+08:00
[INFO] Final Memory: 226M/3733M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.4.1:shade (shade-flink) on project flink-table-planner_2.12: Error creating shaded jar: Problem shading JAR /Users/lzq/Desktop/Projects/Flink/flink/flink-master/flink-table/flink-table-planner/target/flink-table-planner_2.12-1.17-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: 19 -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.4.1:shade (shade-flink) on project flink-table-planner_2.12: Error creating shaded jar: Problem shading JAR /Users/lzq/Desktop/Projects/Flink/flink/flink-master/flink-table/flink-table-planner/target/flink-table-planner_2.12-1.17-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:188)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:184)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.maven.plugin.MojoExecutionException: Error creating shaded jar: Problem shading JAR /Users/lzq/Desktop/Projects/Flink/flink/flink-master/flink-table/flink-table-planner/target/flink-table-planner_2.12-1.17-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:640)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
    ... 11 more
Caused by: java.io.IOException: Problem shading JAR /Users/lzq/Desktop/Projects/Flink/flink/flink-master/flink-table/flink-table-planner/target/flink-table-planner_2.12-1.17-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.plugins.shade.DefaultShader.shadeJars(DefaultShader.java:277)
    at org.apache.maven.plugins.shade.DefaultShader.shade(DefaultShader.java:128)
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:500)
    ... 13 more
Caused by: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.plugins.shade.DefaultShader.addRemappedClass(DefaultShader.java:564)
    at org.apache.maven.plugins.shade.DefaultShader.shadeJarEntry(DefaultShader.java:310)
    at org.apache.maven.plugins.shade.DefaultShader.shadeJars(DefaultShader.java:272)
    ... 15 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 19
    at org.objectweb.asm.ClassReader.readLabel(ClassReader.java:2679)
    at org.objectweb.asm.ClassReader.createLabel(ClassReader.java:2695)
    at org.objectweb.asm.ClassReader.readTypeAnnotations(ClassReader.java:2760)
    at org.objectweb.asm.ClassReader.readCode(ClassReader.java:1928)
    at org.objectweb.asm.ClassReader.readMethod(ClassReader.java:1514)
    at org.objectweb.asm.ClassReader.accept(ClassReader.java:744)
    at org.objectweb.asm.ClassReader.accept(ClassReader.java:424)
    at org.apache.maven.plugins.shade.DefaultShader.addRemappedClass(DefaultShader.java:560)
    ... 17 more
[ERROR]
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException {code}
macOS 13.4.1 

maven 3.2.5

jdk 1.8_144",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 07:32:37 UTC 2023,,,,,,,,,,"0|z1jcp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 08:06;martijnvisser;From the README:

Maven (we recommend version 3.8.6 and require at least 3.1.1)

You'll need to update your used Maven version;;;","25/Jul/23 08:56;chesnay;Also please double-check what Java version you are using. (mvn --version, java --version, javac --version)
We see this error usually because people use a Java version beyond what Flink supports (e.g, Jave 18+)

On a side-note, you should upgrade your Java 8 installation.;;;","26/Jul/23 05:48;lizhiqiang;thx [~martijnvisser]  [~chesnay] reply.

maven I tried 3.2.5 3.6.3 3.8.8 3.9.1 java I upgraded from jdk8_144 to jdk8_372.

jdk8_144  and  maven 3.2.5 , 3.6.3 , 3.9.1  has the same problem.

jdk8_372 + maven 3.8.8 /  3.9.1 package succeed.  this is a jdk problem.;;;","26/Jul/23 06:51;lizhiqiang;I see this restriction in the maven version in the pom.xml file of the master branch
{code:java}
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-enforcer-plugin</artifactId>
    <executions>
        <execution>
            <id>enforce-maven</id>
            <goals>
                <goal>enforce</goal>
            </goals>
            <configuration>
                <rules>
                    <requireMavenVersion>
                        <!-- maven version must be lower than 3.3. See FLINK-3158 -->
                        <version>(,3.3)</version>
                    </requireMavenVersion>
                    <requireJavaVersion>
                        <version>1.8.0</version>
                    </requireJavaVersion>
                </rules>
            </configuration>
        </execution>
    </executions>
</plugin> {code}
 
And maven-enforcer-plugin  does not to restrict minor versions of jdk 1.8 .
 
We would prefer to develop a smaller version of java labeled on the documentation.
 
 ;;;","26/Jul/23 07:32;lizhiqiang;I found that this is a jdk bug, I think we should limit the small version of jdk, to prevent users from using, wasting a lot of time, this error is particularly difficult to troubleshoot. 

[https://bugs.openjdk.org/browse/JDK-8191969]

fixed in 8u172

[~martijnvisser] 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support read null value for csv format,FLINK-32665,13544741,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zjureel,zjureel,25/Jul/23 06:24,08/Oct/23 22:35,04/Jun/24 20:40,,1.18.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,auto-deprioritized-major,pull-request-available,,"when there is null column in a file with csv format, it will throw exception when flink job try to parse these data",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 08 22:35:15 UTC 2023,,,,,,,,,,"0|z1jcoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Oct/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableSourceJsonPlanTest.testReuseSourceWithoutProjectionPushDown is failing,FLINK-32664,13544724,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,24/Jul/23 23:41,25/Jul/23 07:26,04/Jun/24 20:40,25/Jul/23 07:26,1.18.0,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,test-stability,,"Blocker since it's failing on every build and reproduced locally
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51661&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11529",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 07:26:50 UTC 2023,,,,,,,,,,"0|z1jckw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 23:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51663&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11529;;;","25/Jul/23 05:16;Feifan Wang;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51673&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11250;;;","25/Jul/23 06:38;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51670&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11366;;;","25/Jul/23 07:26;twalthr;Fixed in master: f0aec3a69fa0eadaa96d7b52aaaaa476a6a886e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescalingITCase.testSavepointRescalingInPartitionedOperatorStateList fails on AZP,FLINK-32663,13544716,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,Sergey Nuyanzin,Sergey Nuyanzin,24/Jul/23 23:07,04/Aug/23 07:29,04/Jun/24 20:40,02/Aug/23 05:50,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51501&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8665
fails as
{noformat}
Jul 21 01:24:54 01:24:54.146 [ERROR] RescalingITCase.testSavepointRescalingInPartitionedOperatorStateList  Time elapsed: 1.485 s  <<< FAILURE!
Jul 21 01:24:54 java.lang.AssertionError: expected:<530> but was:<30>
Jul 21 01:24:54 	at org.junit.Assert.fail(Assert.java:89)
Jul 21 01:24:54 	at org.junit.Assert.failNotEquals(Assert.java:835)
Jul 21 01:24:54 	at org.junit.Assert.assertEquals(Assert.java:647)
Jul 21 01:24:54 	at org.junit.Assert.assertEquals(Assert.java:633)
Jul 21 01:24:54 	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingPartitionedOperatorState(RescalingITCase.java:621)
Jul 21 01:24:54 	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingInPartitionedOperatorStateList(RescalingITCase.java:508)
Jul 21 01:24:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 21 01:24:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 21 01:24:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28386,,,,,FLINK-32574,,,,,,,,,,,"01/Aug/23 11:48;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13061828/screenshot-1.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 07:29:54 UTC 2023,,,,,,,,,,"0|z1jcj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 23:31;Sergey Nuyanzin;also very similar fail of {{RescalingITCase.testSavepointRescalingOutPartitionedOperatorStateList}} https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51627&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8664;;;","25/Jul/23 11:44;mapohl;[~Yanfei Lei] Is this one a duplicate issue of FLINK-32574. ...since you're looking into FLINK-32574. The tests only differ in the the type of state they are using but have a similar assertion error. Could you close that one as a duplicate of FLINK-32574 if that's the case?;;;","25/Jul/23 12:29;Yanfei Lei;[~mapohl] I think they are duplicate, I'll close  FLINK-32574 and marked it as a duplicate.;;;","31/Jul/23 07:37;mapohl;[~Yanfei Lei] but do you continue looking into that issue?;;;","31/Jul/23 07:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51804&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8036;;;","31/Jul/23 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51817&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8030;;;","01/Aug/23 07:54;renqs;[~fanrui] Could you help investigating this issue? It's probably related to unaligned checkpoint. Thanks a lot!;;;","01/Aug/23 07:57;fanrui;Thanks for the report, I will take a look asap. cc [~renqs];;;","01/Aug/23 11:03;fanrui;Don't find the root cause, just updating some temporary info here.

This test is savepoint instead of checkpoint, and the savepoint doesn't support unaligned checkpoint. We can see this test is _client.triggerSavepoint_[1], and the java doc of RescalingITCase also writes `/** Test savepoint rescaling. */`[2].

Hope these infos are useful for other contributors, I will continue analyze it as well.


[1] https://github.com/apache/flink/blob/c6d58e17e8ce736a062234e1558ac8d7b65990ef/flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java#L563
[2] https://github.com/apache/flink/blob/c6d58e17e8ce736a062234e1558ac8d7b65990ef/flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java#L93;;;","01/Aug/23 11:52;fanrui;This bug is caused by FLINK-28386, this test doesn't enable checkpoint. And the test will fail once the checkpoint is executed, because the addtional checkpoint will update the `PartitionedStateSource.checkCorrectSnapshot`. This bug can always be reproduced when enabling checkpoint.

However FLINK-28386 triggers an immediate checkpoint after all sinks finished. From the log, we can see this job triggered a checkpoint.

Hi [~Yanfei Lei], would you mind I fix this bug?

 !screenshot-1.png! ;;;","01/Aug/23 12:01;fanrui;Hi [~lindong], I see you help reviewed the FLINK-28386, it caused this bug. cc [~Jiang Xin]

May I know whether we just trigger an immediate checkpoint when checkpoint is enabled is reasonable? Or we really want to trigger an immediate checkpoint after all sources finished regardless of whether checkpoint is enabled.

This question will determine how to fix this bug, looking forward to your feedback, thanks a lot:);;;","01/Aug/23 12:09;fanrui;The priority is upgraded to blocker due to it's caused in 1.18. cc [~renqs] [~Sergey Nuyanzin];;;","01/Aug/23 12:52;lindong;Hi [~fanrui], FLINK-28386 will let Flink runtime trigger a checkpoint immediately after all tasks have received end-of-data, instead of after all sources finished. I have updated its JIRA title/description to clarify this.

And yes, I believe it is reasonable to trigger a checkpoint immediately after tall tasks have received end-of-data. This is because if we don't do this, the streaming job will waste time waiting for the triggering of the next periodic checkpoint triggering in order to shutdown, for no good reason.

By ""it caused this bug"", I guess what you mean is that the test failure is caused by a checkpoint triggered by FLINK-28386. However, it does not necessarily mean that FLINK-28386 is the root cause of this bug. For example, if the test can fail due to an immediate checkpoint triggered by FLINK-28386, there is also chance that the test can fail due to periodically triggered checkpoint.

So we probably need to understand the root cause before determining how to fix it.

 ;;;","01/Aug/23 12:59;fanrui;Hi [~lindong], thanks for your quick feedback.

{quote}For example, if the test can fail due to an immediate checkpoint triggered by FLINK-28386, there is also chance that the test can fail due to periodically triggered checkpoint.{quote}

As I said before, this test doesn't enable checkpoint. And the test will fail once the checkpoint is executed, because the addtional checkpoint will update the `PartitionedStateSource.checkCorrectSnapshot`. This bug can always be reproduced when enabling checkpoint.

So my question is should FLINK-28386 trigger an immediate checkpoint even if the checkpoint of this job is disable? If yes, I can fix this test. If no, I can fix the FLINK-28386.
;;;","02/Aug/23 03:09;lindong;[~fanrui] Thanks for the investigation!

FLINK-27386 is expected to trigger an immediate checkpoint only if checkpoint is enabled. According to our offline discussion, it appears that `StateWithExecutionGraph#notifyEndOfData` triggers an immediate checkpoint for this test.

Currently, `StateWithExecutionGraph#notifyEndOfData` relies on `checkpointCoordinatorConfiguration != null` to check whether checkpoint is enabled. I agree it is useful to additionally check whether checkpointInterval is in range (0, Long.MAX_VALUE), similar to what is done in `JobGraph#isCheckpointingEnabled`.

 ;;;","02/Aug/23 05:25;lindong;Merged to apache/flink master branch 7d07ec270cc02a04809d4981fbd1423bdf2822f2;;;","02/Aug/23 06:43;mapohl;The following build failed due to the issue described in this Jira twice. It doesn't contain the above mentioned fix yet. I'll add this build failure anyway for documentation purposes.

* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8341
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8340

;;;","02/Aug/23 06:44;mapohl;[~lindong] Please be reminded to update the fix version as well when merging the PR and closing the Jira. I will do it for this one. :-);;;","02/Aug/23 07:23;lindong;[~mapohl] Thanks for the reminder. Will do it in the future :);;;","04/Aug/23 07:29;Yanfei Lei;[~fanrui] Sorry for the late reply, thanks for the fix!;;;",,,,,,,,,,,,,,,,,,,
JobMasterTest.testRetrievingCheckpointStats fails with NPE on AZP,FLINK-32662,13544715,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,24/Jul/23 22:59,03/Aug/23 09:18,04/Jun/24 20:40,02/Aug/23 11:30,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51452&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8654
fails with NPE as
{noformat}
Jul 20 01:01:33 01:01:33.491 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats  Time elapsed: 0.036 s  <<< ERROR!
Jul 20 01:01:33 java.lang.NullPointerException
Jul 20 01:01:33 	at org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats(JobMasterTest.java:2132)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 20 01:01:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 20 01:01:33 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 20 01:01:33 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32469,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 11:30:11 UTC 2023,,,,,,,,,,"0|z1jciw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 23:09;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51501&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8583;;;","24/Jul/23 23:16;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51555&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8579;;;","24/Jul/23 23:29;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51627&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8583;;;","25/Jul/23 06:41;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51670&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8658;;;","25/Jul/23 07:09;mapohl;[~liangtl] might this be related to FLINK-32469. Can you have a look?;;;","27/Jul/23 09:05;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51745&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8589;;;","27/Jul/23 09:16;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51712&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8664;;;","28/Jul/23 03:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51777&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=7282;;;","28/Jul/23 13:05;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51777&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=7282;;;","31/Jul/23 07:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51798&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=7206;;;","31/Jul/23 07:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51804&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702;;;","31/Jul/23 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51817&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=7206;;;","01/Aug/23 06:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51851&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=7300;;;","02/Aug/23 06:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=7281;;;","02/Aug/23 11:30;mapohl;master: 74babdab7fd5b72d5eec5bff4070532a9621b8a4;;;",,,,,,,,,,,,,,,,,,,,,,,,
OperationRelatedITCase.testOperationRelatedApis fails on AZP,FLINK-32661,13544714,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,Sergey Nuyanzin,Sergey Nuyanzin,24/Jul/23 22:54,29/Oct/23 09:21,04/Jun/24 20:40,29/Oct/23 09:21,1.18.0,,,,,1.19.0,,,,Table SQL / Gateway,,,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51452&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12114
fails as 
{noformat}
Jul 20 04:23:49 org.opentest4j.AssertionFailedError: 
Jul 20 04:23:49 
Jul 20 04:23:49 Expecting actual's toString() to return:
Jul 20 04:23:49   ""PENDING""
Jul 20 04:23:49 but was:
Jul 20 04:23:49   ""RUNNING""
Jul 20 04:23:49 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jul 20 04:23:49 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Jul 20 04:23:49 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jul 20 04:23:49 	at org.apache.flink.table.gateway.rest.OperationRelatedITCase.testOperationRelatedApis(OperationRelatedITCase.java:91)
Jul 20 04:23:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 20 04:23:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 20 04:23:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 20 04:23:49 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 20 04:23:49 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Jul 20 04:23:49 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Jul 20 04:23:49 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Jul 20 04:23:49 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
Jul 20 04:23:49 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
Jul 20 04:23:49 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:21
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 07:06;jiabao.sun;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13063735/screenshot-1.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 29 09:21:45 UTC 2023,,,,,,,,,,"0|z1jcio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 09:16;renqs;[~fsk119] could you take a look at this one? Thanks;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","20/Oct/23 07:10;jiabao.sun; !screenshot-1.png! 

The operation execution is asynchronous and the status will only be updated from Pending to Running after calling from the runBefore method.
Adding an thread suspended breakpoint can reproduce this problem.;;;","29/Oct/23 09:21;leonard;Fixed in master: 83ee295ae046bceebfa2dfb286afb20a814023e8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support external file systems in FileCatalogStore,FLINK-32660,13544652,13541314,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,24/Jul/23 12:04,11/Aug/23 09:57,04/Jun/24 20:40,11/Aug/23 09:57,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 09:57:26 UTC 2023,,,,,,,,,,"0|z1jc4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 12:30;ferenc-csaky;Hi [~Leonard],

can you pls. assign this one to me? I opened the PR with the changes. For now, applied 1.18, but IDK if that's feasible.

Thanks,
Ferenc;;;","24/Jul/23 12:38;hackergin;I think supporting external systems is very useful.  I also wish that this feature can be merged into version 1.18.;;;","11/Aug/23 09:57;zjureel;Thanks [~ferenc-csaky], fixed with f68967a7c938f6258125c3221be74522965d1ff2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DB connection may leak if exception is thrown in JdbcOutputFormat#close,FLINK-32659,13544648,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,24/Jul/23 11:42,16/Jan/24 01:51,04/Jun/24 20:40,,,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 03:36:05 UTC 2023,,,,,,,,,,"0|z1jc40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/23 03:36;aitozi;Can someone help review the fix: https://github.com/apache/flink-connector-jdbc/pull/71 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State should not be silently removed when ignore-unclaimed-state is false,FLINK-32658,13544641,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,24/Jul/23 11:10,17/Sep/23 22:35,04/Jun/24 20:40,,1.17.1,1.18.0,,,,,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,stale-assigned,,"When ignore-unclaimed-state is false and the old state is removed, flink should throw exception. It's similar to removing a stateful operator.

This case occurs not only when the user removes state, but also when the operator is replaced. 

For example: upgrade FlinkKafkaConsumer to KafkaSource. All logical are not changed, so the operator id isn't changed. The KafkaSource cannot resume from the state of FlinkKafkaConsumer. However, the new flink job can start, and the state is silently removed in the new job.(The old state is not physically discarded, it is still stored in the state backend, but the new code will never use it.)

It also brings an additional problem: the KafkaSource will snapshot 2 states, it includes the new state of KafkaSource, and the union list state of FlinkKafkaConsumer. Whenever a job resumes from checkpoint, the union List state is inflated. Eventually the state size of kafka offset exceeded 200MB.

 !screenshot-1.png! 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10121,,,,,,,,,,,"24/Jul/23 11:14;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13061570/screenshot-1.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 17 22:35:05 UTC 2023,,,,,,,,,,"0|z1jc2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/23 04:33;zakelly;[~fanrui] IIUC, you mean there is a problem when the state changes (or be removed) but the operator still exists (the operator id remains the same), Flink will not report an error when `ignore-unclaimed-state` is set to `false` ? ;;;","02/Aug/23 04:56;fanrui;Hi [~Zakelly], thanks for your reply.

{quote}you mean there is a problem when the state changes (or be removed) but the operator still exists (the operator id remains the same), Flink will not report an error when `ignore-unclaimed-state` is set to `false` ? {quote}

That's exactly what I mean. If Flink reports an error in this case, it is more friendly to the flink user. Also, if `ignore-unclaimed-state` is set to `true` flink should discard these removed state inside of the operator to prevent state leak.

;;;","03/Aug/23 07:31;zakelly;Hi [~fanrui] , this make sense. Thanks for reporting this.;;;","18/Aug/23 04:03;masteryhx;Thanks a lot for the proposal, I think it makes sense.

BTW, I saw FLINK-10121 also talk something related to this so just linked it.

Just as discussed offline, I think it's better to consider operator state / corrdinater state / keyed state together which could make the interface more clear.

And seems we have to modify the semantics of `ignore-unclaimed-state` and modify/add new interface in the public API, I think we may need a FLIP firstly.

A blocker is that we could not make sure that users only create state in the open method. I think it's a seprate topic but I'm big +1 to limit the user operation to avoid creating state when processElement:
1. We have seen many cases users createing state when processElement which often bring up many strange problems, e.g. error-prone judging null logic, job stuck in schema evolution even if job is running... Actually, they could move this to open method in all cases.

2. It's better to provide declartion info to state backend early which could make state backend do some previous optimization. This is also described in FLIP-22.
3. I also saw all SQL operators creating state in the open method, so I think it works well for most even all cases.



Of course, it may break changes, it's better to be considered in Flink 2.0;;;","18/Aug/23 06:01;fanrui;Thanks [~masteryhx] for the valuable feedback, I will prepare a FLIP asap :);;;","17/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert upgrading ExecNode versions for StateMetadata,FLINK-32657,13544625,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,twalthr,twalthr,24/Jul/23 09:18,24/Jul/23 13:36,04/Jun/24 20:40,24/Jul/23 13:36,,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"In theory, introducing a new attribute in ExecNode requires upgrading the version of ExecNodeMetadata. However, since this is currently an experimental feature, the attributes that need to be serialized for the final exec node are still being iterated, and upgrading the version will make the serialization scheme of the lower version become immutable (unless a patch is applied to the old version), and the testing framework is not perfect either. Therefore, upgrading the ExecNode version is not necessary if state compatibility can be maintained at the implementation level.

It should be okay to roll back ExecNodeMetadata to version 1 because compatibility handling is enabled at the code level.

Long-term we need a larger testing framework, per-Flink and per-ExecNode version that validates all attributes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 13:36:26 UTC 2023,,,,,,,,,,"0|z1jbyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 13:36;twalthr;Fixed in master: 6a3808213d334de614e162362d1583f3e5322358;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate ManagedTable related APIs,FLINK-32656,13544610,13542776,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,qingyue,qingyue,qingyue,24/Jul/23 07:59,04/Aug/23 08:52,04/Jun/24 20:40,04/Aug/23 08:52,1.18.0,,,,,1.18.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"Please refer to [FLIP-346: Deprecate ManagedTable related APIs|https://cwiki.apache.org/confluence/display/FLINK/FLIP-346%3A+Deprecate+ManagedTable+related+APIs] for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 08:52:09 UTC 2023,,,,,,,,,,"0|z1jbvk:",9223372036854775807,ManagedTable related APIs are deprecated and will be removed in a future major release.,,,,,,,,,,,,,,,,,,,"04/Aug/23 08:52;xtsong;master (1.18): 34729c8891448b8f0a96dbbc12603b44a6e130c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecreateOnResetOperatorCoordinator did not forward notifyCheckpointAborted to the real OperatorCoordinator,FLINK-32655,13544608,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,24/Jul/23 07:38,26/Jul/23 02:07,04/Jun/24 20:40,26/Jul/23 02:07,1.17.1,,,,,1.16.3,1.17.2,1.18.0,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,"{{[RecreateOnResetOperatorCoordinator|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/RecreateOnResetOperatorCoordinator.java#L115]}} does not override {{{}notifyCheckpointAborted{}}}, which causes the {{SplitEnumerator}} in {{SourceCoordinator}} can not receive the checkpoint abort message.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 09:21:11 UTC 2023,,,,,,,,,,"0|z1jbv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 09:21;fanrui;Thanks [~Ming Li] for the fix, merged via:
<master : 1.18> e8a8b05b18f5bf5362d88a548e5c861097f7fa8f
1.17 8a9c943661c227ee58b2d744dee2c92f0f61a51b
1.16 849523ed2f76865e6caf106db76f3a6a4910e3ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate ExecutionConfig#canEqual(obj),FLINK-32654,13544606,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,24/Jul/23 07:33,24/Jul/23 10:47,04/Jun/24 20:40,24/Jul/23 10:47,,,,,,1.18.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,"ExecutionConfig#canEqual(obj) checks whether the object is an instance of ExecutionConfig.
It is not intended to be used by users. It is just used for internal comparison. Unfortunately, is was exposed as `@Public` because ExecutionConfig is `@Public`.
We should deprecate it so that we can remove it in Flink 2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 10:47:00 UTC 2023,,,,,,,,,,"0|z1jbuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 10:47;zhuzh;done via 5cda33d87be1a39e63f24d3c564c5a23ecb699d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for catalog store,FLINK-32653,13544581,13541314,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,24/Jul/23 03:05,18/Aug/23 02:35,04/Jun/24 20:40,18/Aug/23 02:35,,,,,,1.19.0,,,,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 02:35:14 UTC 2023,,,,,,,,,,"0|z1jbp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/23 09:50;hackergin;[~Leonard] please assign this task to me when you are free. Thanks. ;;;","18/Aug/23 02:35;zjureel;Thanks [~hackergin], fixed by 5628c7097875be4bd56fc7805dbdd727d92bdac7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator cannot scale standalone deployments in reactive mode,FLINK-32652,13544553,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,mateczagany,mateczagany,23/Jul/23 13:30,24/Jul/23 06:54,04/Jun/24 20:40,24/Jul/23 06:54,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.6.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"After we upgraded the Fabric8 Kubernetes Client to 6.7.0 the operator can no longer scale standalone deployments in reactive mode because it uses the ""deployments/scale"" API instead of patching the deployment since this commit: [https://github.com/fabric8io/kubernetes-client/commit/c4d3dd14c6ba7261fe4646636d277cba1c2122a2]

We will get the following error: 
{code:java}
org.apache.flink.kubernetes.operator.exception.ReconciliationException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.96.0.1:443/apis/apps/v1/namespaces/flink/deployments/basic-reactive-example-taskmanager/scale. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. deployments.apps ""basic-reactive-example-taskmanager"" is forbidden: User ""system:serviceaccount:flink:flink-operator"" cannot get resource ""deployments/scale"" in API group ""apps"" in the namespace ""flink"". {code}
The fix is easy, we just need to add ""deployments/scale"" to the ClusterRole we create, I'll create a PR soon",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 06:54:26 UTC 2023,,,,,,,,,,"0|z1jbiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 06:54;gyfora;merged to main fea41f2d98cf5cd70254f2a504c751a7be198b65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmark Support for Changelog Statebackend,FLINK-32651,13544548,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ym,ym,23/Jul/23 10:14,23/Jul/23 10:14,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-23 10:14:07.0,,,,,,,,,,"0|z1jbhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added the ability to split flink-protobuf codegen code,FLINK-32650,13544546,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijingwei.5018,lijingwei.5018,lijingwei.5018,23/Jul/23 09:29,13/Nov/23 15:15,04/Jun/24 20:40,13/Nov/23 15:15,1.17.1,,,,,1.19.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,"h3. backgroud

Flink serializes and deserializes protobuf format data by calling the decode or encode method in GeneratedProtoToRow_XXX.java generated by codegen to parse byte[] data into protobuf java objects. The size of the decode/encode codegen method body is strongly related to the number of defined fields in protobuf. When the number of fields exceeds a certain threshold and the compiled method body exceeds 8k, the decode/encode method will not be optimized by JIT, seriously affecting serialization or deserialization performance. Even if the compiled method body exceeds 64k, it will directly cause the task to fail to start.
h3. solution

So I proposed Codegen Splitter for protobuf parsing to split the encode/decode method to solve this problem.

The specific idea is as follows. In the current decode/encode method, each field defined for the protobuf message is placed in the method body. In fact, there are no shared parameters between the fields, so multiple fields can be merged and parsed and written into the split method body. If the number of strings in the current method body exceeds the threshold, a split method will be generated, these fields will be parsed in the split method, and the split method will be called in the decode/encode method. By analogy, the decode/encode method including the split method is finally generated.

after spilt code example

 
{code:java}
//代码占位符
public static RowData decode(org.apache.flink.formats.protobuf.testproto.AdProfile.AdProfilePb message){
RowData rowData=null;
org.apache.flink.formats.protobuf.testproto.AdProfile.AdProfilePb message1242 = message;
GenericRowData rowData1242 = new GenericRowData(5);
split2585(rowData1242, message1242);
rowData = rowData1242;return rowData;
}

{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33533,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 15:15:38 UTC 2023,,,,,,,,,,"0|z1jbhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 13:06;libenchao;+1 for this. Besides the approach mentioned in the description, have you considered utilizing the flink-table-code-splitter to do this?;;;","27/Jul/23 03:33;lijingwei.5018;[~libenchao]  I don't know much about the specific implementation and usage of flink-table-code-splitter. I will give you a conclusion after I understand it.;;;","29/Jul/23 08:57;lijingwei.5018;[~libenchao] I simply use the JavaCodeSpliter in flink-table-code-splitter to split the class from flink-protobuf codegen, and found that it cannot be used directly. The main reason is that the decode/encode method in GeneratedProtoToRow_xxx.java is static, so all the split methods and variables should be static, but JavaCodeSpilter does not take this into consideration, so the split code janino cannot be successfully compiled. Another point is that I think it is a bit strange to let flink-protobuf rely on flink-table-code-splitter. If flink-protobuf have my own splitter, I think it should be more reasonable.
How to make JavaCodeSplitter support the splitting of static methods can be solved in a new issue after refining the problem?;;;","29/Jul/23 15:41;libenchao;[~lijingwei.5018] Thanks for the investigation, I agree with you. I assigned this ticket to you.

Besides, since we also use `protoc` to compile proto to java class, then for large proto defination, it will also encounter the similar problem, what do you think adding a section to the doc to tell the users about this?;;;","30/Jul/23 03:10;maosuhan;[~lijingwei.5018] Thanks for opening this ticket and share the solution. 

Thanks [~libenchao] for reminding that. According to the benchmark test in production usage, the code of single method generated by protoc is probably larger than the code generated by flink protobuf. If the number of fields is too large, it still have impact on the performance. 

I see some related issues of protobuf, [https://github.com/protocolbuffers/protobuf/issues/10247] [https://github.com/protocolbuffers/protobuf/pull/10367] It seems there's someone already knew this problem and tried to fix that in protobuf but it has no progress now.  If we can push protobuf to do the similar change, the performance issue can be nicely solved for this case. Of course, this should not block current ticket's job.;;;","07/Aug/23 07:26;lijingwei.5018;[~maosuhan] Thank you for your suggestion, I will continue to pay attention to the problems you mentioned;;;","13/Nov/23 05:14;libenchao;I've created another ticket to add the document about current restriction, see FLINK-33533. Feel free to pick it up if you are interested.;;;","13/Nov/23 15:15;libenchao;Fixed via a2ec4c3b8bd9f2009fd721ab4c51afedcd8574fb (1.19.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mismatch label and value for prometheus reporter,FLINK-32649,13544538,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,23/Jul/23 03:22,16/Aug/23 03:59,04/Jun/24 20:40,,1.17.0,,,,,,,,,Runtime / Metrics,,,,,,0,pull-request-available,,,"When runing unit test 'org.apache.flink.metrics.prometheus.PrometheusReporterTest#metricIsRemovedWhileOtherMetricsWithSameNameExist', it got wrong response string as 
{code:java}
# HELP flink_logical_scope_metric metric (scope: logical_scope)
# TYPE flink_logical_scope_metric gauge
flink_logical_scope_metric{label1=""some_value"",label2=""value1"",} 0.0 {code}
 

in my opinion, the expected right response is :

 
{code:java}
# HELP flink_logical_scope_metric metric (scope: logical_scope)
# TYPE flink_logical_scope_metric gauge
flink_logical_scope_metric{label1=""value1"",label2=""some_value"",} 0.0
 {code}
 

 

The reason may be we create two metric with same name, but two different order  label-keys in the variables of MetricGroup. And we don't sort the key&value in variables within methond 'org.apache.flink.metrics.prometheus.AbstractPrometheusReporter#notifyOfAddedMetric'.

 

Maybe it won't happen in production env now, howerver it's important to ensure the robustness of method with unpected input param.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-23 03:22:28.0,,,,,,,,,,"0|z1jbfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse Calcite's SqlWindowTableFunction,FLINK-32648,13544529,13397543,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Jul/23 17:59,23/Aug/23 19:35,04/Jun/24 20:40,23/Aug/23 19:35,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,There is Calcite's {{SqlWindowTableFunction}} which could extended with reuse of some existing logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 19:35:19 UTC 2023,,,,,,,,,,"0|z1jbdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/23 19:35;Sergey Nuyanzin;Merged to master as [5164697797dd013672fe0b8d82ecdf8eef5d416b|https://github.com/apache/flink/commit/5164697797dd013672fe0b8d82ecdf8eef5d416b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support config catalog store in python table environment,FLINK-32647,13544519,13541314,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hackergin,hackergin,hackergin,22/Jul/23 13:30,23/Jul/23 18:19,04/Jun/24 20:40,,,,,,,,,,,API / Python,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-22 13:30:33.0,,,,,,,,,,"0|z1jbbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The snapshot is null in CatalogTable when query specify the time snapshot,FLINK-32646,13544511,13541827,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lincoln.86xy,lsy,lsy,22/Jul/23 09:03,22/Jul/23 22:22,04/Jun/24 20:40,22/Jul/23 22:22,1.18.0,,,,,1.18.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"When I debug the test `TimeTravelITCase#testTimeTravelOneTableMultiTimes`, we have specify the time in query, but I found the snapshot is null in `CatalogTable`. I think it's not what was expected, we should fix it.

!image-2023-07-22-17-02-54-246.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/23 09:02;lsy;image-2023-07-22-17-02-54-246.png;https://issues.apache.org/jira/secure/attachment/13061551/image-2023-07-22-17-02-54-246.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 22 22:22:05 UTC 2023,,,,,,,,,,"0|z1jb9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/23 09:10;lincoln.86xy;Had a quick look at the issue, the `TestTimeTravelCatalog` didn't save the snapshot timestamp properly. We can have a quick fix and also add related plan tests.;;;","22/Jul/23 10:12;hackergin;+1，It should be an issue with the implementation of TestTimeTravelCatalog.;;;","22/Jul/23 22:22;lincoln.86xy;Fixed in master: 17be3716be42cd9dc5986e97ae222971d9d19be7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink pulsar sink is having poor performance,FLINK-32645,13544509,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,vbhasvij,vbhasvij,22/Jul/23 08:36,15/Apr/24 10:16,04/Jun/24 20:40,15/Apr/24 10:16,1.16.2,,,,,pulsar-3.0.2,,,,Connectors / Pulsar,,,,,,0,,,,"Found following issue with flink pulsar sink:

 

Flink pulsar sink is always waiting while enqueueing the message and making the task slot busy no matter how many free slots we provide. Attached the screen shot of the same

Just sending messages of less rate 8k msg/sec and stand alone flink job with discarding sink is able to receive full rate if 8K msg/sec

Where as pulsar sink was consuming only upto 2K msg/sec and the sink is always busy waiting. Snapshot of thread dump attached.

Also snap shot of flink stream graph attached

 

 

 ","!Screenshot 2023-07-22 at 1.59.42 PM.png!!Screenshot 2023-07-22 at 2.03.53 PM.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28820,,,,,,,,,,,,,,,,,,,"22/Jul/23 08:36;vbhasvij;Screenshot 2023-07-22 at 2.03.53 PM.png;https://issues.apache.org/jira/secure/attachment/13061549/Screenshot+2023-07-22+at+2.03.53+PM.png","22/Jul/23 10:21;vbhasvij;Screenshot 2023-07-22 at 2.56.55 PM.png;https://issues.apache.org/jira/secure/attachment/13061552/Screenshot+2023-07-22+at+2.56.55+PM.png","22/Jul/23 10:22;vbhasvij;Screenshot 2023-07-22 at 3.45.21 PM-1.png;https://issues.apache.org/jira/secure/attachment/13061554/Screenshot+2023-07-22+at+3.45.21+PM-1.png","22/Jul/23 10:21;vbhasvij;Screenshot 2023-07-22 at 3.45.21 PM.png;https://issues.apache.org/jira/secure/attachment/13061553/Screenshot+2023-07-22+at+3.45.21+PM.png","24/Jul/23 18:55;vbhasvij;pom.xml;https://issues.apache.org/jira/secure/attachment/13061582/pom.xml",,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 10:14:32 UTC 2024,,,,,,,,,,"0|z1jb94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/23 10:23;vbhasvij;After disabling batching of the messages, message throughput increased little bit. 

This shows that, pulsar writer thread queue lock is causing the issue while adding more messages to it, also as shown by thread dump.

One more aspect that confirms it is:

Increased the partitions of sink topic and increased the parallelism of sink equal to partitions. Throughput is increasing.

But following are the problems with this approach
 # We have to use more task slots of flink, i.e., more resources unnecessarily
 # Inspite of increasing back pressure is still remaining same because pulsar sink is 100% busy always
 # !Screenshot 2023-07-22 at 2.56.55 PM.png!!Screenshot 2023-07-22 at 3.45.21 PM.png!!Screenshot 2023-07-22 at 3.45.21 PM.png! no matter the number of slots are attached the screen shot

 

 ;;;","24/Jul/23 03:57;tison;What version of Flink Pulsar connector did you use? 1.16.2 is Flink version and now Pulsar Flink connector takes it own version series https://github.com/apache/flink-connector-pulsar. Generally, 3.0.1 or 4.0.0.;;;","24/Jul/23 04:39;vbhasvij;We are using the following as published here in flink docs: [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/connectors/datastream/pulsar/]

Where connector version is: 1.16.2.  As per this our entire environment is flink 1.16.2 

As you suggested tried mentioning 3.0.1 or 4.0.0, maven unable to download the connector from Maven. Looks like these connectors are not published.;;;","24/Jul/23 04:48;tison;The new version series use a differnt GAV. You can find them at https://mvnrepository.com/artifact/org.apache.flink/flink-connector-pulsar. 3.0.1 should be compatible with Flink 1.16.;;;","24/Jul/23 06:09;syhily;[~vbhasvij] Sorry for the poor performance of Pulsar sink. This is a known issue and has been fixed in 1.17 for a loooong time. We just forget to backport the FLINK-28820 to Flink 1.16. [~tison] Can you help me draw a new v3.0 release of flink-connector-pulsar which contains this fix?;;;","24/Jul/23 15:25;vbhasvij;We are facing upgrade issues with this connector after upgrading to flink 1.17.0, Any help?

 switched from INITIALIZING to FAILED with failure cause: java.lang.NoSuchMethodError: 'void org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.<init>(org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue, java.util.function.Supplier, org.apache.flink.configuration.Configuration)'
        at org.apache.flink.connector.pulsar.source.reader.PulsarSourceFetcherManager.<init>(PulsarSourceFetcherManager.java:71)
        at org.apache.flink.connector.pulsar.source.reader.PulsarSourceReader.create(PulsarSourceReader.java:298)
        at org.apache.flink.connector.pulsar.source.PulsarSource.createReader(PulsarSource.java:137)

 

These line numbers  PulsarSource.java:137  and PulsarSourceReader.java:298 don't exist while i was running my integration test locally. Wondering this connector version really works?

 

 

 ;;;","24/Jul/23 18:56;vbhasvij;1.17.0 connector is crashing repeatedly with pulsar version 2.10.2 and 2.11.  Above call stack i am getting. 

All my local integration tests with pulsar docker 2.10.2 are succeeding,  here is my pom file attached [deleted]

 ;;;","24/Jul/23 22:52;syhily;Hi [~vbhasvij], can you add the flink-connector-base to your pom and shade it into the uber-jar to see if everything is ok?

It seems like your production Flink environment is based on 1.16. Flink introduced a new stop policy since 1.17. So a new {{Configuration}} argument has been added to the constructor of {{SplitFetcherManager}}. The NoSuchMethodError show that you may not use the 1.17 Flink.;;;","01/Aug/23 07:05;vbhasvij;[~syhily]  thanks its working fine now. Will report here any other issues if we see. Please keep the ticket open until that time

 ;;;","15/Apr/24 10:14;syhily;[~tison] I think we can close this issue now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ts,FLINK-32644,13544500,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,weitianpei,weitianpei,22/Jul/23 02:04,22/Jul/23 07:39,04/Jun/24 20:40,22/Jul/23 07:39,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-22 02:04:57.0,,,,,,,,,,"0|z1jb74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce off-heap shared state cache across stateful operators in TM,FLINK-32643,13544499,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,22/Jul/23 01:16,24/Jul/23 04:07,04/Jun/24 20:40,,1.19.0,,,,,,,,,Runtime / State Backends,,,,,,0,,,,"Currently each stateful operator will create an independent db instance if it uses rocksdb as state backend, and we can configure `state.backend.rocksdb.block.cache-size` for each db to speed up state performance. This parameter defaults to 8M, and we cannot set it too large, such as 512M, this may cause OOM and each DB cannot effectively utilize memory. To address this issue, we would like to introduce off-heap shared state cache across multiple db instances for stateful operators in TM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 04:07:52 UTC 2023,,,,,,,,,,"0|z1jb6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 04:07;masteryhx;Hi, Thanks for the proposal.
I have some questions about this, PTAL:
{quote}we cannot set it too large, such as 512M, this may cause OOM
{quote}
We also have a large block cache size in the production env, it works well in most cases.

You mean that lacking strict capacity limit for memory usage for RocksDB [1] may cause OOM ?
{quote}and each DB cannot effectively utilize memory
{quote}
Memory sharing between RocksDB instances has been implemented [2], Could this help to resolve ?
{quote}introduce off-heap shared state cache across multiple db instances for stateful operators in TM.
h4. 
{quote}
What's the cache type ? read-write cache or only read cache ? And What's the data structure ?

What's the cache strategy and granularity ? Caching in the record Level  my increase the overhead per record.

Could this also increase the space overhead compared to Block Cache (due to compression)?

Maybe I missed something about details. I'm also interested in this so wanting to learn more. 


[1] https://issues.apache.org/jira/browse/FLINK-15532
[2] https://issues.apache.org/jira/browse/FLINK-29928;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The upsert mode doesn't work for the compound keys,FLINK-32642,13544469,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jasonliangyc,jasonliangyc,21/Jul/23 16:23,24/Jul/23 01:26,04/Jun/24 20:40,,1.17.0,1.17.1,,,,,,,,Table SQL / Client,,,,,,0,,,," 
Hi, the issue can be produced by following below steps:
 
*1.* Create two tables in sqlserver, one is the sink table, the other one is cdc source table, the sink table has a compound key(date_str,time_str).
*2.* Create the corresponding flink tables in sql-client.
 
{code:java}
--create sink table in sqlserver
CREATE TABLE cumulative_cnt (
  date_str VARCHAR(50),
  time_str VARCHAR(50),
  cnt INTEGER
  CONSTRAINT PK_cumulative_cnt PRIMARY KEY (date_str,time_str)
);

--create source cdc table in sqlserver
CREATE TABLE user_behavior (
  id INTEGER NOT NULL IDENTITY(101,1) PRIMARY KEY,
  create_date datetime NOT NULL,
  click_event VARCHAR(255) NOT NULL
);

EXEC sys.sp_cdc_enable_table  
@source_schema = N'dbo',  
@source_name   = N'user_behavior',  
@role_name     = NULL,  
@supports_net_changes = 1  
GO

--create flink tables through sql-client
CREATE TABLE cumulative_cnt (
    date_str STRING,
    time_str STRING,
    cnt BIGINT,
    PRIMARY KEY (date_str, time_str)NOT ENFORCED
  )  WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:sqlserver://xxxx:1433;databaseName=xxxx',
    'username' = 'xxxx',
    'password' = 'xxxx',
    'table-name' = 'cumulative_cnt'
);

--create flink cdc table through sql-client
CREATE TABLE user_behavior (
   id int,
   create_date TIMESTAMP(0),
   click_event STRING
 ) WITH (
    'connector' = 'sqlserver-cdc',
    'hostname' = 'xxxx',
    'port' = '1433',
    'username' = 'xxxx',
    'password' = 'xxxx',
    'database-name' = 'xxxx',
    'schema-name' = 'dbo',
    'table-name' = 'user_behavior'
 ); {code}
*3.* Run below sql through sql-client to start the job for capturing the cdc data and do the aggregation and finally insert the result into target table.
{code:java}
insert into cumulative_cnt
select
date_str,
max(time_str) as time_str,
count(*) as cnt
from
(
  select
  DATE_FORMAT(create_date, 'yyyy-MM-dd') as date_str,
  SUBSTR(DATE_FORMAT(create_date, 'HH:mm'),1,4) || '0' as time_str
  from user_behavior
) group by date_str; 


{code}
*4.* Insert two records for testing.
{code:java}
INSERT INTO user_behavior(create_date, click_event)VALUES ('2023-06-01 
01:01:00','click1');
INSERT INTO user_behavior(create_date, click_event)VALUES ('2023-06-01 02:20:00','click1');{code}
*5.* Checked the result in db( pls see the screen 1) and found that the target table only have one record, but it is not the expectation cause the two source records have different time, thus the compound key(date_str,  time_str) shoud be different( pls see the screen 2 )
 
There should be two records in the target table:
2023-06-01    01:00    1
2023-06-01    02:20    2
 
screen 1
!image-2023-07-22-00-11-56-363.png|width=274,height=199!
 
screen 2
!image-2023-07-21-23-57-22-186.png|width=572,height=120!
 
 
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/23 15:55;jasonliangyc;image-2023-07-21-23-55-47-399.png;https://issues.apache.org/jira/secure/attachment/13061534/image-2023-07-21-23-55-47-399.png","21/Jul/23 15:56;jasonliangyc;image-2023-07-21-23-56-56-543.png;https://issues.apache.org/jira/secure/attachment/13061533/image-2023-07-21-23-56-56-543.png","21/Jul/23 15:57;jasonliangyc;image-2023-07-21-23-57-22-186.png;https://issues.apache.org/jira/secure/attachment/13061532/image-2023-07-21-23-57-22-186.png","21/Jul/23 16:11;jasonliangyc;image-2023-07-22-00-11-56-363.png;https://issues.apache.org/jira/secure/attachment/13061531/image-2023-07-22-00-11-56-363.png",,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-21 16:23:48.0,,,,,,,,,,"0|z1jb08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json format supports pojo type,FLINK-32641,13544432,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,21/Jul/23 11:54,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 12:26:12 UTC 2023,,,,,,,,,,"0|z1jas0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 12:26;martijnvisser;[~jackylau] Can you be a little more descriptive in this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hard backpressure time should also be measured in hybrid shuffle,FLINK-32640,13544362,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,21/Jul/23 03:47,24/Jul/23 08:23,04/Jun/24 20:40,24/Jul/23 08:23,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 08:23:04 UTC 2023,,,,,,,,,,"0|z1jacg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 08:23;Weijie Guo;master(1.18) via eab99b6de820352490cce580fee2e7f31ef16617.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Filter and Limit exist at the same time, limit cannot take effect",FLINK-32639,13544304,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,dovezhang,dovezhang,20/Jul/23 13:48,21/Jul/23 01:28,04/Jun/24 20:40,21/Jul/23 01:28,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,"Define the environment using Flink Batch.

The Source connector uses filesystem(FileSystemTableSource implements SupportsLimitPushDown/SupportsFilterPushDown)

 
{code:java}
EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
StreamTableEnvironment tabEnv = StreamTableEnvironment.create(env, settings);
tabEnv.executeSql(
        ""CREATE TABLE source(uuid varchar, name varchar, age int, ts timestamp,`partition` varchar)  ""
                + ""WITH ( 'connector' = 'filesystem', 'path'='file:///tmp/file', 'format'='csv' ""
                + "")"");{code}
Case 1: Filter

 
{code:java}
tabEnv.executeSql(""explain select * from source where name is null"").print();

== Optimized Execution Plan ==
Calc(select=[uuid, null:VARCHAR(2147483647) AS name, age, ts, partition], where=[name IS NULL])
+- TableSourceScan(table=[[default_catalog, default_database, source, filter=[IS NULL(name)]]], fields=[uuid, name, age, ts, partition]){code}
 

Case 2: Limit 
{code:java}
tabEnv.executeSql(""explain select * from source limit 10"").print();

== Optimized Execution Plan ==
Limit(offset=[0], fetch=[10], global=[true])
+- Exchange(distribution=[single])
   +- Limit(offset=[0], fetch=[10], global=[false])
      +- TableSourceScan(table=[[default_catalog, default_database, source, limit=[10]]], fields=[uuid, name, age, ts, partition]) {code}
Case 3: Filter + Limit
{code:java}
tabEnv.executeSql(""explain select * from source where name is null limit 10"").print();

== Optimized Execution Plan ==
Limit(offset=[0], fetch=[10], global=[true])
+- Exchange(distribution=[single])
   +- Limit(offset=[0], fetch=[10], global=[false])
      +- Calc(select=[uuid, null:VARCHAR(2147483647) AS name, age, ts, partition], where=[name IS NULL])
         +- TableSourceScan(table=[[default_catalog, default_database, source, filter=[IS NULL(name)]]], fields=[uuid, name, age, ts, partition]) {code}
When the Filter condition is in effect, Limit does not appear to be able to be pushed down to Source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 01:28:06 UTC 2023,,,,,,,,,,"0|z1j9zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 01:28;luoyuxia;It's by design. If the limit is pushed down to table source, we'll get the wrong result.

If we only get 10 rows from the TableSource if limit is pused down, after the filter, it well not fiter out some rows which result we can't get actaul 10 row.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI build failed because e2e_1_ci throw error Bash exited with code '1',FLINK-32638,13544299,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,337361684@qq.com,337361684@qq.com,20/Jul/23 13:12,24/Jul/23 07:02,04/Jun/24 20:40,24/Jul/23 07:02,1.18.0,,,,,1.18.0,,,,Build System / CI,,,,,,0,,,,CI build failed because e2e_1_ci throw error Bash exited with code '1',,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 07:02:57 UTC 2023,,,,,,,,,,"0|z1j9yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 07:52;337361684@qq.com;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51510&view=results;;;","24/Jul/23 07:02;mapohl;Thanks for reporting this issue, [~337361684@qq.com]. We covered this issue already in FLINK-32632. Therefore, I'm closing this one.

As a hint for future bug reports on test instabilities: ""Bash exited with code '1'"" is not the actual problem but rather a symptom of something else failing. You have to scroll further up an analyze the logs to figure out what's wrong.

In your specific case, you would see a bit further up (see [CI log line 6202|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51510&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=6202]) that the e2e test ""Run Kubernetes test"" failed. The logs for this test are printed prior to this line. Mentioning the actual test that failures helps organizing the Jiras more easily (because you can search for the test name).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The UNNEST function will cause the Delayed Retry Strategy For Lookup doesn't work,FLINK-32637,13544265,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lyndon.lee,lyndon.lee,20/Jul/23 09:20,21/Jul/23 02:04,04/Jun/24 20:40,,1.16.1,,,,,,,,,Table SQL / JDBC,Table SQL / Planner,,,,,0,,,,"{{The UNNEST function will cause the Delayed Retry Strategy For Lookup doesn't work}}

When I use the *UNNEST* *funcation* in my sql statement to achieve array expansion, the *Delayed Retry Strategy For Lookup has no effect.* 
The retry is not performed according to the configuration, and there are no logs or exceptions.

I tried *flink on yarn* mode and *standalone* mode and it doesn't work.
See the attachment for the error statements、success statments and source dml in case. 
It is easy to reproduce in sql-client

Here are some information:

 
||index||value||
|version|flink-1.16.1|
|source|kafka|
|source data|canal-binlog data|
|source format|json|
|look up source|mysql|
|sink|kafka|
|sink format|json|

 

 

Please developers judge whether my case is a bug. 
Looking forward to your answer!

feature doc：[https://cwiki.apache.org/confluence/display/FLINK/FLIP-234%3A+Support+Retryable+Lookup+Join+To+Solve+Delayed+Updates+Issue+In+External+Systems]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/23 09:19;lyndon.lee;dml.sql;https://issues.apache.org/jira/secure/attachment/13061467/dml.sql","20/Jul/23 09:19;lyndon.lee;error-flink-sqls.sql;https://issues.apache.org/jira/secure/attachment/13061466/error-flink-sqls.sql","20/Jul/23 09:19;lyndon.lee;success-flink-sqls.sql;https://issues.apache.org/jira/secure/attachment/13061465/success-flink-sqls.sql",,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 02:04:23 UTC 2023,,,,,,,,,,"0|z1j9qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/23 10:07;lincoln.86xy;Looks like might be a problem with hints propagation cc [~xuyangzhong] 
[~lyndon.lee] can you try with 1.17.1 to see if it works? (note that only alias name can be used in the LOOKUP hint option if table has an alias name 
 https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/queries/hints/#lookup-hint-options);;;","21/Jul/23 02:04;lyndon.lee;[~lincoln.86xy]  Thanks for your reply. I try it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promethues metric exporter expose stopped job metric,FLINK-32636,13544261,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,20/Jul/23 09:03,20/Jul/23 09:05,04/Jun/24 20:40,,1.14.0,,,,,,,,,Runtime / Metrics,,,,,,0,,,,"Firstly, I have submitted two jobs(let's say jobA/jobB ) to flink session. When I stopped one jobA, the promethues reporter continutly reports the metric of jobA and jobB. 

However, the jmx reporter only reports  the metric of jobB.

 

 It's unreasonable to report the metric of stopped jobA.

 

JMX reporter metric snapshot:

!image-2023-07-20-17-02-01-660.png|width=794,height=266!

 

promethues metric snapshot:

!image-2023-07-20-17-04-53-069.png|width=787,height=274!

 

 ",flink-1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/23 09:02;stupid_pig;image-2023-07-20-17-02-01-660.png;https://issues.apache.org/jira/secure/attachment/13061462/image-2023-07-20-17-02-01-660.png","20/Jul/23 09:02;stupid_pig;image-2023-07-20-17-02-37-625.png;https://issues.apache.org/jira/secure/attachment/13061461/image-2023-07-20-17-02-37-625.png","20/Jul/23 09:04;stupid_pig;image-2023-07-20-17-04-18-489.png;https://issues.apache.org/jira/secure/attachment/13061463/image-2023-07-20-17-04-18-489.png","20/Jul/23 09:04;stupid_pig;image-2023-07-20-17-04-53-069.png;https://issues.apache.org/jira/secure/attachment/13061464/image-2023-07-20-17-04-53-069.png",,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-20 09:03:07.0,,,,,,,,,,"0|z1j9q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Lag function to Table API ,FLINK-32635,13544255,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,padavan,padavan,20/Jul/23 08:45,20/Jul/23 08:45,04/Jun/24 20:40,,,,,,,,,,,Table SQL / API,,,,,,0,,,,"I was surprised that the Table API doesn't have a LAG function.

 

Expected result: 

 
{code:java}
Table win = t
                .window(Tumble.over(lit(10).seconds()).on($(""proctime"")).as(""w""))
                .groupBy($(""w""), $(""userId""))

 .select( $(""userId""), $(""myfield"").lag(1) );{code}
or 
{code:java}
$(""myfield"").over($(""w"")).lag(1){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-20 08:45:54.0,,,,,,,,,,"0|z1j9oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate StreamRecordTimestamp and ExistingField,FLINK-32634,13544231,13542776,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,qingyue,qingyue,qingyue,20/Jul/23 06:37,29/Jul/23 07:09,04/Jun/24 20:40,29/Jul/23 07:09,1.18.0,,,,,1.18.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"`StreamRecordTimestamp` and `ExistingField` extends the deprecated `TimestampExtractor`, and thus should be deprecated as well",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 29 07:09:26 UTC 2023,,,,,,,,,,"0|z1j9jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/23 07:09;qingyue;Fixed in master: 9ef67388a0236dbb38520c73909a6d84d6141d73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes e2e test is not stable,FLINK-32633,13544198,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zjureel,zjureel,20/Jul/23 00:51,20/Jul/23 08:22,04/Jun/24 20:40,20/Jul/23 08:22,1.18.0,,,,,,,,,Deployment / Kubernetes,Kubernetes Operator,,,,,0,,,,"The output file is: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51444&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117

Jul 19 17:06:02 Stopping minikube ...
Jul 19 17:06:02 * Stopping node ""minikube""  ...
Jul 19 17:06:13 * 1 node stopped.
Jul 19 17:06:13 [FAIL] Test script contains errors.
Jul 19 17:06:13 Checking for errors...
Jul 19 17:06:13 No errors in log files.
Jul 19 17:06:13 Checking for exceptions...
Jul 19 17:06:13 No exceptions in log files.
Jul 19 17:06:13 Checking for non-empty .out files...
grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
Jul 19 17:06:13 No non-empty .out files.
Jul 19 17:06:13 
Jul 19 17:06:13 [FAIL] 'Run Kubernetes test' failed after 4 minutes and 28 seconds! Test exited with exit code 1
Jul 19 17:06:13 
17:06:13 ##[group]Environment Information
Jul 19 17:06:13 Jps
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-20 00:51:05.0,,,,,,,,,,"0|z1j9c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run Kubernetes test is unstable on AZP,FLINK-32632,13544182,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,19/Jul/23 20:47,24/Jul/23 07:02,04/Jun/24 20:40,23/Jul/23 08:59,1.18.0,,,,,1.16.3,1.17.2,1.18.0,,,,,,,,0,pull-request-available,test-stability,,"This test 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51447&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=6213

fails with

{noformat}
2023-07-19T17:14:49.8144730Z Jul 19 17:14:49 deployment.apps/flink-task-manager created
2023-07-19T17:15:03.7983703Z Jul 19 17:15:03 job.batch/flink-job-cluster condition met
2023-07-19T17:15:04.0937620Z error: Internal error occurred: error executing command in container: http: invalid Host header
2023-07-19T17:15:04.0988752Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-11919909188/out/kubernetes_wc_out*': No such file or directory
2023-07-19T17:15:04.1017388Z Jul 19 17:15:04 FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected e682ec6622b5e83f2eb614617d5ab2cf.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32633,FLINK-32638,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 08:59:22 UTC 2023,,,,,,,,,,"0|z1j98g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/23 15:25;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51492&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=6212;;;","21/Jul/23 01:18;lincoln.86xy;another one: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51488&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117;;;","21/Jul/23 04:04;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51495&view=logs&j=81be5d54-0dc6-5130-d390-233dd2956037&t=cfb9de70-be4e-5162-887e-653276e3edee;;;","21/Jul/23 05:21;Weijie Guo;another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51498&view=logs&j=9ebab7cc-5a3d-5028-7e83-8e0bead53d56&t=515332c6-0554-5b46-7cd1-70c9d451224a.;;;","21/Jul/23 05:46;tanyuxin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51490&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=6208;;;","21/Jul/23 06:42;luoyuxia;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51491&view=logs&j=81be5d54-0dc6-5130-d390-233dd2956037&t=cfb9de70-be4e-5162-887e-653276e3edee;;;","21/Jul/23 09:50;mapohl;[~Sergey Nuyanzin] where did you find the logs that you included in this Jira's description? I cannot find the following part (but the other log lines are included):
{code:java}
2023-07-19T17:15:04.0937620Z error: Internal error occurred: error executing command in container: http: invalid Host header
2023-07-19T17:15:04.0988752Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-11919909188/out/kubernetes_wc_out*': No such file or directory {code};;;","21/Jul/23 09:54;Sergey Nuyanzin;[~mapohl] that's a bit strange behavior of Azure i guess...
if you try to look it in fancy azure terminal window it can not show it...
However if you switch to raw log ({{View raw log}} button in top right) then it's possible to see it;;;","21/Jul/23 09:57;mapohl;Thanks for the hint. I assumed the watchdog output being aligned with the raw Azure logs. But looks like there are not (the watchdog doesn't include those lines either);;;","21/Jul/23 12:04;mapohl;Ok, the issue is also appearing in 1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51502&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=5235]

But I guess it's still worth it to label it as a blocker because of the frequency the error appears. It might be a configuration issue.

One first guess was that minikube got updated. But I checked: We're using a fixed Minikube version 1.28.0 (see [common_kubernetes.sh|https://github.com/apache/flink/blob/0e4fb6bbfbcf9bb623a2b26826e53d720b74c898/flink-end-to-end-tests/test-scripts/common_kubernetes.sh#L27]);;;","21/Jul/23 12:13;mapohl;There is an issue raised in the minikube repo with the same error message ""Internal error occurred: error executing command in container: http: invalid Host header"": https://github.com/kubernetes/minikube/issues/16909;;;","21/Jul/23 12:40;mapohl;It might be that it's caused by a Go-Lang update. The Github issue above and others (e.g. [https://github.com/moby/moby/issues/45935).] We don't use a fixed version of go to build cri-dockerd. Maybe, switching to the generated binaries of cri-dockerd is worth it, anyway: One less dependency to worry about.;;;","21/Jul/23 15:41;mapohl;Looks like relying on binaries for {{cri-dockerd}} solved the issue. I also verified that the Go version is indeed 1.20.6 (see [console output in CI build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51541&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=202]):
{code:java}
Jul 21 15:16:10 Installed go version: go version go1.20.6 linux/amd64 {code}
I'm gonna prepare the PR and make it ready to be reviewed.;;;","23/Jul/23 08:59;mapohl;master: bc964a102a8ecd628d63405ee63346f5e45aa86f
1.17: fb340251f3567d04a9d461422cfd75eca7dfd086
1.16: ec3d1112566add69338b19941ee224106042b17a;;;",,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSessionJob stuck in Created/Reconciling state because of No Job found error in JobManager,FLINK-32631,13544155,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bhupixb,bhupixb,19/Jul/23 16:43,05/Jan/24 07:34,04/Jun/24 20:40,,1.16.0,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"{*}Background{*}: We are using FlinkSessionJob for submitting jobs to a session cluster and flink kubernetes operator 1.5.0.

{*}Bug{*}: We frequently encounter a problem where the job gets stuck in CREATED/RECONCILING state. On checking flink operator logs we see the error {_}Job could not be found{_}. Full trace [here|https://ideone.com/NuAyEK].
 # When a Flink session job is submitted, the Flink operator submits the job to the Flink Cluster.
 # Assume the job is finished(or reached a terminal state) and the job manager (JM) restarts for some reason, the job will no longer exist in the JM.
 # Upon reconciliation, the Flink operator queries the JM's REST API for the job using its jobID, but it receives a 404 error, indicating that the job is not found.
 # The operator then encounters an error and logs it, leading to the job getting stuck in an indefinite state.
 # Attempting to restart or suspend the job using the operator's provided mechanisms also fails because the operator keeps calling the REST API and receiving the same 404 error.

{*}Expected Behavior{*}: Ideally, when the Flink operator reconciles a job and finds that it no longer exists in the Flink Cluster, it should handle the situation gracefully. Instead of getting stuck and logging errors indefinitely, the operator should mark the job as failed or deleted, or set an appropriate status for it.",Local,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-19 16:43:36.0,,,,,,,,,,"0|z1j92g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The log level of job failed info should change from INFO to WARN/ERROR if job failed,FLINK-32630,13544107,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,wangm92,wangm92,wangm92,19/Jul/23 11:53,24/Jul/23 16:51,04/Jun/24 20:40,24/Jul/23 16:51,1.17.1,,,,,,,,,Client / Job Submission,Runtime / Coordination,,,,,0,pull-request-available,,,"When a job fails to submit or run, the following log level should be changed to WARN or ERROR, INFO will confuse users
{code:java}
2023-07-14 20:05:26,863 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job flink_test_job (0000000008eefd500000000000000000) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by FailureRateRestartBackoffTimeStrategy(FailureRateRestartBackoffTimeStrategy(failuresIntervalMS=2400000,backoffTimeMS=20000,maxFailuresPerInterval=100)
 
2023-07-14 20:05:26,889 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 0000000008eefd500000000000000000 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by FailureRateRestartBackoffTimeStrategy(FailureRateRestartBackoffTimeStrategy(failuresIntervalMS=2400000,backoffTimeMS=20000,maxFailuresPerInterval=100)

2023-07-14 20:05:26,956 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application FAILED: 
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-6206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 12:43:28 UTC 2023,,,,,,,,,,"0|z1j8rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 11:57;wangm92;hi, [~tison] [~Weijie Guo] I think this is a point that can be optimized, can you guys assign this Jira to me;;;","19/Jul/23 12:43;tison;You can ping me on a patch ready.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for dynamic CEP ,FLINK-32629,13544074,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhangyf,zhangyf,19/Jul/23 07:46,19/Jul/23 07:55,04/Jun/24 20:40,19/Jul/23 07:55,1.18.0,,,,,,,,,Library / CEP,,,,,,0,,,,"When using CEP as a complex event processing engine, when the logic is frequently modified and the threshold is frequently adjusted, the entire program needs to be stopped, the code should be modified, the program should be repackaged, and then submitted to the cluster. Dynamic logic modification and external dynamic injection cannot be realized. Currently, Realized the dynamic injection of CEP logic, based on message-driven logic modification, you can manually inject specific messages into the source end to achieve fine-grained control of logic injection perception",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-7129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-19 07:46:18.0,,,,,,,,,,"0|z1j8kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
build_wheels_on_macos fails on AZP,FLINK-32628,13544064,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,19/Jul/23 06:40,27/Jul/23 00:05,04/Jun/24 20:40,27/Jul/23 00:05,1.16.3,1.17.2,1.18.0,,,1.16.3,1.17.2,1.18.0,,Build System / CI,,,,,,0,pull-request-available,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51394&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102
fails as
{noformat}
2023-07-19T00:18:36.5467620Z       Failed to build fastavro
2023-07-19T00:18:36.5507410Z       ERROR: Could not build wheels for fastavro, which is required to install pyproject.toml-based projects
2023-07-19T00:18:36.5540080Z       [end of output]
2023-07-19T00:18:36.5568470Z   
2023-07-19T00:18:36.5603540Z   note: This error originates from a subprocess, and is likely not a problem with pip.
2023-07-19T00:18:36.5633470Z error: subprocess-exited-with-error
2023-07-19T00:18:36.5669130Z 
2023-07-19T00:18:36.5709780Z × pip subprocess to install build dependencies did not run successfully.
2023-07-19T00:18:36.5737700Z │ exit code: 1
2023-07-19T00:18:36.5764350Z ╰─> See above for output.
2023-07-19T00:18:36.5791010Z 
2023-07-19T00:18:36.5819050Z note: This error originates from a subprocess, and is likely not a problem with pip.
2023-07-19T00:18:36.5847430Z ##[endgroup]
2023-07-19T00:18:36.5884460Z                                                              [31m✕ [0m56.47s
2023-07-19T00:18:36.5921230Z [91mError[0m: Command ['python', '-m', 'pip', 'wheel', '/Users/runner/work/1/s/flink-python', '--wheel-dir=/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/cibw-run-pzvjusx9/cp37-macosx_x86_64/built_wheel', '--no-deps'] failed with code 1. None
{noformat}

probably this is the reason of failing 
{quote}
is required to install pyproject.toml-based projects
{quote}

however not clear why it is started to fail only recently
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 00:05:06 UTC 2023,,,,,,,,,,"0|z1j8i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 06:41;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51353&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102;;;","24/Jul/23 23:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51501&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102;;;","24/Jul/23 23:10;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51502&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102;;;","24/Jul/23 23:13;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51502&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102;;;","24/Jul/23 23:16;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51555&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102;;;","24/Jul/23 23:17;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51556&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=117;;;","24/Jul/23 23:21;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51591&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=120;;;","24/Jul/23 23:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51592&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=105;;;","24/Jul/23 23:28;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51627&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=105;;;","24/Jul/23 23:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51628&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=120;;;","24/Jul/23 23:37;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51629&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=105;;;","25/Jul/23 06:40;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51670&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=105;;;","27/Jul/23 00:05;Sergey Nuyanzin;Merged to 1.16: [5aed79cc3165e1df1cde6719b58ae2aa291f5ed4|https://github.com/apache/flink/commit/5aed79cc3165e1df1cde6719b58ae2aa291f5ed4]
1.17: [3d68f53520e85c00577e9f05f406edfe0c124ef3|https://github.com/apache/flink/commit/3d68f53520e85c00577e9f05f406edfe0c124ef3]
master: [49998998ca81e384975d5d81e09e8cb57eac337d|https://github.com/apache/flink/commit/49998998ca81e384975d5d81e09e8cb57eac337d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for dynamic time window function,FLINK-32627,13544045,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhangyf,zhangyf,zhangyf,19/Jul/23 00:58,18/Aug/23 22:35,04/Jun/24 20:40,,1.18.0,,,,,,,,,API / DataStream,,,,,,0,pull-request-available,stale-assigned,,"When using windows for calculations, when the logic is frequently modified and adjusted, the entire program needs to be stopped, the code is modified, the program is repackaged and then submitted to the cluster. It is impossible to achieve logic dynamic modification and external dynamic injection. The window information can be obtained from the data to trigger Redistribution of windows to achieve the effect of dynamic windows",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:00 UTC 2023,,,,,,,,,,"0|z1j8e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 08:28;martijnvisser;Introducing this will require a FLIP, given that it introduces new public APIs. Can you please do that first? https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals;;;","19/Jul/23 09:04;zhangyf;[~martijnvisser]  How do I create a FLIP, I don't seem to have permission;;;","19/Jul/23 09:12;martijnvisser;[~zhangyf] I've just granted you the permissions;;;","19/Jul/23 09:16;zhangyf;[~martijnvisser] Thank you;;;","18/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinguish non-existent job from non-existent savepoint in Get Savepoint REST API,FLINK-32626,13543985,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,austince,austince,18/Jul/23 15:48,18/Jul/23 16:33,04/Jun/24 20:40,,1.17.1,,,,,,,,,Runtime / REST,,,,,,0,,,,"The current `GET /jobs/:jobid/savepoints/:triggerid` API endpoint [1], when given *either* a Job ID or a Trigger ID that does not exist it will always respond with an exception that indicates {*}the Savepoint doesn't exist{*}, like:
{code:java}
{""errors"":[""org.apache.flink.runtime.rest.handler.RestHandlerException: There is no savepoint operation with triggerId=TRIGGER ID for job JOB ID.\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler.maybeCreateNotFoundError(SavepointHandlers.java:325)\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler.lambda$handleRequest$1(SavepointHandlers.java:308)\n\tat java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)\n\tat java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)\n\tat org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:260)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1275)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)\n\tat org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)\n\tat akka.dispatch.OnComplete.internal(Future.scala:299)\n\tat akka.dispatch.OnComplete.internal(Future.scala:297)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622)\n\tat akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25)\n\tat akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)\n\tat akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)\n\tat akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n""]} {code}
There are operational cases where it would be helpful to differentiate the Job being not found vs. the Savepoint being not found.

For example, if a Job doesn't exist, we can stop trying to take a Savepoint on it.

 

Additionally, we could consider removing the full stack trace as it doesn't add value for these cases.

 

[1]: [https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/rest_api/#jobs-jobid-savepoints-triggerid]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-18 15:48:42.0,,,,,,,,,,"0|z1j80o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniClusterTestEnvironment supports customized MiniClusterResourceConfiguration,FLINK-32625,13543971,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,18/Jul/23 14:50,19/Jul/23 07:55,04/Jun/24 20:40,19/Jul/23 07:55,,,,,,1.18.0,,,,Test Infrastructure,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 07:55:47 UTC 2023,,,,,,,,,,"0|z1j7xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 07:55;leonard;Resolved in master: 3886f1f39635fb94b488ec556f1143327b1a1ead;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TieredStorageConsumerClientTest.testGetNextBufferFromRemoteTier failed on CI,FLINK-32624,13543959,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Wencong Liu,lincoln.86xy,lincoln.86xy,18/Jul/23 14:13,19/Jul/23 07:55,04/Jun/24 20:40,19/Jul/23 07:54,1.18.0,,,,,1.18.0,,,,API / Core,,,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51376&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8]

errors:

{code}
Jul 18 11:18:35 11:18:35.412 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageConsumerClientTest.testGetNextBufferFromRemoteTier  Time elapsed: 0.014 s  <<< FAILURE!
Jul 18 11:18:35 java.lang.AssertionError: 
Jul 18 11:18:35 
Jul 18 11:18:35 Expecting Optional to contain a value but it was empty.
Jul 18 11:18:35 	at org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageConsumerClientTest.testGetNextBufferFromRemoteTier(TieredStorageConsumerClientTest.java:127)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 07:55:12 UTC 2023,,,,,,,,,,"0|z1j7uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 01:11;lincoln.86xy;another failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51376&view=ms.vss-test-web.build-test-results-tab;;;","19/Jul/23 01:20;lincoln.86xy;[~Wencong Liu] would you help to take a look?;;;","19/Jul/23 01:38;Wencong Liu;Sorry for the late reply, I'll take a look. [~lincoln.86xy] ;;;","19/Jul/23 02:04;Wencong Liu;Thanks [~lincoln.86xy] .  I've opened a hot fix. [[hotfix] Fix the TieredStorageConsumerClientTest by WencongLiu · Pull Request #23017 · apache/flink (github.com)|https://github.com/apache/flink/pull/23017];;;","19/Jul/23 06:36;Sergey Nuyanzin;increased to critical since it's reproduced on ci mirror

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51394&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8578;;;","19/Jul/23 07:55;Weijie Guo;master(1.18) via 99c652ce347357a73f53564fb1cffd3e343ff0d2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest api doesn't return minimum resource requirements correctly,FLINK-32623,13543955,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,18/Jul/23 13:53,19/Jul/23 10:40,04/Jun/24 20:40,19/Jul/23 10:40,1.18.0,,,,,1.18.0,,,,Runtime / REST,,,,,,0,pull-request-available,,,The resource requirements returned by the rest api always return a hardcoded 1 lower bound for each vertex.,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 10:40:00 UTC 2023,,,,,,,,,,"0|z1j7u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 10:40;chesnay;master: 6b3d291f6a573fb34a528313e5683d3a48a66771;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not add mini-batch assigner operator if it is useless,FLINK-32622,13543950,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeyhunkarimov,zjureel,zjureel,18/Jul/23 13:09,30/Apr/24 04:40,04/Jun/24 20:40,29/Apr/24 02:48,1.19.0,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"Currently if user config mini-batch for their sql jobs, flink will always add mini-batch assigner operator in job plan even there's no agg/join operators in the job. Mini-batch operator will generate useless event and cause performance issue for them. If the mini-batch is useless for the specific jobs, flink should not add mini-batch assigner even when users turn on mini-batch mechanism. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 04:40:43 UTC 2024,,,,,,,,,,"0|z1j7sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 13:10;zjureel;cc [~libenchao] [~zoudan];;;","20/Jul/23 14:49;libenchao;+1 for this;;;","29/Apr/24 02:48;jingge;master: b1544e4e513d2b75b350c20dbb1c17a8232c22fd;;;","29/Apr/24 02:50;jingge;[~jeyhunkarimov] could you please backport it to 1.19?;;;","30/Apr/24 04:40;jingge;1.19: c370b20c8e2d1ea55dd1fc4c283430832d9f9aae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics for DataGeneratorSource,FLINK-32621,13543948,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,18/Jul/23 13:04,18/Jul/23 13:04,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,,0,,,,{{DataGeneratorSource}} use {{GeneratingIteratorSourceReader}} instead of {{SourceReaderBase}} as sourceReader. So it lacks metrics like numRecordIn contained by SourceReaderBase.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-18 13:04:10.0,,,,,,,,,,"0|z1j7sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate DiscardingSink to sinkv2,FLINK-32620,13543947,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,18/Jul/23 12:47,08/Sep/23 05:26,04/Jun/24 20:40,08/Sep/23 05:26,1.19.0,,,,,1.19.0,,,,Connectors / Common,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 05:26:43 UTC 2023,,,,,,,,,,"0|z1j7s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Sep/23 05:26;Weijie Guo;master(1.19) via c4a76e2f006d834771092a8fac96bfc63361e20b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigOptions to support fallback configuration,FLINK-32619,13543937,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,liangtl,liangtl,18/Jul/23 11:42,19/Jul/23 12:44,04/Jun/24 20:40,19/Jul/23 12:44,1.16.2,1.17.1,,,,,,,,Runtime / Configuration,,,,,,0,,,,"ConfigOptions has no option to specify a ""fallback configuration"" as the default.

 

For example, if we want {{rest.cache.checkpoint-statistics.timeout}} to fallback to web.refresh-interval instead of a static default value, we have to specify

 
{code:java}
@Documentation.OverrideDefault(""web.refresh-interval"")
@Documentation.Section(Documentation.Sections.EXPERT_REST)
public static final ConfigOption<Duration> CACHE_CHECKPOINT_STATISTICS_TIMEOUT =
        key(""rest.cache.checkpoint-statistics.timeout"")
                .durationType()
                .noDefaultValue()
                .withDescription(
                        ""...."");
 {code}
 

 

The {{.noDefault()}} is misleading as it actually has a default.

 

We should introduce a {{.fallbackConfiguration()}} that is handled gracefully by doc generators.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 12:44:21 UTC 2023,,,,,,,,,,"0|z1j7q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 02:32;wangm92;hi, i saw flink  has supported this, like this:
{code:java}
/** Timeout for identifying inactive slots. */
@Documentation.Section(Documentation.Sections.ALL_TASK_MANAGER)
public static final ConfigOption<Duration> SLOT_TIMEOUT =
        key(""taskmanager.slot.timeout"")
                .durationType()
                .defaultValue(AkkaOptions.ASK_TIMEOUT_DURATION.defaultValue())
                .withFallbackKeys(AkkaOptions.ASK_TIMEOUT_DURATION.key())
                .withDescription(
                        Description.builder()
                                .text(
                                        ""Timeout used for identifying inactive slots. The TaskManager will free the slot if it does not become active ""
                                                + ""within the given amount of time. Inactive slots can be caused by an out-dated slot request. If no ""
                                                + ""value is configured, then it will fall back to %s."",
                                        code(AkkaOptions.ASK_TIMEOUT_DURATION.key()))
                                .build()); {code};;;","19/Jul/23 12:44;liangtl;That's a great callout [~wangm92] . Will use that instead and close this JIRA;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the dependency of the flink-core in the flink-sql-jdbc-driver-bundle,FLINK-32618,13543924,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,fsk119,fsk119,fsk119,18/Jul/23 10:29,21/Jul/23 02:47,04/Jun/24 20:40,21/Jul/23 02:47,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-18 10:29:50.0,,,,,,,,,,"0|z1j7n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkResultSetMetaData throw exception for most methods,FLINK-32617,13543920,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fangyong,fsk119,fsk119,18/Jul/23 10:17,21/Jul/23 02:41,04/Jun/24 20:40,21/Jul/23 02:41,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"I think most methods, e.g.

 

```

boolean supportsMultipleResultSets() throws SQLException;

boolean supportsMultipleTransactions() throws SQLException;

boolean supportsMinimumSQLGrammar() throws SQLException;

```

We can just return true or false.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 02:41:35 UTC 2023,,,,,,,,,,"0|z1j7m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 02:41;fsk119;Merged into master: a5a741876ab4392b922fc5858f9ad679fb7eabc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkStatement#executeQuery resource leaks when the input sql is not query,FLINK-32616,13543917,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fangyong,fsk119,fsk119,18/Jul/23 10:09,21/Jul/23 02:47,04/Jun/24 20:40,21/Jul/23 02:47,1.18.0,,,,,1.18.0,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,"The current implementation just throw the exception if the input sql is not query. No one is responsible to close the StatementResult.

 

BTW, the current implementation just submit the sql to the gateway, which means the sql is executed. I just wonder do we need to expose this features to the users?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 02:46:48 UTC 2023,,,,,,,,,,"0|z1j7lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 02:46;fsk119;Merged into master: b9c5dc3d73161622908f90ac876f4ec002b1186e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache AutoscalerInfo for each resource,FLINK-32615,13543914,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,18/Jul/23 09:32,19/Jul/23 14:48,04/Jun/24 20:40,19/Jul/23 14:48,,,,,,kubernetes-operator-1.6.0,,,,Kubernetes Operator,,,,,,0,,,,Currently we always get the autoscaler info configmap through the Kubernetes rest api. This is a very heavy and unnecessary operation as the lifecycle is directly tied to the resource. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 14:48:02 UTC 2023,,,,,,,,,,"0|z1j7kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 14:48;gyfora;merged to main 0c341ebe13645f4e9802cfd780c5b50f59e29363;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avro mappings aren't always named as pojos,FLINK-32614,13543897,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,msillence,msillence,18/Jul/23 07:42,18/Jul/23 11:23,04/Jun/24 20:40,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,"Debezium with the flatten SMT:

    ""transforms.unwrap.type"": ""io.debezium.transforms.ExtractNewRecordState"",

{{Will create avro with the field}}

{{    {}}
{{      ""default"": null,}}
{{      ""name"": ""__deleted"",}}
{{      ""type"": [}}
{{        ""null"",}}
{{        ""string""}}
{{      ]}}
{{    }}}

 

This has the expected field:
  private java.lang.String __deleted;

{{and constructor but the getter and setter are named:}}
{{  public java.lang.String getDeleted$1() {}}
{{    return __deleted;}}
{{  }}}

{{ }}{{  public void setDeleted$1(java.lang.String value) {}}
{{    this.__deleted = value;}}
{{  }}}

{{Trying to use this generate class throws:}}

 

Exception in thread ""main"" java.lang.IllegalStateException: Expecting type to be a PojoTypeInfo
    at org.apache.flink.formats.avro.typeutils.AvroTypeInfo.generateFieldsFromAvroSchema(AvroTypeInfo.java:72)
    at org.apache.flink.formats.avro.typeutils.AvroTypeInfo.<init>(AvroTypeInfo.java:55)
    at org.apache.flink.formats.avro.utils.AvroKryoSerializerUtils.createAvroTypeInfo(AvroKryoSerializerUtils.java:87)
    at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1939)
    at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1840)
    at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:982)
    at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:802)
    at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:749)
    at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:745)
    at org.apache.flink.api.common.typeinfo.TypeInformation.of(TypeInformation.java:210)
    at com.fnz.flink.AvroDeserialization.<init>(AvroDeserialization.java:22)
    at com.fnz.flink.DataStreamJob.main(DataStreamJob.java:90)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-18 07:42:16.0,,,,,,,,,,"0|z1j7h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable check for single rowtime attribute for sinks,FLINK-32613,13543815,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,17/Jul/23 16:26,18/Jul/23 13:11,04/Jun/24 20:40,18/Jul/23 13:11,,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"A common error that users face is the following:
{code}
The query contains more than one rowtime attribute column [%s] for writing into table '%s'.
Please select the column that should be used as the event-time timestamp for the 
table sink by casting all other columns to regular TIMESTAMP or TIMESTAMP_LTZ.
{code}

However, not every sink requires the rowtime set on the stream record. For example, the Kafka sink does not use it because it exposes metadata columns. The user can define which column is the rowtime column by projection.

Either we introduce a config option for disabling this check. Or we come up with an interface that a connector can implement. I would vote for the config option as an easy solution to get rid of this error message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 13:11:08 UTC 2023,,,,,,,,,,"0|z1j6yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 13:11;twalthr;Fixed in master: 663a68f2c56fe6bcdb50fb2c07229cffc3e948e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Pulsar version to 3.0.0 for the connector,FLINK-32612,13543796,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,17/Jul/23 13:28,18/Jul/23 07:44,04/Jun/24 20:40,18/Jul/23 07:34,,,,,,pulsar-4.1.0,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 07:34:48 UTC 2023,,,,,,,,,,"0|z1j6uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 07:34;tison;master via https://github.com/apache/flink-connector-pulsar/commit/279a678dd4f39e1e268d911dc4096d23ab15bc73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redirect to Apache Paimon's link instead of legacy flink table store,FLINK-32611,13543791,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,17/Jul/23 13:02,04/Dec/23 13:05,04/Jun/24 20:40,04/Dec/23 13:05,,,,,,1.18.1,1.19.0,,,Documentation,Project Website,,,,,0,pull-request-available,stale-assigned,,"Current Flink's official web site would always point to the legacy flink table store. However, we should point to the new Apache Paimon website and docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 04 13:05:45 UTC 2023,,,,,,,,,,"0|z1j6tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","04/Dec/23 13:05;yunta;merged in flink-web: 4cd8ab2fa927f48d74ed53e79a5e83efa674a720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON format supports projection push down,FLINK-32610,13543788,13543785,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,17/Jul/23 12:58,20/Jul/23 15:35,04/Jun/24 20:40,20/Jul/23 15:33,,,,,,1.18.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 20 15:33:32 UTC 2023,,,,,,,,,,"0|z1j6sw:",9223372036854775807,"The JSON format introduced JsonParser as a new default way to deserialize JSON data. JsonParser is a Jackson JSON streaming API to read JSON data which is much faster and consumes less memory compared to the previous JsonNode approach. This should be a compatible change, if you encounter any issues after upgrading, you can fallback to the previous JsonNode approach by setting `json.decode.json-parser.enabled` to `false`. ",,,,,,,,,,,,,,,,,,,"20/Jul/23 15:33;jark;Fixed in master: 58f7ceb45013bca847b39901f62ce746680e4f0c...d6967dd7301e82fa102f756e16635dabce1c550d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka table source supports projection push down,FLINK-32609,13543787,13543785,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,sudewei.sdw,lsy,lsy,17/Jul/23 12:58,26/Jan/24 11:43,04/Jun/24 20:40,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 11:43:11 UTC 2024,,,,,,,,,,"0|z1j6so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 08:28;sudewei.sdw;[~lsy] Hi dalong, do you have time to continue this work? If not, I’m willing to doing this, shall I have a try ? ;;;","26/Jan/24 11:43;lsy;assigned to you;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve source reusing with projection push down,FLINK-32608,13543786,13543785,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,17/Jul/23 12:57,15/Sep/23 05:59,04/Jun/24 20:40,23/Jul/23 06:48,,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32730,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 06:39:05 UTC 2023,,,,,,,,,,"0|z1j6sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 06:48;lincoln.86xy;fixed in master: c0044be734a74353ddb9768f7559aaf27c04a1fa;;;","11/Aug/23 06:39;aitozi;I think this issue is duplicated of https://issues.apache.org/jira/browse/FLINK-29088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka table source and json format support projection pushdown,FLINK-32607,13543785,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,17/Jul/23 12:56,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,kafka-4.0.0,,,Connectors / Kafka,Table SQL / Planner,,,,,1,,,,"ProjectionPushDown has a huge performance impact and is not currently implemented in Kafka Source, so we can support it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 12:21:43 UTC 2023,,,,,,,,,,"0|z1j6s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/23 14:47;libenchao;+1 for this, and for protobuf format.;;;","02/Aug/23 08:23;zhoujira86;[~libenchao] Not sure if someone has taken the pb projection pushdown. if no , Can you please assign it to me?;;;","02/Aug/23 12:21;libenchao;[~zhoujira86] Feel free to add a subtask for pb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support namespace and flink version specific default configs,FLINK-32606,13543777,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,17/Jul/23 11:49,19/Jul/23 08:14,04/Jun/24 20:40,19/Jul/23 08:14,,,,,,kubernetes-operator-1.6.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"The main goal is to support setting default configuration on a per-namespace and per-flink version level.

This would allow us for example to set config defaults differently for Flink 1.18 (enable adaptive scheduler by default)

Or to use different reconciliation/operator settings for different namespaces.

This can be a very important feature in large scale heterogeneous environments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 08:14:13 UTC 2023,,,,,,,,,,"0|z1j6qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 08:14;gyfora;merged to main aaf26aec8bdfedff0116c5897bd89b17b4454606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinITCase.testFullOuterJoinWithMultipleKeys fails with TimeoutException: Futures timed out after [20 seconds],FLINK-32605,13543749,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,17/Jul/23 09:35,23/Sep/23 22:34,04/Jun/24 20:40,,1.18.0,,,,,,,,,Runtime / RPC,Table SQL / API,,,,,0,auto-deprioritized-major,test-stability,,"While execution of https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51254&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11681

there was an exception 
{noformat}
Jul 14 04:35:32 Caused by: java.lang.Exception: Could not create actor system
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startLocalActorSystem(AkkaBootstrapTools.java:238)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:349)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:327)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:247)
Jul 14 04:35:32 	at org.apache.flink.runtime.minicluster.MiniCluster.createLocalRpcService(MiniCluster.java:1188)
Jul 14 04:35:32 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:355)
Jul 14 04:35:32 	at org.apache.flink.client.program.PerJobMiniClusterFactory.submitJob(PerJobMiniClusterFactory.java:77)
Jul 14 04:35:32 	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:85)
Jul 14 04:35:32 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2218)
Jul 14 04:35:32 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:110)
Jul 14 04:35:32 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:992)
Jul 14 04:35:32 	... 102 more
Jul 14 04:35:32 Caused by: java.util.concurrent.TimeoutException: Futures timed out after [20 seconds]
Jul 14 04:35:32 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
Jul 14 04:35:32 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
Jul 14 04:35:32 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:223)
Jul 14 04:35:32 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57)
Jul 14 04:35:32 	at scala.concurrent.Await$.result(package.scala:146)
Jul 14 04:35:32 	at akka.stream.SystemMaterializer.<init>(SystemMaterializer.scala:90)
Jul 14 04:35:32 	at akka.stream.SystemMaterializer$.createExtension(SystemMaterializer.scala:39)
Jul 14 04:35:32 	at akka.stream.SystemMaterializer$.createExtension(SystemMaterializer.scala:32)
Jul 14 04:35:32 	at akka.actor.ActorSystemImpl.registerExtension(ActorSystem.scala:1165)
Jul 14 04:35:32 	at akka.actor.ActorSystemImpl.$anonfun$loadExtensions$1(ActorSystem.scala:1208)
Jul 14 04:35:32 	at scala.collection.Iterator.foreach(Iterator.scala:943)
Jul 14 04:35:32 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:54)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaUtils.createActorSystem(AkkaUtils.java:421)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startActorSystem(AkkaBootstrapTools.java:253)
Jul 14 04:35:32 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startLocalActorSystem(AkkaBootstrapTools.java:236)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 23 22:34:59 UTC 2023,,,,,,,,,,"0|z1j6k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","23/Sep/23 22:34;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink end-to-end fails  with kafka-server-stop.sh: No such file or directory ,FLINK-32604,13543747,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,17/Jul/23 09:29,04/Oct/23 17:30,04/Jun/24 20:40,,1.18.0,,,,,,,,,API / Python,,,,,,0,auto-deprioritized-critical,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51253&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=7883
fails as
{noformat}
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/kafka-common.sh: line 117: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-27379214502/kafka_2.12-3.2.3/bin/kafka-server-stop.sh: No such file or directory
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/kafka-common.sh: line 121: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-27379214502/kafka_2.12-3.2.3/bin/zookeeper-server-stop.sh: No such file or directory
Jul 13 19:43:07 [FAIL] Test script contains errors.
Jul 13 19:43:07 Checking of logs skipped.
Jul 13 19:43:07 
Jul 13 19:43:07 [FAIL] 'PyFlink end-to-end test' failed after 0 minutes and 40 seconds! Test exited with exit code 1
Jul 13 19:43:07 

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 04 17:30:23 UTC 2023,,,,,,,,,,"0|z1j6js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 09:29;Sergey Nuyanzin;//cc [~dianfu], [~hxbks2ks];;;","26/Jul/23 10:00;hxbks2ks;It is a downloading error. We can keep observing the frequency of this failed case.
;;;","04/Aug/23 12:54;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51978&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=6843;;;","18/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Oct/23 17:30;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53434&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=8897;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid consuming twice when two pipelines have the same table source,FLINK-32603,13543742,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,17/Jul/23 08:59,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"Here is an example to describe the issue. We have a source table that generates numbers from 1 to 5. Then we derive two tables from the source and convert them to datastream and sink to console.

If we debug the program, we can find that the `org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource` is created twice and the numbers are generated twice. It is a waste to consume the same source data twice.
{code:java}
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.getConfig().enableObjectReuse();
    env.setParallelism(1);
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

    tEnv.executeSql(
                    ""CREATE TABLE source(\n""
                            + ""  f_sequence INT\n""
                            + "") WITH (\n""
                            + ""  'connector' = 'datagen',\n""
                            + ""  'rows-per-second' ='1',\n""
                            + ""  'fields.f_sequence.kind' ='sequence',\n""
                            + ""  'fields.f_sequence.start'='1',\n""
                            + ""  'fields.f_sequence.end'='5'\n""
                            + "")"")
            .await();

    Table source = tEnv.from(""source"");
    Table left = source.filter($(""f_sequence"").isGreater(3));
    Table right = source.filter($(""f_sequence"").isLessOrEqual(3));

    DataStream<Row> leftDataStream = tEnv.toDataStream(left);
    DataStream<Row> rightDataStream = tEnv.toDataStream(right);

    leftDataStream.addSink(new PrintSinkFunction<>());
    rightDataStream.addSink(new PrintSinkFunction<>());

    env.execute();
} {code}
The reason is that every time the `StreamTableEnvironmentImpl#toDataStream` is called, a new planner is created and translates the graph from the end nodes. So the two graphs do not aware that they have the same source node.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 07:15:53 UTC 2023,,,,,,,,,,"0|z1j6io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 07:00;martijnvisser;[~Jiang Xin] I do wonder if this is FLIP worthy: I can imagine this specifically is a problem when using the DataStream API, but if you remain in Table API, you could solve this with a statement set. ;;;","19/Jul/23 07:15;Jiang Xin;Thanks, [~martijnvisser]. Yes, the problem only cause when we convert the Table to DataStream, but now that we have provides such APIs we should make it as good as possible right? Moreover, I wonder if it would introduce data loss if the source table is KafkaSource with ""group.id"" set, in such case, multiple consumers with the same ""group.id"" are running at the same time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nightly builds for Elasticsearch are failing with ""pull access denied for flink-base""",FLINK-32602,13543739,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Jul/23 08:40,17/Jul/23 11:47,04/Jun/24 20:40,17/Jul/23 11:47,,,,,,elasticsearch-3.0.2,,,,Connectors / ElasticSearch,,,,,,0,test-stability,,,"{code:java}
Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: pull access denied for flink-base, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
{code}

https://github.com/apache/flink-connector-elasticsearch/actions/runs/5564892055/jobs/10164792729#step:13:17389",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 11:47:04 UTC 2023,,,,,,,,,,"0|z1j6i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 08:40;martijnvisser;[~snuyanzin] Didn't we see this issue when we were looking into testcontainers / JNA upgrades? ;;;","17/Jul/23 09:44;Sergey Nuyanzin;caused by section was similar, however not sure that the reason is same
the strange thing is that it was ok during PR and started to fail after merge to main...;;;","17/Jul/23 11:13;martijnvisser;Perhaps it was rebased when merged, and prior to that it wasn't tested against Flink 1.18-SNAPSHOT?;;;","17/Jul/23 11:47;martijnvisser;Fixed in:

apache/flink-connector-elasticsearch@main ce44a13cf740cb1e60d4966b96f67826961569a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable RemoteChannelThroughputBenchmark_remoteRebalance_jmhTest,FLINK-32601,13543735,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,17/Jul/23 08:27,17/Jul/23 08:27,04/Jun/24 20:40,,,,,,,,,,,Benchmarks,,,,,,0,,,,"It's an exising exception which may occur accidentally, see workflow [#74|https://github.com/apache/flink-benchmarks/actions/runs/5219158886/jobs/9450453549#logs], [#40,|https://github.com/apache/flink-benchmarks/actions/runs/4915916989/jobs/8779014239], [#75,|https://github.com/apache/flink-benchmarks/actions/runs/5527523559/jobs/10171425495]

Exception stack as below:
{code:java}
<shutdown timeout of 30 seconds expired, forcing forked VM to exit>
2310ERROR: org.openjdk.jmh.runner.RunnerException: Benchmark caught the exception
2311Benchmark had encountered error, and fail on error was requested
2312	at org.openjdk.jmh.runner.Runner.runBenchmarks(Runner.java:570)
2313	at org.openjdk.jmh.runner.Runner.internalRun(Runner.java:313)
2314	at org.openjdk.jmh.runner.Runner.run(Runner.java:206)
2315	at org.openjdk.jmh.Main.main(Main.java:71)
2316Caused by: org.openjdk.jmh.runner.BenchmarkException: Benchmark error during the run
2317	at org.openjdk.jmh.runner.BenchmarkHandler.runIteration(BenchmarkHandler.java:428)
2318	at org.openjdk.jmh.runner.BaseRunner.runBenchmark(BaseRunner.java:282)
2319	at org.openjdk.jmh.runner.BaseRunner.runBenchmark(BaseRunner.java:234)
2320	at org.openjdk.jmh.runner.BaseRunner.doSingle(BaseRunner.java:139)
2321	at org.openjdk.jmh.runner.BaseRunner.runBenchmarksForked(BaseRunner.java:76)
2322	at org.openjdk.jmh.runner.ForkedRunner.run(ForkedRunner.java:72)
2323	at org.openjdk.jmh.runner.ForkedMain.main(ForkedMain.java:84)
2324	Suppressed: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2325		at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2326		at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:1010)
2327		at org.apache.flink.benchmark.RemoteChannelThroughputBenchmark.remoteRebalance(RemoteChannelThroughputBenchmark.java:76)
2328		at org.apache.flink.benchmark.generated.RemoteChannelThroughputBenchmark_remoteRebalance_jmhTest.remoteRebalance_thrpt_jmhStub(RemoteChannelThroughputBenchmark_remoteRebalance_jmhTest.java:123)
2329		at org.apache.flink.benchmark.generated.RemoteChannelThroughputBenchmark_remoteRebalance_jmhTest.remoteRebalance_Throughput(RemoteChannelThroughputBenchmark_remoteRebalance_jmhTest.java:85)
2330		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2331		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2332		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2333		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2334		at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
2335		at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
2336		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2337		at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2338		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2339		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2340		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2341		at java.base/java.lang.Thread.run(Thread.java:829)
2342	Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2343		at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
2344		at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getGlobalFailureHandlingResult(ExecutionFailureHandler.java:126)
2345		at org.apache.flink.runtime.scheduler.DefaultScheduler.handleGlobalFailure(DefaultScheduler.java:328)
2346		at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyGlobalFailure(UpdateSchedulerNgOnInternalFailuresListener.java:57)
2347		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.failGlobal(DefaultExecutionGraph.java:1073)
2348		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.failGlobalIfExecutionIsStillRunning(DefaultExecutionGraph.java:1061)
2349		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph$1.lambda$failJobDueToTaskFailure$1(DefaultExecutionGraph.java:477)
2350		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453)
2351		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2352		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453)
2353		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218)
2354		at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2355		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2356		at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2357		at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2358		at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
2359		at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
2360		at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2361		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
2362		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
2363		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
2364		at akka.actor.Actor.aroundReceive(Actor.scala:537)
2365		at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2366		at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2367		at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
2368		at akka.actor.ActorCell.invoke(ActorCell.scala:547)
2369		at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2370		at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2371		at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2372		at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2373		at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2374		at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2375		at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2376		at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2377	Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold. The latest checkpoint failed due to Asynchronous task checkpoint failed., view the Checkpoint History tab or the Job Manager log to find out why continuous checkpoints failed.
2378		at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:212)
2379		at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:191)
2380		at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:124)
2381		at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2151)
2382		at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1100)
2383		at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
2384		at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
2385		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2386		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2387		at java.base/java.lang.Thread.run(Thread.java:829) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-17 08:27:32.0,,,,,,,,,,"0|z1j6h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Server JDBC Connector wrong quoteIdentifier + issue when there is no update statement in upsert statement,FLINK-32600,13543733,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mradigue,mradigue,mradigue,17/Jul/23 08:13,17/Jul/23 08:39,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,Connectors / JDBC,,,,,,0,,,,"Hi, i've discovered 2 Issues in the MS SQL JDBC connector. They are located in the file SqlServerDialect.java

 
 * the MS SQL Server quote identifier is [identifier].
 * Assuming you want to upsert in this sql table: MyTable(id1,id2) with primary key is the coumpound (id1,id2). There is an syntax error because there are no fields to update

 

These are the fix I've tested against the 2 functions involved:

 

    *@Override*
    *public String quoteIdentifier(String identifier) {*
        return ""["" + identifier + ""]"";
    *}*


    *@Override*
    *public Optional<String> getUpsertStatement(*
            String tableName, String[] fieldNames, String[] uniqueKeyFields) {
        List<String> nonUniqueKeyFields =
                Arrays.stream(fieldNames)
                        .filter(f -> !Arrays.asList(uniqueKeyFields).contains(f))
                        .collect(Collectors.toList());
        String fieldsProjection =
                Arrays.stream(fieldNames)
                        .map(this::quoteIdentifier)
                        .collect(Collectors.joining("", ""));

        String valuesBinding =
                Arrays.stream(fieldNames)
                        .map(f -> "":"" + f + "" "" + quoteIdentifier(f))
                        .collect(Collectors.joining("", ""));

        String usingClause = String.format(""SELECT %s"", valuesBinding);
        String onConditions =
                Arrays.stream(uniqueKeyFields)
                        .map(
                                f ->
                                        ""[TARGET].""
                                                + quoteIdentifier(f)
                                                + ""=[SOURCE].""
                                                + quoteIdentifier(f))
                        .collect(Collectors.joining("" AND ""));
        String updateSetClause =
                nonUniqueKeyFields.stream()
                        .map(
                                f ->
                                        ""[TARGET].""
                                                + quoteIdentifier(f)
                                                + ""=[SOURCE].""
                                                + quoteIdentifier(f))
                        .collect(Collectors.joining("", ""));

        String insertValues =
                Arrays.stream(fieldNames)
                        .map(f -> ""[SOURCE]."" + quoteIdentifier(f))
                        .collect(Collectors.joining("", ""));

        StringBuilder sb = new StringBuilder();
        sb.append(
                String.format(
                        ""MERGE INTO %s AS [TARGET] USING (%s) AS [SOURCE] ON (%s)"",
                        quoteIdentifier(tableName), usingClause, onConditions));
        if (StringUtils.isNotEmpty(updateSetClause)) {
            sb.append(String.format("" WHEN MATCHED THEN UPDATE SET %s"", updateSetClause));
        }

        sb.append(
                String.format(
                        "" WHEN NOT MATCHED THEN INSERT (%s) VALUES (%s);"",
                        fieldsProjection, insertValues));

        return Optional.of(sb.toString());
    *}*

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 08:26:29 UTC 2023,,,,,,,,,,"0|z1j6go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 08:24;martijnvisser;[~mradigue] Do you want to open a PR for fixing the issues?;;;","17/Jul/23 08:26;mradigue;yes, I would be glad to;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks in Slack have failed consistently since 2023-07-14,FLINK-32599,13543729,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,martijnvisser,martijnvisser,17/Jul/23 07:41,16/Aug/23 02:51,04/Jun/24 20:40,16/Aug/23 02:51,,,,,,,,,,Benchmarks,,,,,,0,,,,"As reported in the #flink-dev-benchmarks channel on Slack, all Jenkins build have failed:

Failed build 1457 of flink-master-benchmarks-java8 (Open): hudson.AbortException: script returned exit code 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 02:51:40 UTC 2023,,,,,,,,,,"0|z1j6fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 07:46;leonard;[~Yanfei Lei]  Could you help to take a look at this ticket?;;;","17/Jul/23 07:54;Yanfei Lei;[~leonard] This is because of https://issues.apache.org/jira/browse/FLINK-23484 , after  [https://github.com/apache/flink-benchmarks/pull/75] merging, I think later builds will succeed.;;;","17/Jul/23 07:57;leonard;Thanks [~Yanfei Lei] for the clarification, I'll close this one after later CIs be green.;;;","17/Jul/23 08:56;masteryhx;The later PR has been merged, the CI has been green: https://github.com/apache/flink-benchmarks/actions/runs/5572887997;;;","16/Aug/23 02:51;masteryhx;Since it has been fixed and #flink-dev-benchmarks channel works well now, I closed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spill data from feedback edge to disk to avoid possible OOM,FLINK-32598,13543726,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhangzp,zhangzp,17/Jul/23 07:29,17/Jul/23 07:43,04/Jun/24 20:40,,,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,,,,"In Flink ML, we use feedback edge to implement the iteration module. Suppose the job topology is like `OpA -> HeadOperator -> OpB -> TailOperator`, then the basic process of each iteration is as follows:
 * At the first iteration, HeadOperator takes the input from OpA and forward it to OpB.
 * Later, OpB consumes the input from HeadOperator and forward the output to TailOperator.
 * Finally, TailOperator puts the records into a memory message queue and HeadOperator consumes the message queue.

When the output from OpB contains many records and these records cannot be consumed soon, the message queue would grow big and finally lead to OOM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-17 07:29:56.0,,,,,,,,,,"0|z1j6f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop Yarn specific get rest endpoints ,FLINK-32597,13543717,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,17/Jul/23 05:07,05/Dec/23 06:42,04/Jun/24 20:40,,1.18.0,,,,,,,,,Deployment / YARN,,,,,,0,2.0-related,,,"As listed in in the 2.0 release, we need to Drop YARN-specific mutating GET REST endpoints (yarn-cancel, yarn-stop)
We shouldn't continue having such hacks in our APIs to work around YARN deficiencies.

https://cwiki.apache.org/confluence/display/FLINK/2.0+Release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-17 05:07:47.0,,,,,,,,,,"0|z1j6d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The partition key will be wrong when use Flink dialect to create Hive table,FLINK-32596,13543708,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,walls.flink.m,luoyuxia,luoyuxia,17/Jul/23 02:36,04/Mar/24 01:38,04/Jun/24 20:40,,1.15.0,1.16.0,1.17.0,,,,,,,Connectors / Hive,,,,,,0,,,,"Can be reproduced by the following SQL:

 
{code:java}
tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);
tableEnv.executeSql(
        ""create table t1(`date` string, `geo_altitude` FLOAT) partitioned by (`date`)""
                + "" with ('connector' = 'hive', 'sink.partition-commit.delay'='1 s',  'sink.partition-commit.policy.kind'='metastore,success-file')"");
CatalogTable catalogTable =
        (CatalogTable) hiveCatalog.getTable(ObjectPath.fromString(""default.t1""));

// the following assertion will fail
assertThat(catalogTable.getPartitionKeys().toString()).isEqualTo(""[date]"");{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/24 10:36;walls.flink.m;image-2024-02-14-16-06-13-126.png;https://issues.apache.org/jira/secure/attachment/13066715/image-2024-02-14-16-06-13-126.png","14/Feb/24 21:35;walls.flink.m;image-2024-02-15-03-05-22-541.png;https://issues.apache.org/jira/secure/attachment/13066732/image-2024-02-15-03-05-22-541.png","14/Feb/24 21:36;walls.flink.m;image-2024-02-15-03-06-28-175.png;https://issues.apache.org/jira/secure/attachment/13066733/image-2024-02-15-03-06-28-175.png","14/Feb/24 21:38;walls.flink.m;image-2024-02-15-03-08-50-029.png;https://issues.apache.org/jira/secure/attachment/13066734/image-2024-02-15-03-08-50-029.png",,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 01:38:55 UTC 2024,,,,,,,,,,"0|z1j6b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 18:28;walls.flink.m;Hi [~luoyuxia] , are you working on the same? I can take this up. 

I faced the same issue, where partition key always choose the last one or last two columns, no matter what we choose.;;;","24/Jul/23 01:18;luoyuxia;[~walls.flink.m] Thanks for voluntering. Feel free to take it. I'lll help review. Thxs.;;;","14/Feb/24 21:40;walls.flink.m;[~luoyuxia] 

Hive Metastore expects the partitioned column should be last while inserting data. [hiveql - Hive partition column - Stack Overflow|https://stackoverflow.com/questions/60510174/hive-partition-column]

So what flink does is, it uses the last 'n' columns as PartitionColumns irrespective of what user passes as partition Columns: [https://github.com/apache/flink/blob/403694e7b9c213386f3ed9cff21ce2664030ebc2/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java#L515]

 

And select , insert Queries follows the same logic of finding partition columns at the last!

 

As a test, I made a chng here.  

[https://github.com/apache/flink/commit/df47ceaba82a3d4f3392c1b53bb52f34d520cc3d]

 

Results:

!image-2024-02-15-03-05-22-541.png|width=600,height=106!

!image-2024-02-15-03-06-28-175.png|width=468,height=267!

The partitions will always come at the last due to HMS. Either we use insert stmt like: 

_INSERT INTO testHive2 PARTITION (ts='22:16:46', active='TRUE') SELECT 1, 46, 'false';_

_SELECT query output:_

_!image-2024-02-15-03-08-50-029.png|width=525,height=328!_;;;","20/Feb/24 01:36;luoyuxia;[~walls.flink.m] Thanks for your investagtion. So, do you mean Hive metastore will always take the last columns as partition column whatever what columns we specific as partition column?;;;","02/Mar/24 22:03;walls.flink.m;[~luoyuxia] Yes, correct. MetaStore will take last columns as partition Columns. I think Documentation should clearly specify this. I can put add this in documentation. ;;;","04/Mar/24 01:38;luoyuxia;[~walls.flink.m] Thanks. Feel free to open a pr for ducumentation. I can help review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis connector doc show wrong deserialization schema version,FLINK-32595,13543707,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,17/Jul/23 02:30,17/Jul/23 05:52,04/Jun/24 20:40,17/Jul/23 05:40,aws-connector-4.1.0,,,,,aws-connector-4.1.0,aws-connector-4.2.0,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,"[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kinesis/#the-deserializationschema]

GlueSchemaRegistryJsonDeserializationSchema and GlueSchemaRegistryAvroDeserializationSchema show the wrong version(flink version), but they have been moved to the repo of aws-connector. 
So we should fix the version number.

Note that the module name has been changed for the module
flink-json-glue-schema-registry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/23 02:31;tanyuxin;image-2023-07-17-10-31-42-435.png;https://issues.apache.org/jira/secure/attachment/13061355/image-2023-07-17-10-31-42-435.png","17/Jul/23 02:32;tanyuxin;image-2023-07-17-10-32-14-627.png;https://issues.apache.org/jira/secure/attachment/13061356/image-2023-07-17-10-32-14-627.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 05:40:17 UTC 2023,,,,,,,,,,"0|z1j6aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 02:32;tanyuxin;I have tested the version number locally.  !image-2023-07-17-10-31-42-435.png|width=560,height=209!

and 
!image-2023-07-17-10-32-14-627.png|width=523,height=222!;;;","17/Jul/23 05:40;Weijie Guo;v4.1 via 158a495a00d823efaf1615f63f14a3a6067a5488.
main via 29c4828bbd498277e58f1326c73bdb8700abdbc3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use blocking ResultPartitionType if operator only outputs records on EOF (FLIP-331),FLINK-32594,13543645,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lindong,lindong,15/Jul/23 12:22,12/Sep/23 06:10,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-15 12:22:22.0,,,,,,,,,,"0|z1j5x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelimitedInputFormat will cause record loss for multi-bytes delimit when a delimit is seperated to two splits,FLINK-32593,13543630,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,thlzhf00,thlzhf00,15/Jul/23 02:31,15/Jul/23 02:42,04/Jun/24 20:40,,1.16.1,1.16.2,1.17.1,,,,,,,API / Core,,,,,,0,,,,"Run the following test to reproduce this bug.
{code:java}
// code placeholder
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.io.DelimitedInputFormat;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.junit.Test;

import javax.xml.bind.DatatypeConverter;
import java.io.IOException;

public class MyTest {

  @Test
  public void myTest() throws Exception {
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(5);

    String path = MyTest.class.getClassLoader().getResource(""5parallel.dat"").getPath();

    final DelimitedInputFormat<byte[]> inputFormat = new TestInputFormat();
    // The delimiter is ""B87E7E7E""
    inputFormat.setDelimiter(new byte[]{(byte) 184, (byte) 126, (byte) 126, (byte) 126});
    // Set buffer size less than default value of 1M for easily debugging
    inputFormat.setBufferSize(128);

    DataStreamSource<byte[]> source = env.readFile(inputFormat, path);

    source.map(new MapFunction<byte[], Object>() {
      @Override
      public Object map(byte[] value) throws Exception {
        System.out.println(DatatypeConverter.printHexBinary(value));
        return value;
      }
    }).setParallelism(1);

    env.execute();
  }

  private class TestInputFormat extends DelimitedInputFormat<byte[]> {
    @Override
    public byte[] readRecord(byte[] reuse, byte[] bytes, int offset, int numBytes) throws IOException {
      final int delimiterLen = this.getDelimiter().length;

      if (numBytes > 0) {
        byte[] record = new byte[delimiterLen + numBytes];
        System.arraycopy(this.getDelimiter(), 0, record, 0, delimiterLen);
        System.arraycopy(bytes, offset, record, delimiterLen, numBytes);
        return record;
      }

      return new byte[0];
    }
  }
}
 {code}
 

The actually output result is:
{code:java}
// code placeholder
B87E7E7E1A00EB900A4EDC6850160070F6BED4631321ADDC6F06DC137C221E99
B87E7E7E1A00EB900A4EDC6A51160070F61A8AFE022A3EC67718002A217C2181
B87E7E7E1A00EB900A4EDC6D5516 {code}
 

The expected output result shoud be:
{code:java}
// code placeholder
B87E7E7E1A00EB900A4EDC6850160070F6BED4631321ADDC6F06DC137C221E99
B87E7E7E1A00EB900A4EDC6B52150070F6BE468EFD20BEEEB756E03FD7F653D0
B87E7E7E1A00EB900A4EDC6D5516
B87E7E7E1A00EB900A4EDC6A51160070F61A8AFE022A3EC67718002A217C2181 {code}
The view of a delimit is seperated to two splits (The tail of line 2 and head of line 3):

!image-2023-07-15-10-30-03-740.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/23 02:27;thlzhf00;5parallel.dat;https://issues.apache.org/jira/secure/attachment/13061349/5parallel.dat","15/Jul/23 02:30;thlzhf00;image-2023-07-15-10-30-03-740.png;https://issues.apache.org/jira/secure/attachment/13061348/image-2023-07-15-10-30-03-740.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 15 02:42:47 UTC 2023,,,,,,,,,,"0|z1j5ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/23 02:42;thlzhf00;One fix for this is to adjust the #open(FileInputSplit split) method in the file of DelimitedInputFormat.java.

 

Current method implementation:
{code:java}
// code placeholder
@Override
public void open(FileInputSplit split) throws IOException {
    super.open(split);
    initBuffers();

    this.offset = splitStart;
    if (this.splitStart != 0) {
        this.stream.seek(offset);
        readLine();
        // if the first partial record already pushes the stream over
        // the limit of our split, then no record starts within this split
        if (this.overLimit) {
            this.end = true;
        }
    } else {
        fillBuffer(0);
    }
    initializeSplit(split, null);
} {code}
Adjusted method implementation:
{code:java}
// code placeholder
@Override
public void open(FileInputSplit split) throws IOException {
  super.open(split);

  // The main idea is to include the seperated delimit part in the tail of last split to the head of the next split
  if (split.getStart() > 0) {
    int delimLength = this.delimiter.length;
    int splitHeadLength = 2 * (delimLength - 1);

    byte[] splitHead = new byte[splitHeadLength];

    this.stream.seek(split.getStart() - delimLength + 1);
    this.stream.read(splitHead, 0, splitHeadLength);

    int delimPos = 0;
    int searchPos = 0;
    while ((searchPos < splitHeadLength && delimPos < delimLength)) {
      if (splitHead[searchPos] == this.delimiter[delimPos]) {
        // Found the expected delimiter character. Continue looking for the next
        // character of delimiter.
        delimPos++;
      } else {
        // Delimiter does not match.
        // We have to reset the read position to the character after the first matching
        // character
        //   and search for the whole delimiter again.
        searchPos -= delimPos;
        delimPos = 0;
      }
      searchPos++;
    }

    if (delimPos == delimLength) {
      FileInputSplit extendedSplit = new FileInputSplit(split.getSplitNumber(), split.getPath(),
          split.getStart() - delimLength + 1, split.getLength() + delimLength - 1, split.getHostnames());
      super.open(extendedSplit);
    }
  }

  initBuffers();

  this.offset = splitStart;
  if (this.splitStart != 0) {
    this.stream.seek(offset);
    readLine();
    // if the first partial record already pushes the stream over
    // the limit of our split, then no record starts within this split
    if (this.overLimit) {
      this.end = true;
    }
  } else {
    fillBuffer(0);
  }
  initializeSplit(split, null);
} {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(Stream)ExEnv#initializeContextEnvironment isn't thread-safe,FLINK-32592,13543555,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,fabiowanner,fabiowanner,14/Jul/23 12:26,21/Jul/23 16:49,04/Jun/24 20:40,21/Jul/23 16:05,1.15.4,1.17.1,1.18.0,,,1.16.3,1.17.2,1.18.0,,Client / Job Submission,,,,,,0,pull-request-available,,,"*Context*

We are using the flink-k8s-operator to deploy multiple jobs (up to 32) to a single session cluster. The job submissions done by the operator happen concurrently, basically at the same time.

Operator version: 1.5.0

Flink version:  1.15.4, 1.7.1, 1.18 (master@f37d41cf)

*Problem*

Rarely (~once every 50 deployments) one of the jobs will not be executed. In the following incident 4 jobs are deployed at the same time:
 * gorner-task-staging-e5730831
 * gorner-facility-staging-e5730831
 * gorner-aepp-staging-e5730831
 * gorner-session-staging-e5730831

 
The operator submits the job, they all get a reasonable jobID:
{code:java}
2023-07-14 10:25:35,295 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-task-staging-e5730831] Submitting job: 4968b186061e44390000000000000002 to session cluster.
2023-07-14 10:25:35,297 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-facility-staging-e5730831] Submitting job: 91a5260d916c4dff0000000000000002 to session cluster.
2023-07-14 10:25:35,301 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-aepp-staging-e5730831] Submitting job: 103c0446e14749a10000000000000002 to session cluster.
2023-07-14 10:25:35,302 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-session-staging-e5730831] Submitting job: de59304d370b4b8e0000000000000002 to session cluster.
{code}
In the cluster the JarRunHandler's handleRequest() method will get the request, all 4 jobIDs are present (also all args, etc are correct):
{code:java}
2023-07-14 10:25:35,320 WARN  org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - handleRequest - requestBody.jobId: 4968b186061e44390000000000000002
2023-07-14 10:25:35,321 WARN  org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - handleRequest - requestBody.jobId: de59304d370b4b8e0000000000000002
2023-07-14 10:25:35,321 WARN  org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - handleRequest - requestBody.jobId: 91a5260d916c4dff0000000000000002
2023-07-14 10:25:35,321 WARN  org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - handleRequest - requestBody.jobId: 103c0446e14749a10000000000000002
{code}
But once the EmbeddedExecutor's submitAndGetJobClientFuture() method is called instead of getting 1 call per jobID we have 4 calls but one of the jobIDs twice:
{code:java}
2023-07-14 10:25:35,616 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[4968b186061e44390000000000000002]
2023-07-14 10:25:35,616 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[103c0446e14749a10000000000000002]
2023-07-14 10:25:35,616 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[de59304d370b4b8e0000000000000002]
2023-07-14 10:25:35,721 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[de59304d370b4b8e0000000000000002]
{code}
If this is important: the jobGraph obtained does not match the jobID. We get 2 times de59304d370b4b8e0000000000000002 but the jobgraph for this jobID is never returned by getJobGraph() in EmbeddedExecutor.submitAndGetJobClientFuture().

This will then lead to the job already existing:
{code:java}
2023-07-14 10:25:35,616 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []
2023-07-14 10:25:35,616 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []
2023-07-14 10:25:35,616 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []
2023-07-14 10:25:35,721 WARN  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: [de59304d370b4b8e0000000000000002]
{code}
But since the jobs are completely different the execution will fail. Depending on the timing with one of the following exceptions:
 * RestHandlerException: No jobs included in application
 * ClassNotFoundException: io.dectris.aelps.pipelines.gorner.facility.FacilityEventProcessor

 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-32552,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 16:49:30 UTC 2023,,,,,,,,,,"0|z1j5d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 12:31;fabiowanner;I would be happy to assist in fixing this issue, but could really use some pointers to find the root cause of the problem...;;;","14/Jul/23 16:15;chesnay;Sounds like a duplicate of FLINK-30596.;;;","14/Jul/23 16:16;chesnay;Are you sure you tested this with the latest mater?;;;","14/Jul/23 16:21;chesnay;ahhh could be the usual ExecutionEnvironment concurrency thing.

Try to not submits jobs to jar/:jarid/run in parallel but sequentially.;;;","16/Jul/23 06:03;fabiowanner;Hey! Sorry for my late response and thanks a lot for looking into the issue! As the job submission is done by the official flink k8s operator we do not have direct control over how the jobs are run.

I am currently on vacation, but I will be able to verify your fix on Wednesday!;;;","19/Jul/23 10:28;chesnay;master: 13d35365f677813d5f0090f121e14e8bdec646d1
1.17: 32c657c14266531d3c7b1f448d96f513661a59d6
1.16: 903f122c5acf64e16daf073ec64a0d0a23ade4f1;;;","21/Jul/23 16:05;chesnay;Please re-open if the issue still occurs.;;;","21/Jul/23 16:49;fabiowanner;We did a few hundred deployments and did not run into the issue anymore! Looks resolved! Thanks a lot! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update document of Kafka Source: Enable Dynamic Partition Discovery by Default in Kafka Source,FLINK-32591,13543529,13534275,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,loserwang1024,loserwang1024,loserwang1024,14/Jul/23 09:23,19/Jul/23 02:38,04/Jun/24 20:40,18/Jul/23 08:40,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,"Based on Flip 288,  dynamic partition discovery is enabled by Default in Kafka Source  now. some corresponding document in Chinese and English should be modified:
 * ""Partition discovery is *disabled* by default. You need to explicitly set the partition discovery interval to enable this feature"" in [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/]
 * 
h5. scan.topic-partition-discovery.interval is (none) in [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kafka/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 08:40:43 UTC 2023,,,,,,,,,,"0|z1j57c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 09:24;loserwang1024;[~renqs] CC, would you please assign this issue to me?;;;","17/Jul/23 02:33;renqs;[~loserwang1024] Assigned to you. Thanks for the contribution!;;;","18/Jul/23 08:40;renqs;Merged to main: 79ae2d70499f81ce489911956c675354657dd44f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to read flink parquet filesystem table stored in hive metastore service.,FLINK-32590,13543493,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yangguozhen,yangguozhen,14/Jul/23 03:53,14/Jul/23 03:53,04/Jun/24 20:40,,1.17.1,,,,,,,,,Connectors / Hive,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,"h2. Summary:

Fail to read flink parquet filesystem table stored in hive metastore service.
h2. The problem:

When I try to read a flink parquet filesystem table stored in hive metastore service, I got the following exception.
{noformat}
java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:417) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_345]
Caused by: java.lang.NoSuchMethodError: shaded.parquet.org.apache.thrift.TBaseHelper.hashCode(J)I
	at org.apache.parquet.format.ColumnChunk.hashCode(ColumnChunk.java:812) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at java.util.AbstractList.hashCode(AbstractList.java:541) ~[?:1.8.0_345]
	at org.apache.parquet.format.RowGroup.hashCode(RowGroup.java:704) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at java.util.HashMap.hash(HashMap.java:340) ~[?:1.8.0_345]
	at java.util.HashMap.put(HashMap.java:613) ~[?:1.8.0_345]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.generateRowGroupOffsets(ParquetMetadataConverter.java:1411) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.access$600(ParquetMetadataConverter.java:144) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.format.converter.ParquetMetadataConverter$3.visit(ParquetMetadataConverter.java:1461) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.format.converter.ParquetMetadataConverter$3.visit(ParquetMetadataConverter.java:1437) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter.accept(ParquetMetadataConverter.java:1207) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:1437) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:583) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:777) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:658) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:127) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:75) ~[flink-sql-parquet-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.file.table.FileInfoExtractorBulkFormat.createReader(FileInfoExtractorBulkFormat.java:109) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:112) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:65) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114) ~[flink-connector-files-1.17.1.jar:1.17.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_345]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_345]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_345]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_345]
	... 1 more
{noformat}
h2. Possible reason:

When I start the cluster with the ""-verbse:class"" opt, I got classloading message shown below.
{code:bash}
# how I start the cluster
FLINK_ENV_JAVA_OPTS='-verbose:class' bin/start-cluster.sh
{code}
{noformat}
[Loaded shaded.parquet.org.apache.thrift.TBaseHelper from file:/Users/guozhenyang/Tools/flink-1.17.1/lib/flink-sql-connector-hive-3.1.3_2.12-1.17.1.jar]
[Loaded org.apache.parquet.format.ColumnChunk from file:/Users/guozhenyang/Tools/flink-1.17.1/lib/flink-sql-parquet-1.17.1.jar]
{noformat}
I assume there maybe conflict between the libthrift libs contained in _flink-sql-connector-hive-3.1.3_2.12-1.17.1.jar_ and {_}flink-sql-parquet-1.17.1.jar{_}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-14 03:53:15.0,,,,,,,,,,"0|z1j4zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Carry over parallelism overrides to prevent users from clearing them on updates,FLINK-32589,13543439,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,mxm,mxm,13/Jul/23 14:30,08/Aug/23 09:39,04/Jun/24 20:40,19/Jul/23 14:47,,,,,,kubernetes-operator-1.6.0,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,"The autoscaler currently sets the parallelism overrides via the Flink config {{pipeline.jobvertex-parallelism-overrides}}. Whenever the user posts specs updates, special care needs to be taken in order to carry over existing overrides. Otherwise the job will reset to the default parallelism configuration. Users shouldn't have to deal with this. Instead, whenever a new spec is posted which does not contain the overrides, the operator should automatically apply the last-used overrides (if autoscaling is enabled).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32774,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 14:47:45 UTC 2023,,,,,,,,,,"0|z1j4nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/23 15:20;gyfora;I think this is not necessarily something that should be handled on the operator side otherwise the user has no way of actually removing the overrides.

If they use kubectly apply (serversideApply) without the override config defined, the configs would be naturally merged and carried over. By replacing the user can remove it by not setting the override.;;;","13/Jul/23 16:27;mxm;We handle this in our application stack but I think this is something that the operator should do for the user. Most users probably do a POST/PUT on the spec which will clear any overrides. This will come as a surprise to users.

It is really an implementation detail that the overrides are handled via an undocumented Flink configuration option. The overrides are more part of the applications status than actual configuration.

Users can turn off autoscaling which should clear any overrides.;;;","13/Jul/23 16:57;gyfora;So just to be clear, do you suggest adding a new status field with the autoscaler overrides and always applying them to the spec?

This way we could actually get rid of any spec modification done by the autoscaler module and let the reconciler simply apply it without updating it in k8s.;;;","13/Jul/23 16:58;gyfora;I think this would be a good improvement but we have to consider and document how user specified overrides will interact with this, how the user can override the autoscaler set parallelisms, or completely clear them.;;;","19/Jul/23 14:47;gyfora;merged to main 9fe68251eb1a37333a6e862bf7b048061396498c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML unittest BoundedPerRoundStreamIterationITCase failed,FLINK-32588,13543410,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,13/Jul/23 09:55,13/Jul/23 09:55,04/Jun/24 20:40,,,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,,,,"[https://github.com/apache/flink-ml/actions/runs/5306457279/jobs/9604069705]
[https://github.com/apache/flink-ml/actions/runs/5166305530/jobs/9306327867]

 

The error message is as below.
{code:java}
Error:  testPerRoundIterationWithState  Time elapsed: 7.192 s  <<< FAILURE!
620java.lang.AssertionError: expected:<3> but was:<4>
621	at org.junit.Assert.fail(Assert.java:89)
622	at org.junit.Assert.failNotEquals(Assert.java:835)
623	at org.junit.Assert.assertEquals(Assert.java:647)
624	at org.junit.Assert.assertEquals(Assert.java:633)
625	at org.apache.flink.test.iteration.BoundedPerRoundStreamIterationITCase.testPerRoundIterationWithState(BoundedPerRoundStreamIterationITCase.java:170)
626	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
627	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
628	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
629	at java.lang.reflect.Method.invoke(Method.java:498) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-13 09:55:52.0,,,,,,,,,,"0|z1j4gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The results returned from the CDC sql query are null or the value was changed unexpectly,FLINK-32587,13543406,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,jasonliangyc,jasonliangyc,13/Jul/23 09:42,02/Aug/23 09:28,04/Jun/24 20:40,,1.17.0,1.17.1,,,,,,,,Table SQL / Client,,,,,,0,,,,"I created a CDC table(the sqlserver source table has more than 100 columns) as below and then run the query 'select * from so_cdc' through sql-client, it gives me the unexpected results.
{code:java}
CREATE TABLE so_cdc (
   REC_ID STRING,
   Create_Date TIMESTAMP(3),
   PRIMARY KEY (REC_ID) NOT ENFORCED
 ) WITH (
    'connector' = 'sqlserver-cdc',
    'hostname' = 'xxxx',
    'port' = 'xxxx',
    'username' = 'xxx',
    'password' = 'xxxx',
    'database-name' = 'xxxx',
    'schema-name' = 'xxxx',
    'table-name' = 'xxx',
    'scan.startup.mode' = 'latest-offset'
 ); {code}
Run the query for the first time, the data look normal.

!image-2023-07-13-17-35-32-235.png|width=535,height=141!

 

But after i run the same query multiple times, it gives me the unexpected data, and i'm sure that these two columns of my sqlserver source table don't contain these data.

And i found that the values of 'REC_ID' were actually the values of another column in the sqlserver source table, it seems the query returned the wrong columns.

Above sqlserver source table is in the production database and i can't reproduce the same issue in dev db.

!image-2023-07-21-14-48-31-572.png|width=533,height=159!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/23 09:35;jasonliangyc;image-2023-07-13-17-35-32-235.png;https://issues.apache.org/jira/secure/attachment/13061312/image-2023-07-13-17-35-32-235.png","13/Jul/23 09:37;jasonliangyc;image-2023-07-13-17-37-56-908.png;https://issues.apache.org/jira/secure/attachment/13061311/image-2023-07-13-17-37-56-908.png","21/Jul/23 06:48;jasonliangyc;image-2023-07-21-14-48-31-572.png;https://issues.apache.org/jira/secure/attachment/13061503/image-2023-07-21-14-48-31-572.png","02/Aug/23 08:40;jasonliangyc;image-2023-08-02-16-40-32-476.png;https://issues.apache.org/jira/secure/attachment/13061852/image-2023-08-02-16-40-32-476.png","02/Aug/23 08:43;jasonliangyc;image-2023-08-02-16-43-25-458.png;https://issues.apache.org/jira/secure/attachment/13061853/image-2023-08-02-16-43-25-458.png","02/Aug/23 09:03;jasonliangyc;image-2023-08-02-17-03-05-932.png;https://issues.apache.org/jira/secure/attachment/13061855/image-2023-08-02-17-03-05-932.png","02/Aug/23 09:07;jasonliangyc;image-2023-08-02-17-07-19-295.png;https://issues.apache.org/jira/secure/attachment/13061856/image-2023-08-02-17-07-19-295.png","02/Aug/23 09:09;jasonliangyc;image-2023-08-02-17-09-14-671.png;https://issues.apache.org/jira/secure/attachment/13061857/image-2023-08-02-17-09-14-671.png","02/Aug/23 09:10;jasonliangyc;image-2023-08-02-17-10-03-889.png;https://issues.apache.org/jira/secure/attachment/13061858/image-2023-08-02-17-10-03-889.png","02/Aug/23 09:18;jasonliangyc;image-2023-08-02-17-18-06-375.png;https://issues.apache.org/jira/secure/attachment/13061859/image-2023-08-02-17-18-06-375.png","02/Aug/23 09:25;jasonliangyc;image-2023-08-02-17-25-36-207.png;https://issues.apache.org/jira/secure/attachment/13061861/image-2023-08-02-17-25-36-207.png","02/Aug/23 09:26;jasonliangyc;image-2023-08-02-17-26-08-692.png;https://issues.apache.org/jira/secure/attachment/13061860/image-2023-08-02-17-26-08-692.png",,12.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 09:18:52 UTC 2023,,,,,,,,,,"0|z1j4g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 06:52;jasonliangyc;added more information;;;","01/Aug/23 16:24;jiabao.sun;Thanks [~jasonliangyc] to report this problem.

It is recommended that you create an issue in  [https://github.com/ververica/flink-cdc-connectors/issues] so that more maintainers will see it and try to fix it.
It would be helpful if you could provide the version of sqlserver, the version of the flink cdc connector, and the table structure of sqlserver.
;;;","02/Aug/23 06:17;jasonliangyc;Hi [~jiabao.sun] , got it, I will create an issue in github cdc and my configurations are as below:

    flink: 1.17.0
    flink-connector-jdbc-3.1.1-1.17.jar
    flink-sql-connector-sqlserver-cdc-2.3.0.jar
    mssql-jdbc-12.2.0.jre8.jar
    sqlserver: Microsoft SQL Server 2014;;;","02/Aug/23 06:31;jiabao.sun;Thanks [~jasonliangyc] .


There's a known issue that old version's sqlserver cdc connector will subscribe the same table of different database and that problem was fixed by the 2.4.1 version. But the problem of unexpected data from same table is never reported before.
You can check whether the table of same name exists in different databases and try to use the latest version of the connector to see if this problem still exists.;;;","02/Aug/23 09:18;jasonliangyc;Hi [~jiabao.sun] , thanks for your advices and after serveral tests with the new version file: flink-sql-connector-sqlserver-cdc-2.4.1.jar, now i only found one issue still exist, that is the NULL still returned.

the Create_Date and Last_Update_Date have the same datatype.

sqlserver DDL

!image-2023-08-02-17-25-36-207.png!

 

flink sql DDL

!image-2023-08-02-17-26-08-692.png|width=416,height=141!

!image-2023-08-02-17-09-14-671.png|width=556,height=151!

!image-2023-08-02-17-18-06-375.png|width=549,height=268!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable input locality in SimpleExecutionSlotAllocator,FLINK-32586,13543402,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,13/Jul/23 09:16,19/Jul/23 11:57,04/Jun/24 20:40,19/Jul/23 11:57,1.18.0,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,"At present, the AdaptiveBatchScheduler uses the `SimpleExecutionSlotAllocator` to assign slot to execution, but it currently lacks support for the capability of input locality, which may increase unnecessary data transmission overhead. In this issue, we aim to enable the `SimpleExecutionSlotAllocator` to support the input locality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 11:57:19 UTC 2023,,,,,,,,,,"0|z1j4fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/23 09:24;wanglijie;Thanks your proposal, make sense to me, assigned to you :) [~xiasun];;;","19/Jul/23 11:57;zhuzh;Done via e732edb41a423f19d5eefc397ddbfacadaf0179e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter javax.xml.bind:jaxb-api false positive for Pulsar connector,FLINK-32585,13543298,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,12/Jul/23 15:18,17/Jul/23 17:14,04/Jun/24 20:40,13/Jul/23 17:48,,,,,,,,,,Build System / CI,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 17:48:05 UTC 2023,,,,,,,,,,"0|z1j3s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/23 17:48;tison;master via d7a3b3847dc5c680b32d1997d448b7dac44e529c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make it possible to unset default catalog and/or database,FLINK-32584,13543275,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,12/Jul/23 12:39,26/Jul/23 18:12,04/Jun/24 20:40,17/Jul/23 15:25,,,,,,1.18.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,"In certain scenarios it might make sense to unset the default catalog and/or database. For example in a situation when there is no sane default one, but we want the user make that decision consciously. 

This change has a narrow scope and changes only some checks in the API surface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32691,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 15:25:07 UTC 2023,,,,,,,,,,"0|z1j3n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 14:36;dwysakowicz;Hey [~jark] this is not a big change of behaviour. It only touches a few argument checks see: https://github.com/apache/flink/pull/22986 Do you think this is fine to go without entire FLIP process?;;;","12/Jul/23 14:55;twalthr;[~dwysakowicz] thanks for this proposal. In general, this looks like a very elegant and minimal fix without touching too many components. We could discuss whether we expose {{useCatalog(null)}} and {{useDatabase(null)}} through {{TableEnvironment}} or whether we keep this as an internal feature of {{CatalogManager}}. Nevertheless, forcing users to use a fully qualified name is valid use case.;;;","17/Jul/23 15:25;twalthr;Fixed in master: aa21eac3a14e74d0cdee69db987b7e95adda9932;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestClient can deadlock if request made after Netty event executor terminated,FLINK-32583,13543268,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,plucas,plucas,plucas,12/Jul/23 12:09,01/Aug/23 14:35,04/Jun/24 20:40,31/Jul/23 07:13,1.16.3,1.17.2,1.18.0,,,1.16.3,1.17.2,1.18.0,,Runtime / REST,,,,,,0,pull-request-available,,,"The RestClient can deadlock if a request is made after the Netty event executor has terminated.

This is due to the listener that would resolve the CompletableFuture that is attached to the ChannelFuture returned by the call to Netty to connect not being able to run because the executor to run it rejects the execution.

[RestClient.java|https://github.com/apache/flink/blob/release-1.17.1/flink-runtime/src/main/java/org/apache/flink/runtime/rest/RestClient.java#L471-L482]:
{code:java}
final ChannelFuture connectFuture = bootstrap.connect(targetAddress, targetPort);

final CompletableFuture<Channel> channelFuture = new CompletableFuture<>();

connectFuture.addListener(
        (ChannelFuture future) -> {
            if (future.isSuccess()) {
                channelFuture.complete(future.channel());
            } else {
                channelFuture.completeExceptionally(future.cause());
            }
        });
{code}
In this code, the call to {{addListener()}} can fail silently (only logging to the console), meaning any code waiting on the CompletableFuture returned by this method will deadlock.

There was some work in Netty around this back in 2015, but it's unclear to me how this situation is expected to be handled given the discussion and changes from these issues:
 * [https://github.com/netty/netty/issues/3449] (open)
 * [https://github.com/netty/netty/pull/3483] (closed)
 * [https://github.com/netty/netty/pull/3566] (closed)
 * [https://github.com/netty/netty/pull/5087] (merged)

I think a reasonable fix for Flink would be to check the state of {{connectFuture}} and {{channelFuture}} immediately after the call to {{addListener()}}, resolving {{channelFuture}} with {{completeExceptionally()}} if {{connectFuture}} is done and failed and {{channelFuture}} has not been completed. In the possible race condition where the listener was attached successfully and the connection fails instantly, the result is the same, as calls to {{CompletableFuture#completeExceptionally()}} are idempotent.

A workaround for users of RestClient is to call {{CompletableFuture#get(long timeout, TimeUnit unit)}} rather than {{#get()}} or {{#join()}} on the CompletableFutures it returns. However, if the call throws TimeoutException, the cause of the failure cannot easily be determined.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 14:35:12 UTC 2023,,,,,,,,,,"0|z1j3lk:",9223372036854775807,Fixes a bug in the RestClient where making a request after the client was closed returns a future that never completes.,,,,,,,,,,,,,,,,,,,"12/Jul/23 13:53;plucas;Further investigation suggests that this can only happen when a request is made after the RestClient itself is closed, which asynchronously shuts down the event loop used by Netty.

Many uses of RestClient will use try-with-resources where this wouldn't be an issue, but it should still have the correct behavior when used in other contexts where a request may be attempted after shutdown has already started.;;;","31/Jul/23 07:13;mapohl;master: b2920728a20c99e064c0553f2ec74c190199971f
1.17: 5a553c5ef47a9b777f077a20af96666e34fcc7f6
1.16: 2d6325d0ebbbe418d3e20118da57e8d9d78ea73f;;;","01/Aug/23 14:32;plucas;[~mapohl] regarding the release note, I just wanted to clarify something.
{quote}Fixes a race condition in the RestClient that could happen when submitting a request while closing the client. There was a small chance that the request submission wouldn't complete.
{quote}
That is one effect of this change, but the primary problem it solves is not a race condition, but that if a request is made in _any time after_ the client is closed, then the future will definitely never be resolved, and no exception thrown. If a caller were to call {{.join()}} on the returned {{{}CompletableFuture{}}}—which does not take a timeout—the caller will actually block forever.

I think this is the more serious bug being fixed here than the very rare edge case that required most of the complexity in the change.

Suggested release note:
{quote}Fixes a bug in the RestClient where the response future of a request made after the client was closed is never completed.
{quote}
or
{quote}Fixes a bug in the RestClient where making a request after the client was closed returns a future that never completes.
{quote};;;","01/Aug/23 14:35;mapohl;Good point. Thanks for verifying the release notes (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move TypeSerializerUpgradeTestBase from Kafka connector into flink-connector-common,FLINK-32582,13543229,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,12/Jul/23 07:04,11/Apr/24 10:04,04/Jun/24 20:40,,,,,,,,,,,Connectors / Common,,,,,,0,auto-deprioritized-major,pull-request-available,,"The externalization of connectors caused problems with the Flink's test data generation. The Kafka connector relied on TypeSerializerUpgradeTestBase for some test cases which was fine prior to FLINK-27518 where the test data generation was handled individually.

With FLINK-27518 the process was automated in Flink 1.18. For now, the TypeSerializerUpgradeTestBase class was just copied over into the Kafka connector since it was the only connector that would utilize this test base.

But we might want to provide a more generalized solution where the test base is provided by {{flink-connector-common}} to offer a generalized approach for any connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32455,FLINK-27518,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 10:04:54 UTC 2024,,,,,,,,,,"0|z1j3cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Apr/24 10:04;martijnvisser;[~mapohl] What was the plan on this ticket? We're now in the situation that the Flink Kafka connector can't compile for 1.20-SNAPSHOT, see https://github.com/apache/flink-connector-kafka/actions/runs/8644888490/job/23700957245#step:15:146

{code:java}
Error:  COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/TypeSerializerUpgradeTestBase.java:[139,15] exception java.io.IOException is never thrown in body of corresponding try statement
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/TypeSerializerUpgradeTestBase.java:[150,15] exception java.io.IOException is never thrown in body of corresponding try statement
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/TypeSerializerUpgradeTestBase.java:[182,15] exception java.io.IOException is never thrown in body of corresponding try statement
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/TypeSerializerUpgradeTestBase.java:[194,15] exception java.io.IOException is never thrown in body of corresponding try statement
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/TypeSerializerUpgradeTestBase.java:[206,15] exception java.io.IOException is never thrown in body of corresponding try statement
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for atomic CTAS,FLINK-32581,13543205,13543203,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tartarus,tartarus,12/Jul/23 02:53,10/Aug/23 11:16,04/Jun/24 20:40,10/Aug/23 11:16,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,add docs for atomic CTAS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 11:16:26 UTC 2023,,,,,,,,,,"0|z1j37k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 11:16;luoyuxia;master:

3cd0bf0a67016fe9c0b9ddbf033391ddb87ea496;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-305: Support atomic for CREATE TABLE AS SELECT(CTAS),FLINK-32580,13543203,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,12/Jul/23 02:45,10/Aug/23 11:16,04/Jun/24 20:40,10/Aug/23 11:16,,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,,,,FLIP-305 Support atomic for CREATE TABLE AS SELECT(CTAS) statement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-12 02:45:56.0,,,,,,,,,,"0|z1j374:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The filter criteria on the lookup table of Lookup join has no effect ,FLINK-32579,13543201,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jasonliangyc,jasonliangyc,12/Jul/23 01:53,14/Jul/23 15:29,04/Jun/24 20:40,,1.17.0,1.17.1,,,,,,,,Table SQL / Client,,,,,,0,,,,"*1.* I joined two tables using the lookup join as below query in sql-client, the filter criteria of (p.name = '??????') didn't shows up in the execution detail and it returned the rows only base on one condiction (cdc.product_id = p.id)
{code:java}
select
cdc.order_id,
cdc.order_date,
cdc.customer_name,
cdc.price,
p.name
FROM orders AS cdc
left JOIN products 
FOR SYSTEM_TIME AS OF cdc.proc_time as p ON p.name = '??????' and cdc.product_id = p.id
; {code}
!image-2023-07-12-09-31-18-261.png|width=657,height=132!

 

*2.* It showed the werid results when i changed the query as below, cause there were no data in the table(products) that the value of column 'name' is '??????'  and and execution detail didn't show us the where criteria.
{code:java}
select
cdc.order_id,
cdc.order_date,
cdc.customer_name,
cdc.price,
p.name
FROM orders AS cdc
left JOIN products 
FOR SYSTEM_TIME AS OF cdc.proc_time as p ON cdc.product_id = p.id
where p.name = '??????'
; {code}
!image-2023-07-12-09-42-59-231.png|width=684,height=102!

!image-2023-07-12-09-47-31-397.png|width=685,height=120!

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 01:31;jasonliangyc;image-2023-07-12-09-31-18-261.png;https://issues.apache.org/jira/secure/attachment/13061265/image-2023-07-12-09-31-18-261.png","12/Jul/23 01:42;jasonliangyc;image-2023-07-12-09-42-59-231.png;https://issues.apache.org/jira/secure/attachment/13061264/image-2023-07-12-09-42-59-231.png","12/Jul/23 01:47;jasonliangyc;image-2023-07-12-09-47-31-397.png;https://issues.apache.org/jira/secure/attachment/13061263/image-2023-07-12-09-47-31-397.png","13/Jul/23 09:19;337361684@qq.com;image-2023-07-13-17-19-26-972.png;https://issues.apache.org/jira/secure/attachment/13061305/image-2023-07-13-17-19-26-972.png","13/Jul/23 14:35;jasonliangyc;image-2023-07-13-22-35-35-696.png;https://issues.apache.org/jira/secure/attachment/13061314/image-2023-07-13-22-35-35-696.png","13/Jul/23 14:38;jasonliangyc;image-2023-07-13-22-38-16-709.png;https://issues.apache.org/jira/secure/attachment/13061315/image-2023-07-13-22-38-16-709.png","13/Jul/23 14:43;jasonliangyc;image-2023-07-13-22-43-24-213.png;https://issues.apache.org/jira/secure/attachment/13061316/image-2023-07-13-22-43-24-213.png","13/Jul/23 14:43;jasonliangyc;image-2023-07-13-22-43-45-957.png;https://issues.apache.org/jira/secure/attachment/13061317/image-2023-07-13-22-43-45-957.png","14/Jul/23 15:12;jasonliangyc;image-2023-07-14-23-12-51-696.png;https://issues.apache.org/jira/secure/attachment/13061342/image-2023-07-14-23-12-51-696.png","14/Jul/23 15:20;jasonliangyc;image-2023-07-14-23-20-18-936.png;https://issues.apache.org/jira/secure/attachment/13061343/image-2023-07-14-23-20-18-936.png","13/Jul/23 14:49;jasonliangyc;test_case.sql;https://issues.apache.org/jira/secure/attachment/13061318/test_case.sql",,,11.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 15:29:56 UTC 2023,,,,,,,,,,"0|z1j36o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/23 09:21;337361684@qq.com;Hi [~jasonliangyc] . Are you using Flink version 1.17.0?  I am unable to reproduce the problem in Figure 1 as you shown in my local UT case.  My sql pattern is:
{code:java}
SELECT * FROM MyTable AS T LEFT JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D ON T.b = '?????' and T.a = D.id {code}
My rel plan result is:

!image-2023-07-13-17-19-26-972.png|width=708,height=49!

 

BTW, I don't quite understand what is the problem in your Figure 2? Can you clarify it. Thanks!;;;","13/Jul/23 15:01;jasonliangyc;Hi [~337361684@qq.com]. Yes, I'm using the 1.17.0, but i also tried the 1.17.1, same issues.

I can reproduce the problems by using the attached [^test_case.sql], could you try it again by following the setups in that file? thanks.

!image-2023-07-13-22-35-35-696.png|width=674,height=164!

 

For the second problem, it means:

if 
{code:java}
where p.name = '??????' {code}
then the value of column 'name' return '??????'

if
{code:java}
where p.name = '+++++' {code}
 then the value of column 'name' return '+++++'

but actually there are no such data in the 'products' table, it is weird that it always return the same value as the one in the <where clause>.

 
{code:java}
select
cdc.order_id,
cdc.order_date,
cdc.customer_name,
cdc.price,
p.name
FROM orders AS cdc
left JOIN products 
FOR SYSTEM_TIME AS OF cdc.proc_time as p ON cdc.product_id = p.id
where p.name = '??????'{code}
 

!image-2023-07-13-22-38-16-709.png|width=663,height=120!

 

!image-2023-07-13-22-43-45-957.png|width=662,height=141!

 

 ;;;","14/Jul/23 04:32;337361684@qq.com;Hi, [~jasonliangyc] . I got it.

For question1: there is no filter condition ""p.name = '????'"" in relNode  LookupJoin?
 * This is normal because the filter condition has been pushed down to the jdbc source (jdbc source supports filter pushdown). The pushed down condition will not be displayed in the LookupJoin node.

For question2: wrong join result?
 * I think this is a bug for jdbc lookup source. For the pushed filter condition, the jdbc lookup source did not consume this filter correctly. After reading the code, I speculate that this is because the jdbc source doesn't process this filter condition for dim table.
 * To quickly verify this error. you can disable filter push down by adding config 'table.optimizer.source.predicate-pushdown-enabled'.
 * Also, after verifying, if this error is caused by jdbc source, you can @ [~ruanhang1993] taking a look. 

 ;;;","14/Jul/23 15:29;jasonliangyc;Thanks [~337361684@qq.com].

For 1:

Now it can show the filter condition in the execution plan detail after set 'table.optimizer.source.predicate-pushdown-enabled' = 'false' to disable the filter push down.

!image-2023-07-14-23-12-51-696.png|width=577,height=118!

For 2: 

Your speculation is right, by default the pushdow is enabled, but the jdbc source doesn't process this filter condition correctly and then return the wrong results.

After disabled the pushdown, we can get the normal return.

Hi @ [~ruanhang1993], could you help take a look at this? thanks.

!image-2023-07-14-23-20-18-936.png|width=569,height=263!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cascaded group by window time columns on a proctime window aggregate may result hang for ever,FLINK-32578,13543171,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,11/Jul/23 16:13,20/Jul/23 02:25,04/Jun/24 20:40,19/Jul/23 06:20,1.17.1,,,,,1.17.2,1.18.0,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,"Currently when group by window time columns on a proctime window aggregate result will get a wrong plan which may result hang for ever in runtime.

For such a query:
{code}
insert into s1
SELECT
  window_start,
  window_end,
  sum(cnt),
  count(*)
FROM (
 SELECT
    a,
    b,
    window_start,
    window_end,
    count(*) as cnt,
    sum(d) as sum_d,
    max(d) as max_d
 FROM TABLE(TUMBLE(TABLE src1, DESCRIPTOR(proctime), INTERVAL '5' MINUTE))
 GROUP BY a, window_start, window_end, b
)
GROUP BY a, window_start, window_end
{code}
the inner proctime window works fine, but the outer one doesn't work due to a wrong plan which will translate to a unexpected event mode window operator:
{code}
Sink(table=[default_catalog.default_database.s1], fields=[ws, we, b, c])
+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, CAST(EXPR$2 AS BIGINT) AS b, CAST(EXPR$3 AS BIGINT) AS c])
   +- WindowAggregate(groupBy=[a], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[5 min])], select=[a, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3, start('w$) AS window_start, end('w$) AS window_end])
      +- Exchange(distribution=[hash[a]])
         +- Calc(select=[a, window_start, window_end, cnt])
            +- WindowAggregate(groupBy=[a, b], window=[TUMBLE(time_col=[proctime], size=[5 min])], select=[a, b, COUNT(*) AS cnt, start('w$) AS window_start, end('w$) AS window_end])
               +- Exchange(distribution=[hash[a, b]])
                  +- Calc(select=[a, b, d, PROCTIME() AS proctime])
                     +- TableSourceScan(table=[[default_catalog, default_database, src1, project=[a, b, d], metadata=[]]], fields=[a, b, d])
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Thu Jul 20 02:22:06 UTC 2023,,,,,,,,,,"0|z1j300:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 06:20;lincoln.86xy;Fixed in master: 698b128e20d64db72939d54f7b153e31f261939b;;;","20/Jul/23 02:22;lincoln.86xy;Fixed in 1.17: dc8b70c2fcbb429a27a9cc1e263d9a38c2d7da34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid memory fragmentation when running CI for flink-table-planner module,FLINK-32577,13543143,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,11/Jul/23 11:50,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,,,,Build System / CI,Table SQL / Planner,,,,,0,,,,"This issue is a sub-issue of FLINK-18356.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-11 11:50:02.0,,,,,,,,,,"0|z1j2ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerMergedPartitionFileIndex supports spilling to file,FLINK-32576,13543128,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,11/Jul/23 09:41,19/Jul/23 02:27,04/Jun/24 20:40,19/Jul/23 02:27,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,"When running a very large-scale job, ProducerMergedPartitionFileIndex may occupy too much heap memory and cause OOM.

To resolve the issue, ProducerMergedPartitionFileIndex should support spilling to file to release the occupied memory when necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 02:27:09 UTC 2023,,,,,,,,,,"0|z1j2qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 02:27;Weijie Guo;master(1.18) via 1786e2ddfd1a1df5e907f5719138f63e819cb1a3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unified the Cpu of JobManager Name,FLINK-32575,13543120,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Bo Cui,Bo Cui,11/Jul/23 07:29,17/Sep/23 22:35,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,"To set the jm CPU of the Yarn, use the `yarn.appmaster.vcores`. To set the jm cpu of the k8s, use `kubernetes.jobmanager.cpu`. when there are yarn and k8s clusters, managing these configurations is difficult. Add a unified name for them for ease of use.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 17 22:35:08 UTC 2023,,,,,,,,,,"0|z1j2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","17/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescalingITCase.testSavepointRescalingInPartitionedOperatorState fails on AZP,FLINK-32574,13543117,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,Yanfei Lei,Sergey Nuyanzin,Sergey Nuyanzin,11/Jul/23 07:14,25/Jul/23 12:30,04/Jun/24 20:40,25/Jul/23 12:30,1.18.0,,,,,,,,,Tests,,,,,,0,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51129&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8308
fails with 
{noformat}
Jul 09 01:23:15 01:23:15.355 [ERROR] Tests run: 48, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 247.495 s <<< FAILURE! - in org.apache.flink.test.checkpointing.RescalingITCase
Jul 09 01:23:15 01:23:15.355 [ERROR] RescalingITCase.testSavepointRescalingInPartitionedOperatorState  Time elapsed: 2.037 s  <<< FAILURE!
Jul 09 01:23:15 java.lang.AssertionError: expected:<524> but was:<24>
Jul 09 01:23:15 	at org.junit.Assert.fail(Assert.java:89)
Jul 09 01:23:15 	at org.junit.Assert.failNotEquals(Assert.java:835)
Jul 09 01:23:15 	at org.junit.Assert.assertEquals(Assert.java:647)
Jul 09 01:23:15 	at org.junit.Assert.assertEquals(Assert.java:633)
Jul 09 01:23:15 	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingPartitionedOperatorState(RescalingITCase.java:621)
Jul 09 01:23:15 	at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingInPartitionedOperatorState(RescalingITCase.java:484)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32663,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 12:28:34 UTC 2023,,,,,,,,,,"0|z1j2o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 09:38;Sergey Nuyanzin;also it seems very similar issue with {{RescalingITCase.testSavepointRescalingOutBroadcastOperatorState}}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51283&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8644;;;","25/Jul/23 09:17;renqs;[~Yanfei Lei] Any updates on this one? ;;;","25/Jul/23 12:28;Yanfei Lei;[~renqs] I can't reproduce this bug locally, but by observing the logs of the two failures, I found that they both enabled unaligned checkpoint, I'm trying to reproduce it with `PseudoRandomValueSelector` disabled.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Custom Serialization for Managed State"" page into Chinese",FLINK-32573,13543087,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,pingcai678,pingcai678,11/Jul/23 01:04,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,,,,chinese-translation,Documentation,,,,,0,pull-request-available,,,"The page url is https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/

The markdown file is located in docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/custom_serialization.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 02:55:36 UTC 2023,,,,,,,,,,"0|z1j2hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 12:04;jinsuichen;hi, [~jark] . Would you please assign this to me？;;;","15/Dec/23 02:55;jinsuichen;hi, [~Wencong Liu] . Would you please assign this to me？;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not collect metrics or scale finished job vertices,FLINK-32572,13543063,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,10/Jul/23 19:28,12/Jul/23 08:28,04/Jun/24 20:40,12/Jul/23 08:28,,,,,,kubernetes-operator-1.6.0,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,"The autoscaler currently doesn't handle finished vertices well. These often report weird metrics and cannot be scaled anyways. 

We should exclude those from metric collection and scaling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 08:28:45 UTC 2023,,,,,,,,,,"0|z1j2c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 08:28;gyfora;merged to main daeb4b6559f0d26b1b0f23be5e8230f895b0a03e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prebuild HBase testing docker image,FLINK-32571,13543039,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,10/Jul/23 16:04,05/Jan/24 14:56,04/Jun/24 20:40,,,,,,,,,,,Connectors / HBase,,,,,,0,,,,"For testing we currently build an HBase docker image on-demand during testing. We can improve reliability and testing times by building this image ahead of time, as the only parameter is the HBase version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 13:30:05 UTC 2023,,,,,,,,,,"0|z1j26o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 16:06;chesnay;??the only parameter is the HBase version.??

That being said, I do wonder if different HBase version wouldn't potentially dffferent Hadoop versions.;;;","18/Jul/23 13:30;ferenc-csaky;After HBase 3.x is released and requires Hadoop 3.x it can be a problem. But IDK when that will actually be, it is in alpha for 2 years now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate API that uses Flink's Time implementation (related to FLINK-14638),FLINK-32570,13543018,12972070,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,10/Jul/23 13:52,27/Feb/24 08:43,04/Jun/24 20:40,09/Jan/24 07:11,1.19.0,,,,,1.19.0,,,,,,,,,,0,pull-request-available,stale-assigned,,"The plan is to resolve FLINK-14038 with Flink 2.0. As a preparation, we have to deprecate related @Public API .",,,,,,,,,,,,,,FLINK-14068,,,,,,,,,,,,,,,,,FLINK-34527,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 11:47:37 UTC 2024,,,,,,,,,,"0|z1j22g:",9223372036854775807,"Flink's Time classes are deprecated now and will be subject to deletion with the release of Flink 2.0. Please start to use Java's own Duration class, instead. Methods supporting the Duration class that replace the deprecated Time-based methods were introduced.",,,,,,,,,,,,,,,,,,,"10/Jul/23 14:53;mapohl;[~chesnay] any objections against me picking that one as a 1.18 preparation for the FLINK-14068?;;;","10/Jul/23 16:47;chesnay;Go ahead.;;;","01/Aug/23 09:24;renqs;Downgrading to Major as deprecations will not be treated as must-have in 1.18. ;;;","31/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","01/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Jan/24 11:47;mapohl;* master
** [c943ab49c4aa58286111e2b95e9580a16f4d6b4c|https://github.com/apache/flink/commit/c943ab49c4aa58286111e2b95e9580a16f4d6b4c]
** [6f0d07633a5c8e6511f3d16e04561cb277b65407|https://github.com/apache/flink/commit/6f0d07633a5c8e6511f3d16e04561cb277b65407];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the incomplete serialization of ResolvedCatalogTable caused by the newly introduced  time travel interface,FLINK-32569,13542990,13541827,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,10/Jul/23 10:19,17/Jul/23 01:18,04/Jun/24 20:40,17/Jul/23 01:18,,,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,, The newly added member variables in DefaultCatalogTable are not being serialized in CatalogPropertiesUtil.serializeCatalogTable(this). We should fix this. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 01:18:31 UTC 2023,,,,,,,,,,"0|z1j1w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 10:20;hackergin;[~luoyuxia] I would like to fix this issue, please assign this ticket to me . ;;;","17/Jul/23 01:18;luoyuxia;master:

e8b118598c6acdbf8a529bb0194988783d452bf4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that all subtasks are sorted by busy ratio at the backpressure tab by default,FLINK-32568,13542979,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,10/Jul/23 09:05,10/Jul/23 14:50,04/Jun/24 20:40,10/Jul/23 14:50,1.18.0,,,,,1.18.0,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,"After FLINK-29998 and FLINK-30468, all subtasks are sorted by busy ratio at the backpressure tab by default.

FLINK-30829 makes the backpressure tab could be sort by busy/backpressure/idle seperately. However, the default sort rule is changed.

Following is the picture about it, all subtask are sorted by 3 columns be default:

!image-2023-07-10-17-04-50-823.png|width=795,height=445!",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30829,,,,,,,,,,,,,,,,"10/Jul/23 09:04;fanrui;image-2023-07-10-17-04-50-823.png;https://issues.apache.org/jira/secure/attachment/13061153/image-2023-07-10-17-04-50-823.png",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 14:50:25 UTC 2023,,,,,,,,,,"0|z1j1ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 14:50;fanrui;Merged via 2a6e8ae (master, 1.18);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when Flink write azure data lake storage,error occur",FLINK-32567,13542957,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,chenyu.mr,chenyu.mr,10/Jul/23 06:28,11/Jul/23 02:00,04/Jun/24 20:40,11/Jul/23 02:00,1.17.1,,,,,,,,,API / DataStream,,,,,,0,,,,"When I strictly followed the official website to perform these two operations, I still reported the wrong certification problem, and I wanted to know how I should turn on the certification

!image-2023-07-10-14-26-22-019.png!

 

 

error:

 

!image-2023-07-10-14-28-11-792.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/23 06:26;chenyu.mr;image-2023-07-10-14-26-22-019.png;https://issues.apache.org/jira/secure/attachment/13061149/image-2023-07-10-14-26-22-019.png","10/Jul/23 06:28;chenyu.mr;image-2023-07-10-14-28-11-792.png;https://issues.apache.org/jira/secure/attachment/13061148/image-2023-07-10-14-28-11-792.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-10 06:28:20.0,,,,,,,,,,"0|z1j1ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HeapBytesVector meets java.lang.NegativeArraySizeException,FLINK-32566,13542869,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,catyee,catyee,08/Jul/23 10:07,07/Aug/23 08:55,04/Jun/24 20:40,,1.17.1,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,"When reading the parquet format files, if the data of some fields is too large(to support large fields, we modified the default maxMessageSize value from 100MB to 300MB of thrift in parquet shaded class : shaded.parquet.org.apache.thrift.TConfiguration), HeapBytesVector will exceed the int maximum value when expending the capacity, and then it will meet the java.lang.NegativeArraySizeException.
{code:java}
// code placeholder
switched from RUNNING to FAILED with failure cause:
java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:422)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:961)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:939)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:749)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:564)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.NegativeArraySizeException
	at org.apache.flink.table.data.columnar.vector.heap.HeapBytesVector.reserve(HeapBytesVector.java:102)
	at org.apache.flink.table.data.columnar.vector.heap.HeapBytesVector.appendBytes(HeapBytesVector.java:79)
	at org.apache.flink.formats.parquet.vector.reader.BytesColumnReader.readBinary(BytesColumnReader.java:88)
	at org.apache.flink.formats.parquet.vector.reader.BytesColumnReader.readBatch(BytesColumnReader.java:50)
	at org.apache.flink.formats.parquet.vector.reader.BytesColumnReader.readBatch(BytesColumnReader.java:31)
	at org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.readToVector(AbstractColumnReader.java:189)
	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.nextBatch(ParquetVectorizedInputFormat.java:401)
	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readBatch(ParquetVectorizedInputFormat.java:369)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162)
	... 6 more  {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 08:55:51 UTC 2023,,,,,,,,,,"0|z1j15c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 04:52;yunta;It seems that the {{HeapBytesVector}} would reserve for many times until the length of array exceeds the max value of Integer. [~lsy], do you have more insights here?;;;","07/Aug/23 08:55;yunta;The root cause, which is analysed by [~catyee], is that the size of the batch to read is hard coded by [VectorizedColumnBatch#DEFAULT_SIZE|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/data/columnar/vector/VectorizedColumnBatch.java#L46], and once we read some huge part, the memory would increase to OOM.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support cast from NUMBER to BYTES,FLINK-32565,13542857,13404080,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hanyuzheng,hanyuzheng,hanyuzheng,07/Jul/23 23:46,03/Sep/23 22:35,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,"We are undertaking a task that requires casting from the DOUBLE type to BYTES In particular, we have a INTEGER 1234. Our current approach is to convert this INTEGER to BYTES  using the following SQL query:
{code:java}
SELECT CAST(1234 as BYTES);{code}
{{ }}
However, we encounter an issue when executing this query, potentially due to an error in the conversion between INTEGER and BYTES. Our goal is to identify and correct this issue so that our query can execute successfully. The tasks involved are:
 # Investigate and pinpoint the specific reason for the conversion failure from INTEGER to BYTES.
 # Design and implement a solution that enables our query to function correctly.
 # Test this solution across all required scenarios to ensure its robustness.

 

see also:

1. PostgreSQL: PostgreSQL supports casting from NUMBER types (INTEGER, BIGINT, DECIMAL, etc.) to BYTES type (BYTEA). In PostgreSQL, you can use CAST or TO_BINARY function for performing the conversion. URL: [https://www.postgresql.org/docs/current/sql-expressions.html#SQL-SYNTAX-TYPE-CASTS]

2. MySQL: MySQL supports casting from NUMBER types (INTEGER, BIGINT, DECIMAL, etc.) to BYTES type (BINARY or BLOB). In MySQL, you can use CAST or CONVERT functions for performing the conversion. URL: [https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html]

3. Microsoft SQL Server: SQL Server supports casting from NUMBER types (INT, BIGINT, NUMERIC, etc.) to BYTES type (VARBINARY or IMAGE). You can use CAST or CONVERT functions for performing the conversion. URL: [https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql]

4. Oracle Database: Oracle supports casting from NUMBER types (NUMBER, INTEGER, FLOAT, etc.) to BYTES type (RAW). You can use UTL_RAW.CAST_TO_RAW function for performing the conversion. URL: [https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TO_BINARY_DOUBLE.html]

 

for the problem of bytes order may arise (little vs big endian). 

 

1. Apache Hadoop: Hadoop, being an open-source framework, has to deal with byte order issues across different platforms and architectures. The Hadoop File System (HDFS) uses a technique called ""sequence files,"" which include metadata to describe the byte order of the data. This metadata ensures that data is read and written correctly, regardless of the endianness of the platform.

2. Apache Avro: Avro is a data serialization system used by various big data frameworks like Hadoop and Apache Kafka. Avro uses a compact binary encoding format that includes a marker for the byte order. This allows Avro to handle endianness issues seamlessly when data is exchanged between systems with different byte orders.

3. Apache Parquet: Parquet is a columnar storage format used in big data processing frameworks like Apache Spark. Parquet uses a little-endian format for encoding numeric values, which is the most common format on modern systems. When reading or writing Parquet data, data processing engines typically handle any necessary byte order conversions transparently.

4. Apache Spark: Spark is a popular big data processing engine that can handle data on distributed systems. It relies on the underlying data formats it reads (e.g., Avro, Parquet, ORC) to manage byte order issues. These formats are designed to handle byte order correctly, ensuring that Spark can handle data correctly on different platforms.

5. Google Cloud BigQuery: BigQuery is a serverless data warehouse offered by Google Cloud. When dealing with binary data and endianness, BigQuery relies on the data encoding format. For example, when loading data in Avro or Parquet formats, these formats already include byte order information, allowing BigQuery to handle data across different platforms correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 03 22:35:06 UTC 2023,,,,,,,,,,"0|z1j12o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 13:10;twalthr;[~hanyuzheng] In general, the use case is valid. But I'm wondering whether we should rather provide such functionality through a special function like ""CONVERT"". AFAIK SQL Server also distinguishes between CAST and CONVERT. The problem of bytes order may arise (little vs big endian). Could you perform some research how other big vendors deal with this problem?;;;","24/Jul/23 18:00;hanyuzheng;[~twalthr] Ok;;;","04/Aug/23 15:09;hanyuzheng;[~twalthr] , Through research, It seem that other vendors use cast but not convert.;;;","03/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support cast from BYTES to NUMBER,FLINK-32564,13542856,13404080,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hanyuzheng,hanyuzheng,hanyuzheng,07/Jul/23 23:42,12/Sep/23 22:35,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,"We are dealing with a task that requires casting from the BYTES type to BIGINT. Specifically, we have a string '00T1p'. Our approach is to convert this string to BYTES and then cast the result to BIGINT with the following SQL query:
{code:java}
SELECT CAST((CAST('00T1p' as BYTES)) as BIGINT);{code}
However, an issue arises when executing this query, likely due to an error in the conversion between BYTES and BIGINT. We aim to identify and rectify this issue so our query can run correctly. The tasks involved are:
 # Investigate and identify the specific reason for the failure of conversion from BYTES to BIGINT.
 # Design and implement a solution to ensure our query can function correctly.
 # Test this solution across all required scenarios to guarantee its functionality.

 

see also

1. PostgreSQL: PostgreSQL supports casting from BYTES type (BYTEA) to NUMBER types (INTEGER, BIGINT, DECIMAL, etc.). In PostgreSQL, you can use CAST or type conversion operator （：：） for performing the conversion. URL: [https://www.postgresql.org/docs/current/sql-expressions.html#SQL-SYNTAX-TYPE-CASTS]

2. MySQL: MySQL supports casting from BYTES type (BLOB or BINARY) to NUMBER types (INTEGER, BIGINT, DECIMAL, etc.). In MySQL, you can use CAST or CONVERT functions for performing the conversion. URL: [https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html]

3. Microsoft SQL Server: SQL Server supports casting from BYTES type (VARBINARY, IMAGE) to NUMBER types (INT, BIGINT, NUMERIC, etc.). You can use CAST or CONVERT functions for performing the conversion. URL: [https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql]

4. Oracle Database: Oracle supports casting from RAW type (equivalent to BYTES) to NUMBER types (NUMBER, INTEGER, FLOAT, etc.). You can use the TO_NUMBER function for performing the conversion. URL: [https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TO_NUMBER.html]

5. Apache Spark: Spark DataFrame supports casting binary types (BinaryType or ByteType) to numeric types (IntegerType, LongType, DecimalType, etc.) by using the {{cast}} function. URL: [https://spark.apache.org/docs/latest/api/sql/#cast]

 

for the problem of bytes order may arise (little vs big endian). 

 

1. Apache Hadoop: Hadoop, being an open-source framework, has to deal with byte order issues across different platforms and architectures. The Hadoop File System (HDFS) uses a technique called ""sequence files,"" which include metadata to describe the byte order of the data. This metadata ensures that data is read and written correctly, regardless of the endianness of the platform.

2. Apache Avro: Avro is a data serialization system used by various big data frameworks like Hadoop and Apache Kafka. Avro uses a compact binary encoding format that includes a marker for the byte order. This allows Avro to handle endianness issues seamlessly when data is exchanged between systems with different byte orders.

3. Apache Parquet: Parquet is a columnar storage format used in big data processing frameworks like Apache Spark. Parquet uses a little-endian format for encoding numeric values, which is the most common format on modern systems. When reading or writing Parquet data, data processing engines typically handle any necessary byte order conversions transparently.

4. Apache Spark: Spark is a popular big data processing engine that can handle data on distributed systems. It relies on the underlying data formats it reads (e.g., Avro, Parquet, ORC) to manage byte order issues. These formats are designed to handle byte order correctly, ensuring that Spark can handle data correctly on different platforms.

5. Google Cloud BigQuery: BigQuery is a serverless data warehouse offered by Google Cloud. When dealing with binary data and endianness, BigQuery relies on the data encoding format. For example, when loading data in Avro or Parquet formats, these formats already include byte order information, allowing BigQuery to handle data across different platforms correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 22:35:04 UTC 2023,,,,,,,,,,"0|z1j12g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 13:10;twalthr;[~hanyuzheng] In general, the use case is valid. But I'm wondering whether we should rather provide such functionality through a special function like ""CONVERT"". AFAIK SQL Server also distinguishes between CAST and CONVERT. The problem of bytes order may arise (little vs big endian). Could you perform some research how other big vendors deal with this problem?;;;","24/Jul/23 22:57;hanyuzheng;[~twalthr] ok;;;","04/Aug/23 15:09;hanyuzheng;[~twalthr] , Through research, It seem that other vendors use cast but not convert.;;;","12/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additionalExcludes property to add exclusions to surefire tests,FLINK-32563,13542787,13565110,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,07/Jul/23 09:16,28/Feb/24 16:17,04/Jun/24 20:40,17/Jan/24 16:06,,,,,,connector-parent-1.1.0,,,,Connectors / Parent,,,,,,0,pull-request-available,,,Add an optional property to add exclusions to surefire tests (among other things for skipping archunit tests),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 11 14:28:06 UTC 2023,,,,,,,,,,"0|z1j0n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 09:41;martijnvisser;I think the ticket naming is slightly incorrect, since we already support specifying specific Flink version that should be tested. See https://github.com/apache/flink-connector-shared-utils/blob/ci_utils/.github/workflows/_testing.yml#L25-L28

From the discussion thread I read it as that you want to be able to skip archunit for specific versions? ;;;","07/Jul/23 13:46;echauchot;[~martijnvisser]

> From the discussion thread I read it as that you want to be able to skip archunit for specific versions?

Yes, disable it for all the versions except the main supported by the connector

> See [https://github.com/apache/flink-connector-shared-utils/blob/ci_utils/.github/workflows/_testing.yml#L25-L28]

I did not know __testing.yml workflow. What is the link with connector/push_pr.yml ? I see both of them call ci.yml@ci_utils. Also both run ci.yml workflow on various flink versions but only push_pr.yml allows the connector author to specify which flink versions to test his connector against.;;;","07/Jul/23 15:15;martijnvisser;[~echauchot] The _testing.yml is the testing implementation of ci_utils. So it's just a display of how you can currently test/integrate the available tools;;;","11/Jul/23 16:53;echauchot;[~martijnvisser] I had in mind to be slightly more coercive with connector authors so that they run CI test on last 2 versions and specify which of the 2 is main supported one (for running archunit but also other things). I was thinking of something like this (in _testing.yml style):
{code:java}
jobs:
  compile_and_test:
    strategy:
      matrix:
        include:
          - flink: 1.16.2
            main_version: false
          - flink: 1.17.1
            main_version: true
    uses: ./.github/workflows/ci.yml
    with:
      connector_branch: ci_utils
      flink_version: ${{ matrix.flink }}
      main_flink_version: ${{ matrix.main_version }}
{code}
{code:java}
    inputs:
        main_flink_version:
        description: ""Is the input Flink version, the main version that the connector supports.""
        required: false  // to avoid break the existing connectors
        type: boolean
        default: false
{code}
Do you prefer something like this ?
{code:java}
jobs:
  enable-archunit-tests:
    uses: ./.github/workflows/ci.yml
    with:
      flink_version: 1.17.1
      connector_branch: ci_utils
      run_archunit_tests: true
{code}
{code:java}
    inputs:
      run_archunit_tests:
        description: ""Whether to run the archunit tests""
        required: false // to avoid break the existing connectors
        type: boolean
        default: false 
{code};;;","12/Jul/23 14:34;echauchot;Also the other option (proposed privately by [~chesnay] ) is to integrate archunit tests into the dependency-convergence maven profile. That would allow to have similar sanity checks in the same profile and reuse what is already in place for the run_dependency_convergence github input variable.;;;","11/Aug/23 12:05;echauchot;[~chesnay] [~martijnvisser] WDYT ?;;;","11/Oct/23 14:28;echauchot;I'm implementing that : link together the {{dependency-convergence}} and the {{archunit}} tests inside a sanity-check group. Enable these checks only on connector's main flink version (the one the connector is built against) and disable them for all other versions including snapshots;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink Compactor Service should not use FileWriter from Sink for OutputStreamBasedFileCompactor,FLINK-32562,13542785,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,ysn2233,ysn2233,07/Jul/23 09:02,04/Jun/24 10:53,04/Jun/24 20:40,,1.18.0,,,,,,,,,Connectors / FileSystem,,,,,,0,,,,"Gzip format is designed to be concatenatable but it will be broken by Compactor in FileSink. 

It is because when Compactor Service create new compacted file by using GzipOutputStream, which will create extra bytes at header, which cause the final file will have extra bytes in header. (Gzip header is presented in every finished part file, we don't need an extra header in compacted file). This is because in Compactor Service, it uses the FileWriter specified in FileSink to create the compacted outputstream. I think will should use an simple bytes ouputstream to concat stream instead, or at least give a option.

 

Currently the ConcatFileCompactor only supports pure text file. Many compressed codec support concating like gzip, bzip2. I think we should support those kind of concating, otherwise people must use RecordWiseCompactorFactor which is very ineffcient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-07 09:02:56.0,,,,,,,,,,"0|z1j0mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the status field reconciliationTimestamp from long to Date,FLINK-32561,13542784,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,haoxin,haoxin,07/Jul/23 08:57,11/Jul/23 06:46,04/Jun/24 20:40,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"Can we change the field `status.reconciliationStatus.reconciliationTimestamp` from long to date?

 

At first, this is a broken change for the CRD.

The benefit is that:
 # The date format is more human-readable, this is useful when we debug issues.
 # It will be easy to add this field into additionalPrinterColumns with date duration format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 06:46:03 UTC 2023,,,,,,,,,,"0|z1j0mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/23 06:28;gyfora;We can only change it if we can make it non-breaking otherwise we can make this improvement later when we go from v1beta1 to v1;;;","11/Jul/23 06:46;haoxin;Agreed~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly deprecate all Scala APIs,FLINK-32560,13542782,13542776,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,rskraba,xtsong,xtsong,07/Jul/23 08:44,03/Aug/23 02:10,04/Jun/24 20:40,03/Aug/23 02:10,,,,,,1.18.0,,,,API / Scala,,,,,,0,pull-request-available,,,"We agreed to drop Scala API support in FLIP-265 [1], and have tried to deprecate them in FLINK-29740. Also, both user documentation and roadmap[2] shows that scala API supports are deprecated. However, none of the APIs in `flink-streaming-scala` are annotated with `@Deprecated` atm, and only `ExecutionEnvironment` and `package` are marked `@Deprecated` in `flink-scala`.

[1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-265+Deprecate+and+remove+Scala+API+support
[2] https://flink.apache.org/roadmap/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29740,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 02:10:00 UTC 2023,,,,,,,,,,"0|z1j0m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/23 11:24;rskraba;I can probably help out here!;;;","11/Jul/23 11:27;Sergey Nuyanzin;thanks for volunteering [~rskraba] 

[~xtsong] do you mind if Ryan will help here?;;;","11/Jul/23 12:26;xtsong;The help is very much welcomed. Thanks [~rskraba] and [~Sergey Nuyanzin].;;;","11/Jul/23 13:50;rskraba;OK -- taking a quick look at the current work, I'll take a look at all modules outside of table-planner and add the following annotation and comment *every single time*:

{code}
/**
 * @deprecated
 *   All Flink Scala APIs are deprecated and will be removed in a future Flink version version. You
 *   can still build your application in Scala, but you should move to the Java version of either
 *   the DataStream and/or Table API.
 * @see
 *   <a
 *   href=""https://cwiki.apache.org/confluence/display/FLINK/FLIP-265+Deprecate+and+remove+Scala+API+support"">
 *   FLIP-265 Deprecate and remove Scala API support</a>
 */
@Deprecated
{code}

There's also some {{@Public}} scala code in flink-hadoop-compatibility, and even if {{table-planner}} isn't in scope yet, the {{table-table-api-scala}} and {{table-table-api-scala-bridge}} are likely also candidates for clean-up.;;;","24/Jul/23 09:05;twalthr;[~xtsong] please consider [my comment|https://github.com/apache/flink/pull/23006#pullrequestreview-1543006419] in the PR around this topic.;;;","24/Jul/23 09:38;xtsong;[~twalthr],

Crossposting your comment here in order to keep the discussions at one place.
{quote}TBH I'm questioning this PR. We only deprecated the entry points to the API because we wanted to keep the API usable while it still exists. Nobody wants to develop in an API where every single method is deprecated. I'm fine maybe with class-level deprecations if we need more of those. But spamming the codebase with deprecations while everyones knows that the entire API will be removed, is not very helpful. It will also spam CI/CD logs with hundreds of warnings.
{quote}
I see your point, but I tend to think this differently. Technically speaking, yes, by deprecating the entry points we implicitly send the message that all Scala APIs are deprecated. However, that requires users to carefully read the deprecation message, which many of the users don't from my experience. IMHO, trying to catch the attention of the careless users is probably more important than not disturbing the users who insists to work with the deprecated Scala API.
{quote}I'm fine maybe with class-level deprecations if we need more of those.
{quote}
+1. There are many methods annotated because they have different API stability from the containing class. E.g., the class is annotated as `@Public` while some of its methods are annotated as `@PublicEvolving`. For such methods, I think it's good enough to mark deprecation at the class level.;;;","24/Jul/23 12:34;rskraba;Hello -- I tend to agree that the PR is over-annotated!  This was a bit of fastidious work to make sure that we met ""the Letter of the Law"" for deprecating every exposed API.  This case is slightly different because we're dropping the entire module (as opposed to _guiding_ the developer from one method or class to another).  I think the intention is pretty well met by deprecating the entrypoints.

Adding the class-level annotations for 1.18 could be the compromise, especially because the Scala API is less maintained and tested we can expect it to degrade over time, but I have no objection to just closing the PR.

Taking this into account, I propose:
* We consider the current state (the entrypoints already deprecated) to be sufficient for removing the Scala API in 2.0, and this PR is just to give some *extra* heads up to developers.
* Avoid deprecating methods, only classes, regardless of explicit visibility annotations on annotations.
* Prefer the {{@scala.deprecated}} annotation on Scala code, but don't worry too much about it.  My common tooling (IntelliJ) treats them as the same.  As a scala dev, I prefer the scala annotation on scala code.  Scaladoc notes both, but puts the scala annotation in a separate ""Deprecated"" section.

Regardless, spotless breaks up the link to the FLIP-265 and it's not rendered correctly in scaladoc APIs.  This is an easy fix.

If we agree on this, I can update the PR, or we can leave it all as it is!;;;","25/Jul/23 01:42;xtsong;Thanks, [~rskraba]. Your proposal sounds good to me. The purpose of this PR is indeed to get more attentions on the deprecation of Scala APIs, rather than legalizing its removal in 2.0.;;;","25/Jul/23 08:30;twalthr;Thanks for considering my feedback. And thanks for the nice summary [~rskraba]. Class-level annotations sound like a nice trade-off between annotating everything vs. annotating only entry points.;;;","03/Aug/23 02:10;xtsong;master (1.18): 255d83087c6a6432270e6886ffdcf85dae00c241;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Queryable State,FLINK-32559,13542780,13542776,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xtsong,xtsong,xtsong,07/Jul/23 08:37,02/Aug/23 07:46,04/Jun/24 20:40,02/Aug/23 07:46,,,,,,1.18.0,,,,Runtime / Queryable State,,,,,,0,pull-request-available,,,"Queryable State is described as approaching end-of-life in the roadmap [1], but is neither deprecated in codes nor in user documentation [2]. There're also more negative opinions than positive ones in the discussion about rescuing it [3].

[1] https://flink.apache.org/roadmap/
[2] https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/fault-tolerance/queryable_state/
[3] https://lists.apache.org/thread/9hmwcjb3q5c24pk3qshjvybfqk62v17m",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 07:46:15 UTC 2023,,,,,,,,,,"0|z1j0lk:",9223372036854775807,The Queryable State feature is formally deprecated. It will be removed in future major version bumps.,,,,,,,,,,,,,,,,,,,"02/Aug/23 07:46;xtsong;master (1.18): 8db8119138b492e16969fd363577efa082102538;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly deprecate DataSet API,FLINK-32558,13542777,13542776,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,xtsong,xtsong,07/Jul/23 08:20,11/Aug/23 10:25,04/Jun/24 20:40,04/Aug/23 10:17,,,,,,1.18.0,,,,API / DataSet,,,,,,0,pull-request-available,,,"DataSet API is described as ""legacy"", ""soft deprecated"" in user documentation [1]. The required tasks for formally deprecating / removing it, according to FLIP-131 [2], are all completed.

This task include marking all related API classes as `@Deprecated` and update the user documentation.

[1] https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/dataset/overview/
[2] https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158866741",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32821,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 04 10:17:56 UTC 2023,,,,,,,,,,"0|z1j0kw:",9223372036854775807,"DataSet API is formally deprecated, and will be removed in the next major release.",,,,,,,,,,,,,,,,,,,"04/Aug/23 10:17;xtsong;master (1.18): aa98c18d2ba975479fcfa4930b0139fa575d303e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
API deprecations in Flink 1.18,FLINK-32557,13542776,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,xtsong,xtsong,07/Jul/23 08:15,01/Dec/23 06:39,04/Jun/24 20:40,04/Aug/23 10:18,,,,,,1.18.0,,,,,,,,,,0,,,,"As discussed in [1], we are deprecating multiple APIs in release 1.18, in order to completely remove them in release 2.0.

The listed APIs possibly should have been deprecated already, i.e., already (or won't) have replacements, but are somehow not yet.

[1] [https://lists.apache.org/thread/3dw4f8frlg8hzlv324ql7n2755bzs9h|https://lists.apache.org/thread/3dw4f8frlg8hzlv324ql7n2755bzs9hy]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 09:11:58 UTC 2023,,,,,,,,,,"0|z1j0ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 08:46;xtsong;{{SourceFunction}} are already annotated as `@Deprecated` in FLINK-28045.;;;","01/Aug/23 09:11;renqs;As discussed in the release sync, these deprecations will not block 1.18 release. Downgrading to Major.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Renames contenderID into componentId,FLINK-32556,13542763,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,07/Jul/23 06:30,10/Jul/23 12:45,04/Jun/24 20:40,10/Jul/23 12:45,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"We introduced {{contenderID}} in a lot of places with FLINK-26522. The original multi-component leader election classes of FLINK-24038 used {{componentId}}.

Revisiting that naming made me realize that it's actually wrong. A contender is a specific instance of a component that participates in the leader election. A component, in this sense, is the more abstract concept. {{contenderID}} refers to an ID for the specific contender instance but the IDs we're sharing are actually referring to a Flink component and therefore, are the same between different contenders which compete for leadership for the same component. This contradicts the definition of an identifier.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 12:45:09 UTC 2023,,,,,,,,,,"0|z1j0hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 12:45;mapohl;master: 773feebbb2426ab1a8f7684f59b9a73db8f6a613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a post to record SIGMOD System Award for Apache Flink,FLINK-32555,13542750,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,leonard,leonard,07/Jul/23 03:21,10/Jul/23 04:33,04/Jun/24 20:40,10/Jul/23 04:33,,,,,,1.18.0,,,,Documentation,,,,,,0,pull-request-available,,,"As you all known, Apache Flink has won the 2023 SIGMOD Systems Award [1].

It will be helpful to promote Apache Flink if we can add a page community document to let our users know.


[1][https://sigmod.org/2023-sigmod-systems-award/]

[2]https://spark.apache.org/news/sigmod-system-award.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 04:33:16 UTC 2023,,,,,,,,,,"0|z1j0ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 03:30;ruanhang1993;I'd like to help. Please assign this to me. Thanks.;;;","07/Jul/23 03:49;leonard;Thank you for taking this, I've assigned to you [~ruanhang1993] 
Maybe write a simple blog post for flink-web is enough.;;;","10/Jul/23 04:33;leonard;Resolved in flink-web(asf-site): 65990f39399a51b9d010c4fab110d4d6ca71f127;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Facilitate slot isolation and resource management for global committer,FLINK-32554,13542739,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,allenxwang,allenxwang,06/Jul/23 21:22,07/Jul/23 12:44,04/Jun/24 20:40,,1.16.2,,,,,,,,,API / Core,Connectors / Common,,,,,0,,,,"Flink's global committer executes unique workload compared to the source and sink operators. In some use cases, it may require much higher amount of resources (CPU, memory) than other operators. However, according to this [source code|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/connector/sink2/StandardSinkTopologies.java], currently it is not possible to isolate the global committer to a dedicated task manager or task slot, or assign more resources to it by leveraging the fine grained resource management. Flink would always make the global committer task share with another task in a task slot. (In one test, we tried to have one more task slot than required by the source/sink parallelism, but Flink still assigns the global committer to share a slot with another task.)

As a result, we often see CPU utilization spike on the task manger that runs the global committer compared with other task managers and becomes the bottleneck for the job. Due to slot sharing and inadequate resources on the global committer, the job takes long time to initialize upon restarting and the checkpoints take long time to complete. Our job consumes from Kafka and this bottleneck causes significant increase of consumer lag. The lag in turn causes the Kafka source operator to replay backlogs, causing more CPU consumption on the source operator and making it worse for the global committer that runs in the same task slot.

At minimum, we want the capability to configure the global committer to run in its own task slot, and make that work under reactive scaling. It would also be great to make the fine grained resource management working for global committer.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-06 21:22:28.0,,,,,,,,,,"0|z1j0cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterEntrypointTest.testCloseAsyncShouldNotDeregisterApp failed on AZP,FLINK-32553,13542700,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,06/Jul/23 13:38,18/Aug/23 22:35,04/Jun/24 20:40,,1.17.2,,,,,,,,,Runtime / Coordination,,,,,,0,auto-deprioritized-critical,test-stability,,"
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51013&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7961
{noformat}
Jul 06 05:38:37 [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 71.304 s <<< FAILURE! - in org.apache.flink.runtime.entrypoint.ClusterEntrypointTest
Jul 06 05:38:37 [ERROR] org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.testCloseAsyncShouldNotDeregisterApp  Time elapsed: 22.51 s  <<< ERROR!
Jul 06 05:38:37 org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint TestingEntryPoint.
Jul 06 05:38:37 	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:255)
Jul 06 05:38:37 	at org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.startClusterEntrypoint(ClusterEntrypointTest.java:347)
Jul 06 05:38:37 	at org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.testCloseAsyncShouldNotDeregisterApp(ClusterEntrypointTest.java:175)
Jul 06 05:38:37 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 06 05:38:37 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 06 05:38:37 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 06 05:38:37 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 06 05:38:37 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 06 05:38:37 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 06 05:38:37 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:10 UTC 2023,,,,,,,,,,"0|z1j040:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed up Flink session job deployments,FLINK-32552,13542697,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,fabiowanner,fabiowanner,06/Jul/23 13:38,19/Jul/23 11:50,04/Jun/24 20:40,19/Jul/23 11:50,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"*Context*

In the scope of end-to-end tests we deploy all the Flink session jobs we have regularly in a staging environment. Some of the jobs are bundled together in one helm chart and therefore deployed at the same time. There are around 40 individual Flink jobs (running on the same Flink session cluster). The session cluster is individual for each e2e test run. The problems described below happen scarcely (1 in ~ 50 run maybe).

*Problem*

Rarely the operator seems to ""mix up"" the deployments. This can be seen in the Flink cluster logs as multiple {{Received JobGraph submission '<JOB NAME>' (<JOB_ID>)}} logs are created from jobs with the same job_id. This results in errors such as:

{{DuplicateJobSubmissionException}} or {{ClassNotFoundException.}}

It' also visible in the FlinkSessionJob resource: status.jobStatus.jobName does not match the expected job name of the job being deployed (The job name is passed to the application via argument).

So far we were unable to reliably reproduce the error.

*Details*

The following lines show the status of 3 jobs form the view point of the Flink cluster dashboard, and the FlinkSessionJob ressource:

 

*aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615*

Apache Flink Dashboard:
 * State: Restarting
 * ID: a7d36f3881f943a00000000000000002
 * Exceptions: Cannot load user class: aelps.pipelines.aletsch.smc.SMCUrlMapper

FlinkSessionJob Ressource:
 * State: RUNNING
 * jobId: a1221c743367497b0000000000000002
 * uid: a1221c74-3367-497b-ad2f-8793ab23919d

 

*aletsch_mat_e5730831db8092adb12f5189c4c895ef3a268615*

Apache Flink Dashboard:
 * State: -
 * ID: -

FlinkSessionJob Ressource:
 * State: UPGRADING
 * jobId: -
 * uid: a7d36f38-81f9-43a0-898f-19b950430e9d

Flink K8s Operator:
 * Exceptions: DuplicateJobSubmissionException: Job has already been submitted.

 

*aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615*

Apache Flink Dashboard:
 * State: Running
 * ID: e692c2dfaa18441c0000000000000002
 * Exceptions: -

FlinkSessionJob Ressource:
 * State: RUNNING
 * jobId: e692c2dfaa18441c0000000000000002
 * uid: e692c2df-aa18-441c-a352-88aefa9a3017

As we can see the *aletsch_smc* job is presumably running according to the FlinkSessionJob resource, but crash-looping in the cluster and it has the jobID matching the uid of the resource of {*}aletsch_mat{*}. While *aletsch_mat* is not even running. The following logs also show some suspicious entries: There are several {{Received JobGraph submission}} from different jobs with the same jobID.

 

*Logs*

The logs are filtered by the 3 jobIds from above.

 

JobID: a7d36f3881f943a00000000000000002
{code:bash}
Flink Cluster
    ...
    023-07-06 10:23:50,552 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002) switched from state RUNNING to RESTARTING.
    2023-07-06 10:23:50	    file: '/tmp/tm_10.0.11.159:6122-e9fadc/blobStorage/job_a7d36f3881f943a00000000000000002/blob_p-40c7a30adef8868254191d2cf2dbc4cb7ab46f0d-8a02a0583d91c5e8e6c94f378aa444c2' (valid JAR)
    2023-07-06 10:23:50,522 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
    2023-07-06 10:23:50,522 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
    2023-07-06 10:23:50,522 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
    2023-07-06 10:23:50,522 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
    2023-07-06 10:23:50,512 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002) switched from state RESTARTING to RUNNING.
    2023-07-06 10:23:48,979 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job a7d36f3881f943a00000000000000002
    2023-07-06 10:23:48,853 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
    2023-07-06 10:23:48,853 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
    2023-07-06 10:23:48,853 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
    2023-07-06 10:23:48	    file: '/tmp/tm_10.0.11.159:6122-e9fadc/blobStorage/job_a7d36f3881f943a00000000000000002/blob_p-40c7a30adef8868254191d2cf2dbc4cb7ab46f0d-8a02a0583d91c5e8e6c94f378aa444c2' (valid JAR)
    2023-07-06 10:23:48,661 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002) switched from state RUNNING to RESTARTING.
    2023-07-06 10:23:48,583 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
    2023-07-06 10:23:48,583 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
    2023-07-06 10:23:48,583 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
    2023-07-06 10:23:48,582 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
    2023-07-06 10:23:48,573 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002) switched from state RESTARTING to RUNNING.
    2023-07-06 10:23:47,562 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'aletsch_mat_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:47,518 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job a7d36f3881f943a00000000000000002
    2023-07-06 10:23:47,517 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
    2023-07-06 10:23:47,517 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
    2023-07-06 10:23:47,516 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
    2023-07-06 10:23:47,463 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:47,463 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job a7d36f3881f943a00000000000000002 is submitted.
    2023-07-06 10:23:47,104 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002) switched from state RUNNING to RESTARTING.
    2023-07-06 10:23:46,804 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:46,804 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:46,799 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_2 for job a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:46,577 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 221b24b50413805c9e35d7620b8a00b8 for job a7d36f3881f943a00000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:46,577 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 49d3c8cd1080bd38c0144c3d3cc597cd for job a7d36f3881f943a00000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:46,577 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 819f34cc8957066478fb4b3549367d24 for job a7d36f3881f943a00000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:46,574 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job a7d36f3881f943a00000000000000002 for job leader monitoring.
    2023-07-06 10:23:46,570 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 36802a7de1487f3fb1b6a3b509bd5e20 for job a7d36f3881f943a00000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:46,560 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a7d36f3881f943a00000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
    2023-07-06 10:23:46,556 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager aaa9331f70b07a195b5f09d57d1b40c5@akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_2 for job a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:46,528 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager aaa9331f70b07a195b5f09d57d1b40c5@akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_2 for job a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:46,480 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002) switched from state CREATED to RUNNING.
    2023-07-06 10:23:46,476 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002) under job master id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:46,466 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@62877000 for aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:46,079 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:46,059 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-a7d36f3881f943a00000000000000002-config-map'}.
    2023-07-06 10:23:46,051 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-a7d36f3881f943a00000000000000002-config-map'}.
    2023-07-06 10:23:46,006 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy ExponentialDelayRestartBackoffTimeStrategy(initialBackoffMS=1000, maxBackoffMS=300000, backoffMultiplier=2.0, resetBackoffThresholdMS=3600000, jitterFactor=0.5, currentBackoffMS=1000, lastFailureTimestamp=0) for aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:45,987 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:45,966 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:45,965 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'aletsch_mat_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:45,915 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Added JobGraph(jobId: a7d36f3881f943a00000000000000002) to KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-cluster-config-map'}.
    2023-07-06 10:23:45,859 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:45,857 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a7d36f3881f943a00000000000000002).
    2023-07-06 10:23:45,705 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:45,705 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job a7d36f3881f943a00000000000000002 is submitted.
    2023-07-06 10:23:45,705 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:45,705 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job a7d36f3881f943a00000000000000002 is submitted.
    2023-07-06 10:23:45,705 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=a7d36f3881f943a00000000000000002.
    2023-07-06 10:23:45,705 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job a7d36f3881f943a00000000000000002 is submitted.

    Flink Operator
    2023-07-06 10:26:25,792 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
    2023-07-06 10:25:05,163 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
    2023-07-06 10:24:24,553 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
    2023-07-06 10:24:03,850 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
    2023-07-06 10:23:53,094 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
    2023-07-06 10:23:47,346 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
    2023-07-06 10:23:45,372 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-mat-staging-e5730831] Submitting job: a7d36f3881f943a00000000000000002 to session cluster.
{code}
 
JobID: a1221c743367497b0000000000000002
{code:bash}
Flink Cluster
    2023-07-06 11:23:48,062 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job a1221c743367497b0000000000000002 (48548 bytes, checkpointDuration=107 ms, finalizationTime=33 ms).
    2023-07-06 11:23:47,937 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688635427922 for job a1221c743367497b0000000000000002.
    2023-07-06 10:23:48,567 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job a1221c743367497b0000000000000002.
    2023-07-06 10:23:48,567 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job a1221c743367497b0000000000000002.
    2023-07-06 10:23:48,567 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_7 for job a1221c743367497b0000000000000002.
    2023-07-06 10:23:48,009 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request cae6932e2409d5fece3f6b4636e3c71a for job a1221c743367497b0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,003 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 8a57f3ecff07d300aebb33f6b3545aed for job a1221c743367497b0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,003 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 7a4a0cfd16eec4a1cb043cce5f989db0 for job a1221c743367497b0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,002 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job a1221c743367497b0000000000000002 for job leader monitoring.
    2023-07-06 10:23:48,002 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 92cbc64513fa703e4acf28bbb3088a58 for job a1221c743367497b0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,999 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job a1221c743367497b0000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
    2023-07-06 10:23:47,998 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager aaa9331f70b07a195b5f09d57d1b40c5@akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_7 for job a1221c743367497b0000000000000002.
    2023-07-06 10:23:47,953 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager aaa9331f70b07a195b5f09d57d1b40c5@akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_7 for job a1221c743367497b0000000000000002.
    2023-07-06 10:23:47,922 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a1221c743367497b0000000000000002) switched from state CREATED to RUNNING.
    2023-07-06 10:23:47,887 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a1221c743367497b0000000000000002) under job master id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:47,887 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@2222ba4d for aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a1221c743367497b0000000000000002).
    2023-07-06 10:23:47,880 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a1221c743367497b0000000000000002).
    2023-07-06 10:23:47,872 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-a1221c743367497b0000000000000002-config-map'}.
    2023-07-06 10:23:47,867 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-a1221c743367497b0000000000000002-config-map'}.
    2023-07-06 10:23:47,832 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy ExponentialDelayRestartBackoffTimeStrategy(initialBackoffMS=1000, maxBackoffMS=300000, backoffMultiplier=2.0, resetBackoffThresholdMS=3600000, jitterFactor=0.5, currentBackoffMS=1000, lastFailureTimestamp=0) for aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615 (a1221c743367497b0000000000000002).
    2023-07-06 10:23:47,832 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a1221c743367497b0000000000000002).
    2023-07-06 10:23:47,820 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Added JobGraph(jobId: a1221c743367497b0000000000000002) to KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-cluster-config-map'}.
    2023-07-06 10:23:47,780 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a1221c743367497b0000000000000002).
    2023-07-06 10:23:47,776 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'aletsch_smc_e5730831db8092adb12f5189c4c895ef3a268615' (a1221c743367497b0000000000000002).
    2023-07-06 10:23:47,668 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=a1221c743367497b0000000000000002.
    2023-07-06 10:23:47,668 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job a1221c743367497b0000000000000002 is submitted.

    Flink Operator
    2023-07-06 10:23:48,007 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-smc-staging-e5730831] Submitted job: a1221c743367497b0000000000000002 to session cluster.
    2023-07-06 10:23:47,505 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-smc-staging-e5730831] Submitting job: a1221c743367497b0000000000000002 to session cluster.
    2023-07-06 10:23:45,416 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-smc-staging-e5730831] Submitting job: a1221c743367497b0000000000000002 to session cluster.
{code}
JobID: e692c2dfaa18441c0000000000000002
{code:bash}
Flink Cluster
    2023-07-06 11:23:48,004 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job e692c2dfaa18441c0000000000000002 (8194 bytes, checkpointDuration=125 ms, finalizationTime=28 ms).
    2023-07-06 11:23:47,867 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688635427851 for job e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:48,568 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:48,568 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:48,568 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_6 for job e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:48,002 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 5e5a0e55fac280bf31abf29a20bce684 for job e692c2dfaa18441c0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,002 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 1cdbce54f4376a1df86430f97dab6858 for job e692c2dfaa18441c0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,002 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 352db7288d0e4d1775d5f52dd14c769d for job e692c2dfaa18441c0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,001 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job e692c2dfaa18441c0000000000000002 for job leader monitoring.
    2023-07-06 10:23:48,000 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request bffed3e4a4c8573049a4119bd7e15f19 for job e692c2dfaa18441c0000000000000002 from resource manager with leader id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:48,998 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job e692c2dfaa18441c0000000000000002: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
    2023-07-06 10:23:47,998 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager aaa9331f70b07a195b5f09d57d1b40c5@akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_6 for job e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:47,953 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager aaa9331f70b07a195b5f09d57d1b40c5@akka.tcp://flink@10.0.11.158:6123/user/rpc/jobmanager_6 for job e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:47,851 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615 (e692c2dfaa18441c0000000000000002) switched from state CREATED to RUNNING.
    2023-07-06 10:23:47,845 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615' (e692c2dfaa18441c0000000000000002) under job master id aaa9331f70b07a195b5f09d57d1b40c5.
    2023-07-06 10:23:47,844 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@7eeab246 for aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615 (e692c2dfaa18441c0000000000000002).
    2023-07-06 10:23:47,834 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615 (e692c2dfaa18441c0000000000000002).
    2023-07-06 10:23:47,825 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-e692c2dfaa18441c0000000000000002-config-map'}.
    2023-07-06 10:23:47,813 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-e692c2dfaa18441c0000000000000002-config-map'}.
    2023-07-06 10:23:47,782 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy ExponentialDelayRestartBackoffTimeStrategy(initialBackoffMS=1000, maxBackoffMS=300000, backoffMultiplier=2.0, resetBackoffThresholdMS=3600000, jitterFactor=0.5, currentBackoffMS=1000, lastFailureTimestamp=0) for aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615 (e692c2dfaa18441c0000000000000002).
    2023-07-06 10:23:47,781 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615' (e692c2dfaa18441c0000000000000002).
    2023-07-06 10:23:47,774 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Added JobGraph(jobId: e692c2dfaa18441c0000000000000002) to KubernetesStateHandleStore{configMapName='flink-cluster-aelps-staging-e5730831-cluster-config-map'}.
    2023-07-06 10:23:47,703 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615' (e692c2dfaa18441c0000000000000002).
    2023-07-06 10:23:47,702 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'aletsch_wp_wafer_e5730831db8092adb12f5189c4c895ef3a268615' (e692c2dfaa18441c0000000000000002).
    2023-07-06 10:23:47,650 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=e692c2dfaa18441c0000000000000002.
    2023-07-06 10:23:47,650 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job e692c2dfaa18441c0000000000000002 is submitted.

    Flink Operator
    2023-07-06 10:23:47,973 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-wp-wafer-staging-e5730831] Submitted job: e692c2dfaa18441c0000000000000002 to session cluster.
    2023-07-06 10:23:47,505 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-wp-wafer-staging-e5730831] Submitting job: e692c2dfaa18441c0000000000000002 to session cluster.
    2023-07-06 10:23:45,374 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/aletsch-wp-wafer-staging-e5730831] Submitting job: e692c2dfaa18441c0000000000000002 to session cluster.
{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-32592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 12:29:59 UTC 2023,,,,,,,,,,"0|z1j03c:",9223372036854775807,Not a bug of the flink k8s operator.,,,,,,,,,,,,,,,,,,,"07/Jul/23 04:52;fabiowanner;I would very much appreciate some pointers on where this issue could origin from. So far I was unable to find the root cause (or reproduce it reliably). Ahh and we are running the operator on version 1.5 with this fix included: https://issues.apache.org/jira/browse/FLINK-32412;;;","12/Jul/23 14:50;fabiowanner;I dug a bit deeper and found the following:

Anytime when the error happens there is one job deployment with this exception:

{code:java}
org.apache.flink.runtime.rest.handler.RestHandlerException: No jobs included in application.{code}

This job will be deployed again by the operator and will run without any problems. 

But one of the other jobs deployed in parallel will get mixed up with the failed one and will not get running (using jar file of the failed one leading to classNotFound or same jobId and/or same name)...;;;","14/Jul/23 10:42;fabiowanner;It's definitely not a bug in the operator:

I deployed 4 different jobs at the same time and observed the following:
4 calls are made of the method JarRunHandler.handleRequest() and when looking at the request 4 distinct ids for the 4 jobs are present, but the EmbeddedExecutor's submitAndGetJobClientFuture() method (also called 4 times) will have 3 distinct optJobIds and one duplicate, leading to different flavors of the problem described in this but ticket (depending on the exact timing of the parallel job launches). Btw: Flink versions 1.15.4 & 1.17.1. show this issue.

I have opened a new bug ticket with the relevant information: https://issues.apache.org/jira/browse/FLINK-32592;;;","14/Jul/23 12:29;fabiowanner;BTW: As a really bad workaround the problem can be ""fixed"" by adding big enough random delays in the AbstractFlinkResourceReconcilers reconcile() method on first deployment...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide the possibility to take a savepoint when deleting a flinkdeployment,FLINK-32551,13542665,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,06/Jul/23 09:29,26/Jul/23 06:02,04/Jun/24 20:40,26/Jul/23 06:02,,,,,,kubernetes-operator-1.6.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"Currently if a flinkdeployment is deleted all the HA metadata is removed and no savepoint is taken.

It would be great (for ex. in case of fat finger) to be able to configure deployment in order to take a savepoint if the deployment is deleted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 06:02:33 UTC 2023,,,,,,,,,,"0|z1izw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 09:40;gyfora;What would be the expected behaviour on failing jobs or if the savepoint times out/fails for any reason? This may block deletion for a very long time.;;;","06/Jul/23 11:18;nfraison.datadog;It is probably not perfect but if the user select this option it probably means that he really wants it or wants to manage it by himself if there are some issues to take it before to delete the app

So I would failed the deletion if the savepoint failed and retry.;;;","06/Jul/23 11:22;gyfora;In case the job is accidentally deleted, you would have the last checkpoint available anyways no?
So to some extent accidental deletions are covered.

If this is intentional user can first suspend the job and once suspended, delete it right? ;;;","06/Jul/23 11:36;nfraison.datadog;Yes we could in case of accidental deletion or intentional deletion rely on those actions.

Still I would prefer not to have manual actions from user when doing such or having users to search for last checkpoint in case of accidental deletion (just looking at a log indicating the path of the savepoint taken during job cancelation)

Looks to me also more secure;;;","06/Jul/23 11:42;gyfora;Makes sense , would you like to work on this?;;;","06/Jul/23 11:44;nfraison.datadog;Yes;;;","26/Jul/23 06:02;gyfora;merged to main 18903878cbeff57fa1ad10ccd66cec3d68de7aa2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Single-parallel task consumes multi-partition data exception in pulsarSource-connector,FLINK-32550,13542648,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jianjiao,jianjiao,06/Jul/23 07:45,13/Jul/23 06:50,04/Jun/24 20:40,13/Jul/23 06:50,1.16.0,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,"when i use flink datastream:pulsar-connector. I found that something error when consume data from pulsar.
Send data to pulsar every second through the program. When there is a partitioned-topic with 3 partitions in pulsar, use the Exclusive consumption mode for consumption (verified, it is not a data problem, because there is no abnormal data in normal tasks)

start A 3-parallel task consumes this topic, and everything is normal; but when I use a single-parallel task to consume this topic, an exception occurs when some data is deserialized, and the avro format data is used, so an error is reported (Length is negative: -52); When I create a partitioned-topic with a single partition, I use a single parallelism task to consume, and there is no such problem, all the data is normal, and there is no task error.
I checked the logs and found that the consumer and topic-partition are allocated as expected.
Summarize:
3 topic-partitions and 1 consume work with exception (3 partitions are allocated to a single consumer)
3 topic-partitions and 3 consumers work normally
1 topic-partition 1 consumer works normally

importance:
As long as the number of consumers is less than the number of pulsar topic-partitions, only one consumer will be processing data, and other consumers will not be able to process data properly","flink 1.16.0

pulsar 2.11.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2023-07-06 07:45:12.0,,,,,,,,,,"0|z1izsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tiered storage memory manager supports ownership transfer for buffers,FLINK-32549,13542644,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,06/Jul/23 07:24,12/Jul/23 04:06,04/Jun/24 20:40,12/Jul/23 04:06,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,"Currently, the accumulator is responsible for requesting all buffers, leading to an inaccurate number of requested buffers for each tier. 
To address this issue, buffer ownership must be transferred from the accumulator to the tiers when writing them, which will enable the memory manager to maintain a correct number of requested buffers for different owners.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 03:13:21 UTC 2023,,,,,,,,,,"0|z1izrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 03:13;Weijie Guo;master(1.18) via 2dfff436c09821fb658bf8d289206b9ef85bb25b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make watermark alignment ready for production use,FLINK-32548,13542635,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,06/Jul/23 06:53,02/Aug/23 02:11,04/Jun/24 20:40,17/Jul/23 07:35,1.16.2,1.17.1,,,,1.16.3,1.17.2,1.18.0,,,,,,,,0,,,,"We found a series of watermark alignment bugs and performance issues and hope to reach production availability in 1.18.0. 

And fixes all bugs found in 1.16.3 and 1.17.2.",,,,,,,,,,,,,,,,,,FLINK-32524,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 02:11:23 UTC 2023,,,,,,,,,,"0|z1izpk:",9223372036854775807,"The watermark alignment is ready for production since Flink-1.18, which completed a series of bug fixes and improvements releted to watermark alignment. As proven by the micro benchmarks (screenshots attached in FLINK-32420), with 5000 subtasks, the time to calculate the watermark alignment on the JobManager by a factor of 76x (7664%). Previously such large jobs where actually at large risk of overloading JobManager, now that's far less likely to happen. ",,,,,,,,,,,,,,,,,,,"06/Jul/23 08:00;fanrui;FLINK-32524 is related to watermark alignment as well. It isn't a bug, it's an improvement.;;;","25/Jul/23 11:54;knaufk;[~fanrui] In the documentation (https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/#watermark-alignment-_beta_) this is still marked as ""Beta""? Do you want to change this for Flink 1.18? Is there any other documentation work needed? Thanks, Konstantin (one of the release managers for Flink 1.18);;;","25/Jul/23 13:22;fanrui;Hi [~knaufk] , thanks for the asking.

I think the beta can be removed at flink 1.18. And I want to here more thoughts from core contributors of watermark alignment. cc [~pnowojski] [~dwysakowicz]

I can remove it if most of us think the beta can be removed.;;;","27/Jul/23 08:37;pnowojski;I agree with [~fanrui], given recent efforts from his side and [~cailiuyang], we can remove ""beta"" remark/warning. It doesn't mean there are no remaining bugs, but the beta version seems to be tested by the community quite extensively. ;;;","27/Jul/23 14:10;fanrui;Thanks for the feedback, I created the FLINK-32705 to remove the beta.;;;","27/Jul/23 14:31;fanrui;I see the WatermarkStrategy#withWatermarkAlignment also has the Experimental annotation. 
It's added since 1.15, and we have fixed a series of bugs.  Could we remove the Experimental together?;;;","27/Jul/23 14:49;pnowojski;Good catch, I would bump this to {{PublicEvolving}}.;;;","02/Aug/23 02:11;fanrui;Hi [~knaufk], the beta and Experimental  of watermark alignment have been removed. And I have organized the release note to this JIRA based on [~pnowojski] 's comment at  FLINK-32420.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing doc for Timestamp support in ProtoBuf format,FLINK-32547,13542626,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,06/Jul/23 05:24,08/Jul/23 00:02,04/Jun/24 20:40,08/Jul/23 00:02,1.17.1,,,,,1.17.2,1.18.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,"In FLINK-30093, we have support {{Timestamp}} type, and added the doc for it, but missed to updating the English version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30093,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 08 00:02:24 UTC 2023,,,,,,,,,,"0|z1iznk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/23 00:02;libenchao;Fixed via 
- 1aef1f13b9975f3603952d763d756e4f831a1e75 (1.18.0)
- c66bcb66fd7c201ced53e86a685bc3c6208a5c64 (1.17.2);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update Code Style Guide with Java properties naming convention,FLINK-32546,13542571,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jingge,jingge,05/Jul/23 17:33,05/Jul/23 17:33,04/Jun/24 20:40,,,,,,,,,,,Documentation,,,,,,0,,,,"The class [properties|https://en.wikipedia.org/wiki/Property_(programming)] must be accessible using {_}get{_}, {_}set{_}, _is_ (can be used for boolean properties instead of get), _to_ and other methods (so-called [accessor methods|https://en.wikipedia.org/wiki/Accessor] and [mutator methods|https://en.wikipedia.org/wiki/Mutator_method]) according to a standard [naming convention|https://en.wikipedia.org/wiki/Naming_conventions_(programming)]. 

 

[https://en.wikipedia.org/wiki/JavaBeans]

[https://flink.apache.org/how-to-contribute/code-style-and-quality-preamble/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-05 17:33:33.0,,,,,,,,,,"0|z1izbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improves the performance by optimizing row operations,FLINK-32545,13542540,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,05/Jul/23 12:49,18/Jul/23 05:53,04/Jun/24 20:40,18/Jul/23 05:53,,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,"Currently, dozens of algorithms in Flink ML contain code like `Row.join(row, Row.of(...))` which is expensive. We should avoid creating Rows multiple times.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 05:52:56 UTC 2023,,,,,,,,,,"0|z1iz4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 05:52;lindong;Merged to apache/flink-ml master branch ba327b081878da5a746f1b14ec4d22ec9cc9d8d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonFunctionFactoryTest fails on Java 17,FLINK-32544,13542526,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,chesnay,chesnay,05/Jul/23 10:29,06/Jul/23 09:07,04/Jun/24 20:40,06/Jul/23 09:07,1.18.0,,,,,1.18.0,,,,API / Python,Legacy Components / Flink on Tez,,,,,0,pull-request-available,,,"https://dev.azure.com/chesnay/flink/_build/results?buildId=3676&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=727942b6-6137-54f7-1ef9-e66e706ea068

{code}
Jul 05 10:17:23 Exception in thread ""main"" java.lang.reflect.InaccessibleObjectException: Unable to make field private static java.util.IdentityHashMap java.lang.ApplicationShutdownHooks.hooks accessible: module java.base does not ""opens java.lang"" to unnamed module @1880a322
Jul 05 10:17:23 	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
Jul 05 10:17:23 	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
Jul 05 10:17:23 	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
Jul 05 10:17:23 	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
Jul 05 10:17:23 	at org.apache.flink.client.python.PythonFunctionFactoryTest.closeStartedPythonProcess(PythonFunctionFactoryTest.java:115)
Jul 05 10:17:23 	at org.apache.flink.client.python.PythonFunctionFactoryTest.cleanEnvironment(PythonFunctionFactoryTest.java:79)
Jul 05 10:17:23 	at org.apache.flink.client.python.PythonFunctionFactoryTest.main(PythonFunctionFactoryTest.java:52)
{code}

Side-notes:
* maybe re-evaluate if the test could be run through maven now
* The shutdown hooks business is quite sketchy, and AFAICT would be unnecessary if the test were an ITCase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:07:57 UTC 2023,,,,,,,,,,"0|z1iz1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 09:07;chesnay;master: 0e44dccf694884d60c3cddcb104a3966e11a2dd8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The actual behavior of  restart-strategy.xxx.delay is inconsistent with the document,FLINK-32543,13542503,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,05/Jul/23 08:52,05/Jul/23 08:59,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"For {{restart-strategy.fixed-delay.delay}} and {{restart-strategy.failure-rate.delay}}, the document indicates that they are the delay between two {{consecutive}} restart attempts, but in fact {{jobmaster}} must wait for such time every failover. Even if restart has never happened before, or it has been a long time since the last failover. From the user's perspective, maybe the behavior in the document is more in line with expectations。",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-05 08:52:45.0,,,,,,,,,,"0|z1iywo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to manually specify __op field for starrocks flink connector,FLINK-32542,13542502,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,larajiang,larajiang,05/Jul/23 08:47,05/Jul/23 09:06,04/Jun/24 20:40,05/Jul/23 09:02,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 09:02:47 UTC 2023,,,,,,,,,,"0|z1iywg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 09:02;Weijie Guo;Hi [~larajiang], starrocks connector is not officially maintained by Flink. The best place to ask some question about it is [starrocks-connector-for-apache-flink|https://github.com/StarRocks/starrocks-connector-for-apache-flink].

BTW, please explain your question in English in the next time, as the Flink community has people from all over the world.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the buffer leaking in buffer accumulators when a failover occurs,FLINK-32541,13542498,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,05/Jul/23 08:24,07/Jul/23 07:58,04/Jun/24 20:40,07/Jul/23 07:55,1.18.0,,,,,1.18.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,"When a failover occurs, the buffers in the sort/hash accumulators should be released correctly to avoid buffers leaking. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 07:55:00 UTC 2023,,,,,,,,,,"0|z1iyvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 07:55;Weijie Guo;master(1.18) via 73a0b875407d4c6b1b580a8db3f64a6572106fd2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The issue of not distributing the last batch of data,FLINK-32540,13542484,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,原来你是小幸运001,原来你是小幸运001,05/Jul/23 06:57,05/Jul/23 07:16,04/Jun/24 20:40,05/Jul/23 07:16,,,,,,,,,,,,,,,,0,,,,"I copied the source code of the flat map and wanted to implement my own flat map. One of the logic is to issue the last piece of data at the end of the Flink job, so I executed collector.collect in the close method, but the data was not issued and the operator below cannot receive it.
{code:java}
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.Utils;
import org.apache.flink.api.java.typeutils.TypeExtractor;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator;
import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
import org.apache.flink.streaming.api.operators.TimestampedCollector;
import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
import org.apache.flink.util.Collector;

/**
 * @author LaiYongBIn
 * @date 2023/7/5 10:09
 * @Description Do SomeThing
 */
public class Test {
    public static void main(String[] args) throws Exception {
        ParameterTool parameter = ParameterTool.fromArgs(args);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> stream0 =
                env.addSource(new SourceFunction<String>() {
                            @Override
                            public void run(SourceContext<String> sourceContext) throws Exception {
                                sourceContext.collect(""TEST"");
                                System.out.println(""--------------------cancel--------------------"");
                            }

                            @Override
                            public void cancel() {
                            }
                        })
                        .setParallelism(1);

        MyFlatMapFun flatMapFunc = new MyFlatMapFun();
        TypeInformation<String> outType = TypeExtractor.getFlatMapReturnTypes(env.clean(flatMapFunc), stream0.getType(), Utils.getCallLocationName(), true);

        DataStream<String> flatMap = stream0.transform(""Flat Map"", outType, new MyStreamOperator(env.clean(flatMapFunc))).setParallelism(1);

        flatMap.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String s, Collector<String> collector) throws Exception {
                System.out.println(""----------------------------------------Obtain upstream data is："" + s);
            }
        });

        env.execute();
    }
}


class MyStreamOperator extends AbstractUdfStreamOperator<String, FlatMapFunction<String, String>> implements OneInputStreamOperator<String, String> {


    private transient TimestampedCollector<String> collector;


    public MyStreamOperator(FlatMapFunction<String, String> userFunction) {
        super(userFunction);
    }

    @Override
    public void open() throws Exception {
        collector = new TimestampedCollector<>(output);
    }

    @Override
    public void close() throws Exception {
        // Distribute data during close
        collector.collect(""close message"");
    }

    @Override
    public void processElement(StreamRecord<String> streamRecord) throws Exception {
        // do nothing
    }
}


 class MyFlatMapFun implements FlatMapFunction<String, String> {

    @Override
    public void flatMap(String s, Collector<String> collector) throws Exception {
        // do nothing
    }
} {code}
Then I found out there was a finish method, and I tried to execute 'collector. collect' in the finish method, and the data was successfully distributed。
{code:java}
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.Utils;
import org.apache.flink.api.java.typeutils.TypeExtractor;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator;
import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
import org.apache.flink.streaming.api.operators.TimestampedCollector;
import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
import org.apache.flink.util.Collector;

/**
 * @author LaiYongBIn
 * @date 2023/7/5 10:09
 * @Description Do SomeThing
 */
public class Test {
    public static void main(String[] args) throws Exception {
        ParameterTool parameter = ParameterTool.fromArgs(args);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> stream0 =
                env.addSource(new SourceFunction<String>() {
                            @Override
                            public void run(SourceContext<String> sourceContext) throws Exception {
                                sourceContext.collect(""TEST"");
                                System.out.println(""--------------------cancel--------------------"");
                            }

                            @Override
                            public void cancel() {
                            }
                        })
                        .setParallelism(1);

        MyFlatMapFun flatMapFunc = new MyFlatMapFun();
        TypeInformation<String> outType = TypeExtractor.getFlatMapReturnTypes(env.clean(flatMapFunc), stream0.getType(), Utils.getCallLocationName(), true);

        DataStream<String> flatMap = stream0.transform(""Flat Map"", outType, new MyStreamOperator(env.clean(flatMapFunc))).setParallelism(1);

        flatMap.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String s, Collector<String> collector) throws Exception {
                System.out.println(""----------------------------------------Obtain upstream data is："" + s);
            }
        });

        env.execute();
    }
}


class MyStreamOperator extends AbstractUdfStreamOperator<String, FlatMapFunction<String, String>> implements OneInputStreamOperator<String, String> {


    private transient TimestampedCollector<String> collector;


    public MyStreamOperator(FlatMapFunction<String, String> userFunction) {
        super(userFunction);
    }

    @Override
    public void open() throws Exception {
        collector = new TimestampedCollector<>(output);
    }

    @Override
    public void close() throws Exception {

    }

    @Override
    public void finish() throws Exception {
        // Distribute data during finish
        collector.collect(""close message"");
    }

    @Override
    public void processElement(StreamRecord<String> streamRecord) throws Exception {
        // do nothing
    }
}


 class MyFlatMapFun implements FlatMapFunction<String, String> {

    @Override
    public void flatMap(String s, Collector<String> collector) throws Exception {
        // do nothing
    }
} {code}
But when I executed the program on Yarn, it was still not distributed. May I know the reason for this and how to solve it.I hope that when the program is executed on Yarn, the last batch of data can still be distributed to downstream operators","The above code was executed in IntelliJ IDEA, Flink version 1.16, which also has this issue in 1.14. Other versions have not attempted it

 ",604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Jul 05 07:16:34 UTC 2023,,,,,,,,,,"0|z1iyso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 07:16;martijnvisser;These type of questions should be asked on the User mailing list, Stackoverflow or Slack.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Archunit violations started to fail in test_misc,FLINK-32539,13542477,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,05/Jul/23 05:21,06/Jul/23 15:25,04/Jun/24 20:40,05/Jul/23 09:41,1.18.0,,,,,1.18.0,,,,Tests,,,,,,0,pull-request-available,test-stability,,"blocker since now it fails on every build
to reproduce jdk 8 is required
{noformat}
mvn clean install -DskipTests
mvn verify -pl flink-architecture-tests/flink-architecture-tests-production/ -Darchunit.freeze.store.default.allowStoreUpdate=false
{noformat}
It seems the reason is FLINK-27415
where it was removed line 
{code:java}
checkArgument(fileLength > 0L);
{code}
at the same time it was mentioned in achunit violations  and now should be removed as well

example of failure
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50946&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23064",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 09:41:17 UTC 2023,,,,,,,,,,"0|z1iyr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 05:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50949&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23391;;;","05/Jul/23 05:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50953&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23391;;;","05/Jul/23 05:54;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50967&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23391;;;","05/Jul/23 09:41;Sergey Nuyanzin;Merged to master [44e1397e2e7aa54667bf3b30bc6f6d5db32b5f79|https://github.com/apache/flink/commit/44e1397e2e7aa54667bf3b30bc6f6d5db32b5f79];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI build failed because node is corrupted when compiling,FLINK-32538,13542462,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tanyuxin,tanyuxin,05/Jul/23 02:23,05/Jul/23 06:49,04/Jun/24 20:40,05/Jul/23 06:49,1.18.0,,,,,,,,,Build System / CI,Tests,,,,,0,,,,"[ERROR] The archive file /__w/3/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-linux-x64.tar.gz is corrupted and will be deleted. Please try the build again. 
 
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50896&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10984]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50919&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10984]



[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50925&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10984]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-05 02:23:43.0,,,,,,,,,,"0|z1iyns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add compatibility annotation for REST API classes,FLINK-32537,13542431,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liangtl,liangtl,04/Jul/23 13:46,11/Mar/24 12:43,04/Jun/24 20:40,,1.16.2,1.17.1,,,,1.20.0,,,,Runtime / REST,,,,,,0,,,,"*Why*

We want to standardise the class labelling for Flink classes. Currently, the compatibility annotations like @Public, @PublicEvolving, @Internal are not present for REST API classes.

 

*What*

We should be added @Internal for most Flink classes, unless they change the REST API variables, so we know clearly which components will change our REST API when changed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 15:42:39 UTC 2023,,,,,,,,,,"0|z1iyhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 08:48;chesnay;??so we know clearly which components will change our REST API when changed??

We already know that though due to the compatibility tests.

We'd just unnecessarily restrict us from changing things internally. None of the _classes_ are actually user-facing.;;;","05/Jul/23 15:42;liangtl;> We already know that though due to the compatibility tests.

 

Got it. [~chesnay] Is the proposal to close this Jira and leave as-is?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python tests fail with Arrow DirectBuffer exception,FLINK-32536,13542428,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,chesnay,chesnay,04/Jul/23 13:16,05/Jul/23 11:52,04/Jun/24 20:40,05/Jul/23 11:52,1.18.0,,,,,1.18.0,,,,API / Python,Tests,,,,,0,pull-request-available,,,"https://dev.azure.com/chesnay/flink/_build/results?buildId=3674&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=727942b6-6137-54f7-1ef9-e66e706ea068

{code}
2023-07-04T12:54:15.5296754Z Jul 04 12:54:15 E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
2023-07-04T12:54:15.5299579Z Jul 04 12:54:15 E                   : java.lang.RuntimeException: Arrow depends on DirectByteBuffer.<init>(long, int) which is not available. Please set the system property 'io.netty.tryReflectionSetAccessible' to 'true'.
2023-07-04T12:54:15.5302307Z Jul 04 12:54:15 E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.checkArrowUsable(ArrowUtils.java:184)
2023-07-04T12:54:15.5302859Z Jul 04 12:54:15 E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:546)
2023-07-04T12:54:15.5303177Z Jul 04 12:54:15 E                   	at jdk.internal.reflect.GeneratedMethodAccessor287.invoke(Unknown Source)
2023-07-04T12:54:15.5303515Z Jul 04 12:54:15 E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-07-04T12:54:15.5303929Z Jul 04 12:54:15 E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
2023-07-04T12:54:15.5307338Z Jul 04 12:54:15 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2023-07-04T12:54:15.5309888Z Jul 04 12:54:15 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
2023-07-04T12:54:15.5310306Z Jul 04 12:54:15 E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2023-07-04T12:54:15.5337220Z Jul 04 12:54:15 E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2023-07-04T12:54:15.5341859Z Jul 04 12:54:15 E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2023-07-04T12:54:15.5342363Z Jul 04 12:54:15 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2023-07-04T12:54:15.5344866Z Jul 04 12:54:15 E                   	at java.base/java.lang.Thread.run(Thread.java:833)
{code}

{code}
2023-07-04T12:54:15.5663559Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_empty_to_pandas
2023-07-04T12:54:15.5663891Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_from_pandas
2023-07-04T12:54:15.5664299Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_to_pandas
2023-07-04T12:54:15.5664655Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_to_pandas_for_retract_table
2023-07-04T12:54:15.5665003Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_empty_to_pandas
2023-07-04T12:54:15.5665360Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_from_pandas
2023-07-04T12:54:15.5665704Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas
2023-07-04T12:54:15.5666045Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas_for_retract_table
2023-07-04T12:54:15.5666415Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas_with_event_time
2023-07-04T12:54:15.5666840Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_function
2023-07-04T12:54:15.5667189Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_with_aux_group
2023-07-04T12:54:15.5667526Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_without_keys
2023-07-04T12:54:15.5667882Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_over_window_aggregate_function
2023-07-04T12:54:15.5668242Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_slide_group_window_aggregate_function
2023-07-04T12:54:15.5668607Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_tumble_group_window_aggregate_function
2023-07-04T12:54:15.5668961Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_execute_over_aggregate_from_json_plan
2023-07-04T12:54:15.5669334Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_proc_time_over_rows_window_aggregate_function
2023-07-04T12:54:15.5669714Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_row_time_over_range_window_aggregate_function
2023-07-04T12:54:15.5670085Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_row_time_over_rows_window_aggregate_function
2023-07-04T12:54:15.5670450Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_sliding_group_window_over_count
2023-07-04T12:54:15.5670804Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_sliding_group_window_over_time
2023-07-04T12:54:15.5671168Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_tumbling_group_window_over_count
2023-07-04T12:54:15.5671510Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_tumbling_group_window_over_time
2023-07-04T12:54:15.5671847Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_all_data_types
2023-07-04T12:54:15.5672171Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_basic_functionality
2023-07-04T12:54:15.5672544Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_data_types
2023-07-04T12:54:15.5672864Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_invalid_pandas_udf
2023-07-04T12:54:15.5673188Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_all_data_types
2023-07-04T12:54:15.5673501Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_basic_functionality
2023-07-04T12:54:15.5673881Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_data_types
2023-07-04T12:54:15.5674201Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_invalid_pandas_udf
2023-07-04T12:54:15.5674544Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf
2023-07-04T12:54:15.5674941Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf_without_keys
2023-07-04T12:54:15.5675324Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_map_with_pandas_udf
2023-07-04T12:54:15.5675684Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_aggregate
2023-07-04T12:54:15.5676033Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate
2023-07-04T12:54:15.5676401Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate_list_view
2023-07-04T12:54:15.5676771Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_map_with_pandas_udf
2023-07-04T12:54:15.5677224Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_sql.py::JavaSqlTests::test_java_sql_ddl - sub...
2023-07-04T12:54:15.5677536Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_data_view_clear
2023-07-04T12:54:15.5677856Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_distinct_and_filter
2023-07-04T12:54:15.5678162Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_double_aggregate
2023-07-04T12:54:15.5678476Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_list_view
2023-07-04T12:54:15.5678784Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view
2023-07-04T12:54:15.5679082Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view_iterate
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 11:52:35 UTC 2023,,,,,,,,,,"0|z1iygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 11:52;dianfu;Fixed in master via 7f34df40c0de8e7bffd149196851f89012faf842;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointingStatisticsHandler periodically returns NullArgumentException after job restarts,FLINK-32535,13542426,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,04/Jul/23 13:09,11/Mar/24 12:44,04/Jun/24 20:40,,1.16.2,1.17.1,,,,1.20.0,,,,Runtime / REST,,,,,,0,,,,"*What*

When making requests to /checkpoints REST API after a job restart, we see 500 for a short period of time. We should handle this gracefully in the CheckpointingStatisticsHandler.

 

*How to replicate*
 * Checkpointing interval 1s
 * Job is constantly restarting
 * Make constant requests to /checkpoints REST API.

See [here|https://github.com/apache/flink/pull/22901#issuecomment-1617830035] for more info

 

Stack trace:

{{org.apache.commons.math3.exception.NullArgumentException: input array}}
{{    at org.apache.commons.math3.util.MathArrays.verifyValues(MathArrays.java:1753)}}
{{    at org.apache.commons.math3.stat.descriptive.AbstractUnivariateStatistic.test(AbstractUnivariateStatistic.java:158)}}
{{    at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:272)}}
{{    at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:241)}}
{{    at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics$CommonMetricsSnapshot.getPercentile(DescriptiveStatisticsHistogramStatistics.java:159)}}
{{    at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.getQuantile(DescriptiveStatisticsHistogramStatistics.java:53)}}
{{    at org.apache.flink.runtime.checkpoint.StatsSummarySnapshot.getQuantile(StatsSummarySnapshot.java:108)}}
{{    at org.apache.flink.runtime.rest.messages.checkpoints.StatsSummaryDto.valueOf(StatsSummaryDto.java:81)}}
{{    at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.createCheckpointingStatistics(CheckpointingStatisticsHandler.java:133)}}
{{    at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleCheckpointStatsRequest(CheckpointingStatisticsHandler.java:85)}}
{{    at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleCheckpointStatsRequest(CheckpointingStatisticsHandler.java:59)}}
{{    at org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointStatsHandler.lambda$handleRequest$1(AbstractCheckpointStatsHandler.java:62)}}
{{    at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)}}
{{    at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)}}
{{    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)}}
{{    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)}}
{{    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)}}
{{    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)}}
{{    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)}}
{{    at java.base/java.lang.Thread.run(Thread.java:829)\n}}

 

See graphs here for tests. The dips in the green line correspond to the failures immediately after a job restart.

!https://user-images.githubusercontent.com/35062175/250529297-908a6714-ea15-4aac-a7fc-332589da2582.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25904,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-04 13:09:03.0,,,,,,,,,,"0|z1iygg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exit code 137 (i.e. OutOfMemoryError) in flink-tests module,FLINK-32534,13542411,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,04/Jul/23 11:27,06/Jul/23 09:21,04/Jun/24 20:40,06/Jul/23 09:21,1.17.2,,,,,,,,,Tests,,,,,,0,test-stability,,,"this build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50816&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8056
is failing
{noformat}
Jul 03 11:07:41 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale keyed_different_parallelism from 7 to 12, sourceSleepMs = 0, buffersPerChannel = 0].
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1004  -w /home/agent05_azpcontainer 20ef21acb85dd401bac092e74118bbc65dae54ceb7e28fee9007fd15987f7d27 /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
Finishing: Test - tests
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:20:48 UTC 2023,,,,,,,,,,"0|z1iyd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 07:28;mapohl;The failure you describe happened on agent {{AlibabaCI008-agent05}} on Jul 03 at 11:07:41. I checked the CI builds you reported in FLINK-18356. There is a 137 exit code CI failure (you reported it in this comment) in the {{flink-table}} module on {{AlibabaCI008-agent02}} (i.e. same VM) on Jul 03 at 11:07:07.

The 137 OOM errors make all the JVM processes crash on the same machine. We've seen this in the past where there was always a CI build failing in {{flink-table}} involved. That brought us to the conclusion that FLINK-18356 is the most-likely reason for the OOM. Therefore, you might want to close this Jira issue as a duplicate of FLINK-18356 (it's important to link the Jiras to make sure that we can trace back issues in case the OOM is not only caused by FLINK-18356).;;;","06/Jul/23 07:29;mapohl;In general, we should always check for concurrent failed CI builds when it comes to 137 exit code errors. Here we have to check whether these errors happened on the same machine at the same time.;;;","06/Jul/23 09:20;Sergey Nuyanzin;thanks for looking here ,
yes you are right i will close it in favor of FLINK-18356;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exit code 137 (i.e. OutOfMemoryError) in python module,FLINK-32533,13542410,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,04/Jul/23 11:24,06/Jul/23 09:17,04/Jun/24 20:40,06/Jul/23 09:17,1.17.2,,,,,,,,,API / Python,,,,,,0,test-stability,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50839&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24053

is failing like
{noformat}
Jul 03 15:31:56 pyflink/datastream/tests/test_data_stream.py ........................... [ 42%]
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1004  -w /home/agent05_azpcontainer 262ff527fd71a48209f73ba1736f66a946df80f8d1494b6193462778435a66e2 /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
Finishing: Test - python

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:17:19 UTC 2023,,,,,,,,,,"0|z1iycw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 07:25;mapohl;This issue might be a duplicate to FLINK-18356. See my comment in FLINK-32532 for the reason. The build failure happened around the same time:
{quote}The failure you describe happened on agent AlibabaCI005-agent01 on Jul 03 at 15:33:45. I checked the CI builds you reported in FLINK-18356. There is a 137 exit code CI failure (you reported it in this comment) in the flink-table module on AlibabaCI005-agent04 (i.e. same VM) on Jul 3 at 15:32:38.

The 137 OOM errors make all the JVM processes crash on the same machine. We've seen this in the past where there was always a CI build failing in flink-table involved. That brought us to the conclusion that FLINK-18356 is the most likely reason for the OOM. Therefore, you might want to close this Jira issue as a duplicate of FLINK-18356 (it's important to link the Jiras to make sure that we can trace back issues in case the OOM is not only caused by FLINK-18356).
{quote};;;","06/Jul/23 09:17;Sergey Nuyanzin;thanks for looking here ,
yes you are right i will close it in favor of FLINK-18356;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exit code 137 (i.e. OutOfMemoryError) in flink-s3-fs-hadoop module,FLINK-32532,13542408,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,04/Jul/23 11:21,06/Jul/23 09:09,04/Jun/24 20:40,06/Jul/23 09:09,1.16.3,,,,,,,,,Connectors / Hadoop Compatibility,,,,,,0,test-stability,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50840&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=16093

is failing like
{noformat}
Jul 03 15:33:35 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.267 s - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
Jul 03 15:33:45 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1000  -w /home/agent01_azpcontainer 3e9ac5dd969222db5673644f5c729d323f624390f9dbc3238a1c99b1b3c4679b /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
Finishing: Test - connect_1
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:09:11 UTC 2023,,,,,,,,,,"0|z1iycg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 07:21;mapohl;The failure you describe happened on agent {{AlibabaCI005-agent01}} on Jul 03 at 15:33:45. I checked the CI builds you reported in FLINK-18356. There is a [137 exit code CI failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50841&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11872] (you reported it in [this comment|https://issues.apache.org/jira/browse/FLINK-18356?focusedCommentId=17739727&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17739727]) in the {{flink-table}} module on {{AlibabaCI005-agent04}} (i.e. same VM) on Jul 3 at 15:32:38.

The 137 OOM errors make all the JVM processes crash on the same machine. We've seen this in the past where there was always a CI build failing in {{flink-table}} involved. That brought us to the conclusion that FLINK-18356 is the most likely reason for the OOM. Therefore, you might want to close this Jira issue as a duplicate of FLINK-18356 (it's important to link the Jiras to make sure that we can trace back issues in case the OOM is not only caused by FLINK-18356).;;;","06/Jul/23 09:09;Sergey Nuyanzin;thanks for looking here , 
yes you are right i will close it in favor of FLINK-18356;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Recovery of missing job deployments"" feature not working as described",FLINK-32531,13542395,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,ivan.stoiev,ivan.stoiev,ivan.stoiev,04/Jul/23 09:55,31/Aug/23 06:29,04/Jun/24 20:40,31/Aug/23 06:29,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"Maybe it's not a bug, but some documentation issue.

[Docs|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/#recovery-of-missing-job-deployments] says:
??When HA is enabled, the operator can recover the Flink cluster deployments in cases when it was accidentally deleted by the user or some external process.??

My use cases are stadalone jobs with savepoint as {{{}upgradeMode{}}}, and we want to ensure it's state, even when explicity deleted by the user. Starting a job withouth its previous state is something that we need to avoid at all cost.

In our tests (using Flink 1.16, kubernetes HA and operator 1.4.0), when we delete the FlinkDeployment object, all HA metadata is removed. And when applying the same FlinkDeployment object, flink always start withouth state.

Looking at the code and related issues it seems the normal behaviour, but the docs lead us to wrong conclusions. Is there something that we are missing here? Is there some configuration/situation that could replicate the behaviour described on docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 14:36:41 UTC 2023,,,,,,,,,,"0|z1iy9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 10:52;gyfora;In this case the feature refers to the underlying Deployment object that belongs to the JobManager process itself (not the FlinkDeployment);;;","04/Jul/23 14:28;ivan.stoiev;Thanks [~gyfora] ! Do you think that worth specifing this on the docs? I could submit a PR for this.;;;","04/Jul/23 14:36;gyfora;If you find this confusing then probably other people would also feel the same way. Adding some clarification would be nice, thank you! :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
array_position semantic should align with array_contains instead of spark,FLINK-32530,13542391,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,04/Jul/23 09:04,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,"when i supports array_contains to calcite https://issues.apache.org/jira/browse/CALCITE-5707 i found the spark and flink's behavior is different.
{code:java}
spark: array_contains(array[1, null], null) -> null
flink: array_contains(array[1, null], null) -> true  {code}
so array_remove is also different(the array_remove is  supported by me, which aligns with flink).
{code:java}
spark: array_remove(array[1, null], null) -> null 
flink: array_remove(array[1, null], null) -> 1  {code}
while array_position is align with spark, i think it is not correct.
{code:java}
spark: array_position(array[1, null], null) -> 2 
flink: array_position(array[1, null], null) -> 2   {code}
and i test on postgre which is also 2
{code:java}
postgre:
select array_position(ARRAY[1, null], null); {code}
so the semantic should only follow one way to handle null element. so i think it should be changed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 13:02:30 UTC 2024,,,,,,,,,,"0|z1iy8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 12:04;jackylau;hi [~dwysakowicz] do you have time to have a look?;;;","11/Jan/24 13:02;xuyangzhong;Hi, [~jackylau] . Thanks for pointing it!

Have you looked at the behavior of other DBMS or big data frameworks? This change may cause a little break for users, so we'd better confirm what the correct behavior is.

BTW, does map has the same behavior while using function `MAP[KEY]` like `map_data[cast(null as int)]`?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optional startup probe for JM deployment,FLINK-32529,13542390,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,04/Jul/23 08:56,26/Mar/24 06:46,04/Jun/24 20:40,10/Jul/23 19:26,,,,,,kubernetes-operator-1.6.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,"There are certain cases where the JM enters a startup crash loop for example due to incorrect HA config setup. With the current operator logic these cases require manual user intervention as we don't have HA metadata available for the last checkpoint and it also seems like the JM actually started already.

To solve this properly we suggest adding a default JM startup probe that queries the rest api (/config) endpoint. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 06:46:36 UTC 2024,,,,,,,,,,"0|z1iy8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 19:26;gyfora;merged to main 5ebfc5e4a6c31ea0c212f187bf7f7de70dfd1c09;;;","26/Mar/24 06:35;tbnguyen1407;[~gyfora] The added startupProbe seems to always use HTTP, which fails when cluster is set up with TLS. Currently it needs to be disabled for TLS deployment.;;;","26/Mar/24 06:46;gyfora;[~tbnguyen1407] please open a separate Jira ticket for this. If you could also work on a fix that would be a nice improvement :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The RexCall a = a,if a's datatype is nullable, and when a is null, a = a is null, it isn't true in BinaryComparisonExprReducer",FLINK-32528,13542386,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,shenlang,shenlang,04/Jul/23 08:25,11/Jul/23 12:26,04/Jun/24 20:40,,1.17.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,"Now I'am reading flink sql planner's source code,when I saw the FlinkRexUtil.java, in the org.apache.flink.table.planner.plan.utils.FlinkRexUtil#simplify method,it used the BinaryComparisonExprReducer the deal with BINARY_COMPARISON's operator which the operands are RexInputRef and the operands are same, e.g. a = a, a <> a,a >=a...

In BinaryComparisonExprReducer, a = a,a <=a,a>=a will be simplified the true literal,a <> a,a < a, a > a will be simplified the false literal.

if a's datatype is nullable, and when a is null, a = a is null, a <> a is null. In BinaryComparisonExprReducer's logic,It does not consider the case of the nullable data type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 12:26:32 UTC 2023,,,,,,,,,,"0|z1iy7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 08:49;shenlang;I think in BinaryComparisonExprReducer's logic ,it should consider the case of the nullable data type or  restricted operand's datatype is not nullable.
In Apache Calcite,It considers the case of a=a when the data type of a is nullable.
The code is :https://github.com/apache/calcite/blob/d1a12bb6f4d4f617234dfbc32cb0813c473b0029/core/src/main/java/org/apache/calcite/rex/RexSimplify.java#L520;;;","11/Jul/23 12:26;337361684@qq.com;Hi, [~shenlang] , can you provide a specific sql query as example?  I don't quite understand this 'if a's datatype is nullable, and when a is null, a = a is null, a <> a is null'. Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build failure on Windows,FLINK-32527,13542383,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sunfangbin,sunfangbin,sunfangbin,04/Jul/23 08:18,10/Jul/23 19:25,04/Jun/24 20:40,10/Jul/23 19:25,,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,," 
{code:java}
[INFO] --- maven-antrun-plugin:1.8:run (deployment-crd-compatibility-check) @ flink-kubernetes-operator-api ---
[INFO] Executing tasksmain:
     [java] 2023-07-04 16:07:45,348 o.a.f.k.o.a.v.CrdCompatibilityChecker [INFO ] [.] New schema: file://E:\project\open\flink-operator-main\flink-kubernetes-operator/helm/flink-kubernetes-operator/crds/flinkdeployments.flink.apache.org-v1.yml
     [java] 2023-07-04 16:07:45,350 o.a.f.k.o.a.v.CrdCompatibilityChecker [INFO ] [.] Old schema: https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-1.4.0/helm/flink-kubernetes-operator/crds/flinkdeployments.flink.apache.org-v1.yml
     [java] Exception in thread ""main"" java.net.UnknownHostException: E
     [java]     at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:220)
     [java]     at java.base/java.net.Socket.connect(Socket.java:609)
     [java]     at java.base/sun.net.ftp.impl.FtpClient.doConnect(FtpClient.java:1062)
     [java]     at java.base/sun.net.ftp.impl.FtpClient.tryConnect(FtpClient.java:1024)
     [java]     at java.base/sun.net.ftp.impl.FtpClient.connect(FtpClient.java:1119)
     [java]     at java.base/sun.net.ftp.impl.FtpClient.connect(FtpClient.java:1105)
     [java]     at java.base/sun.net.www.protocol.ftp.FtpURLConnection.connect(FtpURLConnection.java:312)
     [java]     at java.base/sun.net.www.protocol.ftp.FtpURLConnection.getInputStream(FtpURLConnection.java:418)
     [java]     at java.base/java.net.URL.openStream(URL.java:1165)
     [java]     at com.fasterxml.jackson.core.TokenStreamFactory._optimizedStreamFromURL(TokenStreamFactory.java:262)
     [java]     at com.fasterxml.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:400)
     [java]     at com.fasterxml.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:15)
     [java]     at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:3268)
     [java]     at org.apache.flink.kubernetes.operator.api.validation.CrdCompatibilityChecker.getSchema(CrdCompatibilityChecker.java:66)
     [java]     at org.apache.flink.kubernetes.operator.api.validation.CrdCompatibilityChecker.main(CrdCompatibilityChecker.java:60)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Flink Kubernetes: 1.6-SNAPSHOT:
[INFO] 
[INFO] Flink Kubernetes: .................................. SUCCESS [  4.028 s]
[INFO] Flink Kubernetes Standalone ........................ SUCCESS [  8.140 s]
[INFO] Flink Kubernetes Operator Api ...................... FAILURE [ 31.335 s]
[INFO] Flink Kubernetes Operator .......................... SKIPPED
[INFO] Flink Kubernetes Operator Autoscaler ............... SKIPPED
[INFO] Flink Kubernetes Webhook ........................... SKIPPED
[INFO] Flink Kubernetes Docs .............................. SKIPPED
[INFO] Flink SQL Runner Example ........................... SKIPPED
[INFO] Flink Beam Example ................................. SKIPPED
[INFO] Flink Kubernetes Client Code Example ............... SKIPPED
[INFO] Flink Autoscaler Test Job .......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------ {code}
This is due to `JsonNode readTree(URL source)` can not handle the file schema on windows as below:

 
{code:java}
protected InputStream _optimizedStreamFromURL(URL url) throws IOException {
    if (""file"".equals(url.getProtocol())) {
        String host = url.getHost();
        if (host == null || host.length() == 0) {
            String path = url.getPath();
            if (path.indexOf(37) < 0) {
                return new FileInputStream(url.getPath());
            }
        }
    }

    return url.openStream();
} {code}
we should use `JsonNode readTree(File file)` instead.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 19:25:35 UTC 2023,,,,,,,,,,"0|z1iy6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 19:25;gyfora;merged to main f21426201842b5df3c9a4f13ce41973d2ae67430;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update apache parquet to 1.13.1,FLINK-32526,13542381,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,04/Jul/23 08:12,04/Jul/23 14:40,04/Jun/24 20:40,04/Jul/23 14:19,1.18.0,,,,,1.18.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,"Now 1.13.1 is available
https://parquet.apache.org/blog/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 14:19:01 UTC 2023,,,,,,,,,,"0|z1iy6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 14:19;Sergey Nuyanzin;Merged to master as [412118a6e681528919bc5eff90444519a4943437|https://github.com/apache/flink/commit/412118a6e681528919bc5eff90444519a4943437];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update commons-beanutils to 1.9.4,FLINK-32525,13542373,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,04/Jul/23 07:34,10/Sep/23 22:35,04/Jun/24 20:40,,,,,,,,,,,Deployment / YARN,,,,,,0,auto-deprioritized-major,pull-request-available,,"YARN still tests with commons-beanutils 1.8.3 with a remark that beanutil 1.9+ doesn't work with Hadoop, but Hadoop 2.10.2 (which is our minimum supported version) uses beanutils 1.9.4 itself, per https://github.com/apache/hadoop/blob/rel/release-2.10.2/hadoop-project/pom.xml#L861-L863",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 10 22:35:05 UTC 2023,,,,,,,,,,"0|z1iy4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the watermark aggregation performance when enabling the watermark alignment,FLINK-32524,13542368,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Jul/23 06:14,17/Jul/23 07:35,04/Jun/24 20:40,17/Jul/23 07:35,1.18.0,,,,,1.18.0,,,,Connectors / Common,,,,,,0,,,,"Improve the watermark aggregation performance when enabling the watermark alignment, and add the related benchmark.",,,,,,,,,,,,,,,,,FLINK-32548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-04 06:14:46.0,,,,,,,,,,"0|z1iy3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NotifyCheckpointAbortedITCase.testNotifyCheckpointAborted fails with timeout on AZP,FLINK-32523,13542342,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,masteryhx,Sergey Nuyanzin,Sergey Nuyanzin,03/Jul/23 20:21,03/Jun/24 01:43,04/Jun/24 20:40,,1.16.2,1.17.1,1.18.0,1.19.0,1.20.0,,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,stale-assigned,test-stability,"This build
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50795&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8638
 fails with timeout
{noformat}
Jul 03 01:26:35 org.junit.runners.model.TestTimedOutException: test timed out after 100000 milliseconds
Jul 03 01:26:35 	at java.lang.Object.wait(Native Method)
Jul 03 01:26:35 	at java.lang.Object.wait(Object.java:502)
Jul 03 01:26:35 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
Jul 03 01:26:35 	at org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.verifyAllOperatorsNotifyAborted(NotifyCheckpointAbortedITCase.java:198)
Jul 03 01:26:35 	at org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.testNotifyCheckpointAborted(NotifyCheckpointAbortedITCase.java:189)
Jul 03 01:26:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 03 01:26:35 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 03 01:26:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 03 01:26:35 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 03 01:26:35 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 03 01:26:35 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 03 01:26:35 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 03 01:26:35 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 03 01:26:35 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Jul 03 01:26:35 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Jul 03 01:26:35 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jul 03 01:26:35 	at java.lang.Thread.run(Thread.java:748)

{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23636,,,,,,,,,,,,,,,,,"02/Aug/23 07:42;mapohl;failure.log;https://issues.apache.org/jira/secure/attachment/13061849/failure.log",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 01:43:24 UTC 2024,,,,,,,,,,"0|z1ixxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/23 02:19;Wencong Liu;I think we should remove @Test(timeout = TEST_TIMEOUT) in this test and let CI judge whether it's timed out. WDYT?;;;","11/Jul/23 07:29;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51165&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8641;;;","11/Jul/23 07:34;Sergey Nuyanzin;We could try however i'm not sure...  since it's waiting for {{OneShotLatch}} to be triggered could it happen that sometimes it's never triggered?;;;","23/Jul/23 11:49;Sergey Nuyanzin;also for 1.16
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51608&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8503;;;","24/Jul/23 23:27;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51608&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8503;;;","27/Jul/23 09:04;Sergey Nuyanzin;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51745&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=7934;;;","31/Jul/23 07:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51804&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8251;;;","02/Aug/23 06:47;mapohl;Same CI run:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8245
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8013;;;","02/Aug/23 07:40;mapohl;{quote}
I think we should remove @Test(timeout = TEST_TIMEOUT) in this test and let CI judge whether it's timed out. WDYT?
{quote}
While I agree with [~Wencong Liu] generally that the timeout should be removed...
{quote}
We could try however i'm not sure... since it's waiting for OneShotLatch to be triggered could it happen that sometimes it's never triggered?
{quote}
...[~Sergey Nuyanzin]'s observation seems to be correct as well. It appears that the code is waiting for the {{DeclineSink}} to retrieve the aborted signal (at least the corresponding latch isn't triggered).

[~renqs] can you select someone who would be appropriate to look into this checkpointing issue?

I attached the extracted logs for one of the test failures listed in my previous comment.;;;","02/Aug/23 15:03;masteryhx;From the exception stack and attached logs, I saw:
 # the failure reason is not same, timeout and assert error
 # in the timeout cases, the job failed then restored, In the assert error cases, the job aborted two times (these cases are all enabling unaligned checkpoint), the job never failed and ran until finished for every success cases

So I think there are two exceptions in this ITCase:
 # Assert error -> All operators haven't snapshotState together strictly for marked decline checkpoint id in the teste case (This could be reproduced by adding Thread.sleep after first verifyAllOperatorsNotifyAborted() )
 # Timeout exception -> restarting (due to 1 tolerable checkpoint failure number) and notifying aborted occur in different threads, and the order is uncertain, if the job restart firstly, this will cause timeout exception (This could be reproduced by adding Thread.sleep in NormalMap#notifyCheckpointAborted)


For the first exception, we could just make them snapshotState together strictly which I think the ITCase should guarantee.

For the second one, I think it's acceptable that the abort function may not be called if the job failover (notifyCheckpointAborted is a best effort function). So we could just increase the tolerable checkpoint number.

 



Why timeout exception just occured in 1.18 ?

This is because FLINK-32347 which fixes the exception that CompletedCheckpointStore are not registered by the CheckpointFailureManager, After this, the job could fail due to the tolerable checkpoint failure number.;;;","03/Aug/23 14:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51921&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8249;;;","04/Aug/23 06:52;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51955&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8706;;;","14/Aug/23 08:27;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52226&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8661;;;","14/Aug/23 08:36;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52208&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8661;;;","16/Aug/23 06:46;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52293&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8661;;;","18/Aug/23 02:25;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52339&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8661;;;","18/Aug/23 06:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52373&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8837;;;","25/Aug/23 02:21;masteryhx;merge commits 5bbdc469d7f313768cb2d23e1e069fbf5b92232a and 66cc21d4e2c091c0f5211bf558d1a69364519f9b into master & 1.18;;;","30/Aug/23 13:14;mapohl; [~masteryhx] why don't we provide backports for 1.17 and 1.16? Based on the affected version the error also appeared in older versions.;;;","31/Aug/23 03:13;masteryhx;[~mapohl] Thanks for the reminder.
I will merge them into 1.16 & 1.17 after their CI pass.;;;","31/Aug/23 10:19;masteryhx;picked 66cc21d4e2c091c0f5211bf558d1a69364519f9b and merged into 1.16 & 1.17;;;","01/Sep/23 14:00;Sergey Nuyanzin;I reopen this since it is reproduced for 1.16
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52938&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7402

I've also double checked that the failed build contains the latest fix.

[~masteryhx] could you please have a look?;;;","01/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Oct/23 07:51;Sergey Nuyanzin;another reproduction for 1.17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53728&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8498;;;","18/Oct/23 13:26;mapohl;another one in 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53795&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9901;;;","24/Oct/23 07:11;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53902&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8661;;;","24/Oct/23 12:01;mapohl;branch based on {{master}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53969&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8264;;;","30/Oct/23 08:19;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54012&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=9022;;;","14/Nov/23 07:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54267&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10199;;;","16/Nov/23 12:29;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54567&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=9574;;;","16/Nov/23 16:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54631&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8271;;;","17/Nov/23 12:01;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54647&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7;;;","18/Dec/23 09:58;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55602&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8330;;;","15/Feb/24 08:00;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57534&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=7946;;;","20/Feb/24 14:40;mapohl;1.18: [https://github.com/apache/flink/actions/runs/7970805092/job/21759571142#step:10:8508];;;","28/Feb/24 07:05;mapohl;1.20: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57915&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8023;;;","18/Apr/24 09:05;rskraba;1.19 AdaptiveScheduler / Test (module: tests) https://github.com/apache/flink/actions/runs/8731358221/job/23956908384#step:10:8332;;;","22/Apr/24 08:36;rskraba;1.20 Java 17 / Test (module: tests) https://github.com/apache/flink/actions/runs/8761834736/job/24049016938#step:10:8525
;;;","03/Jun/24 01:43;Weijie Guo;1.20 test_cron_hadoop313 tests

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60006&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=9465;;;"
Kafka connector should depend on commons-collections instead of inherit from flink,FLINK-32522,13542300,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,taoran,taoran,03/Jul/23 12:18,11/Oct/23 10:33,04/Jun/24 20:40,,1.17.1,,,,,,,,,Connectors / Kafka,,,,,,0,auto-deprioritized-major,pull-request-available,,"Currently, externalized sql connector rely on flink main repo. but flink main repo has many test cases(especially in flink-python) reference flink-sql-kafka-connector.

If we change the dependencies(e.g.  commons-collections) in flink main repo, it cause exception:
!image-2023-07-03-20-15-47-608.png!

 

!https://user-images.githubusercontent.com/11287509/250120522-6b096a4f-83f0-4287-b7ad-d46b9371de4c.png!

 

So must add this dependency explicitly. Otherwise, it will cause external connectors block the upgrade of flink main. Connectors shouldn't rely on dependencies that may or may not be
available in Flink itself. ",,,,,,,,,,FLINK-30274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/23 12:15;taoran;image-2023-07-03-20-15-47-608.png;https://issues.apache.org/jira/secure/attachment/13061042/image-2023-07-03-20-15-47-608.png","03/Jul/23 12:16;taoran;image-2023-07-03-20-16-03-031.png;https://issues.apache.org/jira/secure/attachment/13061041/image-2023-07-03-20-16-03-031.png",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 16 22:35:16 UTC 2023,,,,,,,,,,"0|z1ixog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 10:10;taoran;As this thread [https://lists.apache.org/thread/l98pc18onxrcrsb01x5kh1vppl7ymk2d] discussed.
Connectors shouldn't rely on dependencies that may or may not be
available in Flink itself.

But currently kafka connector use commons-collections from flink, we should depend on commons-collections and bundle it in shaded-jar.;;;","08/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","16/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support GRPC Read/Write via GCS Connector,FLINK-32521,13542299,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jjayadeep,jjayadeep,03/Jul/23 12:13,03/Jul/23 12:13,04/Jun/24 20:40,,1.17.1,,,,,,,,,FileSystems,,,,,,0,,,,Hadoop GCS connector has enabled an experimental feature for using grpc along with http. This Jira is to enable accessing GCS via grpc via the hadoop gcs connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-03 12:13:53.0,,,,,,,,,,"0|z1ixo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkDeployment recovered states from an obsolete savepoint when performing an upgrade,FLINK-32520,13542286,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,ruibin,ruibin,03/Jul/23 10:53,05/Jul/23 02:56,04/Jun/24 20:40,05/Jul/23 02:56,1.13.1,,,,,,,,,Kubernetes Operator,,,,,,0,,,,"Kubernetes Operator version: 1.5.0
 

When upgrading one of our Flink jobs, it recovered from a savepoint created by the previous version of the job. The timeline of the job is as follows:
 # I upgraded the job for the first time. The job created a savepoint and successfully restored from it.
 # The job was running fine and created several checkpoints.
 # Later, I performed the second upgrade. Soon after submission and before the JobManager stopped, I realized I made a mistake in the spec, so I quickly did the third upgrade.
 # After the job started, I found that it had recovered from the savepoint created during the first upgrade.

 

It appears that there was an error when submitting the third upgrade. However, I'm still not quite sure why this would cause Flink to use the obsolete savepoint after investigating the code. The related logs for the operator are attached below.
 

Although I haven't found the root cause, I came up with some possible fixes:
 # Remove the {{lastSavepoint}} after a job has successfully restored from it.
 # Add options for savepoint, similar to: {{kubernetes.operator.job.upgrade.last-state.max.allowed.checkpoint.age}} The operator should refuse to recover from the savepoint if the max age is exceeded.
 # Create a flag in the status that records savepoint states. Set the flag to false when the savepoint starts and mark it as true when it successfully ends. The job should report an error if the flag for the last savepoint is false.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/23 10:40;ruibin;flink_kubernetes_operator_0615.csv;https://issues.apache.org/jira/secure/attachment/13061035/flink_kubernetes_operator_0615.csv","04/Jul/23 10:14;ruibin;logs-06151328-06151332.csv;https://issues.apache.org/jira/secure/attachment/13061052/logs-06151328-06151332.csv",,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 02:56:10 UTC 2023,,,,,,,,,,"0|z1ixlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 06:56;gyfora;Would it be possible to send the entire operator logs if you can reproduce this again? Would be good to see all the 3 upgrades in the logs;;;","04/Jul/23 06:59;gyfora;""Soon after submission and before the JobManager stopped"" what do you exactly mean here? When did you submit the 3rd upgrade?;;;","04/Jul/23 10:16;ruibin;[~gyfora]  I haven't been able to reproduce this yet. However, I can provide the entire logs during the restart. (see: logs-06151328-06151332.csv),
The name of the deployment is: octopus-flink-octopus-data-proces-8936e.

The times I submitted the second and third upgrades are 2023/06/15 05:30:02 +0000 and 2023/06/15 05:30:25 +0000. They might not match the wall clock of the logs exactly, though.;;;","04/Jul/23 14:35;gyfora;It's a bit difficult to understand the logs with the resource names in each log lines. You can configure the logger to get these from the MDC like in: [https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/conf/log4j-operator.properties#L26]

Otherwise I cannot really see anything suspicious, seems like the operator is always using the HA metadata during these upgrades (due to the LAST_STATE upgradeMode) which should always contain the latest checkpoint.;;;","05/Jul/23 02:56;ruibin;[~gyfora] Sorry, after reviewing, it turns out that the second and third upgrades were `last-state` due to a bug in our web UI. Still the third upgrades did used the savepoint from the first upgrade. Since I can't provide more information right now I will close this issue. I'm going to change the logger settings and try to reproduce this issue. Thanks for your time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for [CREATE OR] REPLACE TABLE AS statement,FLINK-32519,13542285,13542281,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,03/Jul/23 10:31,31/Jul/23 07:28,04/Jun/24 20:40,31/Jul/23 07:28,,,,,,1.18.0,,,,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 31 07:28:57 UTC 2023,,,,,,,,,,"0|z1ixl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/23 07:28;luoyuxia;master: fd0d2db378d42534273c166e6fb664a04ed282b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable atomicity for [CREATE OR] REPLACE table as statement,FLINK-32518,13542284,13542281,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,03/Jul/23 10:31,18/Jul/23 12:44,04/Jun/24 20:40,18/Jul/23 12:44,,,,,,1.18.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 12:44:25 UTC 2023,,,,,,,,,,"0|z1ixkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 12:44;luoyuxia;master:

cc9712bfe92de4a68e87baef8f6937dcdc8a8634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to execute [CREATE OR] REPLACE TABLE AS statement,FLINK-32517,13542283,13542281,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,03/Jul/23 10:29,07/Jul/23 01:16,04/Jun/24 20:40,07/Jul/23 01:16,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 01:16:37 UTC 2023,,,,,,,,,,"0|z1ixko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 01:16;luoyuxia;master:

08ef36eb3d6d08b45fa6b073ba5c6d9473ee97c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to parse [CREATE OR ] REPLACE TABLE AS statement,FLINK-32516,13542282,13542281,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,03/Jul/23 10:28,04/Jul/23 08:26,04/Jun/24 20:40,04/Jul/23 08:26,,,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 08:26:43 UTC 2023,,,,,,,,,,"0|z1ixkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 08:26;luoyuxia;master:

76d2a8d0deb8a81a98ed82b6c0613f79bf2a800c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-303: Support REPLACE TABLE AS SELECT statement,FLINK-32515,13542281,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,03/Jul/23 10:27,31/Jul/23 07:29,04/Jun/24 20:40,31/Jul/23 07:29,,,,,,1.18.0,,,,,,,,,,0,,,,Umbrella issue for [FLIP-303](https://cwiki.apache.org/confluence/display/FLINK/FLIP-303%3A+Support+REPLACE+TABLE+AS+SELECT+statement).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 11:32:05 UTC 2023,,,,,,,,,,"0|z1ixk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:32;knaufk;[~luoyuxia] I will mark this feature as finished for Flink 1.18 and assign the fixVersion accordingly. If this is indeed not done for Flink 1.18, please let me know.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-309: Support using larger checkpointing interval when source is processing backlog,FLINK-32514,13542280,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,03/Jul/23 10:09,02/Feb/24 02:39,04/Jun/24 20:40,24/Aug/23 08:37,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-309%3A+Support+using+larger+checkpointing+interval+when+source+is+processing+backlog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34149,,,,,FLINK-22805,FLINK-18578,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 18:47:17 UTC 2024,,,,,,,,,,"0|z1ixk0:",9223372036854775807,"ProcessingBacklog is introduced to demonstrate whether a record should be processed with low latency or high throughput. ProcessingBacklog can be set by source operators, and can be used to change the checkpoint internal of a job during runtime.",,,,,,,,,,,,,,,,,,,"24/Aug/23 08:37;lindong;Merged to apache/flink master branch 615e824195d23612d4b008e2cada92709d0a14af.;;;","18/Jan/24 11:56;martijnvisser;[~yunfengzhou] [~lindong] I've added support for testing 1.19-SNAPSHOT for the Flink Kafka connector. It seems to fail based on this change, as you can see in https://github.com/apache/flink-connector-kafka/actions/runs/7569481434/job/20612876543#step:14:134 

The FLIP highlighted that this change should be backward compatible, but this run seems to indicate otherwise. Can you take a look?;;;","18/Jan/24 12:26;lindong;[~martijnvisser] Sorry for the issue. It appears that we should added a default implementation (i.e. an empty body) for the newly-added method SplitEnumeratorContext#setIsProcessingBacklog for this PR to be backward compatible.

I think we need to add the default implementation in the next Flink 1.19 minor release. And we would need to update the Flink Kafka connector source code to add an empty implementation for this method to unblock its release.

[~yunfengzhou] Do you have time to create PR for these changes? I can help review the PR.

 

 
 ;;;","18/Jan/24 12:48;martijnvisser;I don't think that we should add the default implementation in the next Flink minor release: we had long discussions about API compatibility, and I think we wanted to bring extra attention to it. From my POV, we should hold off releasing 1.19 until this is addressed. But we can discuss it in the next 1.19 release sync on Tuesday.;;;","18/Jan/24 12:52;lindong;Somehow I thought Flink 1.19 has already been released (by mistake). Given that it is not released yet, I agree that we should fix this compatibility issue in Flink 1.19.;;;","18/Jan/24 18:47;mason6345;Agreed with Martijn. Thanks for catching this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
