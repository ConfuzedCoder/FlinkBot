Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Completes),Inward issue link (Completes),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Outward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
HA storage dir leaks on cluster termination ,FLINK-30513,13515745,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,27/Dec/22 13:40,19/Oct/23 12:42,04/Jun/24 20:41,,1.15.0,1.16.0,1.17.0,1.18.0,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"*Problem*

We found that HA storage dir leaks on cluster termination for a Flink job with HA enabled. The following picture shows the HA storage dir (here on HDFS) of the cluster czh-flink-test-offline (of application mode) after canelling the job with flink-cancel. We are left with an empty dir, and too many empty dirs will greatly hurt the stability of HDFS NameNode!

!image-2022-12-27-21-32-17-510.png|width=582,height=158!

 

Furthermore, in case the user choose to retain the checkpoints on job termination, we will have the completedCheckpoints leaked as well. Note that we no longer need the completedCheckpoints files as we'll directly recover retained CPs from the CP data dir.

*Root Cause*

When we run AbstractHaServices#closeAndCleanupAllData(), we cleaned up blob store, but didn't clean the HA storage dir.

*Proposal*

Clean up the HA storage dir after cleaning up blob store in AbstractHaServices#closeAndCleanupAllData().",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33288,,,,,,,,,,,,,,,,,,,,"27/Dec/22 13:32;Zhanghao Chen;image-2022-12-27-21-32-17-510.png;https://issues.apache.org/jira/secure/attachment/13054145/image-2022-12-27-21-32-17-510.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 19 12:42:29 UTC 2023,,,,,,,,,,"0|z1eekg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 09:25;Zhanghao Chen;Hi [~wangyang0918], looking forward to your opinions on it. Any feedback is much appreciated.;;;","12/Jan/23 11:55;wangyang0918;Given that the cluster-id is configured to k8s-app-ha-1-116, we will have following files in the HA storage directory. The completedCheckpoint and job graph files will be deleted once the job reached global terminal state(e.g. CANCELED, FAILED, FINISHED). So currently we only clean up the blob directory in the {{{}HighAvailabilityServices#closeAndCleanupAllData{}}}. And then the parent directory will be residual.

 

It is not a serious problem when using cloud object storage(S3, OSS, etc.). However, it is critical for HDFS since leaked directories will consume too much memory of NameNode.

 

I am +1 to delete the parent directory in {{HighAvailabilityServices#closeAndCleanupAllData.}}

 
{code:java}
wangyang-pc:scripts danrtsey.wy$ ossutil ls oss://flink-test-yiqi/flink-ha/
LastModifiedTime                   Size(B)  StorageClass   ETAG                                  ObjectName
2023-01-12 19:42:32 +0800 CST            0      Standard   D41D8CD98F00B204E9800998ECF8427E      oss://flink-test-yiqi/flink-ha/job-result-store/k8s-app-ha-1-116/
2023-01-12 19:44:16 +0800 CST            0      Standard   D41D8CD98F00B204E9800998ECF8427E      oss://flink-test-yiqi/flink-ha/k8s-app-ha-1-116/
2023-01-12 19:42:32 +0800 CST            0      Standard   D41D8CD98F00B204E9800998ECF8427E      oss://flink-test-yiqi/flink-ha/k8s-app-ha-1-116/blob/
2023-01-12 19:42:44 +0800 CST      5426525      Standard   EF151FCD3D1F91C3EB512118F05D2E20      oss://flink-test-yiqi/flink-ha/k8s-app-ha-1-116/blob/job_ffffffffe07004590000000000000000/blob_p-a28c3727cd8d501e6b024187013e4311079499be-b327aba6b7fee1f6b0fb4fe66a88a637
2023-01-12 19:44:16 +0800 CST        14288      Standard   0983C3E02FBC7A904207A12295A3AF28      oss://flink-test-yiqi/flink-ha/k8s-app-ha-1-116/completedCheckpoint4679f587e15a
2023-01-12 19:42:44 +0800 CST        34080      Standard   B37673CA07B30B8A0FC76939BBDFA9F7      oss://flink-test-yiqi/flink-ha/k8s-app-ha-1-116/submittedJobGraph427419c216b4 {code};;;","13/Jan/23 02:17;Zhanghao Chen;[~wangyang0918] Could you assign this task to me? I can start preparing a PR for it. Thanks~;;;","20/Feb/23 10:22;Zhanghao Chen;[~wangyang0918] Could you help review the PR when you are free? Much appreciated;;;","09/Mar/23 06:08;Weijie Guo;master(1.18) via 13779ab8e4f5539ca311d9f233d031d818af6450.
revert via 593cc139ab30bb81ab38d94b7b697d00eaaecada as [comments in pull request|https://github.com/apache/flink/pull/21673#discussion_r1130781417].;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","19/Oct/23 12:42;xinchen147;Also one question, in abnormal scenarios, such as when the task is running on Yarn and I kill it, both the directory and subdirectory blob will still exist. Perhaps the community does not consider abnormal scenarios, as we have previously added a hook to clear the appid directory under '/user/hadoop/. flink', It includes jar packages uploaded by Flink when submitting tasks.
But this hook involves Hadoop and it also needs to be modified. In the hook code, hadoop needs to provide us with feedback about the status of the task. I think it may be due to the interaction between multiple communities, and the Flink community did not actively consider clearing in this abnormal situation? And here for 'high availability.storageDir', the situation is the same. 

How to clean up two directories: /user/hadoop/.flink/xxxappId &  'high availability.storageDir' when a task is killed? Whether the community will consider these in the future, and give any better suggestions？;;;",,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL state TTL has no effect when using Interval Join,FLINK-30512,13515738,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wangkang,wangkang,27/Dec/22 11:58,06/Jan/23 03:43,04/Jun/24 20:41,03/Jan/23 09:21,1.15.1,1.16.1,,,,,,,,,Runtime / State Backends,Table SQL / Runtime,,,0,,,,"Take the following join SQL program as an example:
{code:java}
SET 'table.exec.state.ttl' = '900000 ms';
select 
...
from kafka_source_dwdexpose as t1
left join kafka_source_expose_attr_click t3 
ON t1.mid = t3.mid and t1.sr = t3.sr 
and t1.time_local = t3.time_local 
and t1.log_ltz BETWEEN t3.log_ltz - INTERVAL '2' MINUTE  AND t3.log_ltz + INTERVAL '2' MINUTE {code}
!flink1.16.png|width=906,height=278!

the state size is getting bigger and bigger.

we also test the same sql with flink sql 1.13,the state size is stable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/22 11:52;wangkang;flink1.16.png;https://issues.apache.org/jira/secure/attachment/13054144/flink1.16.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 03:43:09 UTC 2023,,,,,,,,,,"0|z1eeiw:",9223372036854775807,"add param : set table.exec.source.idle-timeout=120s; the issue fixed.",,,,,,,,,,,,,,,,,,,"03/Jan/23 09:21;wangkang;add param : set table.exec.source.idle-timeout=120s; the issue fixed.;;;","06/Jan/23 03:43;Yanfei Lei;Thanks for reporting this issue and sharing the solution.

From your screenshot, the job ran for 45min, your ttl was set to 15min and the checkpoint interval is 5min. The full checkpoint size of chk-14 and chk-18 is smaller than the previous one, so I think TTL is in effect. 

> the state size is getting bigger and bigger.
I guess it is because the TPS of your job is relatively high during this period, new states grow faster than old states expire.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore the Exception in user-timer Triggerble when recover form state.,FLINK-30511,13515732,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,RocMarshal,RocMarshal,27/Dec/22 11:27,05/Jan/23 10:00,04/Jun/24 20:41,05/Jan/23 10:00,1.16.0,,,,,,,,,,API / DataStream,,,,0,,,,"* Code segment:

{code:java}
public class OnTimerDemo {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.setString(""taskmanager.numberOfTaskSlots"", ""4"");
        conf.setString(""state.checkpoint-storage"", ""filesystem"");
        conf.setString(""state.checkpoints.dir"", ""file:///tmp/flinkjob"");
        conf.setString(""execution.checkpointing.interval"", ""30s"");

        //conf.setString(""execution.savepoint.path"", ""file:///tmp/flinkjob/159561b8c97c9e0b4f9eeb649086796a/chk-1""); // Anchor-A:

        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);

        env.setParallelism(1);

        EnvironmentSettings envSetting = EnvironmentSettings
                .newInstance()
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableEnv =  StreamTableEnvironment.create(env, envSetting);

        String sourceDDL = ""CREATE TABLE orders (\n"" +
                ""  id           INT,\n"" +
                ""  app          INT,\n"" +
                ""  user_id      STRING"" +
                "") WITH (\n"" +
                ""   'connector' = 'datagen',\n"" +
                ""   'rows-per-second'='1',\n"" +
                ""   'fields.app.min'='1',\n"" +
                ""   'fields.app.max'='10',\n"" +
                ""   'fields.user_id.length'='10'\n"" +
                "")"";

        tableEnv.executeSql(sourceDDL);

        Table query = tableEnv.sqlQuery(""select * from orders"");
        DataStream<Row> rowDataStream = tableEnv.toAppendStream(query, Row.class);

        TypeInformation<?>[] returnTypes = new TypeInformation[4];
        returnTypes[0] = Types.INT;

        returnTypes[1] = Types.INT; // Anchor-B:

        returnTypes[2] = Types.INT;
        returnTypes[3] = Types.INT;


        rowDataStream.keyBy(new KeySelector<Row, String>() {
                    @Override
                    public String getKey(Row value) throws Exception {
                        return value.getFieldAs(2);
                    }
                }).process(new KeyedProcessFunction<String, Row, Row>() {

                    private Row firstRow;

                    @Override
                    public void processElement(Row value, Context ctx, Collector<Row> out) throws Exception {
                        if (firstRow == null) {
                            firstRow = value;
                        }
                        ctx.timerService().registerProcessingTimeTimer(System.currentTimeMillis() + 3000);
                    }

                    @Override
                    public void onTimer(long timestamp, OnTimerContext ctx, Collector<Row> out) throws Exception {
                        Row colRow = new Row(4);
                        colRow.setField(0, 0);
                        colRow.setField(1, 1);
                        colRow.setField(2, 2);
                        colRow.setField(3, 3);

                        out.collect(colRow); // Anchor-C

                    }
                }).name(""TargetTestUDF"")
                .returns(new RowTypeInfo(returnTypes))
                .print();

        env.execute(OnTimerDemo.class.getSimpleName());
    }

}
 {code}
 * Recurrence steps
 ** Run the job without state.
 ** Collect the latest available checkpoint path as 'checkpoint-path-a'
 ** Stop the job.
 ** Fill the real value of 'checkpoint-path-a' into 'Anchor-A' line and un-comment the line.
 ** Set 'returnTypes[1] = Types.INT;' -> 'returnTypes[1] = Types.LONG;' at the 'Anchor-B' line.
 ** Then add break-point at 'StreamTask#handleAsyncException' method.
 ** Run the job. The 'java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long' exception caused at the 'Anchor-C' line will ignore at  'StreamTask#handleAsyncException' method.
 ** So, The framework can't catch the same exception in the case.
 * Root cause:
 ** !截屏2022-12-27 18.51.12.png!
 ** When job started from state data, the Task#restoreAndInvoke would be called. The exception 'java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long' was ignored at the above 'handleAsyncException' method instead of catching at catch-block of 'Task#restoreAndInvoke'.

                       !截屏2022-12-27 19.20.00.png!

Could it be seen as the framework's missing handling of exceptions? 
If so, I prefer to re-throw the exception at 'StreamTask#handleAsyncException', which is suitable for the intention of the 'Task#restoreAndInvoke'.

Thank u.
 
 
 

 ","Flink 1.16.0

java8

deployment Mode: miniCluster in IDC; standalone, yarn-application.",,,,,,,,,,,,,,,,,,,,,,,FLINK-29816,,,,,,,,,,,,,,,,,,,,"27/Dec/22 11:14;RocMarshal;截屏2022-12-27 18.51.12.png;https://issues.apache.org/jira/secure/attachment/13054141/%E6%88%AA%E5%B1%8F2022-12-27+18.51.12.png","27/Dec/22 11:23;RocMarshal;截屏2022-12-27 19.20.00.png;https://issues.apache.org/jira/secure/attachment/13054140/%E6%88%AA%E5%B1%8F2022-12-27+19.20.00.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 10:00:05 UTC 2023,,,,,,,,,,"0|z1eehk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 08:59;kevin.cyj;[~RocMarshal] Thanks for reporting. I guess an easy way to fix the issue is to throw exception for initializing state, which seems the default behavior before FLINK-17012.;;;","05/Jan/23 09:10;Weijie Guo;[~RocMarshal] Thanks for reporting this. IIUC, this is the same problem as FLINK-29816, I will track and fix this in that ticket.;;;","05/Jan/23 09:54;RocMarshal;Thank you [~Weijie Guo] [~kevin.cyj] . Looking forward to the fix.
 
 
 

 ;;;","05/Jan/23 10:00;kevin.cyj;I am closing this ticket for now. We can discuss in FLINK-29816.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow configuring minimum and maximum metrics window size,FLINK-30510,13515724,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,mxm,mxm,mxm,27/Dec/22 10:25,03/Jan/23 14:08,04/Jun/24 20:41,03/Jan/23 14:08,kubernetes-operator-1.3.0,,,,,,,,,,Kubernetes Operator,,,,0,,,,It would be more flexible to have a minimum and maximum window size for metric collection instead of a fixed window size. This would allow for faster metrics evaluation after a rescale operation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30464,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 14:07:43 UTC 2023,,,,,,,,,,"0|z1eefs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 14:07;mxm;After the discussion here we decided not to support this feature for now: https://github.com/apache/flink-kubernetes-operator/pull/493#discussion_r1056547504;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify sql-client.sh,FLINK-30509,13515716,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,yzl,yzl,27/Dec/22 09:07,15/Jan/23 09:43,04/Jun/24 20:41,15/Jan/23 09:42,1.17.0,,,,,1.17.0,,,,,Table SQL / Client,,,,0,pull-request-available,,,New design of SQL client will depend on the sql-gateway module. So add the jar to the jar path in starting script.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 15 09:42:51 UTC 2023,,,,,,,,,,"0|z1eee0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/23 09:42;fsk119;Merged into master: db5affc61599f93ebe59d29114fb29ac6aaa45af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientITCase.testSqlStatements failed with output not matched with expected,FLINK-30508,13515706,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,renqs,renqs,27/Dec/22 07:27,14/Feb/23 02:06,04/Jun/24 20:41,09/Feb/23 03:30,1.16.0,1.17.0,,,,1.16.2,1.17.0,,,,Table SQL / Client,,,,0,test-stability,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44246&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14992,,,,,,,,,,,,,,,,,FLINK-30640,,,,,,,FLINK-30640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 02:06:25 UTC 2023,,,,,,,,,,"0|z1eebs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 07:27;renqs;[~fsk119] any idea on this one?;;;","27/Dec/22 07:30;renqs;Other instances not linked correctly: 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43772&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14992]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43917&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14190]

 ;;;","04/Jan/23 12:16;fsk119;Thanks for reporting this issue. It seems the job has been cleaned up before the client fetch results... Pleaes take a look at the following logs

 
{code:java}
04:10:04,314 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'collect' (17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,317 [flink-akka.actor.default-dispatcher-12] ERROR org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler [] - Unhandled exception.
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (17922fda0de3fe8d5d4d82f496fd8480)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1212) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1227) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:985) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
    at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
    at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
    at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
    at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor f8383d25-f1de-44e9-8c1c-98a7465bc349 because: Stopping JobMaster for job 'collect' (17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [037b6db35609cc3bef4112cb2b2c2301].
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection f91ca618e2bf66e6cd04e4537ba14346: Stopping JobMaster for job 'collect' (17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:3, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (274877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, allocationId: 037b6db35609cc3bef4112cb2b2c2301, jobId: 17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,326 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 17922fda0de3fe8d5d4d82f496fd8480 from job leader monitoring.
04:10:04,326 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 17922fda0de3fe8d5d4d82f496fd8480.
04:10:04,331 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 9234f8e5984b2c41461d696253fe4725@akka://flink/user/rpc/jobmanager_10 for job 17922fda0de3fe8d5d4d82f496fd8480 from the resource manager.
04:10:04,334 [          Thread-164] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: foo[57].
04:10:04,335 [          Thread-164] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: foo[57] closed.
04:10:04,336 [          Thread-157] WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (17922fda0de3fe8d5d4d82f496fd8480)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1212)
    at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1227)
    at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:985)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)End of exception on server side>]
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_292]
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170) ~[flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129) [flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) [flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) [flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222) [flink-table-planner_2.12-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:75) [classes/:?]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (17922fda0de3fe8d5d4d82f496fd8480)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1212)
    at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1227)
    at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:985)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)End of exception on server side>]
    at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:534) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:514) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
04:10:04,437 [          Thread-157] INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
04:10:04,449 [          Thread-157] INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
04:10:04,467 [                main] INFO  org.apache.flink.table.resource.ResourceManager              [] - Added jar resource [file:/tmp/junit3065787421766710063/test-jar/test-classloader-udf.jar] to class path.
04:10:04,467 [                main] INFO  org.apache.flink.table.resource.ResourceManager              [] - Register resource [/tmp/junit3065787421766710063/test-jar/test-classloader-udf.jar] successfully.
04:10:04,498 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore               [] - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
04:10:04,499 [                main] INFO  org.apache.hadoop.hive.metastore.ObjectStore                 [] - ObjectStore, initialize called
04:10:04,504 [                main] INFO  org.apache.hadoop.hive.metastore.MetaStoreDirectSql          [] - Using direct SQL, underlying DB is DERBY
04:10:04,504 [                main] INFO  org.apache.hadoop.hive.metastore.ObjectStore                 [] - Initialized ObjectStore
04:10:04,600 [                main] INFO  org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner [] - Starting generating logical plan
04:10:04,601 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore               [] - 0: get_function: default.hive_add_one
04:10:04,601 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit         [] - ugi=vsts_azpcontainer    ip=unknown-ip-addr    cmd=get_function: default.hive_add_one    
04:10:04,613 [                main] INFO  org.apache.flink.connectors.hive.HiveTableFactory            [] - Successfully loaded Hive udf 'hive_add_one' with class 'HiveAddOneFunc'
04:10:04,613 [                main] INFO  org.apache.flink.connectors.hive.HiveTableFactory            [] - Transforming Hive function 'hive_add_one' into a HiveSimpleUDF
04:10:04,615 [                main] INFO  org.apache.flink.table.functions.hive.HiveSimpleUDF          [] - Creating HiveSimpleUDF from 'HiveAddOneFunc'
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Completed phase 1 of Semantic Analysis
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Get metadata for source tables
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Get metadata for subqueries
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Get metadata for destination tables
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Completed getting MetaData in Semantic Analysis
04:10:04,617 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore               [] - 0: get_function: default.hive_add_one
04:10:04,617 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit         [] - ugi=vsts_azpcontainer    ip=unknown-ip-addr    cmd=get_function: default.hive_add_one {code};;;","09/Jan/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44552&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=13186;;;","09/Jan/23 10:37;mapohl;[~fsk119] will you be able to look into it a come up with a fix for this test stability? May I assign this task to you?;;;","10/Jan/23 10:53;mapohl;This is a 1.16 build:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44635&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=15012;;;","16/Jan/23 07:27;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44824&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=15722;;;","16/Jan/23 09:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44856&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15204;;;","17/Jan/23 07:14;fsk119;Sorry for the late response. I will take a look.;;;","23/Jan/23 08:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45148&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13534;;;","06/Feb/23 07:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=14051;;;","06/Feb/23 07:15;mapohl;Any updates on that one [~fsk119]?;;;","06/Feb/23 08:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45727&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13572;;;","07/Feb/23 11:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45828&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=46497;;;","08/Feb/23 06:22;fsk119;I read recently failed tests are related to the CTAS syntax. It seems the query doesn't wait for the last INSERT INTO statement to finish. Could you share some thoughts about Hive behavior [~lsy] [~luoyuxia];;;","08/Feb/23 06:34;luoyuxia;Hi, the reason maybe ctas statement write data to files and rename files to commit {_}asynchronously{_}, then the following statemt try to read the files written, but the ctas statement may rename these files at same time, so the exception ""can not access file"" as them have been renamed.

[~lsy] Could you please help fix as it's introuced by your changes?;;;","08/Feb/23 09:21;lsy;I have open a PR in another issue.;;;","09/Feb/23 06:20;mapohl;[~lsy] With ""another issue"" you mean FLINK-30640?;;;","10/Feb/23 02:33;fsk119;[~mapohl] you are right. But we haven't cherry pick the fix to the 1.16 yet.;;;","10/Feb/23 06:40;mapohl;Then, the Jira issue management is a bit confusing here: Either, we open this one again to do the 1.16 backport here or (the one I would suggest doing) open FLINK-30640 again, add 1.16.1 as an affected version and provide the backport for it;;;","13/Feb/23 14:58;mapohl;[~fsk119] can you give a bit of guidance here? It looks like this one is fixed by FLINK-30640. But there is no 1.16 backport provided, yet.;;;","14/Feb/23 02:06;fsk119;Sorry for the late response. I have cherry-picked the commit to the 1.16 right now.  Because this is only a test issue, I think either way is fine. I have added 1.16.2 to the fixed versions.

The merged commit in 1.16 is 0994832be8a121273be423de05203390b7bdece4

 ;;;",,,,,,,,,
EventTimeWindowCheckpointingITCase.testTumblingTimeWindowWithKVStateMinMaxParallelism could not be initialized,FLINK-30507,13515700,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,renqs,renqs,27/Dec/22 06:58,05/Jan/23 14:54,04/Jun/24 20:41,05/Jan/23 14:54,1.17.0,,,,,,,,,,Tests,,,,0,test-stability,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44245&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12367,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20523,,,,,,,,,,FLINK-28523,,,,,,,,"29/Dec/22 07:48;Wencong Liu;image-2022-12-29-15-48-55-564.png;https://issues.apache.org/jira/secure/attachment/13054204/image-2022-12-29-15-48-55-564.png","29/Dec/22 07:51;Wencong Liu;image-2022-12-29-15-51-12-230.png;https://issues.apache.org/jira/secure/attachment/13054205/image-2022-12-29-15-51-12-230.png","29/Dec/22 07:52;Wencong Liu;image-2022-12-29-15-52-17-508.png;https://issues.apache.org/jira/secure/attachment/13054207/image-2022-12-29-15-52-17-508.png",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 14:54:46 UTC 2023,,,,,,,,,,"0|z1eeag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 07:52;Wencong Liu;    Hello [~renqs]. For this failed IT case, I found that the main reason is ZookeeperServer startup timed out. ZookeeperServer uses the minSessionTimeout value as the actual timeout parameter by default, and the default value of minSessionTimeout is 6s. Currently, only maxSessionTimeout = 60s is configured in the startup configuration. Since it's larger than the default value of 6s, this configuration does not take effect.

!image-2022-12-29-15-52-17-508.png!

!image-2022-12-29-15-48-55-564.png!

!image-2022-12-29-15-51-12-230.png!    

    Therefore, minSessionTimeout can be set to avoid the slow startup of ZookeeperServer when resources are tight. I tend to set minSessionTimeout to 12s. Do you think this change is reasonable?;;;","05/Jan/23 08:49;mapohl;[~Wencong Liu] could you share logs to support your conclusion?

I looked into the logs and found an {{BindException}} that might be the cause for the timeout:
{code}
05:12:28,231 [         Thread-2155] INFO  org.apache.curator.test.TestingZooKeeperMain                 [] - Could not start server. Waiting and trying one more time.
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_292]
        at sun.nio.ch.Net.bind(Net.java:461) ~[?:1.8.0_292]
        at sun.nio.ch.Net.bind(Net.java:453) ~[?:1.8.0_292]
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222) ~[?:1.8.0_292]
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85) ~[?:1.8.0_292]
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78) ~[?:1.8.0_292]
        at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:687) ~[zookeeper-3.5.9.jar:3.5.9]
        at org.apache.zookeeper.server.ServerCnxnFactory.configure(ServerCnxnFactory.java:76) ~[zookeeper-3.5.9.jar:3.5.9]
        at org.apache.curator.test.TestingZooKeeperMain.internalRunFromConfig(TestingZooKeeperMain.java:244) [curator-test-5.2.0.jar:5.2.0]
        at org.apache.curator.test.TestingZooKeeperMain.runFromConfig(TestingZooKeeperMain.java:136) [curator-test-5.2.0.jar:5.2.0]
        at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:162) [curator-test-5.2.0.jar:5.2.0]
{code}

We've seen several other test failures with this issue (e.g. FLINK-24505 and FLINK-20523). FLINK-20523 already lists the cause. It appears that there is a bug in curator (CURATOR-535) that causes this instability. I would close this issue as a duplicate of FLINK-20523 if you agree.;;;","05/Jan/23 13:50;Wencong Liu;Hello [~mapohl]. I should haven't seen this root cause. This issue can be closed. Thanks for your information. ;;;","05/Jan/23 14:54;mapohl;No worries. Thanks for confirming it. I'm closing this issue in favor of FLINK-20523.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for writing Table Store with Spark3,FLINK-30506,13515681,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,27/Dec/22 02:51,29/Dec/22 02:45,04/Jun/24 20:41,29/Dec/22 02:45,table-store-0.3.0,table-store-0.4.0,,,,table-store-0.3.0,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,Table Store 0.3 supports writing with Spark3. We need to add documentation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 02:45:42 UTC 2022,,,,,,,,,,"0|z1ee68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 02:45;TsReaper;master: 33c30f2e79e85ee15130320d6e712601c1b44d6c
release-0.3: d02276de2fdcf90b50b3cbb1da75055b960704e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close the connection between TM and JM when task executor failed,FLINK-30505,13515491,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,damumu,damumu,26/Dec/22 10:23,02/Feb/23 08:42,04/Jun/24 20:41,02/Feb/23 08:42,1.16.0,,,,,1.17.0,,,,,Runtime / Task,,,,0,pull-request-available,,,"When resource manager detects a task executor has failed, it will close connection with task executor. At this time,jobs running on this tm will fail for other reasons(no longger reachable or heartbeat timeout).
!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/336411/1672047809511-a4b8b5d9-f11f-483c-a113-b42290a33250.png|width=1160,id=uc24b1166!
If close the connection between task executor and job master when resource manager detects a task executor has failed,the real reason for task executor failure will appear in ""Root Exception"".This will make it easier for users to find problems.
!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/336411/1672048733572-2b5b7be4-087d-46ae-9c8d-6ad5a1344019.png|width=1141,id=u947d8c4e!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 08:41:57 UTC 2023,,,,,,,,,,"0|z1ed00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 02:10;xtsong;I don't see how the proposed change makes a difference. The exception in the 2nd screenshot is not the _real reason_ of the TM failure. It practically said the same thing as the exception in the 1st screenshot, that the TM is no longer reachable. To understand the real reason, you need to check the TM/K8s logs anyway.;;;","27/Dec/22 02:56;damumu;I do this to make the exception more obvious when the worker is terminated.;;;","27/Dec/22 03:10;xtsong;On the contrary, I think this can be misleading sometimes. Upon TM becoming unreachable, RM will try to remove the pod/container from K8s/Yarn anyway. That means the TM may still be alive while being unreachable (stuck, network problems, etc.), and will then be terminated by K8s. Thus the termination of the pod can be either the cause or the result of TM being unreachable.;;;","27/Dec/22 04:21;damumu;If the termination of the pod is caused by TM being unreachable,the first exception thrown is tm unreachable,this does not mislead users.On the other hand, close the connection between task executor and job master when resource manager detects a task executor has failed,jobs can failover earlier.;;;","27/Dec/22 05:52;xtsong;bq. If the termination of the pod is caused by TM being unreachable, the first exception thrown is tm unreachable,this does not mislead users.

The exception that causes the execution state transiting to FAILED would be the termination of the pod, if we close JM-TM connection when RM-TM connection is closed.

I think this changes the protocol between JM / RM / TM, in a way that RM can control  the connection between JM and TM. It also adds a O(numJM * numTM) overhead in the RM RPC main thread. With these prices, the benefit is unclear to me. I'm not convinced that a pod termination exception is more suitable than a TM unreachable exception for the ""Root Exception"" on UI.

So I'm overall -1 to this proposal.;;;","27/Dec/22 08:24;damumu;[~xtsong] Thanks for participating in this discussion;;;","02/Feb/23 08:41;gaoyunhaii;Thanks [~damumu] and [~xtsong] for the discussion! Now as it seems we have already reached consistency, I'll first close this issue. If there are future issues let's open the issue again.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix UnsupportedFileSystemSchemeException when writing Table Store on OSS with other engines,FLINK-30504,13515484,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,26/Dec/22 09:06,27/Dec/22 02:14,04/Jun/24 20:41,27/Dec/22 02:14,table-store-0.3.0,table-store-0.4.0,,,,table-store-0.3.0,table-store-0.4.0,,,,Table Store,,,,0,pull-request-available,,,"Currently when writing Table Store tables on OSS with other engines (for example Spark), the following exception will occur.

{code}
22/12/23 17:54:12 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (core-1-1.c-c9f1b761c8946269.cn-huhehaote.emr.aliyuncs.com executor 2): java.lang.RuntimeException: Failed to find latest snapshot id
  at org.apache.flink.table.store.file.utils.SnapshotManager.latestSnapshotId(SnapshotManager.java:81)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.scanExistingFileMetas(AbstractFileStoreWrite.java:87)
  at org.apache.flink.table.store.file.operation.KeyValueFileStoreWrite.createWriter(KeyValueFileStoreWrite.java:113)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.createWriter(AbstractFileStoreWrite.java:227)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.lambda$getWriter$1(AbstractFileStoreWrite.java:217)
  at java.util.HashMap.computeIfAbsent(HashMap.java:1128)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.getWriter(AbstractFileStoreWrite.java:217)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.write(AbstractFileStoreWrite.java:106)
  at org.apache.flink.table.store.table.sink.TableWriteImpl.write(TableWriteImpl.java:63)
  at org.apache.flink.table.store.spark.SparkWrite$WriteRecords.call(SparkWrite.java:124)
  at org.apache.flink.table.store.spark.SparkWrite$WriteRecords.call(SparkWrite.java:105)
  at org.apache.spark.api.java.JavaPairRDD$.$anonfun$toScalaFunction$1(JavaPairRDD.scala:1070)
  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)
  at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)
  at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)
  at org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)
  at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:136)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'oss'. The scheme is directly supported by Flink through the following plugin(s): flink-oss-fs-hadoop. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.
  at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:515)
  at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
  at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274)
  at org.apache.flink.table.store.file.utils.SnapshotManager.findLatest(SnapshotManager.java:164)
  at org.apache.flink.table.store.file.utils.SnapshotManager.latestSnapshotId(SnapshotManager.java:79)
  ... 30 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 02:14:19 UTC 2022,,,,,,,,,,"0|z1ecyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 02:14;TsReaper;master: 929c1110bdbf521ad0a9eb09a65d33f76a2b5990
release-0.3: fe8de4c32c148bb87f5a40649ca2373e88f321d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceITCase$IntegrationTests.testSavepoint failed with timeout,FLINK-30503,13515477,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,renqs,renqs,renqs,26/Dec/22 07:46,16/Oct/23 07:08,04/Jun/24 20:41,16/Oct/23 07:08,1.17.0,,,,,,,,,,Connectors / Kafka,,,,0,stale-assigned,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44202&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37262

{code}
[...]
Dec 24 01:36:26 [ERROR] org.apache.flink.connector.kafka.source.KafkaSourceITCase$IntegrationTests.testSavepoint(TestEnvironment, DataStreamSourceExternalContext, CheckpointingMode)[2]  Time elapsed: 35.447 s  <<< ERROR!
Dec 24 01:36:26 java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
Dec 24 01:36:26 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Dec 24 01:36:26 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Dec 24 01:36:26 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Dec 24 01:36:26 	at org.apache.flink.connector.kafka.testutils.KafkaSourceExternalContext.close(KafkaSourceExternalContext.java:160)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30879,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 22:35:25 UTC 2023,,,,,,,,,,"0|z1ecxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint state Continued to increase,FLINK-30502,13515474,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,douheyu,douheyu,26/Dec/22 07:20,29/Dec/22 08:00,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,,,,"I am using flink incremental checkpoint and configured state ttl.

But I found a lot of files outside the ttl in the hdfs directory (checkpoint/shard).

what happened and how can fix it?",flink-1.13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 08:00:23 UTC 2022,,,,,,,,,,"0|z1ecwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 08:00;Wencong Liu;Hello [~douheyu] , thanks for your feedback. Could you please take a look? [~Yanfei Lei] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink build instruction to deprecate Java 8 instead of requiring Java 11,FLINK-30501,13515436,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,25/Dec/22 08:34,06/Mar/23 03:01,04/Jun/24 20:41,06/Mar/23 02:56,,,,,,1.17.0,,,,,Build System / CI,,,,0,pull-request-available,,,"Flink 1.15 and later versions require at least Java 11 to build from sources [1], whereas the pom.xml specifies the source/target is 1.8. This inconsistency confuses users.

As mentioned in the FLINK-25247 title, the goal of that ticket is to ""Inform users about deprecation"". It will be better to inform users that ""Java 8 is deprecated"" instead of saying ""Fink requires at least Java 11 to build"", so that users have the right information to make the right choice for themselves.

Also note that Flink community is regularly running flink-ml benchmark for both Java 8 and Java 11 [2], which suggests that we are practically ensuring Java 8 is supported.

If we decide to official drop Java 8 support, Flink community probably should give explicit notice regarding the deprecation period so that users can be prepared for this breaking change. We can follow the Kafka website doc [3] for example, which says ""Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0"".

[1] https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/flinkdev/building/
[2] http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&env=2
[3] https://kafka.apache.org/documentation/

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 02:56:06 UTC 2023,,,,,,,,,,"0|z1ecog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 14:02;chesnay;Flink is currently built for Java 8 by default because that's what is actually being released. Changing just the default would result in users that build Flink themselves to have to know about certain flags to actually reproduce the release.
We encourage developers to use Java 11, and nothing beyond that (i.e., exactly Java 11).

FYI, the issue you had was that you used Java 19.;;;","12/Jan/23 14:03;chesnay;I've closed this as ""later"" because Java 11 will be the default once we dropped support for Java 8.;;;","12/Jan/23 14:39;lindong;[~chesnay] I think you have closed this ticket too early before correctly understanding its goal.

As mentioned in the ticket title and description, the goal of this ticket is to ""set requireJavaVersion in pom.xml to be at least 11"". This is because we have explicitly mentioned in the Flink doc here [1] that ""Flink requires at least Java 11 to build"".

If Flink requires exactly Java 11 to be built from source, I think we need to fix the doc[1] properly. The goal of this ticket can be updated to require exactly Java 11.

BTW, I don't think the issue is because I used Java 19. I double checked that I was using Java 8 when observing the error mentioned in the ticket. Maybe you can try it by youself?

 

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/flinkdev/building/;;;","12/Jan/23 15:02;chesnay;What I meant with ""beyond that"" is that we don't support Java 12 or above. That Flink does build with Java 8 was hidden from the docs since the deprecation of Java 8 support, so that new developers start developing with Java 11 from the get go.

ASM errors have traditionally always been about unsupported JDKs, as ASM needs to be updated for each JDK release.
The ""19"" in your error message indicated to me that it was compiled with Java 19.

I just compiled the current Flink master successfully with Java 8, and CI is also working.

I'd suggest to double-check what Java version is shown by {{mvn --version}} and whether some Java 19 JDK isn't on PATH. I remember running into such strange issues in the past.;;;","14/Jan/23 00:40;lindong;[~chesnay] Thanks for the information. In my case, I have not installed Java 19 on my machine. It is hard to tell why Java 8 does not work on my machine but works on yours. it is probably simpler to just ignore this issue since the motivation/reason for this ticket can be explained based on the statement on the Flink website.

I guess we can resolve this ticket after most other tickets in https://issues.apache.org/jira/browse/FLINK-25244 are resolved. What do you think?

 
[1] [https://nightlies.apache.org/flink/flink-docs-master/docs/flinkdev/building/]
 
 

 ;;;","01/Mar/23 13:47;zhongpu314;[~lindong] Currently, 1.8 is still good enough to build Flink from source with Java 8 in my computer [1].

 [1] https://stackoverflow.com/questions/75601233/;;;","01/Mar/23 14:01;lindong;[~zhongpu314] Thanks for the information. It seems that the pom.xml and the doc is inconsistent regarding whether Java 11 is required. I have updated the JIRA description to explain this issue.
;;;","01/Mar/23 14:22;lindong;[~zhongpu314] I will upload a PR to fix the doc so that it is consistent with the existing behavior of Flink.;;;","01/Mar/23 14:37;zhongpu314;Plz have a look at the answer of my SO answer. The remove of “Java 8” in
doc is intentional, and it is to encourage users to switch to Java 11
before Flink really discards Java 8 in code.


-- 
Zhongpu Chen
;;;","01/Mar/23 15:11;lindong;[~zhongpu314] But it is incorrect to say that ""Flink requires at least Java 11 to build"" while we do allow Flink to be built from Java 8, right?

Would it be better to just say ""Java 8 is deprecated""?;;;","02/Mar/23 15:00;martijnvisser;[~lindong] If you want to change the documentation, I would change ""Flink requires at least Java 11 to build."" to ""It is recommended to use Java 11 when building Flink"". ;;;","03/Mar/23 01:19;lindong;[~martijnvisser] I agree we can say we recommend Java 11. How about saying this: ""Flink requires **Java 8 (deprecated) or Java 11 (recommended)** to build""?

Here is the reason I am inclined to explicitly specify ""Java 8 (deprecated)"". Today, many users are still using Java 8 and it is reasonable for users to ask whether Flink supports Java 8. Instead of requiring users to ask this question on mailing list, we probably should provide answer on the Flink website so that it is easy for users to find the right answer by themselves.

I understand we want to encourage users to use Java 11. I just think we should provide the right information to users and let users make their own choice.;;;","03/Mar/23 01:27;lindong;Note that the latest [Spark version 3.3.2|https://spark.apache.org/docs/latest/building-spark.html] still supports Java 8. And the [latest Kafka version 3.3.x|https://kafka.apache.org/documentation/] also supports Java 8. Both projects are widely used and they explicitly list the supported Java versions on their official doc website.

And it is explicitly mentioned on the Kafka website that ""Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0"".

Maybe we should follow their approach regarding whether to specify Java 8 support and how to encourage users to use Java 11.;;;","06/Mar/23 02:56;lindong;Merged to apache/flink master branch 88d28a1745938482f607745c60e7305007a42174.;;;",,,,,,,,,,,,,,,,,
Got an null pointer exception when using table environment in sub-thread,FLINK-30500,13515412,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,SpongebobZ,SpongebobZ,24/Dec/22 09:51,24/Dec/22 10:05,04/Jun/24 20:41,,1.14.3,,,,,,,,,,,,,,0,,,,"I can use the `addInsertSql` function nornally in norm situation, but when I create the table env in main thread and then run this function in sub-thread, I got an ambigious exception that was thrown by calcite.

 
{code:java}
public static void main(String[] args) throws InterruptedException {
    TableEnvironment tableEnvironment = TableEnvironment.create(EnvironmentSettings.newInstance().inBatchMode().build());
    String ddl = ""CREATE TABLE IF NOT EXISTS SOURCE("" +
            ""A DECIMAL(19),"" +
            ""B DECIMAL(19)"" +
            "") WITH ("" +
            ""'connector' = 'jdbc'...)"";
    String ddl2 = ""CREATE TABLE IF NOT EXISTS SINK("" +
            ""A DECIMAL(19),"" +
            ""B DECIMAL(19)"" +
            "") WITH ("" +
            ""'connector' = 'print')"";
    tableEnvironment.executeSql(ddl);
    tableEnvironment.executeSql(ddl2);
    StatementSet statementSet = tableEnvironment.createStatementSet();
    CompletableFuture.runAsync(() -> {
        statementSet.addInsertSql(""INSERT INTO SINK SELECT * FROM SOURCE"");
        try {
            statementSet.execute().await();
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException(e);
        }
    }).exceptionally((e) -> {
        e.printStackTrace();
        return null;
    });

    Thread.sleep(10000);
} {code}
 
{code:java}
Caused by: java.lang.NullPointerException
    at java.util.Objects.requireNonNull(Objects.java:203)
    at org.apache.calcite.rel.metadata.RelMetadataQuery.<init>(RelMetadataQuery.java:144)
    at org.apache.calcite.rel.metadata.RelMetadataQuery.<init>(RelMetadataQuery.java:108)
    at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.<init>(FlinkRelMetadataQuery.java:78)
    at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:59)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)
    at org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)
    at org.apache.calcite.rel.logical.LogicalFilter.create(LogicalFilter.java:108)
    at org.apache.calcite.rel.core.RelFactories$FilterFactoryImpl.createFilter(RelFactories.java:344)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertWhere(SqlToRelConverter.java:1042)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:666)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:644)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3438)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2169)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2093)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:663)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:644)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3438)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2169)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2093)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertJoin(SqlToRelConverter.java:2866)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2162)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertJoin(SqlToRelConverter.java:2864)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2162)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertJoin(SqlToRelConverter.java:2864)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2162)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:663)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:644)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3438)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:570)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:177)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:169)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1057)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1026)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:301)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:639)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:290)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
    at org.apache.flink.table.api.internal.StatementSetImpl.addInsertSql(StatementSetImpl.java:53) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-24 09:51:06.0,,,,,,,,,,"0|z1ecj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Over window codegen error when member code incorrectly used in method body,FLINK-30499,13515402,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Runking,Runking,Runking,24/Dec/22 01:01,14/Aug/23 10:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,"There is a bug in batch over window operator codegen, the comparator codegen logic is wrong that member code incorrectly used in method body. This causes over window code compile failed when operator initialization.

The incorrect generated code for RangeBoundComparator is below, and the wrong place is *private static final java.util.TimeZone timeZone = java.util.TimeZone.getTimeZone(""Asia/Shanghai"");*

which is a member statments that should be placed in class body, not method body.
{code:java}
      public class RangeBoundComparator$40 implements org.apache.flink.table.runtime.generated.RecordComparator {

        private final Object[] references;
        

        public RangeBoundComparator$40(Object[] references) {
          this.references = references;
          
          
        }

        @Override
        public int compare(org.apache.flink.table.data.RowData in1, org.apache.flink.table.data.RowData in2) {
          
                  org.apache.flink.table.data.TimestampData field$41;
                  boolean isNull$41;
                  org.apache.flink.table.data.TimestampData field$42;
                  boolean isNull$42;
                  isNull$41 = in1.isNullAt(0);
                  field$41 = null;
                  if (!isNull$41) {
                    field$41 = in1.getTimestamp(0, 3);
                  }
                  isNull$42 = in2.isNullAt(0);
                  field$42 = null;
                  if (!isNull$42) {
                    field$42 = in2.getTimestamp(0, 3);
                  }
                  if (isNull$41 && isNull$42) {
                     return 1;
                  } else if (isNull$41 || isNull$42) {
                     return -1;
                  } else {
                     
                            private static final java.util.TimeZone timeZone =
                                             java.util.TimeZone.getTimeZone(""Asia/Shanghai"");
                            org.apache.flink.table.data.TimestampData result$43;
                            boolean isNull$43;
                            boolean isNull$44;
                            org.apache.flink.table.data.binary.BinaryStringData result$45;
                            boolean isNull$46;
                            long result$47;
                            org.apache.flink.table.data.TimestampData result$48;
                            boolean isNull$48;
                            boolean isNull$49;
                            org.apache.flink.table.data.binary.BinaryStringData result$50;
                            boolean isNull$51;
                            long result$52;
                            boolean isNull$53;
                            long result$54;
                            boolean isNull$55;
                            boolean result$56;
                            isNull$43 = (org.apache.flink.table.data.TimestampData) field$41 == null;
                            result$43 = null;
                            if (!isNull$43) {
                              result$43 = (org.apache.flink.table.data.TimestampData) field$41;
                            }
                            isNull$48 = (org.apache.flink.table.data.TimestampData) field$42 == null;
                            result$48 = null;
                            if (!isNull$48) {
                              result$48 = (org.apache.flink.table.data.TimestampData) field$42;
                            }
                            
                            
                            
                            
                            
                            isNull$44 = isNull$43;
                            result$45 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
                            
                            if (!isNull$44) {
                              
                              result$45 = org.apache.flink.table.data.binary.BinaryStringData.fromString(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.timestampToString(result$43, 6));
                              isNull$44 = (result$45 == null);
                            }
                            
                            isNull$46 = isNull$44;
                            result$47 = -1L;
                            
                            if (!isNull$46) {
                              
                              result$47 = 
                            org.apache.flink.table.runtime.functions.SqlDateTimeUtils.unixTimestamp(result$45.toString(), timeZone)
                                       ;
                              
                            }
                            
                            
                            
                            
                            isNull$49 = isNull$48;
                            result$50 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
                            
                            if (!isNull$49) {
                              
                              result$50 = org.apache.flink.table.data.binary.BinaryStringData.fromString(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.timestampToString(result$48, 6));
                              isNull$49 = (result$50 == null);
                            }
                            
                            isNull$51 = isNull$49;
                            result$52 = -1L;
                            
                            if (!isNull$51) {
                              
                              result$52 = 
                            org.apache.flink.table.runtime.functions.SqlDateTimeUtils.unixTimestamp(result$50.toString(), timeZone)
                                       ;
                              
                            }
                            
                            isNull$53 = isNull$46 || isNull$51;
                            result$54 = -1L;
                            
                            if (!isNull$53) {
                              
                              result$54 = (long) (result$47 - result$52);
                              
                            }
                            
                            
                            isNull$55 = isNull$53 || false;
                            result$56 = false;
                            
                            if (!isNull$55) {
                              
                              result$56 = result$54 >= ((int) -600000);
                              
                            }
                            
                            if (result$56) {
                              return 1;
                            } else {
                              return -1;
                            }
                          
                  }
               
        }
      }
       {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 10:35:06 UTC 2023,,,,,,,,,,"0|z1ecgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/22 01:08;Runking;This is a bug finded when fixing timestamp cast bug described in https://issues.apache.org/jira/browse/FLINK-25802

[~libenchao] Would you like to take a look? And I'm glad to fix it.;;;","26/Dec/22 12:13;libenchao;[~Runking] Thanks for reporting and volunteering to fix this, assigned to you, happy coding!;;;","27/Dec/22 02:07;Runking;[~libenchao] Thanks a lot, I will create a PR ASAP.;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
frocksdb throw an UnsatisfiedLinkError,FLINK-30498,13515356,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,tanjialiang,tanjialiang,23/Dec/22 10:32,27/Dec/22 06:05,04/Jun/24 20:41,23/Dec/22 11:03,1.14.6,,,,,,,,,,Runtime / State Backends,,,,0,,,,"Using an simple TopN SQL with table.exec.state.ttl options, statebackend use rocksdb
{code:java}
SET table.exec.state.ttl = 1min;

CREATE TABLE `student` (
  id int,
  name string,
  update_time bigint
) WITH (
  'connector' = 'datagen',
  'fields.id.kind' = 'random',
  'fields.id.min' = '0',
  'fields.id.max' = '100'
);

CREATE TABLE `blackhole` (
  id int,
  name string,
  update_time bigint
) WITH (
  'connector' = 'blackhole'
);

INSERT INTO blackhole
SELECT 
    id, 
    name,
    update_time
FROM (
  SELECT
      id, 
      name,
      update_time,
      ROW_NUMBER() OVER (PARTITION BY id ORDER BY update_time DESC) AS row_num 
  FROM student
) AS t
WHERE t.row_num=1; {code}
It will throw an error from frocksdb
{code:java}
java.lang.UnsatisfiedLinkError: org.rocksdb.FlinkCompactionFilter.createNewFlinkCompactionFilterConfigHolder()J
    at org.rocksdb.FlinkCompactionFilter.createNewFlinkCompactionFilterConfigHolder(Native Method) ~[flink-dist_2.12-1.14.2.jar:?]
    at org.rocksdb.FlinkCompactionFilter.access$000(FlinkCompactionFilter.java:14) ~[flink-dist_2.12-1.14.2.jar:?]
    at org.rocksdb.FlinkCompactionFilter$ConfigHolder.<init>(FlinkCompactionFilter.java:115) ~[flink-dist_2.12-1.14.2.jar:?]
    at org.rocksdb.FlinkCompactionFilter$FlinkCompactionFilterFactory.<init>(FlinkCompactionFilter.java:142) ~[flink-dist_2.12-1.14.2.jar:?]
    at org.apache.flink.contrib.streaming.state.ttl.RocksDbTtlCompactFiltersManager.createAndSetCompactFilterFactory(RocksDbTtlCompactFiltersManager.java:84) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.contrib.streaming.state.ttl.RocksDbTtlCompactFiltersManager.setAndRegisterCompactFilterIfStateTtl(RocksDbTtlCompactFiltersManager.java:74) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.createColumnFamilyDescriptor(RocksDBOperationUtils.java:157) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.createStateInfo(RocksDBOperationUtils.java:134) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:643) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:837) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.ttl.TtlStateFactory.createTtlStateContext(TtlStateFactory.java:225) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.ttl.TtlStateFactory.createValueState(TtlStateFactory.java:148) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.ttl.TtlStateFactory.createState(TtlStateFactory.java:132) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.ttl.TtlStateFactory.createStateAndWrapWithTtlIfEnabled(TtlStateFactory.java:72) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getOrCreateKeyedState(AbstractKeyedStateBackend.java:302) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getPartitionedState(AbstractKeyedStateBackend.java:353) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.DefaultKeyedStateStore.getPartitionedState(DefaultKeyedStateStore.java:115) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:60) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:203) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.table.runtime.operators.rank.FastTop1Function.open(FastTop1Function.java:114) ~[flink-table_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.api.operators.KeyedProcessOperator.open(KeyedProcessOperator.java:55) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:110) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.2.jar:1.14.2]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_302] {code}
 

I try in kubernetes application mode, it will throw an error just in 1.14, not found in 1.15.3 and 1.16.0. And i try standalone mode in 1.14, error not found.

It is any change in Dockerfile between 1.14 and 1.15?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 06:05:01 UTC 2022,,,,,,,,,,"0|z1ec6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/22 04:11;Yanfei Lei;Hi [~tanjialiang] , I guess it may be caused by the conflict or loss of the frocksdbjni jar. Could you share how you solved this problem?;;;","26/Dec/22 07:57;tanjialiang;Hi [~Yanfei Lei] , i made a stupid mistake, it was my team's problem, so i close this issue. And yes, it was caused by thre conflict with the frocksdbjni jar, but not caused by the flink official jar, At first i think it was the flink docker's bug, because i found it in the 1.14 not in 1.15 and 1.16, but when i start the 1.14 cluster just in docker and without any extend jar, it was running success. So i think caused by extend jar in 1.14 cluster, at the end, i found it was my team's defined extend jar.;;;","27/Dec/22 06:05;Yanfei Lei;👍Thanks for sharing, this can be an experience for others who encounter similar problems.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableChange to represent DROP change,FLINK-30497,13515353,13515348,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,23/Dec/22 10:03,05/Jan/23 04:27,04/Jun/24 20:41,05/Jan/23 04:27,1.17.0,,,,,1.17.0,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 04:27:36 UTC 2023,,,,,,,,,,"0|z1ec60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 04:27;fsk119;Merged into master: cce059c556bc2538cba8918e281320516c4be222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableChange to represent MODIFY change,FLINK-30496,13515352,13515348,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,23/Dec/22 10:02,03/Jan/23 12:15,04/Jun/24 20:41,03/Jan/23 12:15,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 12:15:05 UTC 2023,,,,,,,,,,"0|z1ec5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 12:15;fsk119;Merged into master: d5b8ad7672cd981cdbc757549ed2ab1fcf007500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableChange to represent ADD change,FLINK-30495,13515351,13515348,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,23/Dec/22 10:01,30/Dec/22 08:10,04/Jun/24 20:41,30/Dec/22 08:10,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 30 08:10:34 UTC 2022,,,,,,,,,,"0|z1ec5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 08:10;fsk119;Merged into master: 5c3658aa06b79e8039043145560a1ad2bcce68b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableChange to represents SET/RESET change,FLINK-30494,13515350,13515348,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,23/Dec/22 10:00,27/Dec/22 12:39,04/Jun/24 20:41,27/Dec/22 12:39,1.17.0,,,,,1.17.0,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 12:39:27 UTC 2022,,,,,,,,,,"0|z1ec5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 12:39;fsk119;Merged into master: 75a92efd7b35501698e5de253e5231d680830c16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP 273: Improve the Catalog API to Support ALTER TABLE syntax,FLINK-30493,13515348,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,,fsk119,fsk119,23/Dec/22 09:59,07/Feb/23 06:49,04/Jun/24 20:41,07/Feb/23 06:49,1.17.0,,,,,1.17.0,,,,,Table SQL / API,,,,1,pull-request-available,,,"Introduce the TableChange to support ALTER TABLE better

https://cwiki.apache.org/confluence/display/FLINK/FLIP-273%3A+Improve+the+Catalog+API+to+Support+ALTER+TABLE+syntax",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-23 09:59:28.0,,,,,,,,,,"0|z1ec4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive overview document given an Incorrect dependency artifactId example,FLINK-30492,13515345,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,Weijie Guo,Weijie Guo,Weijie Guo,23/Dec/22 09:32,20/Jan/23 07:27,04/Jun/24 20:41,30/Dec/22 09:12,1.16.0,1.17.0,,,,,,,,,Documentation,,,,0,pull-request-available,,,"flink-table-api-java-bridge do not have suffix of scala version from 1.15, but document of hive still using the artifactId with scala as example.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 30 14:47:51 UTC 2022,,,,,,,,,,"0|z1ec48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 09:12;martijnvisser;The Hive connector is still tied to the Flink Planner, which requires you to use the Planner with Scala bundled;;;","30/Dec/22 09:29;Weijie Guo;[~martijnvisser] Thanks for the feedback, but I may have some confusion. I know that the `flink-connector-hive` module requires the scala version, so I only remove the suffix of `flink-table-api-java-bridge` from document's pom.xml example. Because In the current code base, the `flink-table-api-java-bridge` module does not need the scala version, and the artifactId of this dependency found in the Maven central repo do not have the scala suffix. ;;;","30/Dec/22 10:07;martijnvisser;[~Weijie Guo] See https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/hive/overview/#moving-the-planner-jar - The Hive connector requires {{flink-table-planner_2.12-1.16.0.jar}} instead of {{flink-table-planner-loader-1.16.0.jar}}. 

See https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/hive/overview/#program-maven for all details for Maven;;;","30/Dec/22 10:55;luoyuxia;If just for `flink-table-api-java-bridge`, I think we don't need the scala suffix.;;;","30/Dec/22 10:59;Weijie Guo;[~luoyuxia] Yes, I only remove the suffix of `flink-table-api-java-bridge` in this pull request, cc [~martijnvisser].;;;","30/Dec/22 14:47;martijnvisser;[~luoyuxia] I do think it is needed, because the Hive connector relies on the Scala version of the planner (instead of the loader). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Hive table partition supports to deserialize later during runtime,FLINK-30491,13515323,13500276,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,23/Dec/22 06:05,10/Jan/23 02:35,04/Jun/24 20:41,10/Jan/23 02:35,1.16.0,,,,,1.17.0,,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 02:35:26 UTC 2023,,,,,,,,,,"0|z1ebzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 02:35;godfrey;Fixed in 1.17.0: d4340d16708010394d7c57063b5dece8362d41d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleted topic from KafkaSource is still included in subsequent restart from savepoint,FLINK-30490,13515322,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,hilmialf,hilmialf,23/Dec/22 05:56,16/Oct/23 15:48,04/Jun/24 20:41,16/Oct/23 15:48,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"It seems that KafkaSource does not handle the removed topic partitions after restarting from offset. So far is still commented out as TODO. ([ref|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/enumerator/KafkaSourceEnumerator.java#:~:text=//%20TODO%3A%20Handle%20removed%20partitions.])

I am wondering if there is an exact plan on when this will be done, what are technical consideration, etc. Since I really need this feature, I would really love to contribute under guidance from maintaner.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31483,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 06:25:55 UTC 2023,,,,,,,,,,"0|z1ebz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 03:03;ruibin;hi, we are encountering the same issue and are actively working on this.;;;","16/Mar/23 03:19;hilmialf;Hi [~ruibin]. Thanks for reply. I wonder is there any related ticket I can follow?;;;","16/Mar/23 06:25;ruibin;[~hilmialf] I created a new ticket for the proposal. FLINK-31483.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-sql-connector-pulsar doesn't shade all dependencies,FLINK-30489,13515281,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,bayard,bayard,22/Dec/22 21:19,13/Feb/23 13:42,04/Jun/24 20:41,13/Feb/23 13:42,1.16.0,,,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"Looking at [https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-pulsar/1.16.0/flink-sql-connector-pulsar-1.16.0.jar] I'm seeing that some dependencies are shaded (com.fasterxml, com.yahoo etc), but others are not (org.sfl4j, org.bouncycastel, com.scurrilous, ...) and will presumably clash with other jar files.

Additionally, this bundling is going on in the '.jar' file rather than in a more clearly indicated separate -bundle or -shaded jar file. 

As a jar file this seems confusing and potentially bug inducing; though I note I'm just a review of the jar and not Flink experienced.",,,,,,,,,,,,FLINK-30606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 13:42:59 UTC 2023,,,,,,,,,,"0|z1ebq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/22 12:14;martijnvisser;[~syhily] WDYT?;;;","30/Dec/22 01:03;syhily;We didn't shade these dependencies because these dependencies are not always required for Pulsar connector. Such as {{org.bouncycastel}}, you will need it only if you use the end-to-end encryption.

But we can truly shade all the dependencies.

{{org.sfl4j}} dependency will be removed in the next Pulsar 2.11 release. So no need to shade it.;;;","13/Feb/23 07:51;syhily;[~tison] Can you assign this issue to me?;;;","13/Feb/23 13:42;tison;master via 5f0bb2db9f1f357a11cce965eb5832bb908523a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenSearch implementation of Async Sink,FLINK-30488,13515272,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,reta,reta,22/Dec/22 18:27,20/Aug/23 10:35,04/Jun/24 20:41,,opensearch-1.0.0,,,,,,,,,,Connectors / Opensearch,,,,0,auto-deprioritized-major,pull-request-available,,"The current OpenSearch connector only uses the SinkFunction. 
 * Implement an asynchronous sink (Async Sink) support for OpenSearch connector
 * Update documentation
 * Add end to end tests

More details to be found [https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink]",,,,,,,,,,,,,,,,,,,,,,,FLINK-24041,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:02 UTC 2023,,,,,,,,,,"0|z1ebo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RabbitMQ Table/SQL connector documentation,FLINK-30487,13515226,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,22/Dec/22 12:31,22/Dec/22 12:31,04/Jun/24 20:41,,,,,,,,,,,,Connectors/ RabbitMQ,,,,0,,,,"While there is a RabbitMQ SQL connector, there is no documentation page for the RabbitMQ SQL connector. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-22 12:31:06.0,,,,,,,,,,"0|z1ebeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the config documentation generator available to connector repos,FLINK-30486,13515225,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,22/Dec/22 12:15,04/Apr/23 04:16,04/Jun/24 20:41,04/Apr/23 04:16,,,,,,,,,,,Connectors / Common,Connectors / Pulsar,Documentation,,0,,,,"Most connectors can be externalized without any issues. This becomes problematic when connectors provide configuration options, like Pulsar does. 

As discussed in https://github.com/apache/flink/pull/21501#discussion_r1046979593 we need to make the config documentation generator available for connector repositories. ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30399,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 02:43:39 UTC 2023,,,,,,,,,,"0|z1ebe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 02:43;syhily;I just copied the code from flink-docs and execute in flink-connector-pulsar to generate all the docs. It would be great to have a related jar file in maven.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-connector-rabbitmq from master branch,FLINK-30485,13515206,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,22/Dec/22 09:36,10/Jan/23 21:19,04/Jun/24 20:41,10/Jan/23 21:19,,,,,,1.17.0,,,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,,"With RabbitMQ being synchronized to https://github.com/apache/flink-connector-rabbitmq we can now remove the RabbitMQ connector from master so it won't be included in the 1.17 release anymore

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29594,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 21:19:00 UTC 2023,,,,,,,,,,"0|z1eba0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 09:11;mapohl;[~martijnvisser] may you remove 1.17 as a fixed version from FLINK-29594 after this issue's PR is merged to {{master}}?;;;","10/Jan/23 21:19;martijnvisser;Fixed in master: cfe794b792b906b2129a24fdac456abbeefa28eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testZooKeeperReelection timed out,FLINK-30484,13515197,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,22/Dec/22 08:33,21/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,,,,,,,,,,Runtime / Coordination,,,,0,auto-deprioritized-major,test-stability,,"{{ZooKeeperLeaderElectionTest.testZooKeeperReelection}} timed out in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44161&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=15416

{code}
Dec 22 05:00:20 ""main"" #1 prio=5 os_prio=0 tid=0x00007f1c7c00b800 nid=0x1ebdc waiting on condition [0x00007f1c82b31000]
Dec 22 05:00:20    java.lang.Thread.State: WAITING (parking)
Dec 22 05:00:20 	at sun.misc.Unsafe.park(Native Method)
Dec 22 05:00:20 	- parking to wait for  <0x000000008070b7c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
Dec 22 05:00:20 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Dec 22 05:00:20 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
Dec 22 05:00:20 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
Dec 22 05:00:20 	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.lambda$waitForNewLeader$0(TestingRetrievalBase.java:50)
Dec 22 05:00:20 	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase$$Lambda$310/1033917063.get(Unknown Source)
Dec 22 05:00:20 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Dec 22 05:00:20 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
Dec 22 05:00:20 	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.waitForNewLeader(TestingRetrievalBase.java:48)
Dec 22 05:00:20 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperReelection(ZooKeeperLeaderElectionTest.java:238)
{code}",,,,,,,,,,,,,,,,,FLINK-26522,FLINK-30338,FLINK-29173,,,,,,,,,,,,,,,,,FLINK-30342,FLINK-28078,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 22:35:20 UTC 2023,,,,,,,,,,"0|z1eb80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/22 12:57;mapohl;This issue has the same cause as FLINK-28078: CURATOR-645 causes it because we're reusing the LeaderLatch client. We could add a sleep (similar to how it got fixed in FLINK-28078), or stop reusing the LeaderLatch client. Alternatively, we could just upgrade to curator 5.4.0 where CURATOR-645 is fixed (FLINK-29173).

Extract from the {{zookeeper-server-1.log}} of the failed build:
{code}
[...]
04:44:47,525 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getData cxid:0xe0 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_73c184d1-e0c3-4884-9476-e55cf87c5963-latch-0000000007
04:44:47,525 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:getData cxid:0xe0 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_73c184d1-e0c3-4884-9476-e55cf87c5963-latch-0000000007
04:44:47,525 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getChildren2 cxid:0xe1 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
04:44:47,525 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:getChildren2 cxid:0xe1 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
04:44:47,526 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:delete cxid:0xe2 zxid:0x37 txntype:2 reqpath:n/a
04:44:47,526 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:delete cxid:0xe2 zxid:0x37 txntype:2 reqpath:n/a
04:44:47,526 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:create2 cxid:0xe3 zxid:0x38 txntype:15 reqpath:n/a
04:44:47,526 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:create2 cxid:0xe3 zxid:0x38 txntype:15 reqpath:n/a
04:44:47,527 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getData cxid:0xe4 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_175e24a6-59f8-486b-a6c6-3630c0cd00bd-latch-0000000008
04:44:47,527 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:getData cxid:0xe4 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_175e24a6-59f8-486b-a6c6-3630c0cd00bd-latch-0000000008
04:44:47,527 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getChildren2 cxid:0xe5 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
04:44:47,527 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:getChildren2 cxid:0xe5 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
04:44:47,528 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:delete cxid:0xe6 zxid:0x39 txntype:2 reqpath:n/a
04:44:47,528 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:delete cxid:0xe6 zxid:0x39 txntype:2 reqpath:n/a
04:44:47,528 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:create2 cxid:0xe7 zxid:0x3a txntype:15 reqpath:n/a
04:44:47,528 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:create2 cxid:0xe7 zxid:0x3a txntype:15 reqpath:n/a
04:44:47,528 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getData cxid:0xe8 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_707dcb78-1f57-497b-8859-c18258fb2e7e-latch-0000000009
04:44:47,529 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:getData cxid:0xe8 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_707dcb78-1f57-497b-8859-c18258fb2e7e-latch-0000000009
04:44:47,529 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getChildren2 cxid:0xe9 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
04:44:47,529 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:getChildren2 cxid:0xe9 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
04:44:47,530 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:delete cxid:0xea zxid:0x3b txntype:2 reqpath:n/a
04:44:47,530 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:delete cxid:0xea zxid:0x3b txntype:2 reqpath:n/a
04:44:47,531 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:create2 cxid:0xeb zxid:0x3c txntype:15 reqpath:n/a
04:44:47,531 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1000026f4a00000 type:create2 cxid:0xeb zxid:0x3c txntype:15 reqpath:n/a
04:44:47,531 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1000026f4a00000 type:getData cxid:0xec zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_87c2ef30-fe84-410f-a2d0-39a634e3a463-latch-0000000010
[...]
{code};;;","23/Dec/22 13:00;mapohl;There are multiple Jiras which would fix this issue:
* updating curator to 5.4.0 (FLINK-29173)
* Migrating the legacy tests to the new per-process leader election implementation (FLINK-30338)
* getting rid of this test as part of FLINK-26522 where we want to improve the leader election classes in general

I'll leave this ticket open for documentation purposes.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 11:27;mapohl;curator 5.4.0 was added in 1.18. The leader election tests were refactored in 1.18 as part of FLINK-26522 (making this test rely on single-process leader election), too. I'm leaving this one open for documentation purposes in case it appears in 1.17-. But it's more likely that we're not going to touch this issue due to it rarely happening. We can close the issue with 1.17 becoming deprecated.;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Make Avro format support for TIMESTAMP_LTZ,FLINK-30483,13515193,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jadireddi,liuml07,liuml07,22/Dec/22 08:22,14/Nov/23 20:53,04/Jun/24 20:41,,1.16.0,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,2,pull-request-available,stale-assigned,,"Currently Avro format does not support TIMESTAMP_LTZ (short for TIMESTAMP_WITH_LOCAL_TIME_ZONE) type. Avro 1.10+ introduces local timestamp logic type (both milliseconds and microseconds), see spec [1]. As TIMESTAMP currently only supports milliseconds, we can make TIMESTAMP_LTZ support milliseconds first.

A related work is to support microseconds, and there is already work-in-progress Jira FLINK-23589 for TIMESTAMP type. We can consolidate the effort or track that separately for TIMESTAMP_LTZ.

[1] https://avro.apache.org/docs/1.10.2/spec.html#Local+timestamp+%28millisecond+precision%29",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33198,,,,,,,,,,,,,,,FLINK-23589,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 14 20:52:07 UTC 2023,,,,,,,,,,"0|z1eb74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 08:29;martijnvisser;[~liuml07] Isn't this a duplicate of FLINK-23589? I don't think this ticket can be resolved until FLINK-17224 is fixed. Do you want to pick that up?;;;","22/Dec/22 08:32;liuml07;Sample code:
{code:java}
  public static void main(String[] args) {
    final TableEnvironment tableEnv =
        TableEnvironment.create(EnvironmentSettings.inStreamingMode());
    tableEnv.executeSql(
        ""CREATE TABLE SinkTable(ts TIMESTAMP_LTZ(3)) ""
            + ""WITH ('connector' = 'filesystem', 'path' = '/tmp/flink-avro-test', 'format' = 'avro')"");
    tableEnv.executeSql(""INSERT INTO SinkTable VALUES (PROCTIME())"");
    tableEnv.executeSql(""SELECT *  FROM SinkTable"").print();
  }
{code}
will get following exception:
{code}
Exception in thread ""main"" java.lang.UnsupportedOperationException: Unsupported to derive Schema for type: TIMESTAMP_LTZ(3)
	at org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:429)
	at org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:400)
	at org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:306)
{code};;;","22/Dec/22 08:35;liuml07;[~martijnvisser] I linked FLINK-23589 in the Jira description. I think it's a related issue, but that is for microseconds support (larger precision for TIMESTAMP) type. This Jira tracks TIMESTAMP_LTZ type, which is not supported regardless of precision.;;;","22/Dec/22 08:38;martijnvisser;But this ticket also requires FLINK-17224 right?;;;","22/Dec/22 09:07;liuml07;I'm not sure this requires FLINK-17224. Today TIME and TIMESTAMP both support milliseconds (precision 3). It seems to me only TIME supporting precision doesn't work correctly and TIMESTAMP just works fine. TIMESTAMP_LTZ in this JIRA is more related to TIMESTAMP and I guess it's not blocked by FLINK-17224.

FLINK-23589 is to add microseconds (precision 6) support for TIMESTAMP. I think it is not blocked by FLINK-17224 either. Meanwhile, FLINK-27331 is to add microseconds precision for TIME which is indeed blocked by FLINK-17224.

I have not looked into those work carefully (or test myself) and could have missed something.;;;","29/Dec/22 10:21;jadireddi;Hi,
I am able to make code changes locally. Can you help me to assign this jira? Will raise a PR.

Regards,

Jagadesh;;;","29/Dec/22 19:46;liuml07;Thanks [~jadireddi] ! I started making changes in a local branch but have not finished. Glad you have a locally working version! I can help review.

I do not have permission to assign Jira to you. We need a committer to help us with that. But as we coordinate here, I think you can file the PR now.;;;","03/Jan/23 19:13;jadireddi;Hi [~liuml07] ,

Thank you for the response. Filed a PR. Could you please review it.
[https://github.com/apache/flink/pull/21594];;;","04/Jan/23 00:06;liuml07;Thanks [~jadireddi] I'll review from next week. This week I'm oncall. I glimpsed and it looks promising to me overall.

[~martijnvisser] I think we need your help for assigning this Jira to Jagadesh and reviewing. I looked again and this does not require FLINK-17224. We can address that separately. ;;;","04/Jan/23 14:55;jadireddi;Thank you [~liuml07] for letting me know. That should be good. ;;;","21/Jan/23 14:18;jadireddi;Hi [~liuml07] , Can you please review PR at your feasible time.;;;","01/Feb/23 11:31;dwysakowicz;[~jadireddi] We did a review of the PR in github. Sorry it took so long. Do you mind telling if you're still interested in finalising the PR? Thanks!;;;","19/Jul/23 03:17;james.johnston.thumbtack;I have some high-level questions/concerns about whether the proposed mapping to Avro's local timestamp logical type is really the right one.  More details in the PR: https://github.com/apache/flink/pull/21594#issuecomment-1641319553;;;","18/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Nov/23 20:52;liuml07;Just a heads up there is a FLIP for this [FLIP-378|https://cwiki.apache.org//confluence/display/FLINK/FLIP-378%3A+Support+Avro+timestamp+with+local+timezone] and related discussions can happen in the maillist and/or related Jira FLINK-33198;;;",,,,,,,,,,,,,,,,
Update catalog documentation ,FLINK-30482,13515162,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,22/Dec/22 03:58,22/Dec/22 03:58,04/Jun/24 20:41,,,,,,,,,,,,Documentation,,,,0,,,,After creating implementation of glue catalog update catalog page documentation ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-22 03:58:51.0,,,,,,,,,,"0|z1eb08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create base aws-glue catalog implementation in flink-connector-aws,FLINK-30481,13515161,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,22/Dec/22 03:57,13/May/24 14:43,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-22 03:57:42.0,,,,,,,,,,"0|z1eb00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add benchmarks for adaptive batch scheduler,FLINK-30480,13515158,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,zhuzh,zhuzh,22/Dec/22 03:27,12/Jun/23 08:00,04/Jun/24 20:41,12/Jun/23 07:58,,,,,,1.18.0,,,,,Benchmarks,Runtime / Coordination,,,0,pull-request-available,,,Currently we only have benchmarks of DefaultScheduler(FLINK-20612). We should also have benchmarks of AdaptiveBatchScheduler to identify initializing/scheduling/deployment performance problems or regressions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32288,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 08:30:15 UTC 2023,,,,,,,,,,"0|z1eazc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 03:30;xiasun;Hi, I would like to work on this issue, could you assign this ticket to me? [~zhuzh] ;;;","22/Dec/22 03:35;zhuzh;Thanks for volunteering. [~xiasun]
I have assigned you the ticket.;;;","22/Dec/22 08:56;Weijie Guo;[~xiasun] Thank you for your willingness to do this job, I just want to remind you that `AdaptiveBatchScheduler` now also supports scheduling job with hybrid type edges. Please don't forget to consider this situation in the benchmark.;;;","22/Dec/22 09:29;xiasun;Thank you very much for  your reminder [~Weijie Guo] !

I'll take this into account.;;;","09/Jun/23 08:30;zhuzh;flink master: 27285d671276f70c197c3ef79adcb518a37ec560
flink-benchmarks master: 2c03dca3c8e8b4aabac1ff803cb363cbd36c74d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Document flink-connector-files for local execution,FLINK-30479,13515149,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liuml07,liuml07,22/Dec/22 00:45,20/Aug/23 10:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Documentation,,,,0,auto-deprioritized-major,pull-request-available,,"The file system SQL connector itself is included in Flink and does not require an additional dependency. However, if a user uses the filesystem connector for [local execution|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/dataset/local_execution/#local-execution], for e.g. running Flink job in the IDE, she will need to add dependency. Otherwise, the user will get validation exception: {{{}Cannot discover a connector using option: 'connector'='filesystem'{}}}. This is confusing and can be documented.

The scope of the files connector dependency should be {{{}provided{}}}, because they should not be packaged into the JAR file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:02 UTC 2023,,,,,,,,,,"0|z1eaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't depend on IPAddressUtil,FLINK-30478,13515115,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sdmoralesma,gunnar.morling,gunnar.morling,21/Dec/22 16:51,24/Apr/23 10:13,04/Jun/24 20:41,24/Apr/23 10:13,,,,,,1.18.0,,,,,API / Core,,,,0,pull-request-available,,,"The class \{{org.apache.flink.util.NetUtils}} uses the JDK-internal class \{{sun.net.util.IPAddressUtil}}. On current JDKs (16+), this causes issues as access to this class is prevented by default and would require an additional \{{--add-opens}} clause. That's undesirable in particular in cases where we don't control the JVM start-up arguments, e.g. when using Flink embedded into a custom Java application.

I suggest to replace this logic using the [IPAddress|https://github.com/seancfoley/IPAddress/] library (Apache License v2), which implements everything we need without relying on internal classes. I have a patch for that ready and will submit it for discussion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 10:13:10 UTC 2023,,,,,,,,,,"0|z1eaqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 10:13;chesnay;master: c02fcfbf9f3b05f62543dbf88af33617f68fb953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not properly blocking retries when timeout occurs in AsyncWaitOperator,FLINK-30477,13515094,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,21/Dec/22 13:24,02/Feb/23 07:54,04/Jun/24 20:41,31/Jan/23 03:00,1.16.0,,,,,1.16.2,1.17.0,,,,API / DataStream,,,,0,pull-request-available,,,"as user reported in ml https://lists.apache.org/thread/n1rqml8h9j8zkhxwc48rdvj7jrw2rjcy
there's issue in AsyncWaitOperator that it not properly blocking retries when timeout occurs

this happens when a retry timer is unfired and then the user function timeout was triggered first, the current RetryableResultHandlerDelegator doesn't take the timeout process properly and will cause more unexpected retries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 07:54:32 UTC 2023,,,,,,,,,,"0|z1eam0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 03:00;lincoln.86xy;fixed in master: 0aab2cd369e700f3ea4c753f31c70802e9017ffc;;;","02/Feb/23 07:54;lincoln.86xy;fixed in 1.16: 8ee93dabf446b28795de431c376b03d41500b634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TrackingFsDataInputStream batch tracking issue,FLINK-30476,13515091,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,WonderBeat,WonderBeat,WonderBeat,21/Dec/22 12:57,14/Aug/23 10:35,04/Jun/24 20:41,,1.15.1,1.15.2,1.15.3,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,stale-assigned,,"{{org.apache.flink.connector.file.src.impl.StreamFormatAdapter.TrackingFsDataInputStream}} wraps underlying InputStream to count bytes consumed.
{{org.apache.flink.connector.file.src.impl.StreamFormatAdapter.Reader}} relies on this to create batches of data.
{code:java}
            while (stream.hasRemainingInBatch() && (next = reader.read()) != null) {
                result.add(next);
            }
{code}
{{org.apache.flink.connector.file.src.impl.StreamFormatAdapter.TrackingFsDataInputStream#read(byte[], int, int)}} contains a bug that can lead to arbitrary size batches due to counter ({{{}remainingInBatch{}}}) underflow.
{code:java}
        public int read(byte[] b, int off, int len) throws IOException {
            remainingInBatch -= len;
            return stream.read(b, off, len);
        }
{code}
Every time we perform a {{stream.read()}} it may return less than {{len}} according to the javadoc.
{code:java}
Params:
b – the buffer into which the data is read. off – the start offset in array b at which the data is written. len – the maximum number of bytes to read.
Returns:
the total number of bytes read into the buffer, or -1 if there is no more data because the end of the stream has been reached.
{code}
But current implementation accounts only bytes that were requested ({{{}{{len}}{}}}).

E.g. S3 Hadoop FS can return less than {{len}} as a result of {{{}stream.read(b, off, len){}}}. This is expected and readers are aware of this 
{{org.apache.parquet.io.DelegatingSeekableInputStream#readFully(java.io.InputStream, byte[], int, int)}}

As a result reading parquet file may result in underflow {{TrackingFsDataInputStream#read(byte[], int, int)}} because parquet reader tries to read the whole Row Group (large) and may execute {{read()}} multiple times. Underflow leads to unlimited batch size that may lead to OOM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Mon Aug 14 10:35:06 UTC 2023,,,,,,,,,,"0|z1ealc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improved speed of RocksDBMapState clear() using rocksDB.deleteRange,FLINK-30475,13515077,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,b923,b923,21/Dec/22 10:22,20/Aug/23 10:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Runtime / State Backends,,,,0,auto-deprioritized-major,pull-request-available,,"Currently {{RocksDBMapState#clear()}} is processed via keyRange traversing and inserting particular keys into BatchWrite for deletion.

RocksDb offer much faster way how to delete key range - {{deleteRange}}

This issue is follow-up for [FLINK-9070|https://issues.apache.org/jira/browse/FLINK-9070] where {{deleteRange}} was also considered. But at that time it implied slower read, it was buggy and not even available in the Java API of RocksDB. All of these problems were solved since that time (see also RocksDB [blog article for deleteRange|https://rocksdb.org/blog/2018/11/21/delete-range.html])

Delete range enables to clear {{RocksDBMapState}} for one key in constant computational complexity whereas the old solution requires O(n ).",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28010,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:02 UTC 2023,,,,,,,,,,"0|z1eai8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 13:16;b923;[~masteryhx] Thank you for linking the issue. Unfortunately I was unable to find it before posting this issue and related pull request.

I am wondering wheter used clear method shlould not be matter of configuration choice?

Because our use-case is to clear millions of entries when clear is called and this results in very long (tens of minutes) clearing.;;;","22/Dec/22 07:03;masteryhx;Hi, Have you compared the whole performance of using deleteRange with current clear ?
Just as FLINK-28010 said, ""it moved the cost of delete with Iterator to read, and this cost will persist unless a compaction occurs"".;;;","26/Dec/22 04:24;Yanfei Lei;Hi [~b923],  this optimization may increase the number of tombstones. As the [blog|https://rocksdb.org/blog/2018/11/21/delete-range.html] says, deleteRange would bring some regression on Point Lookups and Range Scans.

 

FLINK-28010 has been tried to optimize RocksDBMapState#clear() with deleteRange(), but the experimental results of [iterator|https://github.com/apache/flink/pull/20405#discussion_r951119238] are not as expected. Do you have different experimental results?;;;","02/Jan/23 11:33;b923;Hello [~Yanfei Lei] [~masteryhx] ,

Sorry for the late response.

I do not expect, that performance results with {{deleteRange()}} changed since FLINK-28010.

But if I describe our use-case scenario:
  * Write extensive keyed map-state,
  * where the state basically represents a queue of elements waiting for processing.
  * This state is periodically (e.g.: after 3 days) cleared (with millions of items).

When {{MapState#clear()}} with iterator is called, then clearing of the state lasts for hours(!) with occasional checkpoint failing and big degradation of pipeline performance.
So little performance drop mentioned in [blog|https://rocksdb.org/blog/2018/11/21/delete-range.html] is not really a problem in comparison to {{clear()}} with scan and delete.
On the other hand, I admit that for most use-cases performance drop can be problematic.

As I understood the problem, then:
* {{deleteRange()}} is better for clearing map states with big amount of keys,
* whereas clear via scan and delete is optimal for map states with a low amount of keys.

This leads me to the solution proposal, where the {{RocksDBMapState#clear()}} method implementation can be chosen via configuration.

I can imagine two ways how to implement configurable clear operation:
* Switchable: e.g.: config {{state.backend.rocksdb.map-state.clear-op}} with default option {{scan-and-delete}} and optional {{delete-range}}.
* With threshold: e.g.: config {{state.backend.rocksdb.map-state.clear-with-delete-range-threshold}} where user can configure threshold of map state size from which will be used {{delete-range}}. And default value will be {{-1}} which means never use {{delete-range}} .

What do you think?
 ;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
DefaultMultipleComponentLeaderElectionService triggers HA backend change even if it's not the leader,FLINK-30474,13515060,13432464,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Problem,mapohl,mapohl,mapohl,21/Dec/22 09:09,19/Apr/23 06:45,04/Jun/24 20:41,19/Apr/23 06:45,1.15.3,1.16.0,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"{{DefaultMultipleComponentLeaderElectionService}} calls {{LeaderElectionEventHandler#onLeaderInformationChange}} in any case even though the contracts of that method states that it should be only called by the leader to update the HA backend information (see [JavaDoc|https://github.com/apache/flink/blob/5a2f220e31c50306a60aae8281f0ab4073fb85e1/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/LeaderElectionEventHandler.java#L46-L50]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30878,,,,,FLINK-30462,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 06:45:30 UTC 2023,,,,,,,,,,"0|z1eaeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 10:26;mapohl;The leader check is actually handled in the {{MultipleComponentLeaderElectionDriver}} implementations (see [ZooKeeperMultipleComponentLeaderElectionDriver:203|https://github.com/apache/flink/blob/8ddfd590ebba7fc727e79db41b82d3d40a02b56a/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriver.java#L203] and [ZooKeeperMultipleComponentLeaderElectionDriver:223|https://github.com/apache/flink/blob/8ddfd590ebba7fc727e79db41b82d3d40a02b56a/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriver.java#L223] for ZooKeeper and [KubernetesMultipleComponentLeaderElectionDriver:238|https://github.com/apache/flink/blob/0290715a57b8d243586ab747b0cd2416c8081012/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesMultipleComponentLeaderElectionDriver.java#L238] for Kubernetes. But this only triggers the sending of the change information which is then executed in a separate thread in [DefaultMultipleComponentLeaderElectionService#sendLeaderInformationChange|https://github.com/apache/flink/blob/715c3fdef515c4b7ac70c4abd52910c08d6ca662/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L247]. Therefore, the leader role could have changed in the meantime.;;;","25/Jan/23 11:53;mapohl;Added PR #21537 which resolves this issue along FLINK-30462;;;","01/Feb/23 10:00;mapohl;master: fd4f918bbaa6f3fb1876dc90d9528d190b98a2f7
1.16: edaa148fcfa6ae427af8d840032903817b8ea5ef
1.15: ce38381c15ee83fad782677d51b10941c843b82e;;;","02/Feb/23 11:03;mapohl;I'm reopening the issue because the above mentioned fixes were reverted in FLINK-30878;;;","19/Apr/23 06:45;mapohl;This isn't actually an issue because the {{onLeaderInformationChange}} will only trigger updating the information if there's actually confirmed leader information in place. This isn't the case iff:
# The leadership wasn't granted, yet.
# The leadership was granted but the leader information was not confirmed, yet. In that case, a {{confirmLeadership}} call will follow resulting in the update of the leader information in the HA backend.
# The leadership was revoked.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the InputGate network memory management for TaskManager,FLINK-30473,13515058,13515048,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,21/Dec/22 08:43,18/Oct/23 12:19,04/Jun/24 20:41,17/Jan/23 02:20,1.17.0,,,,,1.17.0,,,,,Runtime / Network,,,,0,pull-request-available,,,"Based on the [FLIP-266|https://cwiki.apache.org/confluence/display/FLINK/FLIP-266%3A+Simplify+network+memory+configurations+for+TaskManager], this issue mainly focuses on the first issue.

This change proposes a method to control the maximum required memory buffers in an inputGate according to parallelism size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 02:20:38 UTC 2023,,,,,,,,,,"0|z1eae0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 02:20;xtsong;master (1.17): ae8de97ef2acc798dae34ad2096ece8886bcf308;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the default value of the max network memory config option,FLINK-30472,13515055,13515048,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,21/Dec/22 08:30,17/Jan/23 02:25,04/Jun/24 20:41,17/Jan/23 02:21,1.17.0,,,,,1.17.0,,,,,Runtime / Network,,,,0,pull-request-available,,,"This issue mainly focuses on the second issue in [FLIP-266|https://cwiki.apache.org/confluence/display/FLINK/FLIP-266%3A+Simplify+network+memory+configurations+for+TaskManager], modifying the default value of taskmanager.memory.network.max",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 02:21:39 UTC 2023,,,,,,,,,,"0|z1eadc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 02:21;xtsong;master (1.17): afdf4a73e43f2e6f5b2dd984aa3471f32658f9d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the enriching network memory process in SsgNetworkMemoryCalculationUtils,FLINK-30471,13515054,13515048,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,21/Dec/22 08:26,17/Jan/23 02:24,04/Jun/24 20:41,17/Jan/23 02:21,1.17.0,,,,,1.17.0,,,,,Runtime / Network,,,,0,pull-request-available,,,"In SsgNetworkMemoryCalculationUtils#enrichNetworkMemory, getting PartitionTypes is run in a separate loop, which is not friendly to performance.  If we want to add inputPartitionTypes in the subsequential PR, a new separate loop may be introduced too, which I think is not a good choice.

Using a separate loop to get each collection just looks simpler in code style, but it will affect the performance. We can get all the results of maxSubpartitionNums and partitionTypes through one loop instead of multiple loops, which will be faster. In this way, when we need to add inputPartitionTypes later, we do not need to add a new loop logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 02:21:12 UTC 2023,,,,,,,,,,"0|z1ead4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 02:21;xtsong;master (1.17): fc7defb14d11e270d539ee0d80a2076ae55a4ea2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change groupId for flink-connector-parent,FLINK-30470,13515049,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,martijnvisser,martijnvisser,21/Dec/22 07:54,16/Oct/23 09:09,04/Jun/24 20:41,28/Mar/23 10:25,,,,,,,,,,,Connectors / Common,,,,0,pull-request-available,,,"During the migration of the connectors to the external connector framework, we've used io.github.zentol.flink as groupId for the artifact flink-connector-parent to have the ability to quickly iterate on the connector parent. 

With the first wave of migrations and releases being completed, we should change this back to org.apache.flink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 16 09:09:14 UTC 2023,,,,,,,,,,"0|z1eac0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 20:22;Sergey Nuyanzin;hbase: [20c42df2a8f3edc2ee707c42ce7b48f1160ea3f0|https://github.com/apache/flink-connector-hbase/commit/20c42df2a8f3edc2ee707c42ce7b48f1160ea3f0];;;","16/Oct/23 09:09;dannycranmer;AWS: [ca891733b6941bcedadba1463c075c6aa5187fcc|https://github.com/apache/flink-connector-aws/commit/ca891733b6941bcedadba1463c075c6aa5187fcc] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-266: Simplify network memory configurations for TaskManager,FLINK-30469,13515048,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,21/Dec/22 07:45,17/Jan/23 06:30,04/Jun/24 20:41,17/Jan/23 06:17,1.17.0,,,,,1.17.0,,,,,Runtime / Network,,,,0,,,,"When using Flink, users may encounter the following issues that affect usability.
1. The job may fail with an ""Insufficient number of network buffers"" exception.
2. Flink network memory size adjustment is complex.

When encountering these issues, users can solve some problems by adding or adjusting parameters. However, multiple memory config options should be changed. The config option adjustment requires understanding the detailed internal implementation, which is impractical for most users.

To resolve the issues, we propose some improvement solutions. For more details see [FLIP-266|https://cwiki.apache.org/confluence/display/FLINK/FLIP-266%3A+Simplify+network+memory+configurations+for+TaskManager].

This is the umbrella ticket to track all the changes of this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29351,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-21 07:45:58.0,,,,,,,,,,"0|z1eabs:",9223372036854775807,"The default value of `taskmanager.memory.network.max` has changed from `1g` to `Long#MAX_VALUE`, to reduce the number of config options user needs to tune when trying to increase the network memory size. This may affect the performance when this option is not explicitly configured, due to potential changes of network memory size, and heap and managed memory sizes when the total memory size is fixed. To go back to the previous behavior, user can explicitly configure this option to the previous default value `1g`.

A threshold is introduced for controlling the number of required buffers among all buffers needed for reading data from upstream tasks. Reducing the number of required buffers helps reduces the chance of failures due to insufficient network buffers, at the price of potential performance impact. By default, the number of required buffers is only reduced for batch workloads, while stay unchanged for streaming workloads. This can be tuned via `taskmanager.network.memory.read-buffer.required-per-gate.max`. See the description of the config option for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SortOrder of BusyRatio should be descend by default,FLINK-30468,13515024,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,21/Dec/22 05:15,30/Dec/22 11:20,04/Jun/24 20:41,30/Dec/22 07:53,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"Currently, the sort order is ascend by default, it should be descend.

The most busy subtask should be displayed on top.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29998,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 30 07:53:30 UTC 2022,,,,,,,,,,"0|z1ea6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 07:53;yunta;merged in master: 4b1d8f65f6b19ed05f5d850f2bbd1c1d0315b801;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the accuracy of estimating the row count of LocalHashAgg to make more two-phase HashAgg operators effective ,FLINK-30467,13515001,13444558,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,21/Dec/22 01:25,10/Apr/24 06:58,04/Jun/24 20:41,,1.17.0,,,,,1.20.0,,,,,Table SQL / Planner,,,,0,,,,Improve the accuracy of estimating the row count of LocalHashAgg to make more two-phase HashAgg operators effective.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 06:58:08 UTC 2024,,,,,,,,,,"0|z1ea1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 06:58;kevinmingtarja;Hi, is this done yet? If not I'm interested in doing this task.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detect ineffective scaling operations,FLINK-30466,13514946,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,morhidi,gyfora,gyfora,20/Dec/22 16:53,05/Jan/23 09:06,04/Jun/24 20:41,05/Jan/23 09:06,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"The autoscaler logic currently does not consider whether a given proposed parallelism change will achieve the desired processing rate based on historic metrics.

It might be possible that we are for example trying to scale down to a parallelism that we have also done in the past which resulted in an immediate scale up.

This could be avoided by comparing the expected processing rate at a certain parallelism with past metrics from that parallelism setting. There should be some flexibility and a limited ""memory"" but some simple checks could reduce instable scaling caused by non-linear performance changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 09:06:37 UTC 2023,,,,,,,,,,"0|z1e9s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 09:06;gyfora;I am closing this in order to replace with some more concrete tasks around scaling decisions. This ticket was too broad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-connector-jdbc from master branch,FLINK-30465,13514898,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,20/Dec/22 12:55,10/Jan/23 00:05,04/Jun/24 20:41,10/Jan/23 00:05,,,,,,1.17.0,,,,,Connectors / JDBC,,,,0,pull-request-available,,,With JDBC being synchronized to https://github.com/apache/flink-connector-jdbc we can now remove the JDBC connector from {{master}} so it won't be included in the 1.17 release anymore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 00:05:41 UTC 2023,,,,,,,,,,"0|z1e9i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 06:49;taoran;Hi, [~martijnvisser] .  i found the external flink-connector-jdbc parent pom groupId is not standard by using github username. Can we use apache group id? 

FYI.

```

<parent>
    <groupId>io.github.zentol.flink</groupId>
    <artifactId>flink-connector-parent</artifactId>
    <version>1.0</version>
</parent>

<modelVersion>4.0.0</modelVersion>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-jdbc-parent</artifactId>
<version>3.1-SNAPSHOT</version>

```

 

 ;;;","21/Dec/22 07:52;martijnvisser;[~lemonjing] Good reminder, we still need to change that back. Let me create a ticket for that;;;","21/Dec/22 07:54;martijnvisser;[~lemonjing] I've created FLINK-30470 for this;;;","10/Jan/23 00:05;martijnvisser;Fixed in master: 13fd4f270a3af178a338b778bc8adedf2762a40a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Metrics are collected in stabilization phase and do not always span a full metric window,FLINK-30464,13514892,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,20/Dec/22 12:05,03/Jan/23 17:37,04/Jun/24 20:41,03/Jan/23 17:37,kubernetes-operator-1.4.0,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The current code collects metrics as soon as the job goes into RUNNING mode, regardless of whether we are still in the stabilization period. This means the first metric collection window after rescaling will include metrics from the stabilization period which is not desired. The stabilization period is supposed to allow the job to stabilize and shouldn't be used to make new scaling decisions.

Further, the collected metrics for the scaling decider initially do not span a full metric window. Only after the first metric window is complete, we are guaranteed to have a full windows available. This leads to short-sighted scaling decisions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30463,,,FLINK-30510,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-20 12:05:58.0,,,,,,,,,,"0|z1e9gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stabilization period starts before job goes into RUNNING ,FLINK-30463,13514891,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,20/Dec/22 11:57,23/Dec/22 17:08,04/Jun/24 20:41,23/Dec/22 17:08,kubernetes-operator-1.4.0,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"We are tracking the stabilization period from the job start time which can greatly vary from the time the job goes into RUNNING state. This can result in no stabilization period at all leading to poor scaling decisions.

Thus, we should change the logic to use the update time in combination with a RUNNING job status.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30464,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-20 11:57:22.0,,,,,,,,,,"0|z1e9go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultMultipleComponentLeaderElectionService saves wrong leader session ID,FLINK-30462,13514890,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,20/Dec/22 11:56,02/Feb/23 10:26,04/Jun/24 20:41,01/Feb/23 13:41,1.15.3,1.16.0,1.17.0,,,1.15.4,1.16.2,1.17.0,,,Runtime / Coordination,,,,0,pull-request-available,,,"We save a UUID in [DefaultMultipleComponentLeaderElectionService:195|https://github.com/apache/flink/blob/0290715a57b8d243586ab747b0cd2416c8081012/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L195] but use a different one for granting the leadership to each of the components (see [DefaultMultipleComponentLeaderElectionService:199|https://github.com/apache/flink/blob/0290715a57b8d243586ab747b0cd2416c8081012/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L199]).

We reach an inconsistent state if the JM acquired the leadership before another component registers itself at the {{MultipleComponentLeaderElectionService}}. In that case, a different session ID is passed than the one that is set for components that have been registered before the leadership was acquired by the JobManager. That could be the case for JobMaster leadershipt.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30474,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 09:59:50 UTC 2023,,,,,,,,,,"0|z1e9gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 09:59;mapohl;master: 08507a10ef931ec46db80a419ee19d181ade8c48
1.16: d7039e97be688eb8f872be4f00e9d56b77d05122
1.15: cfef56c2b9b9bdb73c9e020473e4d00849ca9e27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some rocksdb sst files will remain forever,FLINK-30461,13514873,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,20/Dec/22 11:16,23/Aug/23 02:23,04/Jun/24 20:41,07/Feb/23 03:22,1.15.3,1.16.0,1.17.0,,,1.15.4,1.16.2,1.17.0,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,"In rocksdb incremental checkpoint mode, during file upload, if some files have been uploaded and some files have not been uploaded, the checkpoint is canceled due to checkpoint timeout at this time, and the uploaded files will remain.

 
h2. Impact: 

The shared directory of a flink job has more than 1 million files. It exceeded the hdfs upper limit, causing new files not to be written.

However only 50k files are available, the other 950k files should be cleaned up.

!https://user-images.githubusercontent.com/38427477/207588272-dda7ba69-c84c-4372-aeb4-c54657b9b956.png|width=1962,height=364!
h2. Root cause:

If an exception is thrown during the checkpoint async phase, flink will clean up metaStateHandle, miscFiles and sstFiles.

However, when all sst files are uploaded, they are added together to sstFiles. If some sst files have been uploaded and some sst files are still being uploaded, and  the checkpoint is canceled due to checkpoint timeout at this time, all sst files will not be added to sstFiles. The uploaded sst will remain on hdfs.

[code link|https://github.com/apache/flink/blob/49146cdec41467445de5fc81f100585142728bdf/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L328]
h2. Solution:

Using the CloseableRegistry as the tmpResourcesRegistry. If the async phase is failed, the tmpResourcesRegistry will cleanup these temporary resources.

 

POC code:

[https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1]

 

 

!image-2022-12-20-18-45-32-948.png|width=1114,height=442!

!image-2022-12-20-18-47-42-385.png|width=1332,height=552!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30916,,,,,,,,,,"20/Dec/22 10:45;fanrui;image-2022-12-20-18-45-32-948.png;https://issues.apache.org/jira/secure/attachment/13054013/image-2022-12-20-18-45-32-948.png","20/Dec/22 10:47;fanrui;image-2022-12-20-18-47-42-385.png;https://issues.apache.org/jira/secure/attachment/13054012/image-2022-12-20-18-47-42-385.png","06/Feb/23 02:09;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13055166/screenshot-1.png",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 02:23:25 UTC 2023,,,,,,,,,,"0|z1e9co:",9223372036854775807,clean-up left-over rocksdb ssts in the shared scope introduced in FLINK-11008 (Flink version 1.8),,,,,,,,,,,,,,,,,,,"20/Dec/22 11:28;fanrui;Hi [~pnowojski] , could you please help take a look in your free time? thanks :)

And I have a POC commit to fix this bug.

[https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1]

 

BTW, FLINK-28984 is also a bug caused some checkpoint files remain forever. I have fixed these 2 bugs in my test job, no files remain with these fixes.

 

 ;;;","05/Feb/23 15:54;ym;merged commit [{{7326add}}|https://github.com/apache/flink/commit/7326addbb3f2b4867689755a04512412bcf69657] into apache:master;;;","05/Feb/23 16:17;ym;Is this problem introduced by FLINK-24611?

I think we should backport this fix to 1.16 and 1.15. Would you mind helping this? [~fanrui] ;;;","06/Feb/23 02:12;fanrui;Hi [~ym], thanks for your review and merge.

{quote}Is this problem introduced by FLINK-24611?
{quote}
After analysis, this problem was not introduced by FLINK-24611, and it was introduced by FLINK-11008.  FLINK-11008 added all sst files to the Map, once some sst uploads fail, all ssts will not be added to the Map.

{quote} I think we should backport this fix to 1.16 and 1.15. Would you mind helping this? 
{quote}

Sure, it's my pleasure, I will do it asap.


!screenshot-1.png|width=1120,height=743!;;;","06/Feb/23 08:28;mapohl;testUploadedSstCanBeCleanedUp was introduced by this Jira issue but causes a test instability (FLINK-30916). Shall I close FLINK-30916 in favor of this issue or keep it open? ;;;","06/Feb/23 08:33;mapohl;Please be careful on how Jira information is updated:
* Release notes are not meant to be used for sharing the commit hash. The release notes field should include a human-readable explanation of what this Jira issue fixes. This will be then added to the release notes of the corresponding release.
* fixVersion is used to specify what Flink version will include this fix. This Jira issue has 1.15.4 and 1.16.2 as fixVersion even though (accoridng to the comments) there's only a fix provided to {{master}} so far. The fixVersion is, as well, relevant for generating the release notes.;;;","06/Feb/23 08:35;ym;[~mapohl] , thanks for reminding, that's why I am re-opening this ticket.

 

It is fine to close FLINK-30916 and track in this one.

The fix also needs to be back-ported. ;;;","06/Feb/23 10:25;fanrui;Offline discussed with [~ym] , I will submit a PR to master (and 1.17, if needed) for FLINK-30916.

And cherry-pick FLINK-30461 and FLINK-30916 together to 1.15 and 1.16.

master PR: https://github.com/apache/flink/pull/21859
1.16 PR: https://github.com/apache/flink/pull/21853
1.15 PR: https://github.com/apache/flink/pull/21855;;;","07/Feb/23 03:17;ym;Thanks [~fanrui] for the fix and [~yanfei] and [~Zakelly]  for discussion and review!

 

merged commit [{{7326add}}|https://github.com/apache/flink/commit/7326addbb3f2b4867689755a04512412bcf69657] into apache:master (for cleaning-up left-over rocksdb shared ssts)

merged commit [{{d761f51}}|https://github.com/apache/flink/commit/d761f51ab5a76331eb9b8f423e0ffaf1d04f97f5] into apache:master (for the concurrent thread race-condition in unit tests)

 

merged commit [{{e2c3d61}}|https://github.com/apache/flink/commit/e2c3d6152d95074561d062ce0b61b6428f6dd6e1] into apache:release-1.16 (back port the above two to 1.16 branch)

merged commit [{{a63f298}}|https://github.com/apache/flink/commit/a63f2983703f438989d069ababcf4cc173441646] into apache:release-1.15 (back port the above two to 1.15 branch)

 ;;;","23/Aug/23 01:00;mason6345;[~fanrui] Thanks for fixing this! Just curious, how did you figure out that some SST files were not being cleaned up? Are there any tricks to discover the issue outside of reading the code? I recently hit this issue too but all I saw was that SST sizes continuous growth from RocksDB metrics.;;;","23/Aug/23 02:23;fanrui;Hi [~mason6345] , I didn't find out in time that the sst files weren't cleaned up, but our flink user feedbacks the checkpoint fails. After analysis, the root cause is : +_The shared directory of a flink job has more than 1 million files. It exceeded the hdfs upper limit, causing new files not to be written._+

 

The `state.checkpoints.num-retained=1`, I deserialized the _metadata file of the latest checkpoint : only 50k files are depended on, the other 950k files should be cleaned up. So, I think analyzing the  _metadata file can figure out  that some SST files were not being cleaned up.

BTW, please follow the FLINK-28984 as well, it also cause the sst files leak.

 ;;;",,,,,,,,,,,,,,,,,,,,
Support writing HBase's CEIL TTL metadata,FLINK-30460,13514853,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanjialiang,tanjialiang,tanjialiang,20/Dec/22 10:28,16/Nov/23 15:57,04/Jun/24 20:41,16/Nov/23 15:57,,,,,,hbase-3.0.1,,,,,Connectors / HBase,,,,0,pull-request-available,,,"When i using flink sql to sink hbase, i found i can't set the ceil ttl.  Can we support writing the ceil ttl metadata like this?
{code:java}
CREATE TABLE hTable (
 rowkey INT,
 family1 ROW<q1 INT>,
 ttl BIGINT METADATA FROM 'ttl',
 PRIMARY KEY (rowkey) NOT ENFORCED
) WITH (
 'connector' = 'hbase-2.2',
 'table-name' = 'mytable',
 'zookeeper.quorum' = 'localhost:2181'
);{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 15:57:54 UTC 2023,,,,,,,,,,"0|z1e988:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 15:57;martijnvisser;Fixed in apache/flink-connector-hbase

main: 334bd965d299aca2fa597f08bdc89e0275562419
v3.0: d35b703974a32cbb3a00d479ac186fcf73003a07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL Client supports ""SET 'property'""",FLINK-30459,13514847,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liuml07,liuml07,liuml07,20/Dec/22 09:35,14/Aug/23 10:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Client,,,,0,pull-request-available,stale-assigned,,"Currently the SET command in SQL client has two syntax:
- {{SET;}} for listing all config properties
- {{SET 'key'='value'}} for setting a property with (new) value

As in Spark SQL and Flink SQL with Hive dialect, it would be convenient to show one config property using the syntax {{SET 'key';}}. Without this, users will need to find the very one config from all config properties.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 10:35:06 UTC 2023,,,,,,,,,,"0|z1e96w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 10:08;martijnvisser;Can you please first open a discussion on the Dev mailing list if this is indeed the desired syntax? ;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor Table Store Documentation,FLINK-30458,13514838,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,20/Dec/22 08:38,22/Dec/22 02:45,04/Jun/24 20:41,22/Dec/22 02:45,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,Currently the documents of table store are messy and are lacking basic concepts. This ticket refactors table store documentation and at the mean time adding basic concepts.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 02:45:09 UTC 2022,,,,,,,,,,"0|z1e94w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 02:45;TsReaper;master: 4c607a7356a5fd43364f93d70264d1ede98113ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce period compaction to FRocksDB,FLINK-30457,13514817,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,20/Dec/22 04:04,28/Aug/23 08:37,04/Jun/24 20:41,28/Aug/23 08:37,,,,,,1.18.0,,,,,Runtime / State Backends,,,,0,,,,"RocksDB supports period compaction once compaction filter is enabled (see https://github.com/facebook/rocksdb/wiki/Leveled-Compaction#periodic-compaction), which is really useful for Flink TTL feature to pick really old files in compaction.

Current FRocksDB base version: 6.20.3 has included the necessary code on the C++ side: https://github.com/facebook/rocksdb/pull/5166 and https://github.com/facebook/rocksdb/pull/5184. We just need to cherry pick the https://github.com/facebook/rocksdb/pull/8579 on the java side to support it in Flink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30854,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 23 03:06:00 UTC 2022,,,,,,,,,,"0|z1e908:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/22 03:06;masteryhx;Merged [{{ebed4b1}}|https://github.com/ververica/frocksdb/commit/ebed4b1326ca4c5c684b46813bdcb1164a669da1] into FRocksDB-6.20.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLM Bundle Description Version Problems,FLINK-30456,13514799,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,20/Dec/22 01:11,27/Dec/22 15:45,04/Jun/24 20:41,27/Dec/22 15:45,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"OLM is working great with OperatorHub, but noticed a few items that need fixing.

1.  The basic.yaml example version is release-1.1 instead of the latest release (release-1.3).  This needs fixing in two places:

tools/olm/generate-olm-bundle.sh

tools/olm/docker-entry.sh

2.  The label versions in the description are hardcoded to 1.2.0 instead of the latest release (1.3.0)

3. The Provider is blank space "" "" but soon needs to have some text in there to avoid CI errors with the latest operator-sdk versions.  The person who noticed it recommended ""Community"" for now, but maybe we can being making it ""The Apache Software Foundation"" now?  Not sure if we're ready for that yet or not...

 

I'm working on a PR to address these items.  Can you assign the issue to me?  Thanks!

fyi [~tedhtchang] [~gyfora] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/22 16:21;tedhtchang;image-2022-12-20-08-21-02-597.png;https://issues.apache.org/jira/secure/attachment/13054029/image-2022-12-20-08-21-02-597.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 15:45:26 UTC 2022,,,,,,,,,,"0|z1e8w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 16:26;tedhtchang;Hi Jim,  could you also fix the LINKS showing n/a.
[https://operatorhub.io/operator/flink-kubernetes-operator]
!image-2022-12-20-08-21-02-597.png!

https://sdk.operatorframework.io/docs/olm-integration/generation/#csv-fields;;;","27/Dec/22 15:45;gyfora;merged to main 0009746cb3bf96bec0450e99e03c1af20f4038e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Avoid ""cleaning"" of java.lang.String in ClosureCleaner",FLINK-30455,13514751,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,gunnar.morling,gunnar.morling,gunnar.morling,19/Dec/22 15:25,04/Mar/24 11:59,04/Jun/24 20:41,24/Apr/23 09:35,,,,,,,,,,,API / Core,,,,0,pull-request-available,,,"When running a simple ""hello world"" example on JDK 17, I noticed the closure cleaner tries to reflectively access the {{java.lang.String}} class, which fails due to the strong encapsulation enabled by default in JDK 17 and beyond. I don't think the closure cleaner needs to act on {{String}} at all, as it doesn't contain any inner classes. Unless there are objections, I'll provide a fix along those lines. With this change in place, I can run that example on JDK 17.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31919,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 09:35:26 UTC 2023,,,,,,,,,,"0|z1e8ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 15:25;gunnar.morling;[~rmetzger] et al., if you think this makes sense, could you assign this issue to me? Thanks!;;;","19/Dec/22 16:02;gunnar.morling;Sent PR [https://github.com/apache/flink/pull/21532,] seems simple enough.;;;","24/Apr/23 09:35;chesnay;More general approach suggested in FLINK-31919.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent class hierarchy in TaskIOMetricGroup,FLINK-30454,13514744,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gunnar.morling,gunnar.morling,gunnar.morling,19/Dec/22 14:15,05/Jan/23 08:07,04/Jun/24 20:41,05/Jan/23 08:07,,,,,,1.17.0,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"I noticed an interesting issue when trying to compile the flink-runtime module with Eclipse (same for VSCode): the _private_ inner class {{org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.SizeGauge}} has yet another _public_ inner class, {{{}SizeSupplier{}}}. The public method {{org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.registerMailboxSizeSupplier(SizeSupplier<Integer>)}} has a parameter of that type.

The invocation of this method in {{org.apache.flink.streaming.runtime.tasks.StreamTask.StreamTask(Environment, TimerService, UncaughtExceptionHandler, StreamTaskActionExecutor, TaskMailbox)}} can be compiled with the javac compiler of the JDK, but fails to compile with ecj:
{code:java}
The type TaskIOMetricGroup.SizeGauge from the descriptor computed for the target context is not visible here.  
{code}
I tend to believe that the behavior of Eclipse's compiler is the correct one. After all, you couldn't explicitly reference the {{SizeSupplier}} type either.

One possible fix would be to promote {{SizeSupplier}} to the same level as {{{}SizeGauge{}}}. This would be source-compatible but not binary-compatible, though. I.e. code compiled against the earlier signature of {{registerMailboxSizeSupplier()}} would be broken with a {{{}NoSuchMethodError{}}}. Depending on whether {{registerMailboxSizeSupplier()}} are expected in client code or not, this may or may not be acceptable.

Another fix would be to make {{SizeGauge}} public. I think that's the change I'd do. Curious what other folks here think.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 14:28:13 UTC 2022,,,,,,,,,,"0|z1e8k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 14:15;gunnar.morling;[~rmetzger], curious about your thoughts on that compatibility question.;;;","19/Dec/22 15:07;rmetzger;I'm pretty sure these are not public classes. We use the @Public / @PublicEvolving / @Internal annotations to mark interface visibility in Flink.
The TaskIOMetricGroup-stuff is, in my understanding anyways internal. ;;;","19/Dec/22 15:51;gunnar.morling;Ok, cool. Created PR [https://github.com/apache/flink/pull/21531/.|https://github.com/apache/flink/pull/21531/];;;","22/Dec/22 14:28;rmetzger;Thanks for your contribution!

Merged to master (1.17) in f82be845f3f673264a13eaf29e11b19e00f37222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix 'can't find CatalogFactory' error when using FLINK sql-client to add table store bundle jar,FLINK-30453,13514728,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,19/Dec/22 11:51,20/Dec/22 11:41,04/Jun/24 20:41,20/Dec/22 11:40,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"The FLINK 1.16 has introduced new mechanism to allow passing a ClassLoader to EnvironmentSettings[FLINK-15635|https://issues.apache.org/jira/browse/FLINK-15635],  and the Flink SQL client will pass a `ClientWrapperClassLoader`, making that table store CatalogFactory can't be found if it is added by 'ADD JAR' statement through SQL Client.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 11:40:59 UTC 2022,,,,,,,,,,"0|z1e8go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 11:40;TsReaper;master: 7e0d55ff3dc9fd48455b17d9a439647b0554d020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong argument type of null literal is used when call Hive's function,FLINK-30452,13514687,13510724,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,19/Dec/22 07:21,29/Jan/23 10:24,04/Jun/24 20:41,27/Dec/22 07:09,,,,,,1.17.0,,,,,Connectors / Hive,,,,0,pull-request-available,,,"While calling Hive function, if the argument is void type, it will be [considered as string type|[https://github.com/apache/flink/blob/2e5cac1f31aa571276df20e24889994672692a89/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java#L553]] , which may bring wrong type inference and thus may well cause other problems.

 

For example, 
{code:java}
timestamp(if(a = 0, null, a))
{code}
the argument for function `if` is `null` and `a`, null is void type,  `a` is bigint type represents a timestamp, so the inferred return type should be bigint for the `if` function will ignore void type and just consider the other argument type.  But we consider the void type as string type, the return type inferred will be string according to the `if` function inference logic.

And then when we try to converted the value returned by `if(a = 0, null, a)` to timestamp, the unexpected behavior will happen:

Since we infer the returned value of `if(a = 0, null, a)`  as string,  so  `if(a = 0, null, a)` will return some thing like ""1671058803926""，but 

cast (""1671058803926"" as timestamp) will return null.

 

 ",,,,,,,,,,,,,,,,,,,,FLINK-27374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 07:09:35 UTC 2022,,,,,,,,,,"0|z1e87s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 07:52;luoyuxia;I don't think when we do the type inference, void type should be considered as string type.

In the other code path, we just consider void type as [null type |[https://github.com/apache/flink/blob/df0bc117341a4017a1aea0234cb1d63202562be3/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/HiveParserRexNodeConverter.java#L451]]or [unsupport type|[https://github.com/apache/flink/blob/2e5cac1f31aa571276df20e24889994672692a89/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTypeUtil.java#L154]][ |#L154];;;","19/Dec/22 09:06;luoyuxia;We can just return void type instead of string type while doing type inference.

But  HiveParserCalcitePlanner#convertNullLiteral will try to convert null literal to string, then the concern will be for null, the argument type will be null type, but the actuall passed value's type will be string.

But from the call stack of method HiveParserCalcitePlanner#convertNullLiteral, I can conclude such case won't happen for the conversion.

 

 

 ;;;","27/Dec/22 07:09;jingzhang;Fixed in master: 207975b416c324c634f0620e049669afea8cb8db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AlgoOperator for Swing,FLINK-30451,13514667,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhangzp,zhangzp,19/Dec/22 06:01,16/Feb/23 11:30,04/Jun/24 20:41,16/Feb/23 11:30,ml-2.2.0,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 11:30:51 UTC 2023,,,,,,,,,,"0|z1e83c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 11:30;zhangzp;Fixed on master via e85fcf7635154871a49f6feb53553b0e98b21d88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystem supports exporting client-side metrics,FLINK-30450,13514659,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,19/Dec/22 04:09,19/Apr/23 02:41,04/Jun/24 20:41,,,,,,,,,,,,FileSystems,,,,1,,,,"Client-side metrics, or job level metrics for filesystem could help us to monitor filesystem more precisely.

Some metrics (like request rate , throughput, latency, retry count, etc) are useful to monitor the network or client problem of checkpointing or other access cases for a job.  
Some filesystems like s3, s3-presto, gs have supported enabling some metrics, these could be exported in the filesystem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 12:15:33 UTC 2022,,,,,,,,,,"0|z1e81k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 14:39;_anton;Thank you for creating this ticket! To expand on what I mentioned in the original [email thread,|https://lists.apache.org/thread/dpgh6sh0r21sgohjxxbqtm2mrmjdolgr] we'd like to be able to see the following metrics:
 * Request rate (if possible tagged by HTTP method)
 * Request latency
 * Upload / download byte rates
 * Error rate (if possible tagged by error) - would be useful to track throttling errors from S3 for example
 * Retry count
 * Number of active connections

As mentioned in the thread, the S3 Presto client already gathers these metrics [here|https://github.com/prestodb/presto/blob/0.272/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystemStats.java] but they are not exposed anywhere in Flink. The S3A client also has built-in [metrics|https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Metrics] that are actually already exposed via JMX when the client is used in Flink but it would obviously be great to standardize the way we expose FS metrics on the Flink side.

I haven't looked into GCS or Azure Storage yet but definitely interested in metrics from these clients too.;;;","19/Dec/22 14:52;martijnvisser;With regards to this request, are you looking at FileSource / FileSink (to read and/or write files) or also looking at this from a checkpoint/savepointing perspective?

If the first, it would be a duplicate of FLINK-28021 / FLINK-28117 imho. 

Exposing the Presto / Hadoop metrics might not be trivial given that these are loaded via the Flink plugin mechanism https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/plugins/
It would probably require a FLIP and a discussion the Dev mailing list;;;","19/Dec/22 15:06;_anton;I am mostly interested in the checkpoint / savepoint side of things, the main idea is to monitor the pressure on the FS during these operations. I already have some server-side metrics for S3 etc but client-side metrics would offer a more granular view.;;;","20/Dec/22 12:15;stevel@apache.org;if you can grab the IOStatistics from an iOStatisticsSource implementation, s3a, gcs and abfs FS instances collect detailed stats you can snapshot and then marshall as json/serialized java objects. Also works for other classes (input streams, output streams, list iterators....). that stuff is there to play with.

otherwise, hadoop-3.3.2+ optionally annotates the http-referer headr on all S3 requests so you can apportion blame from the S3 server logs, including looking for 503 throttle events. Avoids code changes up the stack and gives a view of the entire cluster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Serialization and Deserialization of FetchResultsResponseBody,FLINK-30449,13514379,13478114,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,yzl,yzl,18/Dec/22 11:49,27/Dec/22 01:58,04/Jun/24 20:41,27/Dec/22 01:58,1.17.0,,,,,1.17.0,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,"Currently, the REST endpoint serialize and deserialize the ResultSet in the FetchResultsResponseBody. But many information of ResultSet will be also put in the FetchResultsResponseBody, so it's not a good way. In this ticket, I propose to introduce a ResultInfo to replace the ResultSet in the FetchResultsResponseBody.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 01:58:07 UTC 2022,,,,,,,,,,"0|z1e6bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 01:58;fsk119;Merged into master: 95cb47ef53c822161ca90cdca43ad95fd1332633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""filter(Objects::nonNull)"" will bring down task with failure cause: ClassCastException",FLINK-30448,13514369,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,float2net,float2net,18/Dec/22 04:16,09/Feb/23 09:24,04/Jun/24 20:41,09/Feb/23 09:24,,,,,,,,,,,API / DataStream,,,,0,,,,"Attached an *all-in-one* java program, which can run locally in DEV environment（e.g. IntelliJ IDEA->run）, consuming from elements stream objects, the object schema is a parent containing two childs(Child1 and Child2) fields, I use *side-output* to map and split out two different sub-streams, each for one child. I put '{*}filter(Objects:nonNUll){*}' for each sub-stream to ignore null objects. When  comming from stream the parent record {*}containing any one of child is null{*}, the program will bring down the task and produce below error:

......

switched from RUNNING to FAILED with failure cause: java.lang.{*}ClassCastException{*}: mytest.TestSideOutput$Child2 cannot be cast to mytest.TestSideOutput$Child1. Failed to push OutputTag with id 'child2' to operator. This can occur when multiple OutputTags with different types but identical names are being used.
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:88)

......

 

However, if I replace '{*}filter(Objects:nonNull){*}' (at line #71 and #90) with logically equivalent  '{*}filter(x->x!=null){*}‘ (at line #70 and #89), everythink will be OK.

 ","test both on flink 1.15.1 and flink 1.16.0

Intellij-Idea dev environment run",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/22 03:49;float2net;TestSideOutput.java;https://issues.apache.org/jira/secure/attachment/13053945/TestSideOutput.java",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:24:00 UTC 2023,,,,,,,,,,"0|z1e694:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 09:52;pltbkd;Hi Yong, 
I looked into the issue and found that this may be a limitation of Java lambda mechanism. 
{code:java}
class Test implements Serializable {

    static boolean nonNull(Object x) {
        return false;
    }

    public static void main(String[] args) throws Exception {
        Test a = new Test();
        FilterFunction f1 = Test::nonNull;
        FilterFunction f2 = Test::nonNull;
        System.out.println(f1 == f2);

        byte[] b1 = InstantiationUtil.serializeObject(f1);
        byte[] b2 = InstantiationUtil.serializeObject(f2);
        System.out.println(Arrays.equals(b1, b2));

        FilterFunction df1 =
                InstantiationUtil.deserializeObject(
                        b1, Thread.currentThread().getContextClassLoader());
        FilterFunction df2 =
                InstantiationUtil.deserializeObject(
                        b2, Thread.currentThread().getContextClassLoader());
        System.out.println(df1 == df2);
    }
}
{code}
Run the code above with the nonNull as a static method in Test class, the results are (false, true, true).
While if you make the nonNull a non-static method, the results will be (false, true, false).

After looked into the internal calls, I found that when the lambda is deserialized, there will be a SerializedLambda. In both case the SerializedLambda objects are different. While it is not the goal, the SerializedLambda should be converted to a FilterFunction. To achieve this, a method called ""$deserializeLambda$"" will be invoked with the SerializedLambda during the deserialization. After that, if the method is not static, the results will be different instances, while if the method is static, the results are exactly the same instance. So when flink call the filter instance with two different types of data(Child1 and Child2), the second call will cause ClassCastException.

On the other hand, if the FilterFunctions are initialized with code `x->x!=null`, there'll be two lambda classes, so that will be fine.

Unfortunately I can find little data about the $deserializeLambda$ method. Here's my supposition. Maybe it makes some optimization to return a singleton instance for a static method lambda. It works well for non-generic classes but fails here.

In one word, I suppose this is the limitation of Java lambda mechanism and Flink can do nothing to avoid it. You have to use `x->x!=null` pattern, or use a non-static method to avoid the exception.;;;","09/Feb/23 09:24;gaoyunhaii;Very thanks [~pltbkd] for the analyzing! Hi [~float2net] based on the above analysis it looks to me that this issue comes from the limitation of using java static method as lambda, thus I'll first close this issue. Feel free to reopen it if there are more need to be checked. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scylladb SQL sink connector,FLINK-30447,13514355,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,krisnaru,krisnaru,krisnaru,17/Dec/22 20:54,18/Dec/22 02:32,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Cassandra,,,,0,,,,Scylladb sql connector for sink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-17 20:54:29.0,,,,,,,,,,"0|z1e660:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flink-playgrounds] Use more recent MySQL image built for linux/arm64/v8 & linux/amd64,FLINK-30446,13514268,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,cameres,cameres,cameres,16/Dec/22 18:17,14/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,0,playground,pull-request-available,stale-assigned,"Trying to work through the [https://nightlies.apache.org/flink/flink-docs-master/docs/try-flink/table_api/,|https://nightlies.apache.org/flink/flink-docs-master/docs/try-flink/table_api/),] I ran into issues w/ my M1 macbook pro. It looks like mysql:8.0.19 hasn't been built for linux/arm64/v8. With a more recent version of mysql, the example starts correctly. Tried to look to see if there were any other tickets, but I couldn't seem to find any. I'll attach a PR, but this is low priority as I was able to just update the docker-compose.yaml.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 10:35:07 UTC 2023,,,,,,,,,,"0|z1e5mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Flink Web3 Connector,FLINK-30445,13514264,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,pegasas,pegasas,16/Dec/22 18:05,20/Dec/22 06:14,04/Jun/24 20:41,16/Dec/22 18:49,,,,,,,,,,,,,,,0,,,,"# Web3 is very hot. But you could search GitHub open source blockchain explorer, the most stars project is blockscout, [https://github.com/blockscout/blockscout|https://github.com/blockscout/blockscout,] which use Elixir as a parallel engine to sync block from blockchain node into a file(CSV format). I think Flink is the best solution of ingestion. Reason:

(1)blockchain needs to match different chain, including Ethereum, Bitcoin, Solana, etc. through JSON RPC.

(2)Like EtherScan, the blockchain needs to fetch the latest block into storage for the index to search.

(3)Also as a supplement to (2), we need a connector to fully sync all block from Blockchain Node. I think Flink Stream/Batch alignment feature is suit for this scenarios.

(4)According to FLIP-27, we could use block number as SourceSplit to read. It is very natural.

(5)Flink Community could use web3 topic to get PR effects on web3 cycle.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,Tue Dec 20 04:59:23 UTC 2022,,,,,,,,,,"0|z1e5ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 18:49;martijnvisser;[~pegasas] New connectors require a FLIP and a discussion the Dev mailing list before creating a Jira ticket. ;;;","17/Dec/22 03:47;pegasas;Hi, [~martijnvisser] , would I have access to create new FLIP myself? https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals;;;","17/Dec/22 06:46;martijnvisser;[~pegasas] I can grant you access, but I think it would be smarter to first start a discussion on the Dev mailing list to see if there's interest in this;;;","20/Dec/22 04:52;pegasas;Is it this one? [https://lists.apache.org/list.html?user@flink.apache.org] [~martijnvisser] ;;;","20/Dec/22 04:59;pegasas;[https://lists.apache.org/list.html?dev@flink.apache.org] posted. [~martijnvisser] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
State recovery error not handled correctly and always causes JM failure,FLINK-30444,13514260,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,dmvk,gyfora,gyfora,16/Dec/22 17:34,17/Aug/23 14:35,04/Jun/24 20:41,,1.14.6,1.15.3,1.16.0,1.17.1,1.18.0,,,,,,Client / Job Submission,,,,0,pull-request-available,stale-assigned,,"When you submit a job in Application mode and you try to restore from an incompatible savepoint, there is a very unexpected behaviour.

Even with the following config:
{noformat}
execution.shutdown-on-application-finish: false
execution.submit-failed-job-on-application-error: true{noformat}

The job goes into a FAILED state, and the jobmanager fails. In a kubernetes environment (when using the native kubernetes integration) this means that the JobManager is restarted automatically.

This will mean that if you have jobresult store enabled, after the JM comes back you will end up with an empty application cluster.

I think the correct behaviour would be, depending on the above mention config:

1. If there is a job recovery error and you have (execution.submit-failed-job-on-application-error) configured, then the job should show up as failed, and the JM should not exit (if execution.shutdown-on-application-finish is false)
2. If (execution.shutdown-on-application-finish is true) then the jobmanager should exit cleanly like on normal job terminal state and thus stop the deployment in Kubernetes, preventing a JM restart cycle",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 14:35:19 UTC 2023,,,,,,,,,,"0|z1e5kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 17:34;gyfora;[~dmvk] ;;;","18/Dec/22 12:02;gyfora;This also not consistent with some other startup errors such as, missing application jar. That causes a jobmanager restart loop, but does not put the job a terminal FAILED state. This behaviour is more desirable as it doesn't lead to empty application clusters on Kubernetes;;;","19/Dec/22 17:20;dmvk;This is a deeper issue with how the savepoints are recovered when JobMaster starts up. If anything goes sideways during execution graph restore, we fail the job master, which is ultimately handled as a fatal exception by the dispatcher (_DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint_ is the relevant code path).

Since the job has already been registered for execution by the dispatcher, we should handle the exception and fail the job accordingly because we can't recover from this.

> This also not consistent with some other startup errors such as, missing application jar. That causes a jobmanager restart loop, but does not put the job a terminal FAILED state. This behaviour is more desirable as it doesn't lead to empty application clusters on Kubernetes

This is a special class of errors that could be qualified as ""pre-main method errors""; I think this is orthogonal to this issue and should be discussed separately.;;;","19/Dec/22 21:14;gyfora;I think the JobManager shutting down is inconsistent with the execution.shutdown-on-application-finish configuration. All other fatal job errors simply leave the jobmanager there.

Also in Application mode, this should trigger the proper shutdown of the whole application, which in Kubernetes mean that it won't restart. ;;;","23/Dec/22 09:57;dmvk;> I think the JobManager shutting down is inconsistent with the execution.shutdown-on-application-finish configuration.

Right now, it depends on when the fatal error happens. Suppose the _fatal error handler_ catches it in the dispatcher. In that case, we might be limited in what we can do because we can no longer guarantee that the APIs are accessible / returning correct results. There might be cases when we can do something, just like with the savepoint recovery, because it's only tied to a particular job and not the whole cluster.

Another consideration is that the _execution.shutdown-on-application-finish_ should only take effect when the job reaches the terminal state (FAILED, FINISHED, CANCELLED), which might not be the case when we encounter fatal error (that usually leads to the process restart and recovery mechanism kicking in).

> All other fatal job errors simply leave the jobmanager there.

It might be a bug if the job doesn't reach the terminal state.

 

I'm off for the holidays. I'll give this more thought next year.;;;","26/Jan/23 21:46;gyfora;Hi [~dmvk] !
Did you make any progress on this? Would be good to hear what you think. I am also happy to help out with any dev/testing efforts.;;;","28/Feb/23 21:44;gyfora;[~dmvk] this problem seems to occur even in simple main method exceptions. Those also cause the JM failure / shutdown which is even more concerning.;;;","01/Mar/23 07:42;dmvk;> this problem seems to occur even in simple main method exceptions

 

Does that occur consistently across Flink versions or just for `1.14.x` (which is in the list of affected versions)? The `

execution.submit-failed-job-on-application-error` flag has been available since 1.15.x [1].

 

[1] https://issues.apache.org/jira/browse/FLINK-25715;;;","01/Mar/23 07:47;gyfora;[~dmvk] I only tested this (the main method error) for 1.16 but it appears there.
I wanted to note it here;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","17/Aug/23 14:35;gyfora;[~dmvk] do you have any update on this by any chance?;;;",,,,,,,,,,,,,,,,,,,,
Expand list of sensitive keys,FLINK-30443,13514246,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gunnar.morling,gunnar.morling,gunnar.morling,16/Dec/22 16:11,22/Dec/22 20:47,04/Jun/24 20:41,22/Dec/22 20:47,,,,,,1.17.0,,,,,API / Core,,,,0,pull-request-available,,,"In {{{}GlobalConfiguration{}}}, there is [a list of known configuration keys|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/configuration/GlobalConfiguration.java#L47-L48] whose values will be masked in log output. In our Flink deployment there's a few more keys which we would like to mask, specifically, the following ones:

* ""auth-params""
* ""service-key""
* ""token""
* ""basic-auth""
* ""jaas.config""

While those are somewhat use-case specific, I feel they are generic enough for being added to that list, and there already is precedence in form of ""fs.azure.account.key"". In that light, I don't think it's worth making this somehow pluggable, but I'm curious what other folks here think. Thanks!
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 20:47:24 UTC 2022,,,,,,,,,,"0|z1e5hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 05:08;jiabao.sun;I think it makes sense to expend sensitive config keys. 
And make it configurable to allow users to expend them in flink-conf.yml.;;;","22/Dec/22 08:32;zhuzh;I think adding a Flink configuration, e.g. security.sensitive-keys, is a good idea which allows users to config according to their own needs. 
It can also make it easier to figure out which keys are considered sensitive by default. Users can find it on the apache Flink configuration [page|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/], instead of trying to find in the code.;;;","22/Dec/22 13:49;gunnar.morling;Hey [~jiabao.sun] and [~zhuzh], thanks for your feedback! I certainly can make this configurable, but do you think it's worth it? I'm not so much concerned about the engineering time for making it happen, this shouldn't take long. But it adds to the configuration surface and complexity of Flink; generally, the less bells and whistles, the better, IMO. It makes things easier for users. How about we add those keys verbatim for now (I've just sent a PR for that), and if there's another request for extending them down the road, we'll make it configurable then?;;;","22/Dec/22 14:17;rmetzger;I suggest to merge this as-is, and file a follow up ticket for making it configurable. 
This will allow us to collect feedback on the ticket.

At my current company, we had to make this change in our internal fork to extend the list of sensitive keys. If this was configurable, it would have been easier for us ... so I see benefits here.
In addition to that, our configuration page distinguishes between normal and ""advanced"" configuration keys...and this is certainly an advanced configuration.;;;","22/Dec/22 20:47;rmetzger;Merged to master (1.17) in https://github.com/apache/flink/commit/d7b63aee3b02e71f837e1f0b18f1b93790765d9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update table walkthrough playground for 1.16,FLINK-30442,13514240,13514237,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,gunnar.morling,danderson,danderson,16/Dec/22 15:34,04/Jan/23 16:42,04/Jun/24 20:41,04/Jan/23 16:42,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 16:42:55 UTC 2023,,,,,,,,,,"0|z1e5gg:",9223372036854775807,The table walkthrough playground now supports M1-based Apple computers.,,,,,,,,,,,,,,,,,,,"04/Jan/23 16:42;danderson;Fixed in master and release-1.16 via

bf2b9f661f3bdc08efb4823a98cb0870d8a3eb05

a2b62f25c7647f5a35e672a9e508bb5c80424977;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update pyflink walkthrough playground for 1.16,FLINK-30441,13514239,13514237,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,gunnar.morling,danderson,danderson,16/Dec/22 15:32,04/Jan/23 16:43,04/Jun/24 20:41,04/Jan/23 16:43,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 16:43:50 UTC 2023,,,,,,,,,,"0|z1e5g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 16:43;danderson;Fixed in master and in release-1.16 via 

4aa9a341bbf49e51809bc9cfcf0e946b2accd8ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update operations playground for 1.16,FLINK-30440,13514238,13514237,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,gunnar.morling,danderson,danderson,16/Dec/22 15:32,04/Jan/23 16:39,04/Jun/24 20:41,04/Jan/23 16:39,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 16:39:04 UTC 2023,,,,,,,,,,"0|z1e5g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/23 14:18;gunnar.morling;[~danderson], thanks for merging that PR! What is the procedure in regards to Jira? It seems you'd have to set the Fix Version and transition this issue to Resolved, as I'm lacking the rights to do so myself. Thanks again.;;;","02/Jan/23 15:11;danderson;With these specific playground tasks, I usually merge each of them to master as soon as they are ready, but wait until all 3 sub-tasks are done before merging all of them to the release branch and then resolving all of the issues.;;;","02/Jan/23 15:33;gunnar.morling;Ok, got it. I'm planning to send the PRs for the two other playgrounds in the course of this week.;;;","04/Jan/23 16:39;danderson;Fixed in master and release-1.16 via:

91d91f871bb7701d62145c4c4ce77876c56544a2

3ac359bd6c141f64f427955238a1b53970a7f4cf

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Update playgrounds for 1.16,FLINK-30439,13514237,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,gunnar.morling,danderson,danderson,16/Dec/22 15:30,04/Jan/23 16:44,04/Jun/24 20:41,04/Jan/23 16:44,,,,,,1.16.0,,,,,Documentation / Training,,,,0,starter,,,"All of the playgrounds should be updated for Flink 1.16. This should include reworking the code as necessary to avoid using anything that has been deprecated.

See [https://github.com/apache/flink-playgrounds] for more info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 15:45:27 UTC 2022,,,,,,,,,,"0|z1e5fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 15:40;gunnar.morling;Got pointed to this one by [~danderson] and would love to give it a try. Seems I lack the right role to have issues assigned, so I hope this comment is enough for now. IIUC, an existing committer can assign it to me?;;;","16/Dec/22 15:43;rmetzger;I assigned you!;;;","16/Dec/22 15:45;gunnar.morling;Exciting! Thanks, [~rmetzger].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The generated schema is not correct when using value.format debezium-avro-confluent,FLINK-30438,13514225,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qinjunjerry,qinjunjerry,16/Dec/22 13:52,16/Dec/22 14:22,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,0,,,,"With the following code:
{code:java}
CREATE TABLE TEST(
    ID BIGINT,
    INTEGRATION_ID STRING,
    PRIMARY KEY(INTEGRATION_ID) NOT ENFORCED
) WITH(
    'connector' = 'kafka',
    'topic' = 'TEST',
    'properties.bootstrap.servers' = 'broker:29092',
    'properties.group.id' = 'TEST',
    'key.format' = 'avro-confluent',
    'key.fields' = 'INTEGRATION_ID',
    'key.avro-confluent.url' = 'http://schema-registry:8081',
    'value.format' = 'debezium-avro-confluent',
    'value.debezium-avro-confluent.url' = 'http://schema-registry:8081',
    'scan.startup.mode' = 'earliest-offset'
); {code}
and this INSERT statement:
{code:java}
INSERT INTO TEST SELECT 1, '1'; {code}
The schema we get in the schema registry is:
{code:java}
[
  ""null"",
  {
    ""fields"": [
      {
        ""default"": null,
        ""name"": ""before"",
        ""type"": [
          ""null"",
          {
            ""fields"": [
              {
                ""default"": null,
                ""name"": ""ID"",
                ""type"": [
                  ""null"",
                  ""long""
                ]
              },
              {
                ""name"": ""INTEGRATION_ID"",
                ""type"": ""string""
              }
            ],
            ""name"": ""record_before"",
            ""type"": ""record""
          }
        ]
      },
      {
        ""default"": null,
        ""name"": ""after"",
        ""type"": [
          ""null"",
          {
            ""fields"": [
              {
                ""default"": null,
                ""name"": ""ID"",
                ""type"": [
                  ""null"",
                  ""long""
                ]
              },
              {
                ""name"": ""INTEGRATION_ID"",
                ""type"": ""string""
              }
            ],
            ""name"": ""record_after"",
            ""type"": ""record""
          }
        ]
      },
      {
        ""default"": null,
        ""name"": ""op"",
        ""type"": [
          ""null"",
          ""string""
        ]
      }
    ],
    ""name"": ""record"",
    ""namespace"": ""org.apache.flink.avro.generated"",
    ""type"": ""record""
  }
] {code}

The first 'null' in the schema shouldn't be there, I think.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 14:22:30 UTC 2022,,,,,,,,,,"0|z1e5d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 14:02;qinjunjerry;[~martijnvisser] Any reason you removed the original problem description part?;;;","16/Dec/22 14:22;martijnvisser;[~qinjunjerry] Apologies. I intended to only remove the ""Can you check and fix"", but removed too much accidentally. Just restored that.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State incompatibility issue might cause state loss,FLINK-30437,13514224,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,16/Dec/22 13:44,20/Dec/22 16:45,04/Jun/24 20:41,19/Dec/22 21:16,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Even though we set:
execution.shutdown-on-application-finish: false
execution.submit-failed-job-on-application-error: true
If there is a state incompatibility the jobmanager marks the Job failed, cleans up HA metada and restarts itself. This is a very concerning behaviour, but we have to fix this on the operator side to at least guarantee no state loss.

The solution is to harden the HA metadata check properly ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 16:45:55 UTC 2022,,,,,,,,,,"0|z1e5cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 21:16;gyfora;merged to main c4e76402f02f05932c6446d97bdc3d60861b9b27;;;","20/Dec/22 16:45;gyfora;merged to release-1.3 4e583a0faa0991c61b37ecd9b937fa3e11c6493a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate Opensearch connector docs into Flink docs v1.16,FLINK-30436,13514208,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,16/Dec/22 11:11,03/Apr/23 10:31,04/Jun/24 20:41,22/Dec/22 10:30,,,,,,1.16.1,,,,,Connectors / Opensearch,Documentation,,,0,pull-request-available,,,Update Flink docs for 1.16 to pull in the opensearch docs from flink-connector-opensearch repo,,,,,,,,,,,,,,,,FLINK-31702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 10:30:36 UTC 2022,,,,,,,,,,"0|z1e59c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 10:30;dannycranmer;Merged commit [{{57cb9da}}|https://github.com/apache/flink/commit/57cb9da51cb3e4866c5128b302f550ffa440e887] into apache:release-1.16 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`SHOW CREATE TABLE` statement shows column comment,FLINK-30435,13514201,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,16/Dec/22 10:30,22/Mar/23 06:11,04/Jun/24 20:41,22/Mar/23 01:58,1.17.0,,,,,1.18.0,,,,,Table SQL / API,,,,0,pull-request-available,,,"After a table with column comments created, we would find that the results generated by `show create table` statement lose column comments.

As created table has been migrated to new schema framework in https://issues.apache.org/jira/browse/FLINK-29679,  It is easy to take it done.

Note: the feature doesn't change sql syntax, just make outputs consistent with ddl and more user-friendly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29679,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 01:58:05 UTC 2023,,,,,,,,,,"0|z1e57s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 01:58;jark;Fixed in master: 268fc1a46f8af171c7102229a010af71c56623d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The non-null check for transactionalIdPrefix is unnecessary in the KafkaWriter constructor,FLINK-30434,13514198,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,Tony Giao,Tony Giao,16/Dec/22 10:26,16/Dec/22 13:26,04/Jun/24 20:41,16/Dec/22 13:26,1.14.6,1.15.3,1.16.0,,,,,,,,Connectors / Kafka,,,,0,,,,"The non-null check for transactionalIdPrefix is unnecessary in the KafkaWriter constructor, especially when EXACTLY_ONCE semantics are not used.


First, when EXACTLY_ONCE semantics are used, a non-null check has been performed for transactionalIdPrefix in KafkaSinkBuilder.


Secondly, although the default value ""kafka-sink"" has been assigned to transactionalIdPrefix in KafkaSinkBuilder, if the user uses the setTransactionalIdPrefix method to set it to null, the construction method of KafkaWriter will throw an exception, even if the user does not use EXACTLY_ONCE semantics at this time.


Obviously, we should allow transactionalIdPrefix to be null when EXACTLY_ONCE semantics are not used.

!image-2022-12-16-18-24-14-706.png|width=692,height=461!",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/22 10:24;Tony Giao;image-2022-12-16-18-24-14-706.png;https://issues.apache.org/jira/secure/attachment/13053916/image-2022-12-16-18-24-14-706.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 13:24:41 UTC 2022,,,,,,,,,,"0|z1e574:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 10:39;martijnvisser;This code is checking if the delivery guarantee is set to EXACTLY_ONCE and *only then* throws an error if a transactionalIdPrefix isn't set. I don't see why that is wrong?;;;","16/Dec/22 13:24;Tony Giao;Sorry, I didn't notice that the non-null check is also performed in the setTransactionalIdPrefix method, and the transactionalIdPrefix already has a default value ""kafka-sink"", so the transactionalIdPrefix in the KafkaWriter constructor cannot be null.
I will close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HiveDialectITCase#testCreateDatabase fails with ""Could not execute CREATE DATABASE""",FLINK-30433,13514153,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,16/Dec/22 08:22,08/Jan/24 00:18,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,Connectors / Hive,,,,0,auto-deprioritized-critical,test-stability,,"{code:java}
Feb 14 02:38:16 [ERROR] Tests run: 27, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 341.271 s <<< FAILURE! - in org.apache.flink.connectors.hive.HiveDialectITCase
Feb 14 02:38:16 [ERROR] org.apache.flink.connectors.hive.HiveDialectITCase.testTemporaryFunctionUDTF  Time elapsed: 49.997 s  <<< ERROR!
Feb 14 02:38:16 org.apache.flink.table.api.TableException: Could not execute CREATE DATABASE: (catalogDatabase: [{}], catalogName: [test-catalog], databaseName: [db1], ignoreIfExists: [false])
Feb 14 02:38:16 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1125)
Feb 14 02:38:16 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:730)
Feb 14 02:38:16 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTemporaryFunctionUDTF(HiveDialectITCase.java:838)
Feb 14 02:38:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 14 02:38:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 14 02:38:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 14 02:38:16 	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
Feb 14 02:38:16 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Feb 14 02:38:16 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create database db1
Feb 14 02:38:16 	at org.apache.flink.table.catalog.hive.HiveCatalog.createDatabase(HiveCatalog.java:376)
Feb 14 02:38:16 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1117)
Feb 14 02:38:16 	... 47 more
Feb 14 02:38:16 Caused by: InvalidObjectException(message:No such catalog hive)
Feb 14 02:38:16 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:1251)
Feb 14 02:38:16 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:1325)
Feb 14 02:38:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 14 02:38:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 14 02:38:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 14 02:38:16 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 14 02:38:16 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
Feb 14 02:38:16 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
Feb 14 02:38:16 	at com.sun.proxy.$Proxy35.create_database(Unknown Source)
Feb 14 02:38:16 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:809)
Feb 14 02:38:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 14 02:38:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 14 02:38:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 14 02:38:16 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 14 02:38:16 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208)
Feb 14 02:38:16 	at com.sun.proxy.$Proxy36.createDatabase(Unknown Source)
Feb 14 02:38:16 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createDatabase(HiveMetastoreClientWrapper.java:179)
Feb 14 02:38:16 	at org.apache.flink.table.catalog.hive.HiveCatalog.createDatabase(HiveCatalog.java:369)
Feb 14 02:38:16 	... 48 more{code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43954&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25508]",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30634,FLINK-31340,FLINK-30011,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 00:18:07 UTC 2024,,,,,,,,,,"0|z1e4x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 07:52;martijnvisser;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45442&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26793;;;","14/Feb/23 07:45;mapohl;{{HiveDialectITCase.testTemporaryFunctionUDTF}} is also affected:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46086&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=27168];;;","13/Mar/23 10:53;renqs;{{HiveDialectITCase.testAlterDatabase}} this time:

release-1.16:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47045&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=27160];;;","13/Mar/23 10:54;renqs;[~luoyuxia] Any idea on this one? ;;;","16/Mar/23 15:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26131;;;","23/Mar/23 07:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47476&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26137;;;","20/May/23 07:48;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49124&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=22404;;;","27/Jul/23 09:10;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51747&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=22056;;;","09/Aug/23 06:27;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52046&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=27243;;;","23/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","31/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jan/24 00:18;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55956&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23898;;;",,,,,,,,,,,,,,,,,,,
HiveSoure doesn't need to maintain all splits when it's for parallelism inference,FLINK-30432,13514126,13444738,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,16/Dec/22 03:36,16/Dec/22 03:39,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Connectors / Hive,,,,0,,,,"By default, Hive source will try to infer parallelism during which it'll split files to HiveSplit  and maintain these splits as a list.

But it's not nessary all the splits since we only need to know the split nums in parallelism inference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-16 03:36:59.0,,,,,,,,,,"0|z1e4r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC Connector fails to reestablish the lost DB connection. ,FLINK-30431,13514107,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jiabao.sun,snadendla,snadendla,15/Dec/22 22:04,10/Nov/23 06:11,04/Jun/24 20:41,,1.15.0,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,stale-assigned,,"Our use case with JDBC connector is to sink records to Amazon Redshift DB table.
At some point in time the connection with redshift gets closed and the Flink's JDBC connector tries to detect & reestablish the connection in the following manner in the @ JdbcOutputFormat.flush() :
{code:java}
1. public synchronized void flush() throws IOException {
2.    ..
3.    
4.    for (int i = 0; i <= executionOptions.getMaxRetries(); i++) {
5.        try {
6.            attemptFlush();
7.            ...
8.        } catch (SQLException e) {
9.            ...
10.            try {
11.                if (!connectionProvider.isConnectionValid()) { <-- TRUE!
12.                    updateExecutor(true);  
13.                }
14.            } catch (Exception exception) {
15.              ....
16.              throw new IOException(""Reestablish JDBC connection failed"", exception);
17.            }
18.            ....
19.        }
20.     }
21.     ....
22. }{code}
updateExecutor() is called (from line#12 of the above code snippet) to close statements and re-establish the DB connection. 

 
{code:java}
1. public void updateExecutor(boolean reconnect) throws SQLException, ...{
2.     jdbcStatementExecutor.closeStatements(); 
3.     jdbcStatementExecutor.prepareStatements(
4.             reconnect
5.                     ? connectionProvider.reestablishConnection()
6.                     : connectionProvider.getConnection());
7. } {code}
h4.  
----
h3. Results:
h4. Expected:

The connection should be re-established and the updates should be reflected on DB.
h4. Actual:

{color:#de350b}*The {{connection re-establish}} code is never reached/invoked !!.*{color} The closeStatements() fails/throws (as the connection is already closed).
{noformat}
Caused by: com.amazon.redshift.util.RedshiftException: This connection has been closed.
    at com.amazon.redshift.jdbc.RedshiftConnectionImpl.checkClosed(RedshiftConnectionImpl.java:1095) ~[?:?]
    at com.amazon.redshift.jdbc.RedshiftConnectionImpl.cancelQuery(RedshiftConnectionImpl.java:1299) ~[?:?]
    at com.amazon.redshift.jdbc.RedshiftStatementImpl.cancel(RedshiftStatementImpl.java:1042) ~[?:?]
    at com.amazon.redshift.jdbc.RedshiftStatementImpl.close(RedshiftStatementImpl.java:748) ~[?:?]
    at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.closeStatements(SimpleBatchStatementExecutor.java:81) ~[?:?]
    at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.updateExecutor(JdbcOutputFormat.java:402) ~[?:?]
    at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.flush(JdbcOutputFormat.java:226) ~[?:?]
    at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.lambda$open$0(JdbcOutputFormat.java:155) ~[?:?]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[?:1.8.0_292]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_292]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
2022-11-09 03:00:07,510 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 0f70cbc56798e19978b509bf0da0107b.
2022-11-09 03:00:07,517 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 0f70cbc56798e19978b509bf0da0107b reached terminal state FAILED.
{noformat}




 

 ","Flink 1.15.2

flink-connector-jdbc: 1.15.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Mon Aug 14 10:35:07 UTC 2023,,,,,,,,,,"0|z1e4mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 02:16;luoyuxia;Thanks for reporting. I think we should fix it. Otherwise, the reconnection will never happen.;;;","16/Dec/22 08:10;martijnvisser;Agree that this should be fixed. Given that we're just externalizing the JDBC connector, can any fixes be created towards {{release-1.16}} and {{release-1.15}} in Flink https://github.com/apache/flink and they should also be created in {{v3.0}} and {{main}} in the external repo at https://github.com/apache/flink-connector-jdbc;;;","16/Dec/22 09:17;jiabao.sun;Hi [~martijnvisser] can you help assign it to me?

I think we can catch the SQLException of closeStatements method.;;;","17/Jan/23 02:40;snadendla;Hi [~jiabao.sun],
Great to see the quick PR.
Just wondering if catching Exception would be safer as it can handle non-SQLException's (if any) and reconnection goes fine?
Can you please help me with release timeline for the fix? 

Thanks!;;;","17/Jan/23 09:15;martijnvisser;[~snadendla] Can you leave your review on the PR instead of this Jira ticket?;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove the useless dependencies in flink-connector-pulsar's dependencyManagement,FLINK-30430,13514067,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Dec/22 15:36,29/Dec/22 15:04,04/Jun/24 20:41,29/Dec/22 15:04,pulsar-3.0.0,pulsar-4.0.0,,,,pulsar-3.0.1,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"I can see a lot of useless dependencies exist in the root pom.xml file of flink-connector-pulsar. They are added in {{dependencyManagement}} part. And we don't setting all the dependency versions in the {{properties}}.

I think we should clean up the pom and move all the versions to {{properties}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 15:04:42 UTC 2022,,,,,,,,,,"0|z1e4eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 15:42;martijnvisser;[~syhily] Feel free to open a PR to clean them up. I do think that all of them right now are needed for dependency convergence https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development#ExternalizedConnectordevelopment-DependencyconvergenceerrorsontransitiveFlinkdependencies;;;","15/Dec/22 15:47;syhily;[~martijnvisser] Tks for your information. I'll check the doc to get the minimal set of required dependencies. Can you assign this issue to me?;;;","29/Dec/22 15:04;tison;master via https://github.com/apache/flink-connector-pulsar/pull/9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink command stops with IllegalArgumentException,FLINK-30429,13514058,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,15/Dec/22 14:51,29/Dec/22 14:53,04/Jun/24 20:41,29/Dec/22 14:53,1.17.0,,,,,1.17.0,,,,,Client / Job Submission,,,,0,pull-request-available,,,"The issue has been introduced [here|https://github.com/apache/flink/pull/20910/files#diff-0f8fa6f2e2f26da108646728a221ce0da9a0f090a040f5ee46f6269f6449fc59R1168].

The exception looks like the following:
{code:java}
java.lang.IllegalArgumentException: 1 > 0
	at java.base/java.util.Arrays.copyOfRange(Arrays.java:3990)
	at java.base/java.util.Arrays.copyOfRange(Arrays.java:3950)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1170)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 14:53:34 UTC 2022,,,,,,,,,,"0|z1e4cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 14:53;gyfora;merged to master: 259c7a4776af4d9d7f174f871fb2d4145787f794;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkReadITCase.testSnapshotsTable is unstable,FLINK-30428,13514041,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Dec/22 12:14,16/Dec/22 03:49,04/Jun/24 20:41,16/Dec/22 03:49,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,https://github.com/apache/flink-table-store/actions/runs/3702469144/jobs/6272779158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 03:49:53 UTC 2022,,,,,,,,,,"0|z1e48o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 03:49;lzljs3620320;master: 39f06f2ea5db8eee10a044316e54aeb9623c9f9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar SQL connector lists not bundled dependencies,FLINK-30427,13514033,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,dwysakowicz,dwysakowicz,15/Dec/22 11:26,15/Dec/22 15:44,04/Jun/24 20:41,15/Dec/22 13:53,pulsar-3.0.0,,,,,pulsar-3.0.0,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"flink-connector-pulsar lists:
{code}
- org.bouncycastle:bcpkix-jdk15on:1.69
- org.bouncycastle:bcprov-ext-jdk15on:1.69
{code}

but does not bundle them. (It uses them in test scope)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 15:44:23 UTC 2022,,,,,,,,,,"0|z1e46w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 13:53;martijnvisser;Fixed in:

main: 06ef03b620dbddb9dbcd55152ac315516e9bc7d4
v3.0: 78324dc30e934c033db25edd162491df35732d1f;;;","15/Dec/22 15:29;syhily;The bouncycastle dependency is introduced by {{pulsar-client-all}}, you can check the dependencies list here: https://mvnrepository.com/artifact/org.apache.pulsar/pulsar-client-all/2.10.2.

I don't know why we need to explicit set the bouncycastle in Pulsar connector? It seems like you mis set the bouncycastle as a test dependency. So just remove them in {{dependencyManagement}} would be better.;;;","15/Dec/22 15:44;martijnvisser;[~syhily] It was needed to resolve dependency convergence issues with the externalization. The scope was incorrect, but they need to be added else dependency convergence occurs. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Archunit can't find any tests in Pulsar repository,FLINK-30426,13514022,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,15/Dec/22 09:43,15/Dec/22 12:24,04/Jun/24 20:41,15/Dec/22 12:24,pulsar-4.0.0,,,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"CI is failing on {{main}} with:

{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-pulsar: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: 'java.lang.Object com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(java.lang.Class)' -> [Help 1]
{code}

It does work for {{v3.0}} but there's no obvious difference between the two branches that might explain this issue. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 12:24:52 UTC 2022,,,,,,,,,,"0|z1e44g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 09:54;Sergey Nuyanzin;[~martijnvisser] since I did update of ArchUnit to 1.0.0 may be I can help here
Could you please give a bit more details like links to branches?;;;","15/Dec/22 09:59;martijnvisser;[~Sergey Nuyanzin] Would be great. See https://github.com/apache/flink-connector-pulsar for the repo

You can find a failed run in https://github.com/apache/flink-connector-pulsar/actions/runs/3695837014/jobs/6258724442 which I can reproduce locally by running the same Maven command as the CI does:

mvn clean deploy -Dscala-2.12 -Prun-end-to-end-tests -DdistDir=/path-to-flink-dist/flink-1.17-SNAPSHOT -Dflink.convergence.phase=install -Pcheck-convergence -Darchunit.freeze.store.default.allowStoreUpdate=false -Dlog4j.configurationFile=file:///path-to/flink-connector-pulsar/tools/ci/log4j.properties

For the {{v3.0}} branch it does pass. Command is the same, expect the distribution dir value of course. 

;;;","15/Dec/22 11:32;Sergey Nuyanzin;Seems that the reason is update ArchUnit to 1.0.0

There is a breaking change done in 0.23.0 (described at [https://github.com/TNG/ArchUnit/releases/tag/v0.23.0)]

After that any rule matching nothing will fail.
Flink-connector-pulsar depends on flink-snapshot and as a result pulls a newer version of flink-architecture-tests-test and flink-architecture-tests-production which are now built with 1.0.0 ArchUnit. And it leads to breaking change mentioned above.

A simple WA is use of {{archRule.failOnEmptyShould=false}} as mentioned in release notes.
At the same time it also questionable if there is a need for {{ProductionCodeArchitectureRules}} in this connector since there is no any {{api}} packages or {{Public}} annotation which is the subject of search for some of those rules;;;","15/Dec/22 12:24;martijnvisser;Fixed in main: d82256b3c7b7f028460bf0212766c5508371ac65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalize token receive side,FLINK-30425,13514018,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,15/Dec/22 09:25,13/Jan/23 08:32,04/Jun/24 20:41,13/Jan/23 08:32,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 08:32:39 UTC 2023,,,,,,,,,,"0|z1e43k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 08:32;mbalassi;324b54c in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add source operator restore readerState log to distinguish split is from newPartitions or split state,FLINK-30424,13513973,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taoran,taoran,taoran,15/Dec/22 04:17,17/Dec/22 03:30,04/Jun/24 20:41,17/Dec/22 03:30,1.15.3,1.16.0,1.16.1,,,1.17.0,,,,,API / DataStream,,,,0,pull-request-available,,,"When a job start firstly, we can find 'assignPartitions' from log。but if source recover from state, we can not distinguish the newPartitions is from timed discover thread or from reader task state.  

We can add a helper log to distinguish and confirm the reader using split state in recover situation.  it's very useful for troubleshooting.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 17 03:30:04 UTC 2022,,,,,,,,,,"0|z1e3tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 06:25;taoran;[~becket_qin]  can u help me to review this pr?;;;","15/Dec/22 08:34;gaoyunhaii;Hi [~lemonjing] could you pointed out to me the position of 'assignedPartitions' or the full content of this log?;;;","15/Dec/22 09:22;taoran;Hi [~gaoyunhaii] . thanks for reviewing. i have added the jm and tm log in the below.

*We can see the recover log in the JM:*
2022-12-15 16:55:05,747 INFO  [pool-10-thread-1] org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2022-12-15 16:55:05,757 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Starting the resource manager.
2022-12-15 16:55:05,763 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Starting the slot manager.
2022-12-15 16:55:05,823 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from ZooKeeperStateHandleStore\{namespace='flink/antc4flink87003521/jobs/20a4149a35ee6a00aba5356c361bf4df/checkpoints'}.
2022-12-15 16:55:05,859 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in ZooKeeperStateHandleStore\{namespace='flink/antc4flink87003521/jobs/20a4149a35ee6a00aba5356c361bf4df/checkpoints'}.
2022-12-15 16:55:05,859 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Trying to fetch 0 checkpoints from storage.
2022-12-15 16:55:05,868 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Initialized DefaultCompletedCheckpointStore in 'ZooKeeperStateHandleStore\{namespace='flink/antc4flink87003521/jobs/20a4149a35ee6a00aba5356c361bf4df/checkpoints'}' with /checkpoints.
2022-12-15 16:55:05,962 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job antc4flink87003521 (20a4149a35ee6a00aba5356c361bf4df).
2022-12-15 16:55:05,962 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-12-15 16:55:06,251 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 1 ms, total 1 pipelined regions currently.
2022-12-15 16:55:06,349 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using job/cluster config to configure application-defined state backend: EmbeddedRocksDBStateBackend\{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=4, writeBatchSize=2097152}
2022-12-15 16:55:06,352 INFO  [jobmanager-io-thread-1] org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
2022-12-15 16:55:06,353 INFO  [jobmanager-io-thread-1] org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using application-defined options factory: DefaultConfigurableOptionsFactory\{configuredOptions={state.backend.antkv.perf-num-occurrence=1000, state.backend.antkv.ttl.ms=129600000, state.backend.antkv.index-block-restart-interval=16, state.backend.antkv.learned.index.enabled=false, state.backend.antkv.level-compaction-dynamic-level-bytes.enabled=false, state.backend.antkv.cache-num-shard-bits=1, state.backend.antkv.level-for-learned-index=0, state.backend.antkv.statistics.enabled=false}}.
2022-12-15 16:55:06,354 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using application-defined state backend: EmbeddedRocksDBStateBackend\{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=4, writeBatchSize=2097152}
2022-12-15 16:55:06,355 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as EmbeddedRocksDBStateBackend
2022-12-15 16:55:06,360 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using job/cluster config to configure application-defined checkpoint storage: org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage@5e36d9c3
2022-12-15 16:55:06,547 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-12-15 16:55:06,548 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Starting job 20a4149a35ee6a00aba5356c361bf4df from savepoint dfs://f-blink-test.sh.aliyun-inc.com:10290/blink/blink-operator/antc4flink87003521/chk-50/_metadata ()
2022-12-15 16:55:06,768 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Reset the checkpoint ID of job 20a4149a35ee6a00aba5356c361bf4df to 51.
2022-12-15 16:55:06,768 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 20a4149a35ee6a00aba5356c361bf4df from Checkpoint 50 @ 0 for 20a4149a35ee6a00aba5356c361bf4df located at dfs://f-blink-test.sh.aliyun-inc.com:10290/blink/blink-operator/antc4flink87003521/chk-50.
2022-12-15 16:55:06,840 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No master state to restore
2022-12-15 16:55:06,840 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator [] - Resetting coordinator to checkpoint.
2022-12-15 16:55:06,842 INFO  [Thread-18] org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]).
2022-12-15 16:55:06,842 INFO  [Thread-18] org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) closed.
2022-12-15 16:55:06,844 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Restoring SplitEnumerator of source Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) from checkpoint.
2022-12-15 16:55:06,862 INFO  [jobmanager-io-thread-1] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@5f14baca for antc4flink87003521 (20a4149a35ee6a00aba5356c361bf4df).
2022-12-15 16:55:06,937 INFO  [flink-akka.actor.default-dispatcher-16] org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with ZookeeperLeaderRetrievalDriver\{connectionInformationPath='/resource_manager/connection_info'}.
2022-12-15 16:55:06,937 INFO  [flink-akka.actor.default-dispatcher-16] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'antc4flink87003521' (20a4149a35ee6a00aba5356c361bf4df) under job master id b54febaa53b5412ad03a423f37644f07.
2022-12-15 16:55:06,941 INFO  [flink-akka.actor.default-dispatcher-16] org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]).
2022-12-15 16:55:06,943 INFO  [flink-akka.actor.default-dispatcher-16] org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2022-12-15 16:55:06,943 INFO  [flink-akka.actor.default-dispatcher-16] org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job flink_demo (20a4149a35ee6a00aba5356c361bf4df) switched from state CREATED to RUNNING.
2022-12-15 16:55:06,943 INFO  [SourceCoordinator-Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt])] com.alipay.flink.connectors.demo.source.enumerator.demoSourceEnumerator [] - Starting the demoSourceEnumerator with partition discovery interval of 60000 ms.

*but if no partitions change, the discover thread just check and return.  Though i know the job was recovered but in the tm log there are no logs related with recover :*

 

2022-12-15 16:55:30,114 INFO  [main] org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2022-12-15 16:55:30,177 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with ZookeeperLeaderRetrievalDriver\{connectionInformationPath='/resource_manager/connection_info'}.
2022-12-15 16:55:30,178 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2022-12-15 16:55:30,180 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /opt/flink/flink-dist-cache-2a59b734-6c51-4379-b18e-3d6dac87f7af
2022-12-15 16:55:30,182 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@100.88.80.8:6123/user/rpc/resourcemanager_2(b54febaa53b5412ad03a423f37644f07).
2022-12-15 16:55:30,426 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2022-12-15 16:55:30,518 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@100.88.80.8:6123/user/rpc/resourcemanager_2 under registration id e85390bf49682d3d723733ac807adb15.
2022-12-15 16:55:30,549 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request cd0f4d4ab80dbc66fe2c42dce23a143a for job 20a4149a35ee6a00aba5356c361bf4df from resource manager with leader id b54febaa53b5412ad03a423f37644f07.
2022-12-15 16:55:30,554 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for cd0f4d4ab80dbc66fe2c42dce23a143a.
2022-12-15 16:55:30,555 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 20a4149a35ee6a00aba5356c361bf4df for job leader monitoring.
2022-12-15 16:55:30,557 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with ZookeeperLeaderRetrievalDriver\{connectionInformationPath='/20a4149a35ee6a00aba5356c361bf4df/connection_info'}.
2022-12-15 16:55:30,561 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request eac343b1f8cef50bd6f636f3595b58bf for job 20a4149a35ee6a00aba5356c361bf4df from resource manager with leader id b54febaa53b5412ad03a423f37644f07.
2022-12-15 16:55:30,561 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for eac343b1f8cef50bd6f636f3595b58bf.
2022-12-15 16:55:30,562 INFO  [main-EventThread] org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@100.88.80.8:6123/user/rpc/jobmanager_1 with leader id d03a423f-3764-4f07-b54f-ebaa53b5412a.
2022-12-15 16:55:30,585 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2022-12-15 16:55:30,618 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@100.88.80.8:6123/user/rpc/jobmanager_1 for job 20a4149a35ee6a00aba5356c361bf4df.
2022-12-15 16:55:30,619 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 20a4149a35ee6a00aba5356c361bf4df.
2022-12-15 16:55:30,622 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 20a4149a35ee6a00aba5356c361bf4df.
2022-12-15 16:55:30,648 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cd0f4d4ab80dbc66fe2c42dce23a143a.
2022-12-15 16:55:30,648 INFO  [flink-akka.actor.default-dispatcher-4] org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot eac343b1f8cef50bd6f636f3595b58bf.
2022-12-15 16:56:19,892 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cd0f4d4ab80dbc66fe2c42dce23a143a.
2022-12-15 16:56:19,907 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2022-12-15 16:56:19,925 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0 (9293cae434983862cfcae1f732528bc7), deploy into slot with allocation id cd0f4d4ab80dbc66fe2c42dce23a143a.
2022-12-15 16:56:19,926 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0 (9293cae434983862cfcae1f732528bc7) switched from CREATED to DEPLOYING.
2022-12-15 16:56:19,930 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot eac343b1f8cef50bd6f636f3595b58bf.
2022-12-15 16:56:19,929 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0 (9293cae434983862cfcae1f732528bc7) [DEPLOYING].
2022-12-15 16:56:19,932 INFO  [flink-akka.actor.default-dispatcher-15] org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0 (3e38d62bdebbb01fd55b0e96deed0856), deploy into slot with allocation id eac343b1f8cef50bd6f636f3595b58bf.
2022-12-15 16:56:19,933 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0 (3e38d62bdebbb01fd55b0e96deed0856) switched from CREATED to DEPLOYING.
2022-12-15 16:56:19,933 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0 (3e38d62bdebbb01fd55b0e96deed0856) [DEPLOYING].
2022-12-15 16:56:20,000 INFO  [Thread-3] com.alibaba.dfs.common.PerformanceCounter                    [] - PERF: dfs://f-blink-test.sh.aliyun-inc.com:10290/blink/ha/antc4flink87003521.OPEN count 1 latency(us): avg 12369 deviation 0
time dist1:  [>10000 1 100.00%]
time dist2:  [<20000 1 100.00%]
2022-12-15 16:56:20,001 INFO  [Thread-3] com.alibaba.dfs.common.PerformanceCounter                    [] - PERF: dfs://f-blink-test.sh.aliyun-inc.com:10290/blink/ha/antc4flink87003521.READ_SYS count 1 latency(us): avg 35153 deviation 0 Bps: avg 29834011.0 deviation 0.0
time dist1:  [>10000 1 100.00%]
time dist2:  [<40000 1 100.00%]
2022-12-15 16:56:20,716 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager [] - Reuse old classloader because the cached libraries is not compatible with new task.
2022-12-15 16:56:20,839 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined state backend: EmbeddedRocksDBStateBackend\{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=4, writeBatchSize=2097152}
2022-12-15 16:56:20,839 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined state backend: EmbeddedRocksDBStateBackend\{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=4, writeBatchSize=2097152}
2022-12-15 16:56:20,841 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
2022-12-15 16:56:20,843 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
2022-12-15 16:56:20,844 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using application-defined options factory: DefaultConfigurableOptionsFactory\{configuredOptions={state.backend.antkv.perf-num-occurrence=1000, state.backend.antkv.ttl.ms=129600000, state.backend.antkv.index-block-restart-interval=16, state.backend.antkv.learned.index.enabled=false, state.backend.antkv.level-compaction-dynamic-level-bytes.enabled=false, state.backend.antkv.cache-num-shard-bits=1, state.backend.antkv.level-for-learned-index=0, state.backend.antkv.statistics.enabled=false}}.
2022-12-15 16:56:20,844 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using application-defined options factory: DefaultConfigurableOptionsFactory\{configuredOptions={state.backend.antkv.perf-num-occurrence=1000, state.backend.antkv.ttl.ms=129600000, state.backend.antkv.index-block-restart-interval=16, state.backend.antkv.learned.index.enabled=false, state.backend.antkv.level-compaction-dynamic-level-bytes.enabled=false, state.backend.antkv.cache-num-shard-bits=1, state.backend.antkv.level-for-learned-index=0, state.backend.antkv.statistics.enabled=false}}.
2022-12-15 16:56:20,845 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: EmbeddedRocksDBStateBackend\{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=4, writeBatchSize=2097152}
2022-12-15 16:56:20,845 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: EmbeddedRocksDBStateBackend\{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=4, writeBatchSize=2097152}
2022-12-15 16:56:20,845 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as EmbeddedRocksDBStateBackend
2022-12-15 16:56:20,845 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as EmbeddedRocksDBStateBackend
2022-12-15 16:56:20,850 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined checkpoint storage: org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage@4413f5e9
2022-12-15 16:56:20,850 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined checkpoint storage: org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage@1095af08
2022-12-15 16:56:20,863 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0 (9293cae434983862cfcae1f732528bc7) switched from DEPLOYING to INITIALIZING.
2022-12-15 16:56:20,863 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0 (3e38d62bdebbb01fd55b0e96deed0856) switched from DEPLOYING to INITIALIZING.
{color:#00875a}2022-12-15 16:56:21,160 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: test_topic-2, startingOffset: 1670947200000, queueCursor: MTY3MTA3NTYzNTY4NDg2ODY3MA==, stoppingOffset: 9223372036854775807]]{color}
2022-12-15 16:56:21,160 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: test_topic-1, startingOffset: 1670947200000, queueCursor: MTY2OTYxNDE0MTI1ODYyMzkzMQ==, stoppingOffset: 9223372036854775807]]
2022-12-15 16:56:21,161 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] com.alipay.flink.connectors.common.source.reader.SplitReaderSupplier [] - Add new split reader.
2022-12-15 16:56:21,161 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] com.alipay.flink.connectors.common.source.reader.SplitReaderSupplier [] - Add new split reader.
2022-12-15 16:56:21,171 INFO  [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-12-15 16:56:21,171 INFO  [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-12-15 16:56:21,177 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (1/2)#0 (9293cae434983862cfcae1f732528bc7) switched from INITIALIZING to RUNNING.
2022-12-15 16:56:21,177 INFO  [Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0] org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, demo_in]], fields=[f0, f1, cnt]) -> Calc(select=[f0, f1]) (2/2)#0 (3e38d62bdebbb01fd55b0e96deed0856) switched from INITIALIZING to RUNNING.

*{color:#172b4d}just with the green line Adding split(s) to reader but this log is existed both in partition discover thread newPartitionsSplit adding and restore situation (it means this adding split could be restored also can be new added splits). We can not distinguish whether task recover from split state or new partitions adding. There are no logs related with restore in the tm side.{color}* 

So i think we can add task restore log in the tm side. It's useful for debugging and troubleshooting.;;;","17/Dec/22 03:30;becket_qin;Merged to master: 528186b62da92ecde0fa308f1df0cc6f95495f4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce cast executor codegen for column type evolution,FLINK-30423,13513961,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Dec/22 00:59,03/Jan/23 03:51,04/Jun/24 20:41,03/Jan/23 03:51,table-store-0.3.0,,,,,table-store-0.4.0,,,,,Table Store,,,,0,pull-request-available,,,Introduce cast executor codegen for column type evolution,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 03:51:52 UTC 2023,,,,,,,,,,"0|z1e3qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 03:51;lzljs3620320;master: 
dcae4c28d69750713fa0d478d6ea348735790dbe
b22ed19740f427852f404d93570e3b0cff1b09ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalize token framework provider API,FLINK-30422,13513636,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,14/Dec/22 16:16,21/Dec/22 13:40,04/Jun/24 20:41,21/Dec/22 13:40,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 13:40:17 UTC 2022,,,,,,,,,,"0|z1e1qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 13:40;gaborgsomogyi;6e5f8694b66d8dea8ae2926adafdb610c09d9b88 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move TGT renewal to hadoop module,FLINK-30421,13513635,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,14/Dec/22 16:13,21/Dec/22 13:39,04/Jun/24 20:41,21/Dec/22 13:39,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 13:39:47 UTC 2022,,,,,,,,,,"0|z1e1qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 13:39;gaborgsomogyi;21a1bf53ad5a5ed1842341624dcfe36c3a385f45 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE thrown when using window time in Table API,FLINK-30420,13513622,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,14/Dec/22 15:34,14/Dec/22 16:59,04/Jun/24 20:41,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"Run the following unit test.
{code:java}
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource;
import org.apache.flink.streaming.api.functions.source.datagen.SequenceGenerator;
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.Tumble;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.functions.AggregateFunction;
import org.apache.flink.table.functions.TableFunction;

import org.junit.Before;
import org.junit.Test;

import java.sql.Timestamp;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

import static org.apache.flink.table.api.DataTypes.TIMESTAMP;
import static org.apache.flink.table.api.Expressions.$;
import static org.apache.flink.table.api.Expressions.call;
import static org.apache.flink.table.api.Expressions.currentTimestamp;
import static org.apache.flink.table.api.Expressions.lit;

public class TestWindowTime {

    private StreamTableEnvironment tEnv;
    private StreamExecutionEnvironment env;

    @Before
    public void before() {
        Configuration config = new Configuration();
        env = StreamExecutionEnvironment.getExecutionEnvironment(config);
        env.setParallelism(1);
        tEnv = StreamTableEnvironment.create(env);
    }

    @Test
    public void testWindowTime() {
        DataStream<Integer> stream =
                env.addSource(
                                new DataGeneratorSource<>(
                                        SequenceGenerator.intGenerator(0, 30), 1, 30l))
                        .returns(Integer.class);
        DataStream<Tuple2<Integer, Long>> streamWithTime =
                stream.map(x -> Tuple2.of(x, System.currentTimeMillis()))
                        .returns(Types.TUPLE(Types.INT, Types.LONG))
                        .assignTimestampsAndWatermarks(
                                WatermarkStrategy.<Tuple2<Integer, Long>>forBoundedOutOfOrderness(
                                                Duration.ofSeconds(2))
                                        .withTimestampAssigner(
                                                (ctx) -> (element, recordTimestamp) -> element.f1));
        Schema schema =
                Schema.newBuilder()
                        .column(""f0"", DataTypes.INT())
                        .column(""f1"", DataTypes.BIGINT())
                        .columnByMetadata(""rowtime"", ""TIMESTAMP_LTZ(3)"")
                        .watermark(""rowtime"", ""SOURCE_WATERMARK()"")
                        .build();

        Table table = tEnv.fromDataStream(streamWithTime, schema);
        table = table.select($(""rowtime""));

        Table windowedTable =
                table.window(Tumble.over(""5.seconds"").on(""rowtime"").as(""w""))
                        .groupBy($(""w""))
                        .select(
                                call(UDAF.class, $(""rowtime"")).as(""row_times""),
                                $(""w"").rowtime().as(""window_time""),
                                currentTimestamp().as(""current_timestamp""));

        windowedTable =
                windowedTable
                        .joinLateral(call(SplitFunction.class, $(""row_times"")).as(""rowtime""))
                        .select(
                                $(""rowtime"").cast(TIMESTAMP(3)).as(""rowtime""),
                                $(""window_time""),
                                $(""current_timestamp""));
        windowedTable.printSchema();
        windowedTable.execute().print();
    }

    public static class SplitFunction extends TableFunction<Timestamp> {

        public void eval(List<Timestamp> times) {
            for (int i = 0; i < times.size(); i++) {
                collect(times.get(i));
            }
        }
    }

    public static class UDAF extends AggregateFunction<List<Timestamp>, List<Timestamp>> {

        public UDAF() {}

        @Override
        public List<Timestamp> createAccumulator() {
            return new ArrayList<>();
        }

        public void accumulate(List<Timestamp> accumulator, Timestamp num) {
            accumulator.add(num);
        }

        @Override
        public List<Timestamp> getValue(List<Timestamp> accumulator) {
            return accumulator;
        }
    }
} {code}
Then the following exception occurs.
{code:java}
java.lang.NullPointerException
    at org.apache.calcite.sql2rel.RelDecorrelator.getNewForOldInputRef(RelDecorrelator.java:1359)
    at org.apache.calcite.sql2rel.RelDecorrelator.access$400(RelDecorrelator.java:122)
    at org.apache.calcite.sql2rel.RelDecorrelator$DecorrelateRexShuttle.visitInputRef(RelDecorrelator.java:1638)
    at org.apache.calcite.sql2rel.RelDecorrelator$DecorrelateRexShuttle.visitInputRef(RelDecorrelator.java:1595)
    at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
    at org.apache.calcite.rex.RexShuttle.visitList(RexShuttle.java:158)
    at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:110)
    at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:33)
    at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateExpr(RelDecorrelator.java:348)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:759)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:723)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:525)
    at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:687)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1170)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:1153)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:525)
    at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:687)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:734)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:723)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:525)
    at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:687)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:391)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:525)
    at org.apache.calcite.sql2rel.RelDecorrelator.getInvoke(RelDecorrelator.java:687)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelate(RelDecorrelator.java:276)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateQuery(RelDecorrelator.java:200)
    at org.apache.calcite.sql2rel.RelDecorrelator.decorrelateQuery(RelDecorrelator.java:165)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkDecorrelateProgram.optimize(FlinkDecorrelateProgram.scala:41)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:59)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1(FlinkGroupProgram.scala:56)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1$adapted(FlinkGroupProgram.scala:51)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.immutable.Range.foreach(Range.scala:155)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:51)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:175)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:82)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:75)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:307)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:187)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
    at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:605)
    at org.apache.flink.ml.clustering.TestAggWithSourceWatermark.testWindowTime(TestAggWithSourceWatermark.java:109)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 16:59:13 UTC 2022,,,,,,,,,,"0|z1e1nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 16:59;Sergey Nuyanzin;Looks like an old issue
I was able to reproduce it with 1.16.0, 1.15.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow tuning of transaction timeout,FLINK-30419,13513614,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Gerrrr,vicky_papavas,vicky_papavas,14/Dec/22 14:50,29/Mar/23 03:12,04/Jun/24 20:41,29/Mar/23 03:12,,,,,,,,,,,Table Store,,,,0,,,,FTS sets the producer transaction timeout to 1hr. The maximum allowed by a kafka broker is 15 mins. This causes exceptions to be thrown and the job dies when kafka log is enabled on a table. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 03:12:15 UTC 2023,,,,,,,,,,"0|z1e1ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 22:33;Gerrrr;The 1h default comes from [FlinkKafkaProducer|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer.java] that deprecated in favour of [KafkaSink|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaSink.java].

[~lzljs3620320] is there a reason to keep using FlinkKafkaProducer? If not, [~vicky_papavas] or I will be happy to make a PR that replaces it with KafkaSink.;;;","09/Jan/23 02:57;lzljs3620320;Thanks [~vicky_papavas] and [~Gerrrr].
I don't know whether it can be migrated to KafkaSink, because its caller is StoreWriteOperator, which is a StreamOperator.
At present, the SinkV2 mechanism cannot meet our needs, so we still use StreamOperator.;;;","09/Jan/23 02:59;lzljs3620320;I have an idea that we can not use kafka's transaction mechanism. ;;;","12/Jan/23 01:34;Gerrrr;Thank you for the feedback [~lzljs3620320]! I looked a bit more into the issue we are trying to solve, invalid default producer transaction timeout, and it turns out that the default is the same for both FlinkKafkaProducer and SinkV2, so just switching to KafkaSink won't help us anyway.

I looked a bit into the reason why Flink sets the default timeout to 1 hour.

From [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/connectors/datastream/kafka/]:
{quote}it is highly recommended to tweak Kafka transaction timeout (see Kafka producer transaction.timeout.ms)» maximum checkpoint duration + maximum restart duration or data loss may happen when Kafka expires an uncommitted transaction.{quote}

 From [https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/kafka/#kafka-producer]:

{quote}
Semantic.EXACTLY_ONCE mode relies on the ability to commit transactions that were started before taking a checkpoint, after recovering from the said checkpoint. If the time between Flink application crash and completed restart is larger than Kafka’s transaction timeout there will be data loss (Kafka will automatically abort transactions that exceeded timeout time). Having this in mind, please configure your transaction timeout appropriately to your expected down times.

Kafka brokers by default have transaction.max.timeout.ms set to 15 minutes. This property will not allow to set transaction timeouts for the producers larger than it’s value. FlinkKafkaProducer by default sets the transaction.timeout.ms property in producer config to 1 hour, thus transaction.max.timeout.ms should be increased before using the Semantic.EXACTLY_ONCE mode.
{quote}

It would be helpful to be able to change the default for `kafka.transaction.timeout.ms` for the entire cluster rather than having to override it in each table. Do you know if this is possible to do right now  [~lzljs3620320]?


 ;;;","12/Jan/23 02:02;lzljs3620320;[~Gerrrr] Nice investigation!

> It would be helpful to be able to change the default for `kafka.transaction.timeout.ms` for the entire cluster rather than having to override it in each table.

What is `entire cluster`? Do you mean we can have a default value for table store catalog for `kafka.transaction.timeout.ms`?;;;","12/Jan/23 02:02;lzljs3620320;CC [~renqs] Kafka connector expert;;;","18/Jan/23 23:51;Gerrrr;> Do you mean we can have a default value for table store catalog for `kafka.transaction.timeout.ms`?

Yeah! I think it would be generally useful to be able to override FTS defaults on the Catalog level. In fact, the FTS Catalog already overrides Table's {{connector}} and {{path}} fields using its {{type}} and {{warehouse}} fields. We would extend that to provide arbitrary fields. For example, the table below gets {{log.system}} and kafka options from the Catalog:
{noformat}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store',
    'log.system' = 'kafka',
    'kafka.bootstrap.servers' = '<list of kafka brokers>',
    'kafka.transaction.timeout.ms' = '900000',
 );

USE CATALOG table_store_catalog;

CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH (
    'kafka.topic' = 'word_count_log'
);{noformat}
WDYT [~lzljs3620320] ?;;;","01/Feb/23 01:58;lzljs3620320;Sorry for late reply. I was on vacation before.
[~Gerrrr] Looks good to me!;;;","29/Mar/23 03:12;lzljs3620320;https://github.com/apache/incubator-paimon/issues/754;;;",,,,,,,,,,,,,,,,,,,,,,
Implement synchronous KinesisClient in EFO,FLINK-30418,13513604,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,darenwkt,darenwkt,14/Dec/22 13:28,17/Jan/23 11:57,04/Jun/24 20:41,17/Jan/23 11:57,,,,,,aws-connector-4.1.0,,,,,Connectors / AWS,Connectors / Kinesis,,,0,pull-request-available,,,"h2. What?


Use synchronous KinesisClient for consumer registration/de-registration/describe.
h2.  
h2. Why?


KinesisProxyV2 supports the following operations:
 * subscribeToShard
 * describeStreamSummary
 * describeStreamConsumer
 * registerStreamConsumer
 * deregisterStreamConsumer

 

All the above operations are currently performed via an AsyncHTTPClient. However, only subsribeToShard needs to be asynchronous, the rest are called synchronously.

 
In addition, there are known issue with async client as follows:
 * Async client can cause the following RuntimeException when app parallelism is high.
{quote}Unable to execute HTTP request: The channel was closed before the protocol could be determined.
{quote}
 * It can also cause deadlock situation as described in https://issues.apache.org/jira/browse/FLINK-30304",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 11:57:03 UTC 2023,,,,,,,,,,"0|z1e1jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 13:33;dannycranmer;Thanks Daren, have assigned to you;;;","17/Jan/23 11:57;dannycranmer;Merged commit [{{70c84cb}}|https://github.com/apache/flink-connector-aws/commit/70c84cb7696151f502d9e6f546ee0f3bb6b327cf] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RabbitMQ error is not clear,FLINK-30417,13513598,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yaronshani,yaronshani,yaronshani,14/Dec/22 13:01,12/Jan/24 08:50,04/Jun/24 20:41,20/Dec/22 15:06,1.14.6,,,,,rabbitmq-3.1.0,,,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,,"Hey,

When there is no connection to the RabbitMQ flink prints:
{code:java}
java.lang.RuntimeException: Error while creating the channel{code}
or
{code:java}
java.net.SocketTimeoutException: connect timeout{code}
 

This is making very difficult for people to understand what exactly is failing. Is it the connection between the jobs/tasks manager? is it Kafka? Rabbit? something else?

I am going to make a PR that state that the error is the RabbitMQ connection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 08:49:26 UTC 2024,,,,,,,,,,"0|z1e1i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 13:09;yaronshani;https://github.com/apache/flink/pull/21509;;;","20/Dec/22 15:06;martijnvisser;Fixed in main: 967f713257bf7e933c04808c857b0e549071b863;;;","12/Jan/24 08:49;martijnvisser;This wasn't released with rabbitmq-3.0.1 since it was only made available in {{main}} and not the {{v3.0}} branch. It's now cherry picked to v3.0 and the fix version has been updated to rabbitmq-3.0.2

v3.0: 702f0bd687eca2ca1f460fef9229f3db213c1803;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add configureSession REST API in the SQL Gateway,FLINK-30416,13513591,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,fsk119,fsk119,14/Dec/22 11:59,04/Jan/23 08:22,04/Jun/24 20:41,04/Jan/23 08:22,1.17.0,,,,,1.17.0,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 08:22:04 UTC 2023,,,,,,,,,,"0|z1e1go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 08:22;fsk119;Merged into master: b48cddf12a3171cf1b6ddaa675a6b81eea1254bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync JDBC connector to match with previously released 1.16 version,FLINK-30415,13513570,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,14/Dec/22 10:09,20/Dec/22 13:51,04/Jun/24 20:41,20/Dec/22 13:51,jdbc-3.0.0,,,,,jdbc-3.0.0,,,,,Connectors / JDBC,,,,0,,,,"The current {{3.0}} branch of the externalized JDBC connector contains commits that were synced from Flink's {{master}} branch. Those commits shouldn't be there, but should be in the {{main}} branch for the externalized JDBC connector (which could be released in a next {{3.1}} release)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 12:19:43 UTC 2022,,,,,,,,,,"0|z1e1c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 12:19;martijnvisser;Dropped 5 commits:
+ 00b5b71055ed88b1e101402b7cb215d4a78527c7 Update version to 1.17-SNAPSHOT
+ eceece3505381c46758ef6312e1d003af0fae723 [FLINK-14101][jdbc-connector] Support SQLServer dialect in the jdbc connector.
+ 9efb82b4d86d8326f73a147fa490916306855d30 [FLINK-29466][Connector/JDBC] Bump postgresql in /flink-connectors/flink-connector-jdbc
+ 3e3b40e8cfadfc16a8ab74d4ef6a3ab3ceafa57b [FLINK-16024][connector/jdbc] Support filter pushdown
+ 29c8af85a3495503192b95aede8a223762d67730 [FLINK-30177][Connectors/JDBC] Update pgjdbc to avoid CVE-2022-41946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add unit test time out when run ci.,FLINK-30414,13513567,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Do,,Aiden Gong,Aiden Gong,14/Dec/22 10:00,09/Jan/23 02:54,04/Jun/24 20:41,07/Jan/23 02:54,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,!image-2022-12-14-17-59-56-800.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/22 10:00;Aiden Gong;image-2022-12-14-17-59-56-800.png;https://issues.apache.org/jira/secure/attachment/13053868/image-2022-12-14-17-59-56-800.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:54:36 UTC 2023,,,,,,,,,,"0|z1e1bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 10:01;Aiden Gong;Hi [~lzljs3620320]  ,  What about set timeout to 60 minutes?;;;","06/Jan/23 12:04;nicholasjiang;[~Aiden Gong], it isn't needed to add test timeout for running CI. [~lzljs3620320], could you please help to close this ticket?;;;","09/Jan/23 02:54;lzljs3620320;[~nicholasjiang] I think this is not a true issue, the root reason is that there are some stuck cases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop Shared and Key_Shared subscription support in Pulsar connector,FLINK-30413,13513564,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,14/Dec/22 09:49,16/Jan/23 22:25,04/Jun/24 20:41,13/Jan/23 03:09,1.15.3,1.16.0,1.17.0,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"A lot of Pulsar connector test unstable issues are related to {{Shared}} and {{Key_Shared}} subscription. Because this two subscription is designed to consume the records in an unordered way. And we can support multiple consumers in same topic partition. But this feature lead to some drawbacks in connector.

1. Performance

Flink is a true stream processor with high correctness support. But support multiple consumer will require higher correctness which depends on Pulsar transaction. But the internal implementation of Pulsar transaction on source is record the message one by one and stores all the pending ack status in client side. Which is slow and memory inefficient.

This means that we can only use {{Shared}} and {{Key_Shared}} on Flink with low throughput. This against our intention to support these two subscription. Because adding multiple consumer to same partition can increase the consuming speed.

2. Unstable

Pulsar transaction acknowledge the messages one by one in an internal Pulsar's topic. But it's not stable enough to get it works. A lot of pending issues in Flink JIRA are related to Pulsar transaction and we don't have any workaround.

3. Complex

Support {{Shared}} and {{Key_Shared}} subscription make the connector's code more complex than we expect. We have to make every part of code into ordered and unordered way. Which is hard to understand for the maintainer.

4. Necessary

The current implementation on {{Shared}} and {{Key_Shared}} is completely unusable to use in Production environment. For the user, this function is not necessary. Because there is no bottleneck in consuming data from Pulsar, the bottleneck is in processing the data, which we can achieve by increasing the parallelism of the processing operator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30657,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 22:25:49 UTC 2023,,,,,,,,,,"0|z1e1ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 10:20;syhily;FYI, the delay message delivery is depend on the {{Shared}} subscription.;;;","03/Jan/23 12:11;syhily;[~Tison] Can you close this issue?;;;","03/Jan/23 12:47;tison;master via https://github.com/apache/flink-connector-pulsar/pull/10;;;","11/Jan/23 09:24;mapohl;Can you please provide backports for 1.16 and 1.15? We've seen test instabilities (the once collected under FLINK-28352) in older versions as well. Those are not fixed with your PR.

Example: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44691&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27466 which is caused by FLINK-27917;;;","13/Jan/23 00:33;syhily;[~mapohl] Can we just drop these tests instead? This PR introduces a huge break change for these old release.;;;","13/Jan/23 03:09;tison;I second [~syhily]'s comment that it can be a break change and we'd either fix the case or directly remove the tests. Since we don't maintain those functions in 1.15 or 1.16, I'll close this issue in favor of FLINK-30657.;;;","13/Jan/23 08:50;mapohl;I agree since we, anyway, already disabled tests (FLINK-30351). Noticing that, I realize that this very test that popped up as documented in my previous comment, should actually be disabled already. That's something we might have to look into as part of FLINK-30657. Thanks for taking care of it.;;;","16/Jan/23 13:49;mapohl;{quote}Can you please provide backports for 1.16 and 1.15? We've seen test instabilities (the once collected under FLINK-28352) in older versions as well.

Those are not fixed with your PR. Example: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44691&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27466] which is caused by FLINK-27917
{quote}
I just found out why this test failure popped up: {{PulsarOrderedPartitionSplitReaderTest}} failed but I mixed it up with {{PulsarUnorderedPartitionSplitReaderTest}}. I created FLINK-30703 as a follow-up.;;;","16/Jan/23 22:25;syhily;[~mapohl] Thanks, this really helps. And the {{PulsarOrderedPartitionSplitReaderTest}} failure first occurs in Sep. 12 2022 which is the same as your test logs. I have started a internal discussion with some of the core Pulsar developers to see if we could find the root cause. And thank you for reporting this!;;;",,,,,,,,,,,,,,,,,,,,,,
create many checkpoint empty dir when job not enable checkpoint,FLINK-30412,13513558,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiaodao,xiaodao,14/Dec/22 09:27,29/Dec/22 08:30,04/Jun/24 20:41,,1.12.7,1.13.6,1.15.2,,,,,,,,API / DataStream,,,,0,,,,"when we submit job to flink session cluster , after a long time, we find it create too much

empty checkpoint dir,and it over hdfs max node limit ;

i found StreamingJobGraphGenerator set snapshot whennever the job is open checkpoint;

jobGraph.setSnapshotSettings(settings) 
{code:java}
private void configureCheckpointing() 
CheckpointConfig cfg = streamGraph.getCheckpointConfig(); long interval = cfg.getCheckpointInterval(); if (interval < MINIMAL_CHECKPOINT_TIME) { // interval of max value means disable periodic checkpoint interval = Long.MAX_VALUE; }
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/22 01:25;xiaodao;image-2022-12-23-09-24-58-584.png;https://issues.apache.org/jira/secure/attachment/13054075/image-2022-12-23-09-24-58-584.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 08:30:15 UTC 2022,,,,,,,,,,"0|z1e19c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 08:28;xtsong;[~xiaodao],

I don't see anything unexpected in the code snippet you provided.

Regarding the cleanup of the checkpoint directories, you may take a look at this [blogpost|https://flink.apache.org/2022/05/06/restore-modes.html].;;;","16/Dec/22 03:39;xiaodao;[~xtsong] thank you for your reply。
Supplementary Note:
when we build jobGraph in StreamingJobGraphGenerator#createJobGraph, it call configureCheckpointing();

in this function it will set

jobGraph.setSnapshotSettings(settings);

and the field snapshotSettings in jobGraph never be null;
after submit to jobmanager. it build executionGraph. the code is
org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder#buildGraph
in this function it create chekcpoint when the jobGraph field snapshotSettings is not null;
// configure the state checkpointing
        JobCheckpointingSettings snapshotSettings = jobGraph.getCheckpointingSettings();
        if (snapshotSettings != null) \{...}
and it build CheckpointCoordinator then create empty checkpoint dir in this code;
'''
try {
            this.checkpointStorage = checkpointStateBackend.createCheckpointStorage(job);
            checkpointStorage.initializeBaseLocations();
        } catch (IOException e) {
            throw new FlinkRuntimeException(
                    ""Failed to create checkpoint storage at checkpoint coordinator side."", e);
        }
'''
my doubt is that why we need to open checkpoint when the checkpointInterval is not set(close);                

 ;;;","16/Dec/22 04:15;xtsong;[~xiaodao],
Thanks for the explanation. Now I get your point.

I think the problem is that, in {{StreamingJobGraphGenerator}} we use `interval == Long.MAX_VALUE` to indicate disabling checkpointing, while in {{ExecutionGraphBuilder}} we use `snapshotSettings == null`. This inconsistency will indeed result in creating of empty directories when checkpointing is disabled.

[~gaoyunhaii], could you help double-check on this?;;;","22/Dec/22 08:51;gaoyunhaii;Hi [~xtsong] [~xiaodao]  I have a double check on the issue, the logic seems to be:
 # The `ExecutionGraphBuilder` seems to judge if it will create CheckpointCoordinator based on `snapshotSettings == null`. There is one point that seems a bit unexpected is that it seems in DataStream snapshotSettings is always set, which means we will also create the CheckpointCoordinator in batch mode. This might need some double checks.
 # For streaming mode, it should be reasonable since even if user disable the periodic checkpoints, they may still take savepoints, thus we still need to create the CheckpointCoordinator.

But I'm still not get why the directory is leaked, since it seems after fixed in https://issues.apache.org/jira/browse/FLINK-23180, the directory should not be created if the checkpoint interval == Long.MAX_VALUE : [https://github.com/apache/flink/blob/4ea67f63eb1c43d7adf07c37946b20b525fb015d/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L335.]   [~xiaodao] could you also attach the paths of the leaked directories ? ;;;","23/Dec/22 01:25;xiaodao;hi [~gaoyunhaii] , the dir is deleted, but i have the stack when i happened.

!image-2022-12-23-09-24-58-584.png|width=571,height=294!;;;","29/Dec/22 08:30;gaoyunhaii;Hi [~xiaodao] sorry for the late reply, may I have a double check on the version you are using? Since From the stack, it seems it is still trying to create a JobManagerRunner object, but JobManagerRunner has been changed to be an interface since 1.10: https://issues.apache.org/jira/browse/FLINK-14259 Since https://issues.apache.org/jira/browse/FLINK-23180 is merged in 1.15, if you are using a version earlier than 1.15.0 I reckon it might be fixed by upgrading. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Flink deployment stuck in UPGRADING state when deploy flinkdeployment without resource,FLINK-30411,13513557,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nowke,tanjialiang,tanjialiang,14/Dec/22 09:26,04/Jan/23 13:07,04/Jun/24 20:41,04/Jan/23 13:07,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"In flink kubernetes operator 1.2.0. When i deploy a flinkdeployments without resource, the flink deployment stuck in UPGRADING state.
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
name: socket-window-word-count
spec:
image: flink:1.16.0-scala_2.12-java8
flinkVersion: v1_16
flinkConfiguration:
taskmanager.numberOfTaskSlots: ""1""
serviceAccount: flink
job:
jarURI: local:///opt/flink/examples/streaming/WordCount.jar
parallelism: 2
upgradeMode: stateless{code}
 

when i kubectl describe flinkdeployments, i found this error message

!image-2022-12-14-17-22-12-656.png!

 

maybe we can validate it when apply flinkdeployment?  When it is invalid, throw an error rather than apply flinkdeployment succeed.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/22 09:22;tanjialiang;image-2022-12-14-17-22-12-656.png;https://issues.apache.org/jira/secure/attachment/13053866/image-2022-12-14-17-22-12-656.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 13:07:59 UTC 2023,,,,,,,,,,"0|z1e194:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 09:30;gyfora;Could you please work on adding the required extra validation?;;;","18/Dec/22 07:02;nowke;[~gyfora] Could you assign this issue to me? I can send a PR right away and backport to {{release-1.2}} and {{release-1.3}} as well.;;;","18/Dec/22 08:45;gyfora;Thank you[~nowke] , we can backport this to 1.3 after it’s merged to main , I agree.

 

 ;;;","04/Jan/23 13:07;gyfora;merged to main 4feee47f0175d12ad192fbfc89d483793dcae5b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename 'full' to 'latest-full' and 'compacted' to 'compacted-full' for scan.mode in Table Store,FLINK-30410,13513547,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Dec/22 08:25,19/Dec/22 02:39,04/Jun/24 20:41,19/Dec/22 02:39,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"As discussed in the [dev mailing list|https://lists.apache.org/thread/t76f0ofl6k7mlvqotp57ot10x8o1x90p], we're going to rename some values for {{scan.mode}} in Table Store.

Specifically, {{full}} will be renamed to {{latest-full}} and {{compacted}} will be renamed to {{compacted-full}}, so user can understand that a full snapshot will always be produced, no matter for a batch job or a streaming job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 02:39:40 UTC 2022,,,,,,,,,,"0|z1e16w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 02:39;TsReaper;master: e6156e2f2998a5c7227f0a40512f03312f041e0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support reopening closed metric groups,FLINK-30409,13513507,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,mason6345,mason6345,mason6345,14/Dec/22 03:42,21/Dec/22 07:06,04/Jun/24 20:41,21/Dec/22 07:06,1.17.0,,,,,1.17.0,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"Currently, metricGroup.close() will unregister metrics and the underlying metric groups. If the metricGroup is created again via addGroup(), it will silently fail to create metrics since the metric group is in a closed state.

We need to close metric groups and reopen them because some of the metrics may reference old objects that are no longer relevant/stale and we need to re-create the metric/metric group to point to the new references. For example, we may close `KafkaSourceReader` to remove a topic partition from assignment and then recreate `KafkaSourceReader` with a different set of topic partitions. The metrics should also reflect that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 07:06:42 UTC 2022,,,,,,,,,,"0|z1e0y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 03:43;mason6345;cc: [~chesnay] what do you think about this? ;;;","14/Dec/22 03:45;mason6345;Another way to think about this is to ""clear"" underlying metric groups and metrics without ""closing"" the parent group so that metrics and underlying metric groups could be recreated.;;;","15/Dec/22 08:38;xtsong;Thanks for reporting this, [~mason6345].

Currently, when you call {{addGroup()}} with a name that already exist, it gives you the previously created group, regardless the group is closed or not. (See AbstractMetricGroup#addGroup). I think we can check whether the previous group is closed and if yes return a new group instead here. WDYT?;;;","15/Dec/22 09:01;mason6345;I think that is a good proposal–we can keep the semantic that a group instance will always stay closed. We should then also discard the old metric group from AbstractMetricGroup state and we can do this lazily (if and when a group with same name is requested). Mind if I take a stab at the implementation and testing as I have been thinking about this problem a lot?;;;","15/Dec/22 09:17;xtsong;Of course. Thanks for volunteering. I've assigned you to the ticket.;;;","20/Dec/22 22:03;mason6345;[~xtsong] Thanks for assigning me the ticket. The PR is up with CI passing. I hope you see this message since I don't have permission to select reviewers :) ;;;","21/Dec/22 07:06;xtsong;master (1.17): 535f02c72f3dfdedd4f40a30cc27364a5f5c2cdf;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add unit test for HA metadata check logic,FLINK-30408,13513481,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,13/Dec/22 20:49,01/Jan/23 17:32,04/Jun/24 20:41,19/Dec/22 21:17,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,,,,"The current mechanism to check for the existence of HA metadata in the operator is not guarded by any unit tests which makes in more susceptible to accidental regressions.

We should add at least a few simple test cases to cover the expected behaviour",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 16:46:07 UTC 2022,,,,,,,,,,"0|z1e0sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 21:17;gyfora;merged to main c4e76402f02f05932c6446d97bdc3d60861b9b27;;;","20/Dec/22 16:46;gyfora;merged to release-1.3 4e583a0faa0991c61b37ecd9b937fa3e11c6493a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better encapsulate error handling logic in controllers,FLINK-30407,13513480,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,spencerx,gyfora,gyfora,13/Dec/22 20:47,24/Aug/23 16:43,04/Jun/24 20:41,24/Aug/23 16:43,,,,,,,,,,,Kubernetes Operator,,,,0,stale-assigned,starter,,"The error handling in the FlinkDeployment and SessionJobControllers are a bit adhoc and mostly consist of a series of try catch blocks.

We should introduce a set of error handlers that we can encapsulate nicely and share between the controllers to reduce code duplication.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 16:37:04 UTC 2023,,,,,,,,,,"0|z1e0s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 20:02;spencerx;Hi [~gyfora] can I take this ticket if no one else is working on this?;;;","15/Feb/23 20:11;gyfora;Sure [~spencerx] please go ahead!;;;","21/Feb/23 02:34;spencerx;Hi [~gyfora] , could you tell me more about the context of this ticket?
Currently, DeploymentFailedException, MissingJobManagerException and 
UnknownJobException apply to FlinkDeployment only. MissingSessionJobException applies to SessionJob only. Most of common exception handler code in controllers is in ReconcilationUtils and EventRecorder. 
Do you suggest any specific part of the duplicated code that we can reduce? ;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","24/Aug/23 07:45;gyfora;[~spencerx] are you working on this? Otherwise we can simply close this ticket.;;;","24/Aug/23 16:37;spencerx;Hi [~gyfora], no I am not working on this. We can close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Jobmanager Deployment error without HA metadata should not lead to unrecoverable error,FLINK-30406,13513479,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,13/Dec/22 20:45,01/Jan/23 17:32,04/Jun/24 20:41,28/Dec/22 16:20,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Currently we don't have a good way of asserting that the job never started after savepoint upgrade when the JM deployment fails (such as on an incorrect image).

This easily leads to scenarios which require manual recovery from the user.

We should try to avoid this with some mechanism to greately improve the robustness of savepoint ugrades.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 28 16:20:38 UTC 2022,,,,,,,,,,"0|z1e0s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 16:20;gyfora;merged to 
main 6ff02908b254c0f8e7328d495870d4bd562d54b1
release-1.3 5ed117cb8e3e2b00f2abeb0f98d7555d78e0afe3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ResourceLifecycleStatus to CommonStatus and printer column ,FLINK-30405,13513478,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zch93,gyfora,gyfora,13/Dec/22 20:43,10/Feb/23 00:11,04/Jun/24 20:41,10/Feb/23 00:11,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,starter,,"The CommonStatus api already contains a getter for the ResourceLifecycleState of a Flink resource.

We should remove the JsonIgnore annotation to expose this in the status. 

We should also expose this as a printer column instread of the reconciliation status that is used currently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 11:15;zch93;image-2023-01-31-12-15-18-295.png;https://issues.apache.org/jira/secure/attachment/13054939/image-2023-01-31-12-15-18-295.png","31/Jan/23 12:26;zch93;image-2023-01-31-13-26-23-428.png;https://issues.apache.org/jira/secure/attachment/13054942/image-2023-01-31-13-26-23-428.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 00:11:07 UTC 2023,,,,,,,,,,"0|z1e0rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 14:00;mbalassi;[~morhidi] do you mind if [~zch93] takes a stab at this (expecting it getting merged this week)?

Additional context on printer columns:
https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#additional-printer-columns

So we want the ResourceLifyCycle to show up for running flink custom resources when running e.g.:

{noformat}
kubectl get flinkdeployments
{noformat}
;;;","30/Jan/23 15:12;mbalassi;As per offline discussion with Matyas reassigning.;;;","31/Jan/23 11:15;zch93;for some reason the `CommonStatus` class does not read by the `CrdReferenceDoclet` class (maybe because that is an abstract class), however the `CommonStatus` class also within the `org.apache.flink.kubernetes.operator.api.status` package, so the `run()` method of the `CrdReferenceDoclet` class has to be read this as well, based on this code snippet:
{code:java}
printStream.println(""## Status"");
var status =
sortedByName(
environment.getIncludedElements().stream()
.filter(e -> e.toString().startsWith(STATUS_PACKAGE_PREFIX))
.collect(Collectors.toSet()));
handleAbstractClass(status, environment.getTypeUtils());
se.show(status);{code}
but the reference.md file does not include the `CommonStatus` class at the end of the build, and neither the `flinkdeployments.flink.apache.org-v1.yml` file and I think this is the reason why the `kubectl get flinkdeployments` command does not return a value for the `RESOURCE LIFECYCLE STATE` column as it can be seen below:

!image-2023-01-31-12-15-18-295.png|width=466,height=48!;;;","31/Jan/23 12:19;mbalassi;[~zch93] you are mixing 2 generator utilities:
1. The CrDReferenceDoclet is used to generate the documentation for the CRD yaml file. This was useful for a previous ticket, but it is not in play here.
2. There is the CRDGenerator that we use from Fabric8, such that it can generate the CRD yaml file based on the Java classes that we define to describe our Flink applications:

{noformat}
2023-01-31 04:04:18,325 i.f.c.g.CRDGenerator           [INFO ] [.] Generating 'flinksessionjobs.flink.apache.org' version 'v1beta1' with org.apache.flink.kubernetes.operator.api.FlinkSessionJob (spec: org.apache.flink.kubernetes.operator.api.spec.FlinkSessionJobSpec / status org.apache.flink.kubernetes.operator.api.status.FlinkSessionJobStatus)...
2023-01-31 04:04:19,402 i.f.c.g.CRDGenerator           [INFO ] [.] Generating 'flinkdeployments.flink.apache.org' version 'v1beta1' with org.apache.flink.kubernetes.operator.api.FlinkDeployment (spec: org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec / status org.apache.flink.kubernetes.operator.api.status.FlinkDeploymentStatus)...
{noformat}

This utilitiy comes from:

{noformat}
        <dependency>
            <groupId>io.fabric8</groupId>
            <artifactId>kubernetes-client</artifactId>
            <version>${fabric8.version}</version>
        </dependency>
{noformat}

Ideally you only need to replace this line with a printer column annotation:
https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator-api/src/main/java/org/apache/flink/kubernetes/operator/api/status/CommonStatus.java#L53

And rerun the maven build to trigger the generator.;;;","31/Jan/23 12:34;zch93;Yeah, you are right maybe I mixed up the 2 things, that is my fault and I highlighted the less important thing. But that is still true, if I have a private Resource Lifecycle State field within the CommonStatus class then, that appears with a different structure within the `flinkdeployments.flink.apache.org-v1.yml` (and also in the flinksessionjobs) compared with for example the reconciliation state. My expectation is to have a commonState property under the 'states' part and within that commonState, there should be the resourceLifecycleState as a part of the commonState (like in case of the reconciliationStatus -> reconciliationState). My feeling is that it has an effect of the final result.

And thank you for the advise, I've already tried that what you mentioned, but unfortunately '@PrinterColumn' not applicable to methods.

!image-2023-01-31-13-26-23-428.png|width=553,height=79!

I will continue the investigation.;;;","10/Feb/23 00:11;mbalassi;a9f4045 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Do not redeploy taskmanagers on standalone application scaling ,FLINK-30404,13513456,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gyfora,gyfora,13/Dec/22 16:59,29/Dec/22 15:43,04/Jun/24 20:41,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"The standalone deployment mode allows in-place scale ups when reactive mode is configured but in other cases it deletes and redeploys both task and jobmanagers.

Without reactive mode, we could improve the current behaviour by not redeploying taskmanagers during scale changes. In theory only the jobmanager needs to be redeployed in these cases (as it cannot run multiple jobs in application mode).

This improvement would greately improve the startup times in busy clusters and make scaling more robust",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-13 16:59:43.0,,,,,,,,,,"0|z1e0mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The reported latest completed checkpoint is discarded,FLINK-30403,13513387,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,tisonet,tisonet,13/Dec/22 13:52,23/Feb/23 08:39,04/Jun/24 20:41,23/Feb/23 08:39,1.16.0,,,,,,,,,,Runtime / Checkpointing,,,,0,,,,"There is a small window where the reported latest completed checkpoint can be marked as discarded while the new checkpoint wasn't reported yet. 

The reason is that the function _addCompletedCheckpointToStoreAndSubsumeOldest_  is called before _reportCompletedCheckpoint_ in _CheckpointCoordinator._

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 08:31:31 UTC 2023,,,,,,,,,,"0|z1e07k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 08:54;gaoyunhaii;Hi [~tisonet] , it looks to me we have to keep the current order since _addCompletedCheckpointToStoreAndSubsumeOldest_ still be __ possible get failed, in this case the method will throw exception and _reportCompletedCheckpoint_ will be skipped. 

Also it looks to me there seems no explicit issues caused by this window? ;;;","23/Feb/23 08:19;Weijie Guo;Hi [~tisonet], do you have future concern?;;;","23/Feb/23 08:31;tisonet;Hi, thanks for asking. No, let's close it. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Separate token framework generic and hadoop specific parts,FLINK-30402,13513385,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,13/Dec/22 13:39,14/Dec/22 15:30,04/Jun/24 20:41,14/Dec/22 15:30,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 15:30:07 UTC 2022,,,,,,,,,,"0|z1e074:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 15:30;mbalassi;4ba3366 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for MinHashLSH,FLINK-30401,13513384,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,13/Dec/22 13:36,11/Jan/23 03:25,04/Jun/24 20:41,11/Jan/23 03:25,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Add Estimator and Transformer for MinHashLSH.

Its function would be at least equivalent to Spark's org.apache.spark.ml.feature.MinHashLSH. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test (Optional)
 * Markdown document (Optional)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-13 13:36:15.0,,,,,,,,,,"0|z1e06w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop bundling connector-base in externalized connectors,FLINK-30400,13513368,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,chesnay,chesnay,13/Dec/22 11:58,26/Jan/24 12:56,04/Jun/24 20:41,12/Jan/24 08:40,,,,,,aws-connector-4.2.0,elasticsearch-3.1.0,kafka-3.0.2,kafka-3.1.0,rabbitmq-3.1.0,Connectors / Common,,,,0,pull-request-available,,,"Check that none of the externalized connectors bundle connector-base; if so remove the bundling and schedule a new minor release.

Bundling this module is highly problematic w.r.t. binary compatibility, since bundled classes may rely on internal APIs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 16:25:54 UTC 2023,,,,,,,,,,"0|z1e03c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 06:44;ruanhang1993;Hi, all. I would like to help to check. Thanks.;;;","10/Oct/23 14:18;martijnvisser;[~chesnay] How would this work for SQL FAT jars? IIUC they actually need connector-base;;;","16/Oct/23 09:10;dannycranmer;AWS: [01f112bd5a69f95cd5d2a4bc7e08d1ba9a81d56a|https://github.com/apache/flink-connector-aws/commit/01f112bd5a69f95cd5d2a4bc7e08d1ba9a81d56a];;;","16/Oct/23 09:11;dannycranmer;> Chesnay Schepler How would this work for SQL FAT jars? IIUC they actually need connector-base

[~martijnvisser] flink-connector-base is [included in Flink dist|https://github.com/apache/flink/blob/master/flink-dist/pom.xml#L156], so SQL jars are fine; it will already be on the classpath. ;;;","16/Oct/23 09:35;martijnvisser;[~dannycranmer] Thanks :);;;","16/Oct/23 15:58;martijnvisser;Fixed in apache/flink-connector-kafka:main 37cbb83f55e9d6f0b8dc35bb8da867086dfa4d9e;;;","27/Oct/23 07:56;martijnvisser;Fixed in apache/flink-connector-rabbitmq:

main: fb0566abb45bf545668c688311253056eb7b4192
v3.0: bb9936513803c895b3e2ac7cb9af525bbe685f4c;;;","24/Nov/23 16:16;leonard;Hey [~ruanhang1993], recently we meet some issue when release connector like kafka[1] ,we need pick batch from connector main branch to dev branch, would you like to push things forward ?
[1] https://lists.apache.org/thread/qfb383vl321gcyv10fw6wb6jp9xjzrx4;;;","24/Nov/23 16:25;tzulitai;fixed for apache/flink-connector-kafka:v3.0 as well:
44e49e777f78bd3d13ffff843f0711f617d373e4;;;",,,,,,,,,,,,,,,,,,,,,,
Enable connectors to use config docs generator,FLINK-30399,13513366,13483838,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,13/Dec/22 11:56,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Connectors / Common,Documentation,,,1,,,,publish flink-docs and refactor the configuration as necessary to be usable from externalized connector repos.,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30486,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-13 11:56:06.0,,,,,,,,,,"0|z1e02w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce S3 support for table store,FLINK-30398,13513349,13513317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Dec/22 10:15,14/Dec/22 07:43,04/Jun/24 20:41,14/Dec/22 07:43,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"S3 contains a large number of dependencies, which can easily lead to class conflicts. We need a plugin mechanism to load the corresponding jars through the classloader.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 07:43:58 UTC 2022,,,,,,,,,,"0|z1dzz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 07:43;TsReaper;master: 54fd3ab2d16999d628faf4ff62c701b12ca20725;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Pulsar connector from master branch,FLINK-30397,13513340,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Dec/22 09:47,03/Jan/23 10:17,04/Jun/24 20:41,23/Dec/22 12:11,,,,,,1.17.0,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,FLINK-29755,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 23 12:11:04 UTC 2022,,,,,,,,,,"0|z1dzx4:",9223372036854775807,The Pulsar connector has been externalized and is no longer bundled and released as part of the main Flink release. Downloads can be found at https://flink.apache.org/downloads.html and the source code at https://github.com/apache/flink-connector-pulsar,,,,,,,,,,,,,,,,,,,"23/Dec/22 12:11;martijnvisser;Fixed in master:

[FLINK-30397][Connector/Pulsar] Remove Pulsar connector: 64305b3c2b047d6231db5e23dec26c0983c10a11

[FLINK-30397][Python][Connector/Pulsar] Make PyFlink depend on the latest Pulsar snapshot version: f54ecb75122e73021bd137f844743704acc06949

[FLINK-30397][Docs] Include layout folder for external documentation: 4d64dc1dc2da4029bf91e23e17d2b66eb598cb4b

[FLINK-30397][Docs] Connector documentation integration function now uses parameters in case there are additional folders that need to be integrated, like the Pulsar connector: 10fc197666e9442be9697b78b9e0467a9288b06a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql hint 'LOOKUP' which is defined in outer query block may take effect in inner query block,FLINK-30396,13513327,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lam167,lam167,lam167,13/Dec/22 08:40,27/Feb/23 06:48,04/Jun/24 20:41,27/Feb/23 06:47,1.16.0,,,,,1.17.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"As [flink doc|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/hints/#query-hints] said:

> {{Query hints}} can be used to suggest the optimizer to affect query execution plan within a specified query scope. Their effective scope is current {{{}Query block{}}}([What are query blocks ?|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/hints/#what-are-query-blocks-]) which {{Query Hints}} are specified.

But the sql hint 'LOOKUP' behaves differently like the demo following:
{code:java}
-- DDL
CREATE TABLE left_table (
    lid INTEGER,
    lname VARCHAR,
    pts AS PROCTIME()
) WITH (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path'='xxx'
) 

CREATE TABLE dim_table (
    id INTEGER,
    name VARCHAR,
    mentor VARCHAR,
    gender VARCHAR
) WITH (
    'connector' = 'jdbc',
    'url' = 'xxx',
    'table-name' = 'dim1',
    'username' = 'xxx',
    'password' = 'xxx',
    'driver'= 'com.mysql.cj.jdbc.Driver' 
)

-- DML
SELECT /*+ LOOKUP('table'='outer') */
    ll.id AS lid,
    ll.name,
    r.mentor,
    r.gender
FROM (
    SELECT /*+ LOOKUP('table'='inner') */
    l.lid AS id,
    l.lname AS name,
    r.mentor,
    r.gender,
    l.pts
    FROM left_table AS l
JOIN dim_table FOR SYSTEM_TIME AS OF l.pts AS r
ON l.lname = r.name
) ll JOIN dim_table FOR SYSTEM_TIME AS OF ll.pts AS r ON ll.name=r.name{code}
The inner correlate will have two hints:
{noformat}
{     
    [LOOKUP inheritPath:[0] options:{table=inner}],
    [LOOKUP inheritPath:[0, 0, 0] options:{table=outer}]
}{noformat}
and IMO which maybe is a bug.

The first hint comes from the inner query block and the second hint comes from the outer block, and ClearJoinHintWithInvalidPropagationShuttle will not clear the second hint cause the correlate has no 'ALIAS' hint.

The reason for the above case is that the hint 'ALIAS' now only works for join rel nodes and 'LOOKUP' works for correlate and join rel nodes.

I think maybe the better way would be to make 'ALIAS' support both correlate and join rel nodes like 'LOOKUP'.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 26 06:04:11 UTC 2023,,,,,,,,,,"0|z1dzu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/23 06:04;lincoln.86xy;fixed in master: a72f88f36505e1ce45f2280584a11385c2c2bc14

fixed in release-1.17: 5ee98b544d936e8cde2d4429425da46185031bb3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor module name and documentation for filesystems,FLINK-30395,13513318,13513317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Dec/22 07:54,13/Dec/22 09:16,04/Jun/24 20:41,13/Dec/22 09:16,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"* flink-table-store-filesystem => flink-table-store-filesystems
* flink-table-store-fs-oss-hadoop => flink-table-store-oss
* introduce a new page for oss only",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 09:16:31 UTC 2022,,,,,,,,,,"0|z1dzs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 09:16;TsReaper;master: 0620350d292600b11ba97779bfdb8869b87597db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[umbrella] Refactor filesystem support in table store,FLINK-30394,13513317,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Dec/22 07:53,29/Mar/23 01:44,04/Jun/24 20:41,29/Mar/23 01:44,,,,,,table-store-0.4.0,,,,,Table Store,,,,0,,,,"- Let other computing engines, such as hive, spark, trino, support object storage file systems, such as OSS and s3.
- Let table store access different file systems from Flink cluster according to configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-13 07:53:31.0,,,,,,,,,,"0|z1dzs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSourceUnorderedE2ECase.testScaleDown failed with TimeoutException,FLINK-30393,13513310,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,renqs,renqs,13/Dec/22 07:20,04/Jan/23 02:52,04/Jun/24 20:41,04/Jan/23 02:52,1.17.0,,,,,,,,,,Connectors / Pulsar,,,,0,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43904&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=16386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 02:52:40 UTC 2023,,,,,,,,,,"0|z1dzqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 02:52;tison;After FLINK-30413 dropped the related support, this test was dropped also. Invalid now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase the default value of cluster.thread-dump.stacktrace-max-depth,FLINK-30392,13513294,13501095,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,yunta,yunta,13/Dec/22 05:15,03/Jan/23 03:07,04/Jun/24 20:41,03/Jan/23 03:07,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"Currently, the thread dump function still have the default stack-trace depth as 8, which is the same as before. However, the default value is really too small for developers to know the actual thread info.
From our experiences, we can set this value as 24.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 03:07:56 UTC 2023,,,,,,,,,,"0|z1dzmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 03:07;yunta;merged in master: 187e572768932b121b5f5e785727f0c48ca98aff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show jars command doesn't list client wrapper classloader loaded jars ,FLINK-30391,13513289,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,13/Dec/22 04:00,01/Mar/23 08:29,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Client,,,,0,,,,"passing custom jars for sql-client using -j and -l doesn't gets listed in `show jars` command. 

It seems that jars and URI contained in `ClientWrapperClassloader` (introduced from 1.16.0) doesn't get traversed while show jar 

 
{code:java}
 /usr/lib/flink/bin/sql-client.sh embedded -j hudi-flink-bundle_2.12-0.10.1.jar
Setting HBASE_CONF_DIR=/etc/hbase/conf because no HBASE_CONF_DIR was set.
Using the result of 'hadoop classpath' to augment the Hadoop classpath: /etc/hadoop/conf:/usr/lib/hadoop/lib/:/usr/lib/hadoop/.//:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/:/usr/lib/hadoop-hdfs/.//:/usr/lib/hadoop-mapreduce/.//:/usr/lib/hadoop-yarn/lib/:/usr/lib/hadoop-yarn/.//:/etc/tez/conf:/usr/lib/tez/hadoop-shim-0.10.2-amzn-0.jar:/usr/lib/tez/hadoop-shim-2.8-0.10.2-amzn-0.jar:/usr/lib/tez/lib:/usr/lib/tez/LICENSE:/usr/lib/tez/LICENSE-BSD-3clause:/usr/lib/tez/LICENSE-CDDLv1.1-GPLv2_withCPE:/usr/lib/tez/LICENSE-MIT:/usr/lib/tez/LICENSE-SIL_OpenFontLicense-v1.1:/usr/lib/tez/NOTICE:/usr/lib/tez/tez-api-0.10.2-amzn-0.jar:/usr/lib/tez/tez-aux-services-0.10.2-amzn-0.jar:/usr/lib/tez/tez-build-tools-0.10.2-amzn-0.jar:/usr/lib/tez/tez-common-0.10.2-amzn-0.jar:/usr/lib/tez/tez-dag-0.10.2-amzn-0.jar:/usr/lib/tez/tez-examples-0.10.2-amzn-0.jar:/usr/lib/tez/tez-history-parser-0.10.2-amzn-0.jar:/usr/lib/tez/tez-javadoc-tools-0.10.2-amzn-0.jar:/usr/lib/tez/tez-job-analyzer-0.10.2-amzn-0.jar:/usr/lib/tez/tez-mapreduce-0.10.2-amzn-0.jar:/usr/lib/tez/tez-protobuf-history-plugin-0.10.2-amzn-0.jar:/usr/lib/tez/tez-runtime-internals-0.10.2-amzn-0.jar:/usr/lib/tez/tez-runtime-library-0.10.2-amzn-0.jar:/usr/lib/tez/tez-tests-0.10.2-amzn-0.jar:/usr/lib/tez/tez-ui-0.10.2-amzn-0.war:/usr/lib/tez/tez-yarn-timeline-cache-plugin-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-with-acls-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-with-fs-0.10.2-amzn-0.jar:/usr/lib/tez/lib/async-http-client-2.12.3.jar:/usr/lib/tez/lib/commons-cli-1.2.jar:/usr/lib/tez/lib/commons-codec-1.11.jar:/usr/lib/tez/lib/commons-collections4-4.1.jar:/usr/lib/tez/lib/commons-io-2.8.0.jar:/usr/lib/tez/lib/commons-lang-2.6.jar:/usr/lib/tez/lib/guava-31.1-jre.jar:/usr/lib/tez/lib/hadoop-annotations.jar:/usr/lib/tez/lib/hadoop-auth.jar:/usr/lib/tez/lib/hadoop-hdfs-client-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-common-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-common.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-core-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-yarn-server-web-proxy.jar:/usr/lib/tez/lib/hhadoop-mapreduce-client-core.jar:/usr/lib/tez/lib/javax.servlet-api-3.1.0.jar:/usr/lib/tez/lib/jersey-client-1.19.jar:/usr/lib/tez/lib/jersey-json-1.19.jar:/usr/lib/tez/lib/jettison-1.3.4.jar:/usr/lib/tez/lib/jsr305-3.0.0.jar:/usr/lib/tez/lib/metrics-core-3.1.0.jar:/usr/lib/tez/lib/netty-all-4.1.72.Final.jar:/usr/lib/tez/lib/protobuf-java-2.5.0.jar:/usr/lib/tez/lib/RoaringBitmap-0.7.45.jar:/usr/lib/tez/lib/slf4j-api-1.7.36.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.331.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/animal-sniffer-annotations-1.14.jar:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/checker-qual-2.5.2.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.54.0.jar:/usr/share/aws/emr/emrfs/lib/error_prone_annotations-2.1.3.jar:/usr/share/aws/emr/emrfs/lib/findbugs-annotations-3.0.1.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/j2objc-annotations-1.1.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.12.331.jar:/usr/share/aws/emr/emrfs/lib/jsr305-3.0.2.jar:/usr/share/aws/emr/emrfs/auxlib/:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.3.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jarSLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/flink/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2022-12-07 03:17:05,554 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /var/lib/flink/yarn/.yarn-properties-hadoop.
2022-12-07 03:17:05,554 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /var/lib/flink/yarn/.yarn-properties-hadoop.
                                   ????????
                               ????????????????
                            ???????        ???????  ?
                          ????   ?????????      ?????
                          ???         ???????    ?????
                            ???            ???   ?????
                              ??       ???????????????
                            ?? ?   ???       ?????? ?????
                            ?????   ????      ????? ?????
                         ???????       ???    ??????? ???
                   ????????? ??         ??    ??????????
                  ????????  ??           ?   ?? ???????
                ????  ???            ?  ?? ???????? ?????
               ???? ? ??          ? ?? ????????    ????  ??
              ???? ????          ??????????       ??? ?? ????
           ???? ?? ???       ???????????         ????  ? ?  ???
           ???  ?? ??? ?????????              ????           ???
           ??    ? ???????              ????????          ??? ??
           ???    ???    ????????????????????            ????  ?
          ????? ???   ??????   ????????                  ????  ??
          ????????  ???????????????                            ??
          ?? ????   ???????  ???       ??????    ??          ???
          ??? ???  ???  ???????            ????   ?????????????
           ??? ?????  ????  ??                ??      ????   ???
           ??   ???   ?     ??                ??              ??
            ??   ??         ??                 ??        ????????
             ?? ?????       ??                  ???????????    ??
              ??   ????      ?                    ???????      ??
               ???   ?????                         ?? ???????????
                ????    ????                     ??????? ????????
                  ?????                          ??  ????  ?????
                      ?????????????????????????????????  ?????
    ______ _ _       _       _____  ____  _         _____ _ _            _  BETA
   |  ___| ()     | |     / ___|/ __ | |       / ____| ()          | |
   | |__  | |_ _ __ | | __ | (___ | |  | | |      | |    | |_  ___ _ __ | |_
   |  | | | | ' | |/ /  __ | |  | | |      | |    | | |/ _ \ '_ | __|
   | |    | | | | | |   <   __) | || | |__  | |___| | |  / | | | |
   ||    |||| |||\ |__/ ______|  _||||| ||_|
        Welcome! Enter 'HELP;' to list all available commands. 'QUIT;' to exit.
Command history file path: /home/hadoop/.flink-sql-history
Flink SQL> show jars;
Empty set{code}

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30318,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 07:57:06 UTC 2023,,,,,,,,,,"0|z1dzls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 07:57;samrat007;i would like to work on this issue ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that no compaction is in progress before closing the writer,FLINK-30390,13513282,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,13/Dec/22 02:21,29/Mar/23 01:44,04/Jun/24 20:41,29/Mar/23 01:44,,,,,,table-store-0.4.0,,,,,Table Store,,,,0,,,,"When the writer does not generate a new submission file, it will be closed. (In AbstractFileStoreWrite) However, at this time, there may be asynchronous interactions that have not been completed and are forced to close, which will cause some strange exceptions to be printed in the log.

We can avoid this situation, ensure that no compaction is in progress before closing the writer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-13 02:21:52.0,,,,,,,,,,"0|z1dzk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add retry to read hints,FLINK-30389,13513277,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,lzljs3620320,lzljs3620320,13/Dec/22 01:59,04/Jan/23 08:33,04/Jun/24 20:41,04/Jan/23 08:33,,,,,,table-store-0.3.0,,,,,Table Store,,,,1,pull-request-available,,,"For the oss (object store) filesystem. When writing hint file, delete it first and then add it. Reading hint file may fail frequently. We don't need to return directly in case of failure. We can add a retry.

{code:java}
Failed to read hint file LATEST. Falling back to listing files.
java.io.FileNotFoundException: oss://lake_v4/snapshot/LATEST: No such file or directory!
  at 
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 08:33:29 UTC 2023,,,,,,,,,,"0|z1dzj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 09:02;Wencong Liu;Hello [~lzljs3620320] , I'm quite interested in this issue. Can you provide the exact location where the exception occurs? Thanks.;;;","03/Jan/23 04:16;lzljs3620320;[~Wencong Liu] SnapshotManager.readHints. Feel free to open a PR.;;;","03/Jan/23 06:24;Wencong Liu;Thanks [~lzljs3620320] . A fixed number of retries and detection intervals can be added to readHints. I'd like to take this ticket. Could you please assign to me? ;;;","03/Jan/23 06:35;lzljs3620320;[~Wencong Liu] Assigned;;;","04/Jan/23 08:33;lzljs3620320;master: 2586516e12c9c4141e80741fb604e65c4d9c7445
release-0.3: 9cc9956c5bcdae3980b15b5d12bd4a5e2fcba377;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for ElementConverted open() method for KDS/KDF/DDB,FLINK-30388,13513231,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,dannycranmer,dannycranmer,12/Dec/22 16:33,15/Apr/24 11:56,04/Jun/24 20:41,15/Apr/24 11:56,,,,,,aws-connector-4.3.0,,,,,Connectors / DynamoDB,Connectors / Firehose,Connectors / Kinesis,,0,pull-request-available,,,"FLINK-29938 added support for an {{open()}} method in Async Sink ElementConverter. Once flink-connector-aws upgrades to Flink 1.17 we should implement this method. It was originally implemented [here|https://github.com/apache/flink/pull/21265] but was yanked during the [sync|FLINK-30384]",,,,,,,,,,,,,,,,,,,,,,,,FLINK-35050,,,,,,,,,,,,FLINK-30384,,,FLINK-29938,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 11:56:17 UTC 2024,,,,,,,,,,"0|z1dz8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 10:10;chalixar;[~danny.cranmer] [~liangtl] would be great if you managed to review this tiny [PR|https://github.com/apache/flink-connector-aws/pull/135]
 ;;;","15/Apr/24 11:56;dannycranmer;Merged commit [{{8cafbbc}}|https://github.com/apache/flink-connector-aws/commit/8cafbbced8659c654e8f507979b47566b45ea547] into apache:main 

 

Thanks [~chalixar] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support job singleton within TM,FLINK-30387,13513226,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gzm55,gzm55,12/Dec/22 15:56,09/Feb/23 10:00,04/Jun/24 20:41,,,,,,,,,,,,API / Core,,,,0,,,,"For better managing some shared objects between slots of the same job, such as thread pool, db connections, cache data, etc., we need the ability to create these objects once for each job at the beginning, then to release the bounded resource when the job finished some how with correct order. A simple static filed cannot always archive this when the class is loaded by the TM classloader.

To implement this job level singleton, we can use the flink user classloader. Since each flink job has its own context class loader, we could add some methods to operate the user CL to create a user specify AutoCloseable object only once for each job and close it at last in release hook of the user CL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 10:00:38 UTC 2023,,,,,,,,,,"0|z1dz7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 16:53;chesnay;You can already do this via static fields and the classloader hook. AFAICT there's nothing else needed from the Flink side.;;;","14/Dec/22 07:05;gzm55;The class containing the static fields may loaded by the TM CL, not the user CL, so the static fields may introduce side effects across jobs on some TMs. In addition, the order of release hooks of each job's _static_ field is also not easy to handle. So it is better to improve or provide some APIs, so that we can handle these concerns correctly. ;;;","09/Feb/23 10:00;pltbkd;As you said we can use the flink user classloader, there can be a user defined singleton class in the user jar, which is for sure loaded by the user classloader, and put the shared objects in this class. I suppose that is enough for this purpose if i'm not missing anything.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column constraint lacks primary key not enforced check,FLINK-30386,13513218,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,12/Dec/22 15:12,21/Aug/23 14:50,04/Jun/24 20:41,07/Feb/23 07:16,1.15.2,1.15.3,1.16.0,,,1.17.0,hbase-3.0.0,,,,Table SQL / API,,,,0,pull-request-available,,,"Currently, only table constraint performs the enforced check. Not sure if it is by design or a bug.

The following case can be reproduced on Flink 1.16.0, 1.15.3, and 1.15.2. I think the earlier version might also reveal it.
{code:sql}
Flink SQL> create table T (f0 int not null primary key, f1 string) with ('connector' = 'datagen');
[INFO] Execute statement succeed.

Flink SQL> explain select * from T;
== Abstract Syntax Tree ==
LogicalProject(f0=[$0], f1=[$1])
+- LogicalTableScan(table=[[default_catalog, default_database, T]])

== Optimized Physical Plan ==
TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])

== Optimized Execution Plan ==
TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])


Flink SQL> create table S (f0 int not null, f1 string, primary key(f0)) with ('connector' = 'datagen');
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED  controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode
{code}",,,,,,,,,,,,,,,,,,,,FLINK-19418,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 07:16:04 UTC 2023,,,,,,,,,,"0|z1dz60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 15:35;dwysakowicz;Do you mean that we should support the {{ENFORCED}}  mode? It was a conscious decision to support only {{NOT ENFORCED}} as the data is stored outside of Flink and thus there is no way to ensure the uniqueness of the key.;;;","13/Dec/22 02:16;qingyue;Re [~dwysakowicz] . The `NOT ENFORCED` check only performs on the table constraint but skips the column constraint. As a result, `PRIMARY KEY(a)` will throw an exception, but `a BIGINT NOT NULL PRIMARY KEY` is allowed.

I think we should also throw the exception when encountering `a BIGINT NOT NULL PRIMARY KEY` and inform users to use `a BIGINT NOT NULL PRIMARY KEY NOT ENFORCED` instead.;;;","13/Dec/22 03:22;qingyue;Would you help verify this issue, is it a bug or by design? cc [~lincoln.86xy] and [~fsk119];;;","13/Dec/22 08:12;dwysakowicz;Thanks for the explanation [~qingyue], I think that's fair assessment. ;;;","13/Dec/22 12:50;lincoln.86xy;[~qingyue] this is indeed a bug of current implementation,  `a BIGINT NOT NULL PRIMARY KEY` (without `NOT ENFORCED`) is expected to be unsupported and requires an error here;;;","14/Dec/22 06:14;qingyue;Thanks for the explanation, and let's fix this. Please assign the ticket to me. cc [~lincoln.86xy] ;;;","07/Feb/23 07:16;lincoln.86xy;fixed in master: e69e6514d0b901eb03f1a8bfc499680d076248c9;;;",,,,,,,,,,,,,,,,,,,,,,,,
Remove AWS connectors from master branch,FLINK-30385,13513212,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,12/Dec/22 14:36,06/Jan/23 08:37,04/Jun/24 20:41,06/Jan/23 08:37,,,,,,1.17.0,,,,,Connectors / AWS,Connectors / Firehose,Connectors / Kinesis,,0,,,, KDS/KDF connectors have been externalised and can now be removed from the master branch,,,,,,,,,,,,FLINK-30384,FLINK-30550,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-12 14:36:39.0,,,,,,,,,,"0|z1dz4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync master KDS/KDF connector changes to flink-connector-aws,FLINK-30384,13513211,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,12/Dec/22 14:35,12/Dec/22 20:48,04/Jun/24 20:41,12/Dec/22 20:48,,,,,,aws-connector-4.1.0,,,,,Connectors / AWS,Connectors / Firehose,Connectors / Kinesis,,0,pull-request-available,,,Update flink-connector-aws to take the KDS/KDF/AWS base and docs changes from the Flink codebase.,,,,,,,,,,,,,,FLINK-30385,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30388,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 17:27:31 UTC 2022,,,,,,,,,,"0|z1dz4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 17:27;dannycranmer;Cannot sync some code since we do not support the 1.17 changes yet. Created followup Jira: FLINK-30388;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UseLogicalIdentifier makes datadog consider metric as custom,FLINK-30383,13513209,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Dec/22 14:35,19/Dec/22 11:51,04/Jun/24 20:41,19/Dec/22 11:51,1.16.0,,,,,1.16.1,1.17.0,,,,Runtime / Metrics,,,,0,pull-request-available,,,"Custom metrics in the DD context are meant for metrics not exported by the standard integrations (which includes our reporter), but seemingly enabling the {{useLogicalIdentifier}} flag breaks whatever detection mechanism they have for the reporter.
This results in additional costs to users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 11:51:28 UTC 2022,,,,,,,,,,"0|z1dz40:",9223372036854775807,"The Datadog reporter now adds a ""flink."" prefix to metric identifiers if ""useLogicalIdentifier"" is enabled. This is required for these metrics to be recognized as Flink metrics, not custom ones.",,,,,,,,,,,,,,,,,,,"19/Dec/22 11:51;chesnay;master: 97509a13bf2aee219873c22e47ce67decfb09c5a
1.16: 0293c3b998a75cc6a3ac1c8fa13d6d69b7f0d306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.16 to integrate KDS/KDF docs from flink-connector-aws,FLINK-30382,13513208,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,12/Dec/22 14:30,03/Jan/23 10:45,04/Jun/24 20:41,12/Dec/22 17:59,1.16.0,,,,,1.16.1,,,,,Connectors / AWS,,,,0,pull-request-available,,,KDS/KDF connectors have been externalized to the flink-connector-aws repo. Update the docs to reflect this,,,,,,,,,,,,,,,,FLINK-30550,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-12 14:30:57.0,,,,,,,,,,"0|z1dz3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump minikube kubernetes version for operator,FLINK-30381,13513186,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,12/Dec/22 12:47,13/Dec/22 14:54,04/Jun/24 20:41,13/Dec/22 14:54,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 14:54:19 UTC 2022,,,,,,,,,,"0|z1dyyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 14:54;mbalassi;4ec5220 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Look into enabling parallel compilation on CI,FLINK-30380,13513180,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,12/Dec/22 12:23,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Build System,Build System / CI,,,0,,,,,,,,,,,,,,,,FLINK-30083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-12 12:23:49.0,,,,,,,,,,"0|z1dyxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decouple connector docs integration from connector releases,FLINK-30379,13513179,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,12/Dec/22 12:22,11/Mar/24 12:43,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Connectors / Common,Documentation,,,0,,,,"The Flink docs currently integrate the docs of a specific connector release.
This implies that we need to update this integration for every single connector release, and can't hotfix the connector docs as we do for Flink releases.

We should consider having tag in the connector repos for minor Flink versions (e.g, Flink-1.16) such that Flink doesn't have to select a specific connector version but just uses ""whatever works with 1.16"".

This does imply that we'd change the tag now and then, which isn't perfect but imo worth it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-12 12:22:59.0,,,,,,,,,,"0|z1dyxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rework sql_download_table shortcode,FLINK-30378,13513174,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Dec/22 12:09,19/Jan/23 16:16,04/Jun/24 20:41,19/Jan/23 16:15,,,,,,1.17.0,,,,,Connectors / Common,Documentation,,,0,pull-request-available,,,"The existing sql_[connector_]download_table shortcodes rely on data in the Flink source (data/sql_connectors.yml).

Instead we can move this data to the externalized connectors, and parameterize the data lookup with the connector name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 16:15:36 UTC 2023,,,,,,,,,,"0|z1dyw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 16:15;chesnay;master:
8d5cfe60335f655f7196a0e744d7bbbf8f10ca8a
7f7b030d72f5f824254d83572092a34891ceff8d
aws:
68c3caa3e8df74aa88d878ae7961c5e15ae653cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommonExecSink does not use ClientWrapperClassLoader while extracting Type from KeyedStream,FLINK-30377,13513170,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,prabhujoseph,prabhujoseph,12/Dec/22 11:35,27/Jan/23 14:53,04/Jun/24 20:41,27/Jan/23 14:53,1.16.0,,,,,1.16.1,1.17.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"CommonExecSink does not use ClientWrapperClassLoader while extracting Type from KeyedStream. This will lead to ClassNotFoundException on user classes added through add jar command. This is working fine on Flink 1.15.

 
{code:java}
Caused by: java.lang.ClassNotFoundException: org.apache.hudi.common.model.HoodieRecord
 at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_352]
 at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_352]
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_352]
 at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_352]
 at java.lang.Class.forName0(Native Method) ~[?:1.8.0_352]
 at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_352]
 at org.apache.flink.api.java.typeutils.TypeExtractionUtils.checkAndExtractLambda(TypeExtractionUtils.java:143) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:539) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:415) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:406) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.streaming.api.datastream.KeyedStream.<init>(KeyedStream.java:116) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.streaming.api.datastream.DataStream.keyBy(DataStream.java:300) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.hudi.sink.utils.Pipelines.hoodieStreamWrite(Pipelines.java:339) ~[?:?]
 at org.apache.hudi.table.HoodieTableSink.lambda$getSinkRuntimeProvider$0(HoodieTableSink.java:104) ~[?:?]
 at org.apache.hudi.adapter.DataStreamSinkProviderAdapter.consumeDataStream(DataStreamSinkProviderAdapter.java:35) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.applySinkProvider(CommonExecSink.java:483) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.createSinkTransformation(CommonExecSink.java:203) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:176) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:158) ~[?:?]
 at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85) ~[?:?]
 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84) ~[?:?]
 at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197) ~[?:?]
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1733) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:825) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
 at org.apache.flink.table.client.gateway.local.LocalExecutor.executeModifyOperations(LocalExecutor.java:219) ~[flink-sql-client-1.16.0.jar:1.16.0]

 {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 27 14:52:47 UTC 2023,,,,,,,,,,"0|z1dyvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:52;dannycranmer;Merged commit [{{18a0ea9}}|https://github.com/apache/flink/commit/18a0ea99d6eb48cd4a0963c0e8a42b4d9d4833cf] into apache:master

Merged commit [{{d71cf8e}}|https://github.com/apache/flink/commit/d71cf8ee314d92f3425ae2dc80d0cc83e0d3c44e] into apache:release-1.16 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a new flink bushy join reorder rule which based on greedy algorithm,FLINK-30376,13513169,13444558,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,12/Dec/22 11:32,18/Jan/23 07:25,04/Jun/24 20:41,17/Jan/23 13:19,1.17.0,,,,,1.17.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,Introducing a new Flink bushy join reorder strategy which based on the greedy algorithm. The old join reorder rule will also be the default join reorder rule and the new bushy join reorder strategy will be optional.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30727,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 13:19:22 UTC 2023,,,,,,,,,,"0|z1dyv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 13:19;godfrey;Fixed in 1.17.0: 14ea1a58e9bf516f0ed0683486252c7e54ea8039;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClient leaks flink-table-planner jar under /tmp,FLINK-30375,13513168,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,samrat007,prabhujoseph,prabhujoseph,12/Dec/22 11:18,11/Jan/23 20:58,04/Jun/24 20:41,11/Jan/23 20:58,1.16.0,,,,,1.17.0,,,,,Table SQL / Client,,,,0,pull-request-available,,,"SqlClient leaks flink-table-planner jar under /tmp

 
{code:java}

[root@ip-172-1-1-3 lib]# ls -lrt /tmp/flink-table-planner_*
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:08 /tmp/flink-table-planner_acada33f-a10b-4a4a-ad16-6bca25a67e10.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:17 /tmp/flink-table-planner_fb2f6e31-48a0-4c1e-ab7b-16129e776125.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:22 /tmp/flink-table-planner_83499393-1621-43de-953c-2000bc6967ce.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:24 /tmp/flink-table-planner_3aa798da-e794-4c6c-ad9f-49b4574da64b.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:36 /tmp/flink-table-planner_ea52d9ea-3148-4fc3-8a58-f83063bb14d5.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:44 /tmp/flink-table-planner_fdc64e21-6bc7-4e4a-b8b2-2a97629d0727.jar
-rw-rw-r-- 1 hadoop hadoop 39137545 Dec 12 11:05 /tmp/flink-table-planner_84c00b13-c6b5-4e8b-80cb-5631a2fa3150.jar
-rw-rw-r-- 1 hadoop hadoop 39137601 Dec 12 11:10 /tmp/flink-table-planner_d00b8a7b-e615-46f7-bd21-b5efa684c184.jar
-rw-rw-r-- 1 hadoop hadoop 39137601 Dec 12 11:11 /tmp/flink-table-planner_a413520d-ce15-41f9-aa5e-05a30f7eaff5.jar {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 20:58:08 UTC 2023,,,,,,,,,,"0|z1dyuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 17:21;samrat007;I will like to work on this issue ;;;","11/Jan/23 09:58;samrat007;Please help review the pr. ;;;","11/Jan/23 20:58;dannycranmer;Merged commit [{{086f13a}}|https://github.com/apache/flink/commit/086f13a9cd625f856309d1d57d80a0b6a66c687b] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultExecutionGraphCacheTest fails with 239 exit code,FLINK-30374,13513158,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,12/Dec/22 10:21,19/Dec/22 09:39,04/Jun/24 20:41,19/Dec/22 08:58,1.17.0,,,,,,,,,,Runtime / REST,,,,0,test-stability,,,"

{noformat}
 
Dec 12 00:58:17 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Dec 12 00:58:17 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter3948027586539982758.jar /__w/1/s/flink-runtime/target/surefire 2022-12-12T00-54-35_333-jvmRun3 surefire6600007105913438334tmp surefire_23874743947436015186tmp
Dec 12 00:58:17 [ERROR] Error occurred in starting fork, check output in log
Dec 12 00:58:17 [ERROR] Process Exit Code: 239
Dec 12 00:58:17 [ERROR] Crashed tests:
Dec 12 00:58:17 [ERROR] org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest
Dec 12 00:58:17 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Dec 12 00:58:17 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Dec 12 00:58:17 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Dec 12 00:58:17 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Dec 12 00:58:17 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Dec 12 00:58:17 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Dec 12 00:58:17 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Dec 12 00:58:17 [ERROR] at java.lang.Thread.run(Thread.java:748)
Dec 12 00:58:17 [ERROR] -> [Help 1]
Dec 12 00:58:17 [ERROR] 
Dec 12 00:58:17 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Dec 12 00:58:17 [ERROR] Re-run Maven using the -X switch to enable full debug logging.

{noformat}



https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43871&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 08:58:24 UTC 2022,,,,,,,,,,"0|z1dyso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 08:58;mapohl;I'm closing this issue in favor of FLINK-30299. The test failure is caused by a fatally exited JVM in the {{TaskManagerRunnerTest}}:
{code}
00:56:39,525 [flink-akka.actor.default-dispatcher-5] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
00:56:39,538 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
00:56:39,544 [flink-akka.remote.default-remote-dispatcher-6] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-6' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1
        at akka.remote.transport.netty.NettyFutureBridge$.apply(NettyTransport.scala:65) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_1278afb8-452c-4cc3-8616-1a3d914bbd6d.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.remote.transport.netty.NettyFutureBridge$$anon$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:113) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 16 more
00:56:39,546 [FileChannelManagerImpl-netty-shuffle shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-af2e440d-55d1-4b21-a3d8-a12e746207bc
00:56:39,547 [FileChannelManagerImpl-netty-shuffle shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-9ddec9fe-5d23-42d9-a20c-12b28fc836d3
00:56:39,547 [FileChannelManagerImpl-netty-shuffle shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-d886b38a-1c87-494b-8ef1-6eabd1bb0170
00:56:39,547 [FileChannelManagerImpl-netty-shuffle shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-476aadef-2a55-4359-b1d5-a2aa71f784c3
00:56:39,553 [     flink-metrics-7] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-table-runtime free for flink-table-store-codegen,FLINK-30373,13513150,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,12/Dec/22 09:49,13/Dec/22 03:27,04/Jun/24 20:41,13/Dec/22 03:27,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"To remove the dependence of flink-table-store-core  on flink-table-runtime, flink-table-store-codegen is the first step.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 03:27:23 UTC 2022,,,,,,,,,,"0|z1dyqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 03:27;lzljs3620320;master: 3e47657054e75d714cd941f502a899e5059cffac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dependencyManagement bom for flink-dist,FLINK-30372,13513141,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,gzm55,gzm55,12/Dec/22 09:07,12/Dec/22 14:15,04/Jun/24 20:41,12/Dec/22 14:15,,,,,,,,,,,API / Core,,,,0,,,,"The flink-dist and other artifacts shipped within the prebuilt .tgz archives should be depended with the 'provided' scope when developing a flink app. But many artifacts are shaded directly or indirectly, and not clear to a flink app developer.

Can we publish a new pom in bom style for org.apache.fink:flink-dist with only <dependencyManagement> section, which declares all the shaded or shipped dependencies with 'provided' or 'test' scopes. Then we can import this pom when in the app pom like this:

 
{noformat}
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-dist-bom</artifactId>
      <version>1.16.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 14:15:51 UTC 2022,,,,,,,,,,"0|z1dyow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 14:15;chesnay;This is essentially a duplicate of FLINK-29866 which tries to address the same issue, but in a different way.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcOutputFormat is at risk of database connection leaks,FLINK-30371,13513140,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Echo Lee,Echo Lee,Echo Lee,12/Dec/22 09:02,17/Apr/24 09:52,04/Jun/24 20:41,,1.16.0,1.16.1,jdbc-3.0.0,,,jdbc-3.3.0,,,,,Connectors / JDBC,,,,0,pull-request-available,stale-assigned,,"When writing to the target table fails for some reason, for example, the target table does not exist.

The internal call sequence of JdbcOutputFormat is:
JdbcOutputFormat#flush(throws IOException) --> JdbcOutputFormat#close --> JdbcOutputFormat#flush(throws RuntimeException).

Will not call the close method of the database connection, when the restart strategy is fixeddelay, maxNumberRestartAttempts is Integer.MAX, this will cause the number of database connections to continue to rise and reach the limit.

 
{code:java}
2022-12-07 10:49:32,050 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat [] - JDBC executeBatch error, retry times = 3
java.sql.BatchUpdateException: ORA-00942: table or view does not exist{code}
{code:java}
Caused by: java.sql.SQLException: Listener refused the connection with the following error:
ORA-12519, TNS:no appropriate service handler found {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 12:28:12 UTC 2024,,,,,,,,,,"0|z1dyoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 09:19;Echo Lee;CC [~martijnvisser] 

Can you assign this ticket to me?;;;","26/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","25/Sep/23 04:09;kevindick;Hello, i reopend the PR because in the old one there where nothing going on. Hope this solves this issue;;;","09/Apr/24 12:28;kevindick;Hello [~Echo Lee] [~martijnvisser] any updates on this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing delegation token framework authentication to providers,FLINK-30370,13513139,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,12/Dec/22 09:01,13/Dec/22 14:45,04/Jun/24 20:41,13/Dec/22 14:45,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 14:45:53 UTC 2022,,,,,,,,,,"0|z1dyog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 14:45;mbalassi;4abdf2d in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FlinkFilterJoinRule push wrong filters to another side while join type is left/right outer join,FLINK-30369,13513118,13444558,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,337361684@qq.com,337361684@qq.com,12/Dec/22 07:08,21/Dec/22 01:19,04/Jun/24 20:41,21/Dec/22 01:19,1.16.1,,,,,1.17.0,,,,,Table SQL / Planner,,,,0,,,,"Now, if join type is left/right outer join,  FlinkFilterJoinRule will push wrong filters to another side by extracting filters from join conditions. If conditions contain IS_NULL or IS_NOT_NULL filters and join type is left/right join, it cannot push this filter to another side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-12 07:08:41.0,,,,,,,,,,"0|z1dyjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix calcite method RelMdUtil$numDistinctVals() wrongly return zero if the method input domainSize much larger than numSelected ,FLINK-30368,13513105,13444558,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,12/Dec/22 04:21,27/Dec/22 08:39,04/Jun/24 20:41,27/Dec/22 08:39,1.17.0,,,,,1.17.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,Fix calcite method RelMdUtil$numDistinctVals() wrongly return zero if the method input domainSize much larger than numSelected. This wrong zero value will affect the selection of join type。 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-5431,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 08:39:15 UTC 2022,,,,,,,,,,"0|z1dygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 09:11;martijnvisser;[~337361684@qq.com] Why do this over upgrading to the Calcite version that fixes this? There's a lot of work being done atm the upgrade our Calcite dependencies ;;;","27/Dec/22 08:39;godfrey;Fixed in master: dc862dae28a172f674a9b8a2198c603275304550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enrich the thread dump info with deeper stack,FLINK-30367,13513103,13501095,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Yu Chen,yunta,yunta,12/Dec/22 04:06,13/Dec/22 05:11,04/Jun/24 20:41,13/Dec/22 05:11,,,,,,1.17.0,,,,,Runtime / Task,Runtime / Web Frontend,,,0,,,,"Currently, we only have the thread dump info with very few stack depth, and we cannot see the thread information in details. It would be useful to enrich the thread dump info.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 05:11:20 UTC 2022,,,,,,,,,,"0|z1dygg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 14:56;pvary;The depth of the stack trace could be set by the {{cluster.thread-dump.stacktrace-max-depth}} configuration value.

See: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#cluster-thread-dump-stacktrace-max-depth;;;","13/Dec/22 05:11;yunta;[~pvary] Thanks for the information, I did not notice this change as the default value is still 8 as before. I think we can create another ticket to increase the default value for better experiences.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Group Agg failed in cleaning the idle state,FLINK-30366,13513096,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,hxb,hxb,12/Dec/22 03:39,12/Dec/22 11:49,04/Jun/24 20:41,12/Dec/22 11:49,1.15.3,1.16.0,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,0,pull-request-available,,,"{code:java}
# aggregate_fast.pyx
cpdef void on_timer(self, InternalRow key):
    if self.state_cleaning_enabled:
        self.state_backend.set_current_key(key) # The key must be a list, but it is a InternalRow here.
        accumulator_state = self.state_backend.get_value_state(
            ""accumulators"", self.state_value_coder)
        accumulator_state.clear()
        self.aggs_handle.cleanup() {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 11:49:00 UTC 2022,,,,,,,,,,"0|z1dyew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 11:49;hxb;Merged into master via 72a70313b59352736514b4927a1dfadc2e8e4232

Merged into release-1.16 via 041d863552396105d08af097a456ee291263d434

Merged into release-1.15 via d3413718bc5751a10dda6c5a7b41626666753c07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New dynamic partition pruning strategy to support more dpp patterns,FLINK-30365,13513086,13500276,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,12/Dec/22 01:47,09/Jan/23 04:32,04/Jun/24 20:41,09/Jan/23 04:32,1.17.0,,,,,1.17.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"New dynamic partition pruning strategy to support more dpp patterns. Now, dpp rules is coupled with the join reorder rules, which will affect the result of join reorder. At the same time, the dpp rule don't support these patterns like union node in fact side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 04:32:35 UTC 2023,,,,,,,,,,"0|z1dyco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 02:51;lsy;Please convert it as the subtask of [FLINK-29942|https://issues.apache.org/jira/browse/FLINK-29942];;;","09/Jan/23 04:32;godfrey;Fixed in 1.17.0: d9f9b55f82dfbc1676572cc36b718a99001497f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job failed without exception stack,FLINK-30364,13513077,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,dianfu,dianfu,12/Dec/22 01:38,29/Dec/22 08:15,04/Jun/24 20:41,29/Dec/22 08:15,1.13.0,1.14.0,1.15.0,1.16.0,,,,,,,Runtime / Task,,,,0,,,,"{code}
2022-12-12 08:01:19,985 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - route map function -> Filter -> Timestamps/Watermarks with job vertex id 4bf7c1955ffe56e2106d666433eaf137 (3/30) (96f1efb2ad67ab19cae4d81548ae58e9) switched from RUNNING to FAILED on job-ee140d6c-483b-48c3-961d-b02981d4ba99-taskmanager-1-1.
java.lang.NullPointerException: null
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/22 02:43;dianfu;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13053773/screenshot-1.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 08:15:01 UTC 2022,,,,,,,,,,"0|z1dyao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 08:28;zhuzh;[~dianfu] is there a way to reproduce this issue?

I tested a job which periodically encounters NPE problems but the error stacks are always printed.;;;","22/Dec/22 08:20;wanglijie;I think this was because the error stack is optimized by the jvm due to appearing too many times. See [https://stackoverflow.com/questions/2411487/nullpointerexception-in-java-with-no-stacktrace] for details and you can add {{-XX:-OmitStackTraceInFastThrow}} to jvm options to get the error stack back.;;;","29/Dec/22 08:15;zhuzh;Close this ticket for now because it's likely to be the case [~wanglijie] mentioned above, and therefore not a problem of Flink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable HadoppConf and Kerboros Decorator by default,FLINK-30363,13512588,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,ZhenqiuHuang,ZhenqiuHuang,11/Dec/22 19:29,16/Jan/23 20:02,04/Jun/24 20:41,16/Jan/23 20:02,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Flink natively replies on these two decorator to create config maps and k8s secrets before creating job manager pod in K8s. It works well in job submisstion through Flink Cli.

org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator
org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecorator

But It doesn't work in operator mode:
1) Operator class path doesn't had HADOOP or Kerberos related info, so resources willn't be created.
2) When FlinkResourceManager create TM Pod. If JM POD has HADOOP env variables or Kerberos config are included application flink conf, then one of decorators will be enabled. TM Pod will mount on config maps or secrets that are not created yet.

Thus, we should help user to disable these two Decorators by default.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 05:38:36 UTC 2022,,,,,,,,,,"0|z1dva0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/22 19:30;ZhenqiuHuang;[~bamrabi] [~gyfora]

Would you please assign this jira to me, if you think it is a reasonable improvement?;;;","12/Dec/22 05:38;ZhenqiuHuang;Discussed with [~gyfora] offline, the change will wait for the flink upstream feature.
https://issues.apache.org/jira/browse/FLINK-28831;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-connector-hive can't build with maven 3.8,FLINK-30362,13512222,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,foxss,foxss,11/Dec/22 16:30,12/Dec/22 09:07,04/Jun/24 20:41,12/Dec/22 09:07,1.15.3,1.16.0,1.17.0,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"Flink connector hive pull in hive-exec  which depends on org.pentaho:pentaho-aggdesigner-algorithm in blocked jboss mirror. 

This is cve related issue which blockeds upgrade to maven 3.8.1+

[https://maven.apache.org/docs/3.8.1/release-notes.html#cve-2021-26291]

 
{code:java}
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.17-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [repository.jboss.org (http://repository.jboss.org/nexus/content/groups/public/, default, disabled), conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> [Help 1]{code}","install maven 3.8.1+

git clone flink repo

run mvn clean pcakge",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,Sun Dec 11 16:42:30 UTC 2022,,,,,,,,,,"0|z1dt0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/22 16:42;Sergey Nuyanzin;*Looks like a duplicate of* https://issues.apache.org/jira/browse/FLINK-27640

as stated in comments of the mentioned issue this should help
use of maven wrapper
{noformat}
./mvnw <goals>
{noformat}
or add this to your maven settings.xml:
{code:xml}
<settings>
    <mirrors>
        <mirror>
            <id>conjars-https</id>
            <url>https://conjars.org/repo/</url>
            <mirrorOf>conjars</mirrorOf>
        </mirror>
    </mirrors>
</settings>
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster deleted and created back while updating replicas,FLINK-30361,13512207,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Swathi Chandrashekar,Swathi Chandrashekar,Swathi Chandrashekar,11/Dec/22 12:19,01/Jan/23 16:18,04/Jun/24 20:41,14/Dec/22 15:02,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Whenever we try to update the replicas of the task manager for a flink standalone session cluster using the flink CR, any change in CR triggers a redeploy of the flink cluster ( delete + create of all the components - JM and TM ).

This might not be required for replica update and this should not affect the existing pods and only a new TM pod will added during a scale up and a TM pod should be deleted during a scale down.

Example tried --> Change the TM replicas from 2 to 3.

{quote}PS C:\Users\cswathi\Documents\flink-OSS-operator> kubectl get pods -w
NAME                                                              READY   STATUS              RESTARTS   AGE
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     ContainerCreating   0          1s
flink-kubernetes-operator-676897686f-5fc8r                        2/2     Running             0          18m
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            1/1     Running             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   1/1     Running             0          1s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   1/1     Running             0          13s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   1/1     Terminating         0          65s
*basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            1/1     Terminating         0          65s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   1/1     Terminating         0          65s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   1/1     Terminating         0          66s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            1/1     Terminating         0          66s*
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   1/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     Terminating         0          67s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     Terminating         0          67s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     Terminating         0          67s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     Terminating         0          67s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     Terminating         0          67s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     Terminating         0          67s
basic-session-deployment-only-example-588474bf97-nng85            0/1     Pending             0          0s
basic-session-deployment-only-example-588474bf97-nng85            0/1     Pending             0          0s
basic-session-deployment-only-example-588474bf97-nng85            0/1     ContainerCreating   0          0s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     Pending             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     Pending             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-588474bf97-nng85            0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   1/1     Running             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   1/1     Running             0          1s
basic-session-deployment-only-example-588474bf97-nng85            1/1     Running             0          2s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   1/1     Running             0          17s{quote}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 01 16:18:08 UTC 2023,,,,,,,,,,"0|z1dsxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/22 12:25;Swathi Chandrashekar;In SessionReconciler.java ( flink-kubernetes-operator), we are checking if there is any spec change, delete the session cluster, wait for it to be terminated and later deploy the new changes
{quote}@Override
protected void reconcileSpecChange(-----) throws Exception {
deleteSessionCluster(deployment, observeConfig);

// We record the target spec into an upgrading state before deploying
ReconciliationUtils.updateStatusBeforeDeploymentAttempt(deployment, deployConfig);
statusRecorder.patchAndCacheStatus(deployment);

deploy( deployment,deployment.getSpec(), deployment.getStatus(), ctx, deployConfig,Optional.empty( false
{quote}
The ask is, we can remove the deletion of session cluster whenever there is any update, as the kubernetes deployment will handle any kind of update [ i.e if there is a replica change, no change to the existing pods - only added/deletion of diff , there is a config change, depending on the rolling strategy, the update is passed to the corresponding pods by the kubernetes ].;;;","11/Dec/22 12:38;gyfora;We have similar logic in place for standalone applications with reactive scheduler enabled, so this should be a relatively easy fix.;;;","11/Dec/22 15:40;gyfora;[~Swathi Chandrashekar] do you want to give this issue a try? ;;;","11/Dec/22 17:38;Swathi Chandrashekar;Yes [~gyfora] , I would like to try this;;;","14/Dec/22 15:02;gyfora;merged to main efefd31b5155436b36e53101e92a51ada2f240b0;;;","01/Jan/23 16:18;gyfora;release-1.3 7da3cdd6ec4bfd20f24d27dc70df1c40abe6a341;;;",,,,,,,,,,,,,,,,,,,,,,,,,
A null pointer exception is thrown when data is written to the HBase table using the specified column name,FLINK-30360,13512206,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,wjc920,wjc920,wjc920,11/Dec/22 12:00,14/Aug/23 10:35,04/Jun/24 20:41,,1.12.2,1.17.0,,,,,,,,,Connectors / HBase,,,,0,pull-request-available,stale-assigned,,"A null pointer exception is thrown when data is written to the HBase table using the specified column name.

The SQL examples are as follows:
{code:java}
CREATE TABLE testTable4(
  rowkey INT,  
  family1 ROW<col1 INT>,
  family2 ROW<col1 VARCHAR, col2 BIGINT>,
  family3 ROW<col1 DOUBLE, col2 BOOLEAN, col3 VARCHAR>
) WITH (
  'connector' = 'hbase-2.2',
  'table-name' = 'testTable4',
  'zookeeper.quorum' = 'localhost:2181',
  'zookeeper.znode.parent' = '/hbase'
)

-- Specifies the column name to insert
INSERT INTO testTable4 (rowkey, family3)  SELECT 1, ROW(1.0, false, 'c')
{code}
The exception information is as follows:
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
        at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
        at akka.actor.ActorCell.invoke(ActorCell.scala:547)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        ... 4 more
Caused by: java.lang.NullPointerException
        at org.apache.flink.connector.hbase.util.HBaseSerde.lambda$createNullableFieldEncoder$a1c61e38$1(HBaseSerde.java:332)
        at org.apache.flink.connector.hbase.util.HBaseSerde.createPutMutation(HBaseSerde.java:147)
        at org.apache.flink.connector.hbase.sink.RowDataToMutationConverter.convertToMutation(RowDataToMutationConverter.java:53)
        at org.apache.flink.connector.hbase.sink.RowDataToMutationConverter.convertToMutation(RowDataToMutationConverter.java:32)
        at org.apache.flink.connector.hbase.sink.HBaseSinkFunction.invoke(HBaseSinkFunction.java:194)
        at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
        at StreamExecCalc$4.processElement(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
        at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
        at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
        at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
        at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}
The following pull request is my modification to this problem.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 10:35:08 UTC 2023,,,,,,,,,,"0|z1dsx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 15:03;martijnvisser;[~wjc920] I don't know HBase that well, but why does the example from your Jira return a NullPointerException ? Is it because in your example family1 and family2 are not provided?;;;","26/Dec/22 05:50;wjc920;Yes, the NullPointerException is present because no family1 and family2 fields are provided.

The FLINK-18726  explain how to support INSERT INTO specific columns. So I think we can INSERT INTO hbase specific columns. [~martijnvisser] ;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountered NoClassDefFoundError when using  flink-sql-connector-elasticsearch6 ,FLINK-30359,13511533,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Dec/22 17:31,28/Feb/23 16:52,04/Jun/24 20:41,18/Jan/23 15:35,1.15.0,elasticsearch-3.0.0,,,,1.16.1,elasticsearch-3.0.1,elasticsearch-3.1.0,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,"When I use sql-client to submit job that sink data to elasticsearch, it seems that the `NoClassDefFoundError` arises as shown in the figure below. Note that the connector jar I used is flink-sql-connector-elasticsearch6-4.0-SNAPSHOT.jar, it is a uber jar compiled from flink-connector-elasticsearch repo's main branch.

!image-2022-12-11-01-09-47-337.png|width=596,height=285!

Through some investigations, I found that in flink-sql-connector-elasticsearch6 module's pom.xml, we manually excluded the `com.carrotsearch: hppc` dependency, which is no problem for the 6.3.1 version of elasticsearch. However, FLINK-25189 bump this to 6.8.20, which does requires the dependence of hppc. Therefore, an `NoClassDefFoundError` occurred.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31263,,,,,"10/Dec/22 17:09;Weijie Guo;image-2022-12-11-01-09-47-337.png;https://issues.apache.org/jira/secure/attachment/13053750/image-2022-12-11-01-09-47-337.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 15:35:48 UTC 2023,,,,,,,,,,"0|z1dork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/22 17:36;Weijie Guo;To fix this problem, I think we no longer need to exclude `com.carrotsearch: hppc` from the sql connector jar of elasticsearch-6. In addition, I tested the 6.3.1 version of es excluding `hppc`, and the 6.8.20 version of es including `hppc`, all of which work well, hope this can help confirm which versions of flink(since the es connector was previously maintained in apache/flink repo) has this problem.

I am willing to fix this, [~chesnay] would you like to assign this ticket to me and I will open a pull request later.;;;","12/Dec/22 09:01;chesnay;[~Weijie Guo] Let's remove the exclusion from the shade--plugin. Ideally we also convert the shade-plugin exclusions into actual dependency exclusions so catch stuff like this earlier.;;;","17/Jan/23 12:03;martijnvisser;Fixed in main: 045a6a3acc6d3a6b76f4046f83f02d0b9c1bbb2d

[~Weijie Guo] Can you also open a PR towards the {{release-1.16}} branch of Flink and {{v3.0}} of the externalized Elasticsearch repo?
;;;","18/Jan/23 03:40;Weijie Guo;[~martijnvisser] Yes, I opened back-port PR towards these two repo respectively:

flink-1.16: https://github.com/apache/flink/pull/21708

v3.0: https://github.com/apache/flink-connector-elasticsearch/pull/51;;;","18/Jan/23 13:32;martijnvisser;release-1.16: f37048e78c94dcf2386e80c347fda02ff4b35d3d
v3.0: c6b15c4891147a0b39548db1897de4f3628c9eaa;;;","18/Jan/23 15:35;martijnvisser;Thanks [~Weijie Guo] - All fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Show the task manager id on the exception history page,FLINK-30358,13511517,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,10/Dec/22 12:08,08/May/23 12:37,04/Jun/24 20:41,26/Dec/22 08:23,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"At present, the web UI exception history page only displays the TM host and port. However, we generally need to search for problems according to the pod name or container ID.

Therefore, it is more convenient to add resource id (pod name on k8s, container id on yarn) to the location column and a link to the task manager id to jump to the task manager page.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29660,FLINK-24945,,,,,,,,,FLINK-31931,,,,,,,,,,"11/Dec/22 03:34;JunRuiLi;8371ECF0-4F7F-45EE-93B0-7EC6F7F57D72.png;https://issues.apache.org/jira/secure/attachment/13053757/8371ECF0-4F7F-45EE-93B0-7EC6F7F57D72.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 26 08:23:08 UTC 2022,,,,,,,,,,"0|z1doo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/22 12:15;JunRuiLi;Hi [~zhuzh] Can I take this ticket?;;;","11/Dec/22 02:27;zhuzh;This sounds a good improvement. Thanks for proposing this! [~JunRuiLi]
I have assigned you the ticket.;;;","26/Dec/22 08:23;zhuzh;master:
d4f9656d56cc495e1df78eaf1f25f82c18640e02
a49f1aaec6239401cc8b1dac731d290e95290caf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong link in connector/jdbc doc.,FLINK-30357,13511505,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Aiden Gong,Aiden Gong,Aiden Gong,10/Dec/22 07:41,21/Dec/22 10:42,04/Jun/24 20:41,21/Dec/22 10:42,1.15.2,1.16.0,,,,1.16.1,1.17.0,jdbc-3.0.1,jdbc-3.1.0,,Documentation,,,,0,doc-site,pull-request-available,,!image-2022-12-10-15-40-50-043.png!!image-2022-12-10-15-40-50-117.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/22 07:40;Aiden Gong;image-2022-12-10-15-40-50-043.png;https://issues.apache.org/jira/secure/attachment/13053739/image-2022-12-10-15-40-50-043.png","10/Dec/22 07:40;Aiden Gong;image-2022-12-10-15-40-50-117.png;https://issues.apache.org/jira/secure/attachment/13053738/image-2022-12-10-15-40-50-117.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 10:42:23 UTC 2022,,,,,,,,,,"0|z1dolc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 10:21;Aiden Gong;Hi [~Leonard]  , Please assign to me and code review. Thank you ~;;;","20/Dec/22 12:48;martijnvisser;flink-connector-jdbc:

main: 89656c3c15a510da03c774d6bb1cc5d6ed240408
v3.0: 8dd07942a1215eabf7fa705c6602c5ee448bacbc;;;","21/Dec/22 10:42;martijnvisser;flink:

master: b9fb8b8a2d59badfeaa83d7c64ecdd1a90964b83
release-1.16: 91e6038328987e67fd65370c55919a88a27fba4e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update NOTICE files to say ""Apache Flink AWS connectors""",FLINK-30356,13511170,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,martijnvisser,dannycranmer,dannycranmer,09/Dec/22 16:21,21/Dec/22 10:30,04/Jun/24 20:41,21/Dec/22 10:30,aws-connector-3.0.0,aws-connector-4.0.0,,,,aws-connector-3.1.0,aws-connector-4.1.0,,,,Connectors / AWS,,,,0,pull-request-available,,,"[https://lists.apache.org/thread/8bb8kh3w5ohztj50k4cgsqt97466t9fj]

 

Update all NOTICE files as per:

- ""Not a blocking issue, but the source NOTICE currently says ""Apache Flink"", where it should say ""Apache Flink AWS connector"" or something.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 10:30:41 UTC 2022,,,,,,,,,,"0|z1dmiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 10:30;dannycranmer;Merged [f520aa7|https://github.com/apache/flink-connector-aws/commit/f520aa76714a789f6638c21e54446f97a0121e89] into main

Merged [b15cb86|https://github.com/apache/flink-connector-aws/commit/b15cb866f23e8b7659ec89bec93c37b95d048a5d] into v4.0

Merged [ba35b67|https://github.com/apache/flink-connector-aws/commit/ba35b670315df13d4cbc7d20a7143dc6faf80a53] into v3.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
crictl causes long wait in e2e tests,FLINK-30355,13511142,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,mapohl,mapohl,09/Dec/22 16:04,19/Dec/22 13:47,04/Jun/24 20:41,19/Dec/22 13:47,1.17.0,,,,,1.17.0,,,,,Test Infrastructure,Tests,,,0,pull-request-available,test-stability,,"We observed strange behavior in the e2e test where the e2e test run times out: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=7446

The issue seems to be related to {{crictl}} again because we see the following error message in multiple tests. No logs are produced afterwards for ~30mins resulting in the overall test run taking too long:
{code}
Dec 09 08:55:39 crictl
fatal: destination path 'cri-dockerd' already exists and is not an empty directory.
fatal: a branch named 'v0.2.3' already exists
mkdir: cannot create directory ‘bin’: File exists
Dec 09 09:26:41 fs.protected_regular = 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 13:47:38 UTC 2022,,,,,,,,,,"0|z1dmco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 09:39;martijnvisser;[~mapohl] I don't think this is related to crictl but it is related to cri-dockerd. The test is cloning that repository. Let me see if I can harden it a bit. ;;;","19/Dec/22 13:47;martijnvisser;Fixed in master: f9557068cf203a2eabf31ffd3911c40373d7e9ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reducing the number of ThreadPools in LookupFullCache and related cache-loading classes,FLINK-30354,13511115,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,09/Dec/22 15:49,20/Dec/22 11:11,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"In the course of reviewing FLINK-29405, I came up with a proposal to reduce the complexity of the {{LookupFullCache}} implementation and shrinking the amount of threadpools being used from 3 to 2. Here's the proposal I also shared in the [FLINK-29405 PR comment|https://github.com/apache/flink/pull/20919#pullrequestreview-1208332584]:

About the responsibilities how I see them:
* {{LookupFullCache}} is the composite class for combining the {{CacheLoader}} and the {{CacheReloadTrigger}} through the {{ReloadTriggerContext}}
* {{ReloadTriggerContext}} provides an async call to trigger the reload but also some utility methods for providing processing or event time (where it's not clear to me why this is connected with the reload. It looks like a future task based on the TODO comments)
* {{CacheLoader}} is in charge of loading the data into memory (if possible concurrently).
{{CacheReloadTrigger}} provides different strategies to trigger new reloads.

About the different executors:
* The {{CacheReloadTrigger}} utilize a {{SingleThreadScheduledExecutor}} which triggers {{ReloadTriggerContext::reload}} subsequently. If the loading takes longer, subsequently triggered calls pile up. Here, I'm wondering whether that's what we want. thinking
* {{CacheLoader}} utilizes a {{SingleThreadExecutor}} in {{CacheLoader#reloadExecutor}} which is kind of the ""main"" thread for reloading the data. It triggers {{CacheLoader#updateCache}} with {{CacheLoader#reloadLock}} being acquired. {{(InputFormat)CacheLoader#updateCache}} is implemented synchronously. The data is loaded concurrently if possible using a {{FixedThreadPool}}.

My proposal is now to reduce the number of used thread pools: Instead of having a {{SingleThreadExecutor}} and a {{FixedThreadPool}} in the {{CacheLoader}} implementation, couldn't we come up with a custom {{ThreadPoolExecutor}} where we specify the minimum number of threads being 1 and the maximum being the number of cores (similar to what is already there with [ThreadUtils#newThreadPool|https://github.com/apache/flink/blob/d067629d4d200f940d0b58759459d7ff5832b292/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/utils/ThreadUtils.java#L36]). That would free the {{CacheLoader}} from starting and shutting down thread pools by moving its ownership from {{CacheLoader}} to {{LookupFullCache}} calling it the {{cacheLoadingThreadPool}} (or similar). Additionally, the {{ScheduledThreadPool}} currently living in the {{CacheReloadTrigger}} implementations could move into {{LookupFullCache}} as well calling it something like {{cacheLoadSchedulingThreadPool}}. LookupFullCache would be in charge of managing all cache loading-related threads. Additionally, it would manage the current execution through {{CompletableFutures}} (one for triggering the reload and one for executing the reload. Triggering a reload would require cancelling the current future (if it's not completed, yet) or ignoring the trigger if we want a reload to finish before triggering a new one.

{{CacheLoader#updateCache}} would become {{CacheLoader#updateCacheAsync(ExecutorService)}} returning a {{CompletableFuture}} that completes as soon as all subtasks are completed. {{CacheLoader#reloadAsync}} would return this {{CompletableFuture}} instead of creating its own future. The lifecycle (as already explained in the previous paragraph) would be managed by {{LookupFullCache}}. The benefit would be that we wouldn't have to deal interrupts in {{CacheLoader}}.

I see the following benefits:
* {{ReloadtriggerContext}} becomes obsolete (one has to clarify what the event time and processing time functions are for, though).
* {{CacheLoader#awaitFirstLoad}} becomes obsolete as well. We can verify the completion of the cache loading in {{LookupFullCache}} through the {{CompletableFuture}} instances.
* {{CacheReloadTrigger}} can focus on the strategy implementation without worrying about instantiating threads. This is duplicated code right now in {{PeriodicCacheReloadTrigger}} and {{TimedCacheReloadTrigger}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29405,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-09 15:49:06.0,,,,,,,,,,"0|z1dm6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit concurrency for external connector repositories,FLINK-30353,13511081,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,martijnvisser,martijnvisser,09/Dec/22 14:43,13/Dec/22 09:50,04/Jun/24 20:41,13/Dec/22 09:50,,,,,,,,,,,Connectors / Common,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-09 14:43:56.0,,,,,,,,,,"0|z1dlz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document missing configuration properties,FLINK-30352,13511078,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,reta,reta,reta,09/Dec/22 14:20,03/Jan/23 03:52,04/Jun/24 20:41,12/Dec/22 16:07,1.15.3,1.16.0,elasticsearch-3.0.0,,,elasticsearch-3.0.1,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,"There is a number of configuration properties which are not documented:
 - sink.delivery-guarantee
 - connection.request-timeout
 - connection.timeout
 - socket.timeout
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 16:07:02 UTC 2022,,,,,,,,,,"0|z1dlyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 16:07;chesnay;main: e205d907ce53b2bfa8e3152dfc1308d19311d347
v3.0: 2d5900da633ccc6dd6a8dc2f91877828eb78f3ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable test cases again,FLINK-30351,13511061,13469647,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,09/Dec/22 12:19,13/Jan/23 08:51,04/Jun/24 20:41,04/Jan/23 08:14,1.15.4,1.16.1,,,,,,,,,Connectors / Pulsar,,,,0,test-stability,,,"The following Pulsar-related tests were disabled due to test instabilities (see linked subtasks):
 * PulsarSourceUnorderedE2ECase
 * 
PulsarUnorderedPartitionSplitReaderTest
 * 
PulsarUnorderedSourceITCase

This issue can be resolved by enabling the tests again on {{master}} and the release branches after the test instabilities have been resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29835,FLINK-27917,FLINK-29836,FLINK-30340,FLINK-29755,,,FLINK-30397,,,FLINK-30657,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 08:14:00 UTC 2023,,,,,,,,,,"0|z1dluo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 08:05;mapohl;I removed affected version 1.17.0 because we're removing the Pulsar code in 1.17 (see FLINK-30397);;;","22/Dec/22 08:25;mapohl;Tests are disabled:
1.16: 9215ef5bbd76e11dd5b373cce4acdbbc9f1207fa
1.15: aa2c0395e8a368d7dd19605b94a0af4d77c519db;;;","04/Jan/23 06:00;syhily;All these tests have been removed in latest Pulsar connector. So I think we can just leave these tests disabled in 1.16 and 1.15 branches.;;;","04/Jan/23 08:14;mapohl;Ok, I will close this issue then based on what [~syhily] said in [his comment|https://issues.apache.org/jira/browse/FLINK-30351?focusedCommentId=17654301&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17654301].

Ideally, we would just remove the code instead of having dead code in the codebase. But considering that this issue is linked in the {{@Disabled}} annotation, I think it's documented well enough.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Write dependency-reduced pom to default directory,FLINK-30350,13511051,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Dec/22 10:47,16/Jan/23 13:06,04/Jun/24 20:41,16/Jan/23 13:06,,,,,,1.17.0,,,,,Build System,,,,0,pull-request-available,,,"The dependency-reduced pom is currently written to the target/ directory. This makes sense in general as it is a generated artifact, and should be automatically cleaned-up when doing a mvn clean.

The shade-plugin however marks this pom as the modules pom during the build, and maven has a requirement that the basedir of a module is where the pom resides.
As a result the basedir is changed whenever dependency-reduction is applied, causing weird side-effects like changing the working directory during IT cases.

Let's bite the bullet and keep the dependency-reduced pom next to the original pom, with some extra configuration to remove it during a mvn clean.
This side-steps all the issues encountered in FLINK-30083.",,,,,,,,,,,,,,FLINK-30083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 13:06:14 UTC 2023,,,,,,,,,,"0|z1dlsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 13:06;chesnay;master: 60101e120fb2bd2393e9da00f07e1d53cec35c23;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync missing HBase e2e tests to external repo,FLINK-30349,13511046,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ferenc-csaky,martijnvisser,martijnvisser,09/Dec/22 10:31,12/Jul/23 16:24,04/Jun/24 20:41,10/Jul/23 16:03,hbase-3.0.0,,,,,hbase-3.0.0,,,,,Connectors / HBase,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 16:24:50 UTC 2023,,,,,,,,,,"0|z1dlrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 16:03;chesnay;main: 8fda39b1e81f63c3db1be2295fb3efeeb60fd638;;;","10/Jul/23 19:27;martijnvisser;[~ferenc-csaky] [~chesnay] For my info, the next logical step would be to create an HBase release and then remove the HBase connector code from the Flink repo, correct?;;;","12/Jul/23 15:41;ferenc-csaky;Correct I think. Although the E2E tests are failing ATM. Currently, as far I see from the logs, the tests are executed multiple times (which is weird, might be related to the parametrization) until the records won't show up in HBase. I did some refactor, but the [CI tests on my fork|https://github.com/ferenc-csaky/flink-connector-hbase/actions/runs/5520614808/jobs/10067489222] now fails on downloading the {{flink-base}} docker img for one of the tests, complains about a missing docker login, which is also weird. For the record, on my machine both the current implementation and my changes ran successfully.

And as far as I see the newer commits, some of the non-E2E IT cases also fail for Flink 1.17, I'll take a look at that.;;;","12/Jul/23 16:24;ferenc-csaky;Cherry-picked the commit for fixing the tests to 1.17 from the Flink repo: https://github.com/apache/flink-connector-hbase/pull/14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Refine Transformer for RandomSplitter,FLINK-30348,13510873,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,09/Dec/22 03:37,19/Apr/23 01:44,04/Jun/24 20:41,19/Apr/23 01:44,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:43:48 UTC 2023,,,,,,,,,,"0|z1dkow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:43;lindong;Merged to apache/flink-ml master branch 6f8aa576f7cadaa2307c73cb84fc141fc8ce1691;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connecting SqlGateway via beeline gets java.lang.UnsupportedOperationException:: Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS,FLINK-30347,13510861,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,paul8263,paul8263,09/Dec/22 03:31,09/Dec/22 03:35,04/Jun/24 20:41,09/Dec/22 03:35,1.16.0,,,,,,,,,,Table SQL / Gateway,,,,0,,,,"Connecting to Flink SqlGateway with hiveserver2 endpoint enabled via beeline(provided by Hive 3.1.0) resulted in the following error:
{code:java}
2022-12-09 10:24:28,600 ERROR org.apache.flink.table.endpoint.hive.HiveServer2Endpoint     [] - Failed to GetInfo.
java.lang.UnsupportedOperationException: Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.
        at org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.GetInfo(HiveServer2Endpoint.java:371) [flink-connector-hive_2.12-1.16.0.jar:1.16.0]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1537) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1522) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2022-12-09 10:24:28,600 ERROR org.apache.thrift.server.TThreadPoolServer                   [] - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'infoValue' is unset! Struct:TGetInfoResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*java.lang.UnsupportedOperationException:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.:9:8, org.apache.flink.table.endpoint.hive.HiveServer2Endpoint:GetInfo:HiveServer2Endpoint.java:371, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1537, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1522, org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39, org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39, org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286, java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1142, java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:617, java.lang.Thread:run:Thread.java:745], errorMessage:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.), infoValue:null)
        at org.apache.hive.service.rpc.thrift.TGetInfoResp.validate(TGetInfoResp.java:379) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.validate(TCLIService.java:5228) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5285) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5254) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.write(TCLIService.java:5205) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2022-12-09 10:24:28,600 WARN  org.apache.thrift.transport.TIOStreamTransport               [] - Error closing output stream.
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118) ~[?:1.8.0_121]
        at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[?:1.8.0_121]
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) ~[?:1.8.0_121]
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) ~[?:1.8.0_121]
        at java.io.FilterOutputStream.close(FilterOutputStream.java:158) ~[?:1.8.0_121]
        at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.transport.TSocket.close(TSocket.java:235) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:303) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121] {code}
The same problem occured when I tried the beeline tool provided by Kyuubi.

However, the SQL Gateway works well with DBeaver and JDBC Java code.","Flink 1.16.0

Hive 3.1.0

Hadoop 3.1.1",,,,,,,,,,,,,,,,,,,,,,,FLINK-29839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 09 03:35:18 UTC 2022,,,,,,,,,,"0|z1dkm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 03:35;luoyuxia;Similar issue as FLINK-29839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong code comment in class Trigger,FLINK-30346,13510857,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,ruanliang,ruanliang,09/Dec/22 02:28,12/Dec/22 02:33,04/Jun/24 20:41,09/Dec/22 08:03,,,,,,,,,,,,,,,0,pull-request-available,,,wrong code comment in class Trigger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 09 08:03:01 UTC 2022,,,,,,,,,,"0|z1dklc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 08:03;martijnvisser;Please check out the code style & quality guide https://flink.apache.org/contributing/code-style-and-quality-pull-requests.html - There is no need for a Jira for typos;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the serializer performace of state change of changelog,FLINK-30345,13510855,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,09/Dec/22 01:59,30/Dec/22 06:12,04/Jun/24 20:41,30/Dec/22 06:12,1.16.0,,,,,1.17.0,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"Currently, AbstractStateChangeLogger use sync 

DataOutputViewStreamWrapper to serialize state change which is unnecessary because it will always be executed in single thread.

So replace it with a unsync one could improve the performance of serialization.

In my simple stateful WordCount case, it could improve TPS by 10% at least.

Furthermore, because the serialization and deserialization of key and value have been executed in some delegaed state backend, maybe we could avoid double serialization. It may improve the performance if the serialization logic is complex and even is the bottleneck.

This ticket focuses on the sync serializer problem.
The second problem about double serialization could also be disscussed, and I will create a new ticket if necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 30 06:12:11 UTC 2022,,,,,,,,,,"0|z1dkkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 08:07;martijnvisser;Note: new features/optimizations should not be introduced in a patch version (1.16.1) so removing that from the fix versions list.;;;","16/Dec/22 07:25;masteryhx;Actually, I think it should be tagged with ""Bug"".

So I have changed the type.
I'd like to make it involved in 1.16.1.

[rkhachatryan |https://github.com/rkhachatryan] [~tangyun] [~klion26] WDYT?;;;","16/Dec/22 08:15;martijnvisser;[~masteryhx] Why do you think it's a bug? Based on the given description, this is an optimization (improve performance of serialization). If it's a bug, I would expect a runtime failure or incorrect results. ;;;","16/Dec/22 11:18;roman;I second Martjins comment, this is a performance optimization rather than a bug fix.;;;","30/Dec/22 06:12;yunta;merged in master: a5c12c5589b92d5e8b22a4aa952a0c06543424a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate KubernetesHighAvailabilityTestBase and implementing test classes,FLINK-30344,13510807,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,08/Dec/22 15:52,20/Jun/23 15:24,04/Jun/24 20:41,20/Jun/23 15:24,1.16.0,1.17.0,,,,1.18.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 15:24:34 UTC 2023,,,,,,,,,,"0|z1dka8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 15:24;mapohl;master: f1ca261e1e917fa1c79715ff6ed415076aa4295a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate KubernetesLeaderElectionAndRetrievalITCase,FLINK-30343,13510804,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,08/Dec/22 15:49,21/Jun/23 06:54,04/Jun/24 20:41,21/Jun/23 06:51,1.16.0,1.17.0,,,,1.18.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 06:51:48 UTC 2023,,,,,,,,,,"0|z1dk9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 10:41;Wencong Liu;Hi [~mapohl] , could you please illustrate this issue in detail ?;;;","02/Jan/23 08:15;mapohl;Hi [~Wencong Liu], I put more context into the parent issue's description (FLINK-30338). But here's a summary: We missed the removal of certain legacy code in FLINK-25806. This includes the tests collected in FLINK-30338 (i.e. its subtasks). But we shouldn't blindly delete these. We should rather verify which test cases are covered by the new test classes (i.e. {{MultipleComponent*}}). We should migrate tests that are not covered by the new test classes, yet.

But that said, we might close FLINK-30338 and go ahead with FLINK-26522, instead. Where we change the {{LeaderElectionService}} interface. That would enable us to keep the old (legacy) test cases.;;;","02/Jan/23 15:12;Wencong Liu;[~mapohl] Thanks for your reply. I think that even though FLINK-26522 is currently in progress, the current changes to FLINK-30338 are still meaningful. After FLINK-30338 is completed, only a few minor changes may be needed. Old (legacy) test cases should not be needed, right? If you think it's OK, could you please assign the remaining subtasks to me?;;;","03/Jan/23 06:43;mapohl;I'm not sure whether it's worth it to start on FLINK-30338 while FLINK-26522. If FLINK-26522 is resolved in the way it is described in its comment section right now, the {{MultipleComponent*}} classes would either go away or become what's {{DefaultLeaderElectionService}} right now with slight adaptations. Therefore, it would make sense to rather migrate the test cases that are implemented for the {{MultiComponent*}} as part of FLINK-26522 than migrating the legacy test cases to what's now under the {{MultipleComponent*}} leader election as part of FLINK-30338 just to migrate them back as part of FLINK-26522 again. It seems to be extra effort without any additional value. WDYT?;;;","03/Jan/23 07:00;Wencong Liu;It makes sense. Let's follow the progress of FLINK-26522.;;;","21/Jun/23 06:51;mapohl;master: 5553ea78488be0354a5746cae0e514fe4b5e57cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Migrate ZooKeeperLeaderElectionTest,FLINK-30342,13510803,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,08/Dec/22 15:47,21/Jun/23 09:55,04/Jun/24 20:41,21/Jun/23 09:54,1.16.0,1.17.0,,,,1.18.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"||ZooKeeperLeaderElectionTest||ZooKeeperMultipleComponentLeaderElectionDriverTest||
|testZooKeeperLeaderElectionRetrieval|testPublishLeaderInformation|
|testZooKeeperReelection|testLeaderElectionWithMultipleDrivers|
|testZooKeeperReelectionWithReplacement|<missing>|
|testLeaderShouldBeCorrectedWhenOverwritten|testNonLeaderCannotPublishLeaderInformation (slightly different)|
|testExceptionForwarding|<missing>|
|testEphemeralZooKeeperNodes|<missing>|
|testNotLeaderShouldNotCleanUpTheLeaderInformation|<missing> (but similar to testNonLeaderCannotPublishLeaderInformation)|
|testUnExpectedErrorForwarding|<missing>|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30484,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 09:54:44 UTC 2023,,,,,,,,,,"0|z1dk9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/23 09:54;mapohl;master: c4c633e843920bbf18b2ac2242ee48ada408f6ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce audit_log system table,FLINK-30341,13510769,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Dec/22 12:11,12/Dec/22 09:55,04/Jun/24 20:41,12/Dec/22 08:19,,,,,,table-store-0.3.0,,,,,,,,,0,pull-request-available,,,"In some scenarios, users need to get the changelog to do some auditing work, such as determining the number of updates and inserts.

We can provide audit_log system table, users can get the rowkind information column.


{code:java}
INSERT INTO T VALUES ('1', '2', '3');
INSERT INTO T VALUES ('1', '4', '5');

SELECT * FROM T$audit_log;

users can get:
- ""+I"", ""1"", ""2"", ""3""
- ""-U"", ""1"", ""2"", ""3"";
- ""+U"", ""1"", ""4"", ""5""
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 08:19:41 UTC 2022,,,,,,,,,,"0|z1dk1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 08:19;lzljs3620320;master: 
81f4efc101620a134601c69a3b873f2a8d39fb70
23b523cd47bd08dfd9ad8ba402af3720f097b538;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarUnorderedSourceITCase fails with IllegalStateException,FLINK-30340,13510767,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,mapohl,mapohl,08/Dec/22 11:47,04/Jan/23 02:52,04/Jun/24 20:41,04/Jan/23 02:52,pulsar-4.0.0,,,,,,,,,,Connectors / Pulsar,,,,0,test-stability,,,"We have test failures in \{{PulsarUnorderedSourceITCase}} in the following build due to an {{IllegalStateException}} being caused by the job reaching {{CANCELLED}} state: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43793&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=38014]
{code:java}
Dec 08 01:26:44 java.lang.IllegalStateException: Job has entered CANCELED state, but expecting [FINISHED]
Dec 08 01:26:44 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForJobStatus$7(CommonTestUtils.java:284)
Dec 08 01:26:44 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
Dec 08 01:26:44 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Dec 08 01:26:44 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForJobStatus(CommonTestUtils.java:268)
Dec 08 01:26:44 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testSourceSingleSplit(SourceTestSuiteBase.java:159)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,,FLINK-29836,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 02:52:50 UTC 2023,,,,,,,,,,"0|z1dk1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 11:47;mapohl;Linking FLINK-29836 since both times the same ITCase class is affected.;;;","08/Dec/22 11:50;mapohl;Another one was reported in FLINK-29836: https://github.com/apache/flink-connector-pulsar/actions/runs/3593169763/jobs/6049780821
{code:java}
2022-12-01T13:56:49.0365395Z [ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 417.797 s <<< FAILURE! - in org.apache.flink.connector.pulsar.source.PulsarUnorderedSourceITCase
2022-12-01T13:56:49.0368192Z [ERROR] org.apache.flink.connector.pulsar.source.PulsarUnorderedSourceITCase.testSourceSingleSplit(TestEnvironment, DataStreamSourceExternalContext, CheckpointingMode)[2]  Time elapsed: 3.506 s  <<< ERROR!
2022-12-01T13:56:49.0369084Z java.lang.IllegalStateException: Job has entered CANCELED state, but expecting [FINISHED]
2022-12-01T13:56:49.0369723Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForJobStatus$7(CommonTestUtils.java:284)
2022-12-01T13:56:49.0371022Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
2022-12-01T13:56:49.0371778Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
2022-12-01T13:56:49.0372493Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForJobStatus(CommonTestUtils.java:268)
2022-12-01T13:56:49.0373684Z 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testSourceSingleSplit(SourceTestSuiteBase.java:159)
[...] {code};;;","13/Dec/22 12:45;martijnvisser;https://github.com/apache/flink-connector-pulsar/actions/runs/3685357846/jobs/6236223564;;;","15/Dec/22 15:16;martijnvisser;https://github.com/apache/flink-connector-pulsar/actions/runs/3704720693/jobs/6277697882;;;","03/Jan/23 09:27;mapohl;I updated the affected versions. FLINK-30397 removed Pulsar-related code from {{master}}. Therefore, it's only an issue on {{release-1.16}} and {{release-1.15}} where the test disabled (see FLINK-30351). {{pulsar-4.0.0}} was added as affected versions.;;;","04/Jan/23 02:52;tison;After FLINK-30413 dropped the related support, this test was dropped also. Invalid now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add a unified delegation token manager,FLINK-30339,13510766,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,08/Dec/22 11:44,21/Dec/22 13:41,04/Jun/24 20:41,21/Dec/22 13:41,1.16.0,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 13:41:07 UTC 2022,,,,,,,,,,"0|z1dk14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 13:41;gaborgsomogyi;37ad3434b887023ad81df0a013d434ca17f04e89 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean leader election legacy code,FLINK-30338,13510756,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,mapohl,mapohl,mapohl,08/Dec/22 10:49,21/Jun/23 09:56,04/Jun/24 20:41,21/Jun/23 09:56,1.16.0,1.17.0,,,,,,,,,Runtime / Coordination,,,,0,,,,"FLINK-25806 removed not all the legacy classes of Flink 1.15- per-component leader election. {{ZooKeeperLeaderElectionDriver}} and {{KubernetesLeaderElectionDriver}} should be removed. Tests currently implemented in {{{}ZooKeeperLeaderElectionTest{}}}, {{KubernetesLeaderElectionTest}} and {{KubernetesLeaderElectionAndRetrievalITCase}} should be removed or migrated to use {{{}MultipleComponentLeaderElection*{}}}-related classes.",,,,,,,,,,,,,,,,,FLINK-26522,,,FLINK-30484,,,,,,FLINK-26522,,,,,,,,,,FLINK-25806,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 09:56:26 UTC 2023,,,,,,,,,,"0|z1djyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/23 08:16;mapohl;FLINK-26522 might make this issue obsolete. It would make merging the test cases easier. Therefore, I'm stopping the migration efforts for now and continue working on FLINK-26522, instead.;;;","21/Jun/23 09:56;mapohl;This task is merged into the efforts around FLINK-26522;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Functions application throws an exception when stopping a job gracefully creating a final savepoint,FLINK-30337,13510734,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,alibahadirzeybek,alibahadirzeybek,08/Dec/22 09:28,21/Feb/23 10:17,04/Jun/24 20:41,,statefun-3.2.0,,,,,,,,,,Stateful Functions,,,,1,,,,"When running a Stateful Functions applications, if the stop[1] command is executed, the client throws a FlinkException with the following stack trace where *953498833da99ec437758b49b7d5befd* is the specific job id:

 
{code:java}
The program finished with the following exception:org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""953498833da99ec437758b49b7d5befd"".
    at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:581)
    at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:1002)
    at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:569)
    at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1069)
    at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.util.concurrent.TimeoutException
    at java.base/java.util.concurrent.CompletableFuture.timedGet(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
    at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:579)
    ... 6 more {code}
 

How to reproduce:
 # Follow the README[2] of the k8s deployment example of the *flink-statefun-playground* project to have a running application
 # Open the Flink UI that is started to get the *JOB_ID*
 # Detect the *STATEFUN_MASTER_POD_NAME* by running: *kubectl get pods --namespace statefun*
 # Start a shell into the *statefun-master* pod by issuing the: *kubectl exec -it --namespace statefun $STATEFUN_MASTER_POD_NAME – /bin/bash*
 # Run the stop command: *./bin/flink stop --savepointPath /tmp/flink-savepoints $JOB_ID*

 

[1]: [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/cli/#stopping-a-job-gracefully-creating-a-final-savepoint]

[2]: [https://github.com/apache/flink-statefun-playground/blob/main/deployments/k8s/README.md] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 10:16:40 UTC 2023,,,,,,,,,,"0|z1dju0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 10:16;novakov.alex;I have been able to reproduce this issue as well. 

Flink CLI Stop command fails with above exception after timeout and there is one more exception in *statefun-worker* pod:
{code:java}
2023-02-21 10:06:36,818 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - feedback-union -> functions (1/1)#0 - asynchronous part of checkpoint 2 could not be completed.
java.util.concurrent.CancellationException: null
    at java.util.concurrent.FutureTask.report(Unknown Source) ~[?:?]
    at java.util.concurrent.FutureTask.get(Unknown Source) ~[?:?]
    at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
    at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:57) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:177) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [flink-dist_2.12-1.14.3.jar:1.14.3]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
    at java.lang.Thread.run(Unknown Source) [?:?]{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync missing Pulsar e2e tests to external repo,FLINK-30336,13510728,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,08/Dec/22 09:10,24/Apr/23 07:52,04/Jun/24 20:41,12/Dec/22 18:34,pulsar-3.0.0,,,,,pulsar-3.0.0,pulsar-4.0.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 18:34:25 UTC 2022,,,,,,,,,,"0|z1djso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 18:34;martijnvisser;Fixed in:

main: 858931e29e4bcf67f2c695cc742c90652dfb6352
v3.0: 634e158545975e52b6929483a767c8f3e99091ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Improvement for Hive Query Syntax Compatibility-Part2,FLINK-30335,13510724,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,08/Dec/22 08:38,19/Dec/22 07:27,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Hive,,,,0,,,,Move some unfinished issues in FLINK-26360 to here and any issue about Hive dialect are expected be put in here.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-08 08:38:36.0,,,,,,,,,,"0|z1djrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinator error splitRequest check cause HybridSource loss of data and hang,FLINK-30334,13510715,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,taoran,taoran,taoran,08/Dec/22 07:26,27/Dec/22 01:30,04/Jun/24 20:41,27/Dec/22 01:30,1.16.0,1.17.0,,,,1.16.1,1.17.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"If we use hybrid source, for example, filesystem source A read a.csv, filesystem B read b.csv. It's a very simple case, but it will hang in second source with:

10802 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: hybrid_source[1] received split request from parallel task 0 (#0)
10802 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - Subtask 0 (on host '') is requesting a file source split
10803 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner [] - Assigning split to non-localized request: Optional[FileSourceSplit: [file:/Users/xxx/a.csv|file:///Users/xxx/a.csv] [0, 49) (no host info) ID=0000000001 position=null]
10808 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - Assigned split to subtask 0 : FileSourceSplit: [file:/Users/xxx/a.csv|file:///Users/xxx/a.csv] [0, 49) (no host info) ID=0000000001 position=null
10816 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceReader [] - Adding splits subtask=0 sourceIndex=0 currentReader=org.apache.flink.connector.file.src.impl.FileSourceReader@1e8e1971 [HybridSourceSplit

{sourceIndex=0, splitId=0000000001}

]
10817 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [FileSourceSplit: [file:/Users/xxx/a.csv|file:///Users/chucheng/TMP/a.csv] [0, 49) (no host info) ID=0000000001 position=null]
10822 [Source Data Fetcher for Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
10864 [Source Data Fetcher for Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Finished reading from splits [0000000001]
+I[hello_a, flink, 1]
+I[hello_a, hadoop, 2]
+I[hello_a, world, 3]
10866 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Finished reading split(s) [0000000001]
10868 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Closing splitFetcher 0 because it is idle.
10868 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
10868 [Source Data Fetcher for Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
10869 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: hybrid_source[1] received split request from parallel task 0 (#0)
10870 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - Subtask 0 (on host '') is requesting a file source split
10872 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - No more splits available for subtask 0
10872 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
10872 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceReader [] - End of input subtask=0 sourceIndex=0 org.apache.flink.connector.file.src.impl.FileSourceReader@1e8e1971
StaticFileSplitEnumerator:org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator@69906bb9
10874 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator [] - Starting enumerator for sourceIndex=1
10879 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceReader [] - Switch source event: subtask=0 sourceIndex=1 source=org.apache.flink.connector.file.src.FileSource@12ef574f
10879 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
10882 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: hybrid_source[1] received split request from parallel task 0 (#0)

*do not read next source data and hang at 'received split request from parallel task'*

 

The reason is that in the 1.16 & master, latest code add context.hasNoMoreSplits check then call enumerator.handleSplitRequest.  We do understand the comments for reducing the call splits. But it not consider the the situation about HybridSource. When a subtask hasNoMoreSplits, it will switch to next source. But here just set a check without this situation. When first source read finish, the context just let this subtask with noMoreSplit Status. And the later check can't assign splits with next sources. However , the flink 1.15 is correct.

 

*SourceCoordinator*

 
{code:java}
private void handleRequestSplitEvent(int subtask, int attemptNumber, RequestSplitEvent event) {
    LOG.info(
            ""Source {} received split request from parallel task {} (#{})"",
            operatorName,
            subtask,
            attemptNumber);
    // request splits from the enumerator only if the enumerator has un-assigned splits
    // this helps to reduce unnecessary split requests to the enumerator
    if (!context.hasNoMoreSplits(subtask)) {
        enumerator.handleSplitRequest(subtask, event.hostName());
    }
} {code}
SourceCoordinator call `context.hasNoMoreSplits` check cause the subtask not read the other child sources in hybrid source.

 

SourceCoordinatorContext

 
{code:java}
boolean hasNoMoreSplits(int subtaskIndex) { return subtaskHasNoMoreSplits[subtaskIndex]; }


@Override
public void signalNoMoreSplits(int subtask) {
    checkSubtaskIndex(subtask);

    // Ensure the split assignment is done by the coordinator executor.
    callInCoordinatorThread(
            () -> {
                subtaskHasNoMoreSplits[subtask] = true;
                signalNoMoreSplitsToAttempts(subtask);
                return null; // void return value
            },
            ""Failed to send 'NoMoreSplits' to reader "" + subtask);
}

{code}
context set subtask noMoreSplit is true if source is done (without considering the hybrid situation).

 

 

1.15

 
{code:java}
public void handleEventFromOperator(int subtask, OperatorEvent event) {
    runInEventLoop(
            () -> {
                if (event instanceof RequestSplitEvent) {
                    LOG.info(
                            ""Source {} received split request from parallel task {}"",
                            operatorName,
                            subtask);
                    enumerator.handleSplitRequest(
                            subtask, ((RequestSplitEvent) event).hostName());
                }  {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 01:30:02 UTC 2022,,,,,,,,,,"0|z1djps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 07:42;taoran;Hi. [~zhuzh] [~thw]  please take a look?   I think this question is very important because it will lead to loss of data and unexpected runtime behavior.   thanks. ;;;","08/Dec/22 09:00;martijnvisser;[~lemonjing] Isn't your second source also a batch/bounded source, while it should be a streaming/unbounded source?;;;","08/Dec/22 09:11;taoran;[~martijnvisser] yes, my second source is bounded csv. But HybridSource second source no need to must be unbounded (hybridsource without this limitation).  if second source is bounded hybridsource will return bounded. if second source is unbounded (To be precise, it is the last source, because the hybridsource is not necessarily just two) it will return unbounded.  hybrid source can work in batch and streaming. In batch mode, it will read 2 sources and return.

And it doesn't matter, i change second source to be streaming data source, it cause same bug too. The key point is current second child source do not run now (block at split request).

1.15 works well.;;;","09/Dec/22 03:46;zhuzh;Thanks for reporting this issue! [~lemonjing]

It is unexpected that {{HybridSourceSplitEnumerator}} will assign splits after it has signaled a noMoreSplits event. I think this behavior is misleading. 
The solution you proposed may solve the issue of HybridSource. However, it may introduce problems to speculative execution. Therefore, we need a better solution.

At least, we should enable the {{SourceCoordinator}} to distinguish fake noMoreSplits event and true noMoreSplits event, to know whether the source is truly fully consumed, to make correct decisions. To achieve this, we can change {{HybridSourceSplitEnumerator}} to intercept noMoreSplits signaled by the internal enumerators, and notifies {{SourceCoordinatorContext}} in a different way(i.e. introduce a SourceCoordinatorContext#signalIntermediateNoMoreSplits(int)).

Besides that, we should also refine the HybridSource test to cover {{handleRequestSplitEvent}} code path, so that this problem can be identified in ahead.;;;","09/Dec/22 05:46;taoran;[~zhuzh] Yes, the key point is that we need to distinguish the noMoreSplits is intermediated event or final event. Can u assign this ticket to me？ i'm glad to modify the pr and fix this issue. ;;;","09/Dec/22 09:56;zhuzh;Thanks for volunteering to fix it. [~lemonjing]
I have assigned you the ticket. :);;;","20/Dec/22 10:10;taoran;[~zhuzh] Hi, i have updated the pr and tested it. currently it works well. 
Looking forward for your review, thanks. ;;;","27/Dec/22 01:30;zhuzh;master:
6e4e6c68a4a179d932dacafb2771cc84f730bbc0

1.16:
2b5e4ac8df41fe68ae8212eab4a254740e0c5ef4;;;",,,,,,,,,,,,,,,,,,,,,,,
Supports lookup a partial-update table,FLINK-30333,13510712,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Dec/22 07:20,09/Dec/22 09:47,04/Jun/24 20:41,09/Dec/22 09:47,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"The lookup uses streaming read for reading table. (In TableStreamingReader)
- We should support lookup a partial-update table with full compaction.
- But partial-update table without full compaction, we should throw exception.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 09 09:47:01 UTC 2022,,,,,,,,,,"0|z1djp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 09:47;TsReaper;master: e33f36ff07d13e55d26da81ebbc916d1fe0c2e15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsFileDataIndex should supports spilling to file,FLINK-30332,13510699,13510697,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,08/Dec/22 03:47,16/Jan/23 02:23,04/Jun/24 20:41,16/Jan/23 02:23,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,"`HsFileDataIndex` is maintains in a list of treeMap(i.e. in memory), there is no problem in our 10T TPC-DS test. However, with the increase of job's data volume, the index will continue to grow, and there is a risk of OOM. We should support index data spilling and loading to avoid this problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 02:23:29 UTC 2023,,,,,,,,,,"0|z1djm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 02:23;xtsong;master (1.17): 8e50e24797fbb154cdf7c484775f0fafa94cf34c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Further improvement of production availability of hybrid shuffle,FLINK-30331,13510697,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,08/Dec/22 03:42,11/Jul/23 09:41,04/Jun/24 20:41,04/Mar/23 17:03,,,,,,1.17.0,,,,,,,,,0,,,,"Hybrid shuffle is experimental in version 1.16. In order to basically achieve production availability in 1.17, we need to make some further improvements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-08 03:42:53.0,,,,,,,,,,"0|z1djls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude .github from source release(s),FLINK-30330,13510676,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,morhidi,morhidi,07/Dec/22 23:27,07/Dec/22 23:27,04/Jun/24 20:41,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-07 23:27:16.0,,,,,,,,,,"0|z1djh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator helm chart does not work with dynamic config because of use of volumeMount subPath,FLINK-30329,13510656,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ottomata,ottomata,ottomata,07/Dec/22 19:42,28/Aug/23 07:35,04/Jun/24 20:41,12/Dec/22 15:52,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"flink-kubernetes-operator supports dynamic configuration when {{kubernetes.operator.dynamic.config.enabled}} is enabled.  The provided helm charts make use of a k8s ConfigMap to provide the flink-conf.yaml file.  These helm charts make use of the {{subPath}} VolumeMount feature.  When mounted volumes use {{{}subPath{}}}, the volume that uses them is not updated automatically.  So, even you do update the ConfigMap in k8s, the flink-conf.yaml file will never see those updates.
 * [https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically]
 * [https://kubernetes.io/docs/concepts/storage/volumes/#configmap]

Since all 'subPaths' of the volume are being mounted in both the operator and webhook containers at the same paths, I believe an easy fix would be to avoid declaring individual volumeMounts for each file, and just mount the whole ConfigMap as a volume at /opt/flink/conf.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 15:52:46 UTC 2022,,,,,,,,,,"0|z1djco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 15:52;mbalassi;dc173cb in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerWideRocksDbMemorySharingITCase.testBlockCache failed,FLINK-30328,13510463,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,07/Dec/22 16:28,04/Feb/23 19:07,04/Jun/24 20:41,04/Feb/23 19:07,1.17.0,,,,,1.17.0,,,,,Runtime / Coordination,Runtime / State Backends,,,0,pull-request-available,test-stability,,"{{TaskManagerWideRocksDbMemorySharingITCase.testBlockCache}} failed in this build: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43763&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9836]
{code:java}
Dec 06 16:33:59 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.926 s <<< FAILURE! - in org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase
Dec 06 16:33:59 [ERROR] org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase.testBlockCache  Time elapsed: 12.907 s  <<< FAILURE!
Dec 06 16:33:59 java.lang.AssertionError: 
Dec 06 16:33:59 Block cache usage reported by different tasks varies too much: DoubleSummaryStatistics{count=20, sum=3783523840.000000, min=189045056.000000, average=189176192.000000, max=189569600.000000}
Dec 06 16:33:59 That likely mean that they use different cache objects expected:<1.895696E8> but was:<1.89045056E8>
Dec 06 16:33:59 	at org.junit.Assert.fail(Assert.java:89)
Dec 06 16:33:59 	at org.junit.Assert.failNotEquals(Assert.java:835)
Dec 06 16:33:59 	at org.junit.Assert.assertEquals(Assert.java:555)
Dec 06 16:33:59 	at org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase.testBlockCache(TaskManagerWideRocksDbMemorySharingITCase.java:133)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30831,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 04 19:07:03 UTC 2023,,,,,,,,,,"0|z1di5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 16:29;mapohl;[~roman] is there anyone else I could ping on these issues besides you to spread the work a bit?;;;","07/Dec/22 16:31;mapohl;The test failure appeared in a CI run for [hotfix PR #21459|https://github.com/apache/flink/pull/21459]. It's quite likely that the failure is unrelated to the change because the PR addresses deprecated Calcite APIs.;;;","08/Dec/22 23:44;roman;The failure is most likely unrelated and I can look into it. Thanks for reporting and pinging me.;;;","11/Dec/22 07:04;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43861&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","02/Jan/23 09:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44309&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9849;;;","02/Jan/23 11:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44373&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=11054;;;","04/Jan/23 11:39;fsk119;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44456&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=5c8e7682-d68f-54d1-16a2-a09310218a49;;;","09/Jan/23 15:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44613&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7433;;;","19/Jan/23 13:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10185;;;","19/Jan/23 13:36;mapohl;I'm updating the priority to become a blocker since it appears to only happen on master. [~roman]  Are there any updates on that topic?;;;","19/Jan/23 14:09;roman;Sorry for the late reply, [~mapohl] 

I've checked it locally and made sure that it is a test issue, not a production code issue. In particular, I've found no issues with:
 - memory sharing (the feature that's tested)
 - concurrency - except metric reading, which is expected

The problem in test is caused by the timings of metrics collections.

The interval is hardcoded to 5s in Flink currently, but even if changed it still doesn't solve the problem completely. There seems to be no better alternative to testing via metrics either.

So I opened drafted a PR to check deviation instead of min/max which should eliminate false positives: https://github.com/apache/flink/pull/21733
I'll try to finalize it this week.;;;","24/Jan/23 22:11;roman;Merged into master as f8105bb584f8e7b6302a15c1c9270b988ff574cb.;;;","26/Jan/23 07:33;mapohl;I'm reopening this issue because we still see test failures on {{master}}. The following build failed with the fix [f8105bb5|https://github.com/apache/flink/commit/f8105bb584f8e7b6302a15c1c9270b988ff574cb] being included:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45196&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9881;;;","30/Jan/23 08:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8015;;;","30/Jan/23 12:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45374&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8043;;;","30/Jan/23 14:00;mapohl;{quote}
I've checked it locally and made sure that it is a test issue, not a production code issue. In particular, I've found no issues with:
 - memory sharing (the feature that's tested)
 - concurrency - except metric reading, which is expected

The problem in test is caused by the timings of metrics collections.
{quote}

[~roman] is there a chance that we can get it fixed before 1.17? Or would you vote for lowering the priority of this issue?;;;","30/Jan/23 15:39;roman;I think the priority can be lowered because it is a test code issue.

I've opened a [PR|https://github.com/apache/flink/pull/21791/] to ignore the test temporarily as a quick solution.

I've also opened a [PR|https://github.com/apache/flink/pull/21778] to fix the test, hope it can get merged soon.;;;","30/Jan/23 15:42;mapohl;thanks for getting back to me on that one. I'm lowering the priority to Major.;;;","30/Jan/23 15:51;mapohl;I created FLINK-30831 after going through the logs once more.;;;","31/Jan/23 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45408&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8037;;;","04/Feb/23 19:07;roman;Fixed by 7b65644161767e90f0834db76ba116d4d92449d5 in master.;;;",,,,,,,,,,
Adding a values.schema.json to the Helm chart source to support schema validation on Helm chart configuration,FLINK-30327,13510448,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,07/Dec/22 14:34,20/Aug/23 10:35,04/Jun/24 20:41,,kubernetes-operator-1.2.0,,,,,,,,,,Kubernetes Operator,,,,1,auto-deprioritized-major,starter,,"Helm charts support [configuration validation|https://helm.sh/docs/topics/charts/#schema-files] through {{{}values.schema.json{}}}. I'm wondering whether that's something worth adding.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:02 UTC 2023,,,,,,,,,,"0|z1di2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 02:15;xxxinli1;Hi [~mapohl] I would like to try this. ;;;","29/Mar/23 07:29;mapohl;You might want to check with the Kubernetes Operator people whether that's a reasonable improvement before going ahead with it. I just added this Jira as a proposal based on some work I did with Helm in the past.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client should not create table directory structure,FLINK-30326,13510425,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Gerrrr,Gerrrr,07/Dec/22 12:33,29/Mar/23 01:49,04/Jun/24 20:41,29/Mar/23 01:49,,,,,,,,,,,Table Store,,,,0,,,,"The SQL client creates the schema part of the directory structure for new tables. This behavior is incorrect because the SQL client could be attached to a SQL gateway that does not necessarily have access to DFSes for all catalogs/tables. 

I propose to change the behavior such that FTS sink creates the schema, either on the fly if `auto-create=true` or just as a separate job if `auto-create=false`.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:49:24 UTC 2023,,,,,,,,,,"0|z1dhxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 12:36;Gerrrr;I used the following example to conclude that the SQL client creates the table structure.
h2. Setup

I used a docker-compose with 1 JM and 1 TM that do not have shared volumes as well as an SQL client started on the JM.

 

_NOTE_ I used the local file system for the warehouse to find out what component was responsible for creation of what directories. I understand that it is not the production-ready setup.
h3. Step 1: Create FTS catalog and schema via a SQL client on JM and see where the schema files appear
{noformat}
Flink SQL> CREATE CATALOG my_catalog WITH (
>   'type'='table-store',
>   'warehouse'='file:/tmp/table_store'
> );
[INFO] Execute statement succeed.Flink SQL> USE CATALOG my_catalog;
>
Flink SQL> CREATE TABLE word_count (
>     word STRING PRIMARY KEY NOT ENFORCED,
>     cnt BIGINT
> );
[INFO] Execute statement succeed.{noformat}
h3. Step 2: Check where the directory was created

JM:
{noformat}
root@flink:/opt/flink# ls /tmp/table_store/default.db/word_count/schema/schema-0
/tmp/table_store/default.db/word_count/schema/schema-0
{noformat}
TM:
{noformat}
ls /tmp/table_store/default.db/
ls: cannot access '/tmp/table_store/default.db/': No such file or directory
{noformat}
h3. Step 3: Do a single insert and see where the new files appear
{noformat}
Flink SQL> INSERT INTO word_count (word, cnt) VALUES ('foo', 1);
[INFO] Submitting SQL update statement to the cluster...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.17-SNAPSHOT.jar) to field java.lang.Class.ANNOTATION
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: c9c175d0137280aeeba83cc7b2ce8454
{noformat}
On JM: nothing changed
On TM:
{noformat}
$ ls /tmp/table_store/default.db/word_count/
bucket-0  manifest  snapshot
{noformat};;;","07/Dec/22 12:59;Gerrrr;[~lzljs3620320] [~TsReaper] Could you please comment if you agree with the issue statement and would review a design proposal that addresses it?;;;","09/Dec/22 02:32;lzljs3620320;Thanks [~Gerrrr]for reporting.
I think it is the configuration of local files that causes some behavior ambiguity.
Maybe we just need to remind the user that there is a problem with this operation?;;;","12/Dec/22 18:13;Gerrrr;Hey [Jingsong Lee|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lzljs3620320] ,

I agree that the local filesystem should not be used as storage, and we might warn users about it. I used the local filesystem only to show that the SQL client creates a directory structure on disk.

My main point was that the SQL session should not create files on DFS because it does not necessarily have access. AFAIU, neither SQL gateway nor JM require DFS credentials for anything but creating schema files on {{{}CREATE TABLE{}}}. The SQL gateway might run in a different DC and not have access to the DFS at all.

I suggest that TMs should always create the table files. If {{{}auto-create=true{}}}, FTS queries would do it on start. If {{{}auto-create=false{}}}, {{CREATE TABLE}} would submit a job that creates the directory structure and the schema files.;;;","29/Mar/23 01:49;lzljs3620320;I close this, I think it is difficult to implement in paimon, if necessary, please raise the issue again in paimon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"the column lost comments, also find some issue ,but that commit is not mr. i have some diff  solution",FLINK-30325,13510411,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,blackpighe,blackpighe,07/Dec/22 10:13,09/Dec/22 08:57,04/Jun/24 20:41,09/Dec/22 08:43,1.14.3,1.16.0,,,,,,,,,Connectors / Hive,Table SQL / Planner,,,0,columns,comments,,"In the process of using flink in the company, column level comment were found to be lost, and there was also an issue in the community discussing these issues, but they try to modify the structure of the table schema to fix it. I have a better solution here. The basic idea is to upgrade TableSchema to ResolvedSchema. However, there are some problems if an external component uses a lower version of flink and may not have a ResolvedSchema.",flink: 1.14.3,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://issues.apache.org/jira/browse/FLINK-27449,,Patch,,,,,,,,,9223372036854775807,,,,Fri Dec 09 08:57:48 UTC 2022,,,,,,,,,,"0|z1dhu8:",9223372036854775807,"1.convert tableSchema to ResolvedSchema
2.update hive connector to load metedata use ResolvedSchema  to Serialization and deserialization
3.update flink-api-java module,by update method buildShowCreateTableRow of TableEnvironmentImpl
to handle show create table and view",,,,,,,,,,,,,,,,,,,"08/Dec/22 01:42;luoyuxia;FYI: FLINK-29679 is going to fix the comment loss issue.

But what's your better solution? Is it upgrad TableSchema to ResolvedSchema? If so, that's the solution that FLINK-29679 try to use.

Seems you want to propose a solution when  ResolvedSchema is not available for a lower flink version? ;;;","09/Dec/22 07:18;blackpighe;Yes, I only want to be responsible for 1.14.3 now, because our company used 1.14.3. At that time, the version of tableschema had been marked for deletion and partially used the new ResolvedSchema. The lack of column comment made our product experience poor. Does the flink community have any ideas about fixing the bug in the lower version, even though this change is specific to 1.14.3 and may not be incorporated into the latest version? I can provide solutions and code;;;","09/Dec/22 08:13;luoyuxia;I'm afraid of not. It'll break compatibility which we defintly want to guarantee in bug-fix version like 1.14.4.

It won't be accepted by Flink community.;;;","09/Dec/22 08:42;martijnvisser;Not to mention that Flink 1.14 is no longer support by the community: only the last two releases are supported, so currently Flink 1.15 and Flink 1.16;;;","09/Dec/22 08:43;martijnvisser;Duplicate of https://issues.apache.org/jira/browse/FLINK-29679;;;","09/Dec/22 08:57;blackpighe;ok,i see,thank you;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Support column/partition statistics in table store,FLINK-30324,13510410,13510408,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,07/Dec/22 10:10,22/Mar/23 02:20,04/Jun/24 20:41,22/Mar/23 02:20,table-store-0.3.0,,,,,,,,,,Table Store,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-07 10:10:13.0,,,,,,,,,,"0|z1dhu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add statistics and support table statistics in table store,FLINK-30323,13510409,13510408,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,07/Dec/22 10:09,08/May/23 09:15,04/Jun/24 20:41,22/Mar/23 02:20,table-store-0.3.0,,,,,,,,,,Table Store,,,,0,pull-request-available,,,Add statistics related class in table store and supports table statistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-07 10:09:47.0,,,,,,,,,,"0|z1dhts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support table/column/partition statistics in table store,FLINK-30322,13510408,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,07/Dec/22 10:08,22/Mar/23 02:20,04/Jun/24 20:41,22/Mar/23 02:20,table-store-0.3.0,,,,,,,,,,Table Store,,,,0,,,,Support table/column/partition statistics in table sotre,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-07 10:08:40.0,,,,,,,,,,"0|z1dhtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ZLIB of FRocksDB to 1.2.13,FLINK-30321,13510404,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Dec/22 09:54,31/Mar/23 01:12,04/Jun/24 20:41,12/Dec/22 09:07,1.17.0,,,,,1.17.0,,,,,Runtime / State Backends,,,,0,,,,"In FRocksDB, the ZLIB version is 1.2.11, which may result in memory corruption, see [cve-2018-25032|https://nvd.nist.gov/vuln/detail/cve-2018-25032#vulnCurrentDescriptionTitle]

https://lists.apache.org/thread/rm40f45qfw6rls70k35o2dt0k4tz9bsr",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 01:12:32 UTC 2023,,,,,,,,,,"0|z1dhso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 09:07;Yanfei Lei;Merged [3eac409|https://github.com/ververica/frocksdb/commit/3eac409606fcd9ce44a4bf7686db29c06c205039] into FRocksDB-6.20.3.;;;","30/Mar/23 03:12;songwenbin;How do you solve the problem with cve-2018-25032 in Flink rockdb? [~Yanfei Lei] ;;;","30/Mar/23 05:44;Yanfei Lei;By upgrading the version of ZLIB.;;;","31/Mar/23 01:12;songwenbin;Thank you very much for your answer. In flink1.16.1 , upgrade the frocksdbjni of the pom.xml of the flick-statebackend-rocksdb to 6.20.3-veverica-2.0 ,is is ok ? [~Yanfei Lei] 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase connector insert timestamp  type  data error,FLINK-30320,13510386,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhengshubin,zhengshubin,07/Dec/22 08:17,07/Dec/22 08:19,04/Jun/24 20:41,,1.15.2,,,,,,,,,,Table SQL / Client,,,,0,,,,"hbase table :

CREATE TABLE h_code_wip(
 rowkey STRING,
 info ROW<code_event_time TIMESTAMP>

) WITH (
 'connector' = 'hbase-2.2',
 'table-name' = 'default:code_wip',
 'zookeeper.quorum' = 'xx:2181"" 
 )

insert sql:

INSERT INTO h_code_wip

SELECT 
rowkey,
ROW(  CAST (code_event_time AS TIMESTAMP(3)) ) info

FROM code_wip

code_event_time is JAVA Timestamp type  before  I had transform datastream to table.

Whatever I  had cast  code_event_time or not, it occur error:

The precision 6 of TIMESTAMP type is out of the range [0, 3] supported by HBase connector

 ","flink 1.15.2

hbase 2.4.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-07 08:17:25.0,,,,,,,,,,"0|z1dhoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate illegal column name in table store,FLINK-30319,13510345,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,07/Dec/22 03:55,08/Dec/22 07:23,04/Jun/24 20:41,08/Dec/22 07:23,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"There're some specific column names such as `_KEY_` prefix, `_VALUE_COUNT`, `_SEQUENCE_NUMBER` and `_VALUE_KIND`, we should validate column names and throw exception in DDL",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 07:23:59 UTC 2022,,,,,,,,,,"0|z1dhfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 07:23;lzljs3620320;master: 51d57803c3eee1943bc5a4ddf1228aff42aa88d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql-client failed to load jar passed with -j args in flink 1.16.0 ,FLINK-30318,13510341,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,samrat007,samrat007,07/Dec/22 03:21,23/Jun/23 07:29,04/Jun/24 20:41,13/Dec/22 04:48,1.16.0,,,,,,,,,,Table SQL / Client,,,,0,,,,"In Flink 1.16.0 , /usr/lib/flink/bin/sql-client.sh embedded -j hudi-flink-bundle_2.12-0.10.1.jar failed to load jar passed through argument 

 
{code:java}
 /usr/lib/flink/bin/sql-client.sh embedded -j hudi-flink-bundle_2.12-0.10.1.jar
Setting HBASE_CONF_DIR=/etc/hbase/conf because no HBASE_CONF_DIR was set.
Using the result of 'hadoop classpath' to augment the Hadoop classpath: /etc/hadoop/conf:/usr/lib/hadoop/lib/:/usr/lib/hadoop/.//:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/:/usr/lib/hadoop-hdfs/.//:/usr/lib/hadoop-mapreduce/.//:/usr/lib/hadoop-yarn/lib/:/usr/lib/hadoop-yarn/.//:/etc/tez/conf:/usr/lib/tez/hadoop-shim-0.10.2-amzn-0.jar:/usr/lib/tez/hadoop-shim-2.8-0.10.2-amzn-0.jar:/usr/lib/tez/lib:/usr/lib/tez/LICENSE:/usr/lib/tez/LICENSE-BSD-3clause:/usr/lib/tez/LICENSE-CDDLv1.1-GPLv2_withCPE:/usr/lib/tez/LICENSE-MIT:/usr/lib/tez/LICENSE-SIL_OpenFontLicense-v1.1:/usr/lib/tez/NOTICE:/usr/lib/tez/tez-api-0.10.2-amzn-0.jar:/usr/lib/tez/tez-aux-services-0.10.2-amzn-0.jar:/usr/lib/tez/tez-build-tools-0.10.2-amzn-0.jar:/usr/lib/tez/tez-common-0.10.2-amzn-0.jar:/usr/lib/tez/tez-dag-0.10.2-amzn-0.jar:/usr/lib/tez/tez-examples-0.10.2-amzn-0.jar:/usr/lib/tez/tez-history-parser-0.10.2-amzn-0.jar:/usr/lib/tez/tez-javadoc-tools-0.10.2-amzn-0.jar:/usr/lib/tez/tez-job-analyzer-0.10.2-amzn-0.jar:/usr/lib/tez/tez-mapreduce-0.10.2-amzn-0.jar:/usr/lib/tez/tez-protobuf-history-plugin-0.10.2-amzn-0.jar:/usr/lib/tez/tez-runtime-internals-0.10.2-amzn-0.jar:/usr/lib/tez/tez-runtime-library-0.10.2-amzn-0.jar:/usr/lib/tez/tez-tests-0.10.2-amzn-0.jar:/usr/lib/tez/tez-ui-0.10.2-amzn-0.war:/usr/lib/tez/tez-yarn-timeline-cache-plugin-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-with-acls-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-with-fs-0.10.2-amzn-0.jar:/usr/lib/tez/lib/async-http-client-2.12.3.jar:/usr/lib/tez/lib/commons-cli-1.2.jar:/usr/lib/tez/lib/commons-codec-1.11.jar:/usr/lib/tez/lib/commons-collections4-4.1.jar:/usr/lib/tez/lib/commons-io-2.8.0.jar:/usr/lib/tez/lib/commons-lang-2.6.jar:/usr/lib/tez/lib/guava-31.1-jre.jar:/usr/lib/tez/lib/hadoop-annotations.jar:/usr/lib/tez/lib/hadoop-auth.jar:/usr/lib/tez/lib/hadoop-hdfs-client-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-common-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-common.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-core-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-yarn-server-web-proxy.jar:/usr/lib/tez/lib/hhadoop-mapreduce-client-core.jar:/usr/lib/tez/lib/javax.servlet-api-3.1.0.jar:/usr/lib/tez/lib/jersey-client-1.19.jar:/usr/lib/tez/lib/jersey-json-1.19.jar:/usr/lib/tez/lib/jettison-1.3.4.jar:/usr/lib/tez/lib/jsr305-3.0.0.jar:/usr/lib/tez/lib/metrics-core-3.1.0.jar:/usr/lib/tez/lib/netty-all-4.1.72.Final.jar:/usr/lib/tez/lib/protobuf-java-2.5.0.jar:/usr/lib/tez/lib/RoaringBitmap-0.7.45.jar:/usr/lib/tez/lib/slf4j-api-1.7.36.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.331.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/animal-sniffer-annotations-1.14.jar:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/checker-qual-2.5.2.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.54.0.jar:/usr/share/aws/emr/emrfs/lib/error_prone_annotations-2.1.3.jar:/usr/share/aws/emr/emrfs/lib/findbugs-annotations-3.0.1.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/j2objc-annotations-1.1.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.12.331.jar:/usr/share/aws/emr/emrfs/lib/jsr305-3.0.2.jar:/usr/share/aws/emr/emrfs/auxlib/:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.3.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jar
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/flink/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2022-12-07 03:17:05,554 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /var/lib/flink/yarn/.yarn-properties-hadoop.
2022-12-07 03:17:05,554 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /var/lib/flink/yarn/.yarn-properties-hadoop.
                                   ????????
                               ????????????????
                            ???????        ???????  ?
                          ????   ?????????      ?????
                          ???         ???????    ?????
                            ???            ???   ?????
                              ??       ???????????????
                            ?? ?   ???       ?????? ?????
                            ?????   ????      ????? ?????
                         ???????       ???    ??????? ???
                   ????????? ??         ??    ??????????
                  ????????  ??           ?   ?? ???????
                ????  ???            ?  ?? ???????? ?????
               ???? ? ??          ? ?? ????????    ????  ??
              ???? ????          ??????????       ??? ?? ????
           ???? ?? ???       ???????????         ????  ? ?  ???
           ???  ?? ??? ?????????              ????           ???
           ??    ? ???????              ????????          ??? ??
           ???    ???    ????????????????????            ????  ?
          ????? ???   ??????   ????????                  ????  ??
          ????????  ???????????????                            ??
          ?? ????   ???????  ???       ??????    ??          ???
          ??? ???  ???  ???????            ????   ?????????????
           ??? ?????  ????  ??                ??      ????   ???
           ??   ???   ?     ??                ??              ??
            ??   ??         ??                 ??        ????????
             ?? ?????       ??                  ???????????    ??
              ??   ????      ?                    ???????      ??
               ???   ?????                         ?? ???????????
                ????    ????                     ??????? ????????
                  ?????                          ??  ????  ?????
                      ?????????????????????????????????  ?????
    ______ _ _       _       _____  ____  _         _____ _ _            _  BETA
   |  ___| ()     | |     / ___|/ __ | |       / ____| ()          | |
   | |__  | |_ _ __ | | __ | (___ | |  | | |      | |    | |_  ___ _ __ | |_
   |  _| | | | ' | |/ /  ___ | |  | | |      | |    | | |/ _ \ '_ | __|
   | |    | | | | | |   <   ___) | || | |___  | |___| | |  / | | | |
   ||    |||| |||_\ |____/ ______|  ___|||_|| ||_|
        Welcome! Enter 'HELP;' to list all available commands. 'QUIT;' to exit.
Command history file path: /home/hadoop/.flink-sql-history
Flink SQL> show jars;
Empty set
{code}
This works only when keeping the hudi-flink bundle jar in /flink/lib/ path 

 

But this works with flink 1.15.2 
{code:java}
/usr/lib/flink/bin/sql-client.sh embedded -j hudi-flink-bundle_2.12-0.10.1.jar
Setting HBASE_CONF_DIR=/etc/hbase/conf because no HBASE_CONF_DIR was set.
Using the result of 'hadoop classpath' to augment the Hadoop classpath: /etc/hadoop/conf:/usr/lib/hadoop/lib/:/usr/lib/hadoop/.//:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/:/usr/lib/hadoop-hdfs/.//:/usr/lib/hadoop-mapreduce/.//:/usr/lib/hadoop-yarn/lib/:/usr/lib/hadoop-yarn/.//:/etc/tez/conf:/usr/lib/tez/hadoop-shim-0.10.2-amzn-0.jar:/usr/lib/tez/hadoop-shim-2.8-0.10.2-amzn-0.jar:/usr/lib/tez/lib:/usr/lib/tez/LICENSE:/usr/lib/tez/LICENSE-BSD-3clause:/usr/lib/tez/LICENSE-CDDLv1.1-GPLv2_withCPE:/usr/lib/tez/LICENSE-MIT:/usr/lib/tez/LICENSE-SIL_OpenFontLicense-v1.1:/usr/lib/tez/NOTICE:/usr/lib/tez/tez-api-0.10.2-amzn-0.jar:/usr/lib/tez/tez-aux-services-0.10.2-amzn-0.jar:/usr/lib/tez/tez-build-tools-0.10.2-amzn-0.jar:/usr/lib/tez/tez-common-0.10.2-amzn-0.jar:/usr/lib/tez/tez-dag-0.10.2-amzn-0.jar:/usr/lib/tez/tez-examples-0.10.2-amzn-0.jar:/usr/lib/tez/tez-history-parser-0.10.2-amzn-0.jar:/usr/lib/tez/tez-javadoc-tools-0.10.2-amzn-0.jar:/usr/lib/tez/tez-job-analyzer-0.10.2-amzn-0.jar:/usr/lib/tez/tez-mapreduce-0.10.2-amzn-0.jar:/usr/lib/tez/tez-protobuf-history-plugin-0.10.2-amzn-0.jar:/usr/lib/tez/tez-runtime-internals-0.10.2-amzn-0.jar:/usr/lib/tez/tez-runtime-library-0.10.2-amzn-0.jar:/usr/lib/tez/tez-tests-0.10.2-amzn-0.jar:/usr/lib/tez/tez-ui-0.10.2-amzn-0.war:/usr/lib/tez/tez-yarn-timeline-cache-plugin-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-with-acls-0.10.2-amzn-0.jar:/usr/lib/tez/tez-yarn-timeline-history-with-fs-0.10.2-amzn-0.jar:/usr/lib/tez/lib/async-http-client-2.12.3.jar:/usr/lib/tez/lib/commons-cli-1.2.jar:/usr/lib/tez/lib/commons-codec-1.11.jar:/usr/lib/tez/lib/commons-collections4-4.1.jar:/usr/lib/tez/lib/commons-io-2.8.0.jar:/usr/lib/tez/lib/commons-lang-2.6.jar:/usr/lib/tez/lib/guava-31.1-jre.jar:/usr/lib/tez/lib/hadoop-annotations.jar:/usr/lib/tez/lib/hadoop-auth.jar:/usr/lib/tez/lib/hadoop-hdfs-client-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-common-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-common.jar:/usr/lib/tez/lib/hadoop-mapreduce-client-core-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.3.3-amzn-0.jar:/usr/lib/tez/lib/hadoop-yarn-server-web-proxy.jar:/usr/lib/tez/lib/hhadoop-mapreduce-client-core.jar:/usr/lib/tez/lib/javax.servlet-api-3.1.0.jar:/usr/lib/tez/lib/jersey-client-1.19.jar:/usr/lib/tez/lib/jersey-json-1.19.jar:/usr/lib/tez/lib/jettison-1.3.4.jar:/usr/lib/tez/lib/jsr305-3.0.0.jar:/usr/lib/tez/lib/metrics-core-3.1.0.jar:/usr/lib/tez/lib/netty-all-4.1.72.Final.jar:/usr/lib/tez/lib/protobuf-java-2.5.0.jar:/usr/lib/tez/lib/RoaringBitmap-0.7.45.jar:/usr/lib/tez/lib/slf4j-api-1.7.36.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.331.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/animal-sniffer-annotations-1.14.jar:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/checker-qual-2.5.2.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.54.0.jar:/usr/share/aws/emr/emrfs/lib/error_prone_annotations-2.1.3.jar:/usr/share/aws/emr/emrfs/lib/findbugs-annotations-3.0.1.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/j2objc-annotations-1.1.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.12.331.jar:/usr/share/aws/emr/emrfs/lib/jsr305-3.0.2.jar:/usr/share/aws/emr/emrfs/auxlib/:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.3.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jar
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/flink/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2022-12-07 03:19:37,344 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /var/lib/flink/yarn/.yarn-properties-hadoop.
2022-12-07 03:19:37,344 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /var/lib/flink/yarn/.yarn-properties-hadoop.
                                   ????????
                               ????????????????
                            ???????        ???????  ?
                          ????   ?????????      ?????
                          ???         ???????    ?????
                            ???            ???   ?????
                              ??       ???????????????
                            ?? ?   ???       ?????? ?????
                            ?????   ????      ????? ?????
                         ???????       ???    ??????? ???
                   ????????? ??         ??    ??????????
                  ????????  ??           ?   ?? ???????
                ????  ???            ?  ?? ???????? ?????
               ???? ? ??          ? ?? ????????    ????  ??
              ???? ????          ??????????       ??? ?? ????
           ???? ?? ???       ???????????         ????  ? ?  ???
           ???  ?? ??? ?????????              ????           ???
           ??    ? ???????              ????????          ??? ??
           ???    ???    ????????????????????            ????  ?
          ????? ???   ??????   ????????                  ????  ??
          ????????  ???????????????                            ??
          ?? ????   ???????  ???       ??????    ??          ???
          ??? ???  ???  ???????            ????   ?????????????
           ??? ?????  ????  ??                ??      ????   ???
           ??   ???   ?     ??                ??              ??
            ??   ??         ??                 ??        ????????
             ?? ?????       ??                  ???????????    ??
              ??   ????      ?                    ???????      ??
               ???   ?????                         ?? ???????????
                ????    ????                     ??????? ????????
                  ?????                          ??  ????  ?????
                      ?????????????????????????????????  ?????
    ______ _ _       _       _____  ____  _         _____ _ _            _  BETA
   |  ___| ()     | |     / ___|/ __ | |       / ____| ()          | |
   | |__  | |_ _ __ | | __ | (___ | |  | | |      | |    | |_  ___ _ __ | |_
   |  _| | | | ' | |/ /  ___ | |  | | |      | |    | | |/ _ \ '_ | __|
   | |    | | | | | |   <   ___) | || | |___  | |___| | |  / | | | |
   ||    |||| |||_\ |____/ ______|  ___|||_|| ||_|
        Welcome! Enter 'HELP;' to list all available commands. 'QUIT;' to exit.
Command history file path: /home/hadoop/.flink-sql-history
Flink SQL> show jars;
/usr/lib/flink/opt/flink-sql-client-1.15.2.jar
/usr/lib/flink/opt/flink-python_2.12-1.15.2.jar
/home/hadoop/hudi-flink-bundle_2.12-0.10.1.jar
{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30391,FLINK-15635,,FLINK-32418,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 03:52:09 UTC 2022,,,,,,,,,,"0|z1dheo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 04:35;luoyuxia;It should be introduced by FLINK-14055, [~lsy]  is it expected behavior or not?;;;","07/Dec/22 09:46;samrat007;I tested as per [flink-sql-client-doc|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sqlclient/#sql-client-startup-options] ,  `-j ` and `-l` for `flink 1.16`  and `master` , this is not working 

 
FLINK-27659 missed to register the Jar Resources passed through CLIOptions into the ResourceManager (Manager deals with all user defined resources) during SessionContext creation.
This is registered only during AddJar or Create Function. This looks like a simple fix to register the jar dependencies during SessionContext creation.
I can submit a patch for this. 
 ;;;","08/Dec/22 01:27;luoyuxia;[~samrat007] Thanks for the investigation, I had a quick look of the code, it seems the following code will create a classloader with the passed jars. If so, I think we don't need the register these jars again. 
{code:java}
final ClientWrapperClassLoader userClassLoader =
        new ClientWrapperClassLoader(
                ClientClassloaderUtil.buildUserClassLoader(
                        defaultContext.getDependencies(),
                        SessionContext.class.getClassLoader(),
                        new Configuration(configuration)),
                configuration); {code}
I haven't a detail investigation, I may miss somthing. Please correct me if I'm wrong.

 ;;;","12/Dec/22 02:43;lsy;[~samrat007] As [~luoyuxia] explain, the jars added by `-j` and `-i` have been registered to the parent classloader of `ClientWrapperClassLoader` when construct it, so we shouldn't registered it ClientWrapperClassLoader again. Can you debug why we can't find the class in the parent classloader of ClientWrapperClassLoader?;;;","12/Dec/22 04:42;samrat007;[~lsy] , m on it ! i will share the root cause once i find something. 

 

 ;;;","12/Dec/22 12:55;samrat007;Hi [~luoyuxia] , [~lsy] , 

i tried to debug the issue and also tried out with different custom jar flink-sql-connector-mysql-cdc and it worked fine . 

I verified taskmanager.log where it shows BlobClient is downloading the passed jars over `-j` and `-l` command .  

there is one bug that lead to this confusion ,
{code:java}
 show jars{code}
 doesn't list all the jars in the `clientWrapperClassLoader`.

 

Apologies for creating misleading ticket . 

i think show jars should list all the URI added as it was there in flink-1.15.x

Please suggest if thats expected behaviour missing. 

Found some fix for the hudi-bundle-flink.jar in https://issues.apache.org/jira/browse/FLINK-15635 ;;;","13/Dec/22 02:56;luoyuxia;[~samrat007] Hi, thanks for investigation. To me, show jars don't show all the urls added is not expected, and I think it should be fixed.

Hope you can find some reason why it happens if you have time.

About the 

>  /usr/lib/flink/bin/sql-client.sh embedded -j hudi-flink-bundle_2.12-0.10.1.jar failed to load jar

>  tried out with different custom jar flink-sql-connector-mysql-cdc and it worked fine

I'm wondering what the exception for `/usr/lib/flink/bin/sql-client.sh embedded -j hudi-flink-bundle_2.12-0.10.1.jar failed `?

I found a potianl reason is that after FLINK-14055, we won't wrap therad's classload using Thread.currentThread().setContextClassLoader() with the added jars, so if the any class in hudi-flink-bundle_2.12-0.10.1.jar try to load the class using Thread.currentThread().setContextClassLoader(), it may well get an classload without all the added jars which is different from 1.15.

 

 ;;;","13/Dec/22 03:52;samrat007;Thank you [~luoyuxia] , for spending time and looking at it . 

FLINK-15635 explained same, as you have suggested 

 

There are 4 type of classloader introduced 
 # ComponentClassloader
 # AppClassloader
 # ClientWrapperClassloader 
 # FlinkUserCodeClassloader 

We have created a ticket for the fix in sync FLINK-30377 

I will add a patch for it by today , please help review the same . 

i will close this ticket and create different one for the show jars unexpected behaviour in 1.16.0 onwards (FLINK-30391 ). ;;;",,,,,,,,,,,,,,,,,,,,,,,
Create table in FTS catalog with s3 warehouse throws DatabaseNotExistException,FLINK-30317,13510091,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Gerrrr,Gerrrr,06/Dec/22 17:03,29/Mar/23 01:45,04/Jun/24 20:41,29/Mar/23 01:45,,,,,,,,,,,Table Store,,,,0,,,,"{noformat}
Flink SQL> CREATE CATALOG my_catalog WITH (
>   'type'='table-store',
>   'warehouse'='s3://bucket/my-tablestore'
> );
[INFO] Execute statement succeed.

Flink SQL> USE CATALOG my_catalog;
[INFO] Execute statement succeed.

Flink SQL> CREATE TABLE word_count (
>     word STRING PRIMARY KEY NOT ENFORCED,
>     cnt BIGINT
> );
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.catalog.exceptions.DatabaseNotExistException: Database default does not exist in Catalog my_catalog. {noformat}

Creating the table in the default catalog works though:

{noformat}
Flink SQL> use catalog default_catalog;
[INFO] Execute statement succeed.

Flink SQL> CREATE TABLE word_count (
>       word STRING PRIMARY KEY NOT ENFORCED,
>       cnt BIGINT
>  ) WITH (
>    'connector'='table-store',
>    'path'='s3://bucket/my-tablestore',
>    'auto-create'='true'
> );
[INFO] Execute statement succeed.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:45:36 UTC 2023,,,,,,,,,,"0|z1dfvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 06:53;lzljs3620320;Thanks [~Gerrrr] for reporting.
We should check and create default database in FileSystemCatalog and HiveCatalog.;;;","29/Mar/23 01:45;lzljs3620320;https://github.com/apache/incubator-paimon/issues/733;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate flink-statebackend-common,FLINK-30316,13510079,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Dec/22 15:35,07/Dec/22 16:21,04/Jun/24 20:41,07/Dec/22 16:21,,,,,,1.17.0,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"There is a new module was added with junit4

it's better to use junit5 for new modules",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 16:21:11 UTC 2022,,,,,,,,,,"0|z1dfso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 16:21;mapohl;master: a0a5c71f785dace33d05e9f96f122bd1d0425905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more information about image pull failures to the operator log,FLINK-30315,13510066,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,pvary,pvary,06/Dec/22 14:35,14/Dec/22 14:26,04/Jun/24 20:41,14/Dec/22 14:26,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"When there is an image pull error, this is what we see in the operator log:
{code:java}
org.apache.flink.kubernetes.operator.exception.DeploymentFailedException: Back-off pulling image ""flink:1.14""
 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.checkContainerBackoff(AbstractFlinkDeploymentObserver.java:194)
 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeJmDeployment(AbstractFlinkDeploymentObserver.java:150)
 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:84)
 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:55)
 	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:56)
 	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:32)
 	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:113)
 	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
 	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
 	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
 	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
 	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
 	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
 	at java.base/java.lang.Thread.run(Unknown Source) {code}
This is the information we have on kubernetes side:
{code:java}
Normal   Scheduled  2m19s               default-scheduler  Successfully assigned
default/quickstart-base-86787586cd-lb7j6 to minikube
Warning  Failed     20s                 kubelet            Failed to pull image ""flink:1.14"": rpc error: code = Unknown desc = context deadline exceeded
*Warning  Failed     20s                 kubelet            Error*: ErrImagePull
Normal   BackOff    19s                 kubelet            Back-off pulling image ""flink:1.14""
*Warning  Failed     19s                 kubelet            Error*: ImagePullBackOff
Normal   Pulling    7s (x2 over 2m19s)  kubelet            Pulling image ""flink:1.14""
{code}
It would be good to add the additional message (in this case {{{}Failed to pull image ""flink:1.14"": rpc error: code = Unknown desc = context deadline exceeded{}}}) to the message of the {{DeploymentFailedException}} for traceability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29744,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 14:26:46 UTC 2022,,,,,,,,,,"0|z1dfps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 15:04;pvary;The {{ContainerStateWaiting}} contains the message that we want.
The issue is that:
 - For {{ErrImagePull}} we have the correct message: {{Failed to pull image ""flink:1.14"": rpc error: code = Unknown desc = context deadline exceeded}}
 - For {{ImagePullBackOff}} we only have this message: {{Back-off pulling image ""flink:1.14""}} which is not that useful

Based on this, I think we have the following options:
 # Throw {{DeploymentFailedException}} at {{ErrImagePull}} and add provide the enhanced message. Cons: This throws an error on the first image pull error - previously we retried at least once (I am not sure that this is that important as we continue to monitor the state of the deployment and we act on the state changes anyway)
 # Store the message in the state and provide it when the ImagePullBackOff failed

I would like to hear you opinions about the options, or I am interested in any alternatives you have in mind.



Without any different opinions, I would go for option 1.;;;","14/Dec/22 14:26;gyfora;merged to main 40357168fba199320c742a975def11f0c177aea4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to read all records from compressed delimited file input format,FLINK-30314,13510065,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,dyaraev,dyaraev,06/Dec/22 14:30,10/Nov/23 10:11,04/Jun/24 20:41,10/Nov/23 10:11,1.15.2,1.16.0,1.17.1,,,1.19.0,,,,,Connectors / FileSystem,,,,2,pull-request-available,,,"I am reading gzipped JSON line-delimited files in the batch mode using [FileSystem Connector|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/filesystem/]. For reading the files a new table is created with the following configuration:
{code:sql}
CREATE TEMPORARY TABLE `my_database`.`my_table` (
  `my_field1` BIGINT,
  `my_field2` INT,
  `my_field3` VARCHAR(2147483647)
) WITH (
  'connector' = 'filesystem',
  'path' = 'path-to-input-dir',
  'format' = 'json',
  'json.ignore-parse-errors' = 'false',
  'json.fail-on-missing-field' = 'true'
) {code}
In the input directory I have two files: input-00000.json.gz and input-00001.json.gz. As it comes from the filenames, the files are compressed with GZIP. Each of the files contains 10 records. The issue is that only 2 records from each file are read (4 in total). If decompressed versions of the same data files are used, all 20 records are read.

As far as I understand, that problem may be related to the fact that split length, which is used when the files are read, is in fact the length of a compressed file. So files are closed before all records are read from them because read position of the decompressed file stream exceeds split length.

Probably, it makes sense to add a flag to {{{}FSDataInputStream{}}}, so we could identify if the file compressed or not. The flag can be set to true in {{InputStreamFSInputWrapper}} because it is used for wrapping compressed file streams. With such a flag it could be possible to differentiate non-splittable compressed files and only rely on the end of the stream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/22 16:14;dyaraev;input.json;https://issues.apache.org/jira/secure/attachment/13053926/input.json","16/Dec/22 16:14;dyaraev;input.json.gz;https://issues.apache.org/jira/secure/attachment/13053927/input.json.gz","16/Dec/22 16:14;dyaraev;input.json.zip;https://issues.apache.org/jira/secure/attachment/13053925/input.json.zip",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 10 10:11:58 UTC 2023,,,,,,,,,,"0|z1dfpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 09:57;echauchot;[1.15 doc|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/filesystem] states that 
{code:java}
Note JSON format for file system connector is not a typical JSON file but uncompressed newline delimited JSON
{code}
So not really a bug I guess but more a feature to add.;;;","09/Dec/22 10:25;snilard;[~echauchot] But then the Flink should decline to even open such compressed file. This state when it opens these files but read only part of these is super super confusing.;;;","09/Dec/22 10:27;dyaraev;[~echauchot] Thank you for pointing this out. To be honest I saw that statement, but since reading partially works and there is no error messages saying such files/formats are not supported, my assumption was that this functionality is there.


I think it's a fairly common use case when files like JSON or CSV are stored compressed to save disk space. It is a bit surprising that this functionality is not supported. So if it's not a bug and there isn't a ticket for this feature in JIRA, it would be great if this issue could be converted to an improvement.;;;","09/Dec/22 13:02;echauchot;Yes I agree, something needs to be done here. I'll take a look at how compressed files are handled in general regardless of the format of the file. But my guess is that we would need to uncompress them to a resilient distributed storage to have the proper size and be able to split.;;;","09/Dec/22 13:13;echauchot;Actually despite the comment on uncompressed json above in the doc, in [another part of the doc|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/dataset/overview/#read-compressed-files] it is mentioned that reading compressed files is transparent. So I think that the bug label is suited.;;;","09/Dec/22 14:37;dyaraev;??We would need to uncompress them to a resilient distributed storage to have the proper size and be able to split.??

Do we really have to do this? Won't it bring additional overhead? I would say that we could consider such files non-splittable and parallelize jobs on the file level. We cannot parallelize decompression of text files (JSON, CSV, etc.), so it will anyway be done on a single worker. If we read from the compressed stream until end, we will not need uncompressed file size. As soon as files are decompressed we can re-distribute the data using *{{rebalance}}* or *{{shuffle}}* operations.;;;","09/Dec/22 15:48;echauchot;yes that is what I had in mind to uncompress with parallelism 1 and then redistribute the uncompressed data to read in parallel but it has overhead indeed that might not be worth it. That was more of a proposition to start the discussion. But indeed, the other projects I work on treat the compressed files as non-splittable and the parallelization is indeed at the file level.;;;","14/Dec/22 18:12;echauchot;[~dyaraev] I tried to reproduce the behavior you observed [with this code|https://github.com/echauchot/flink-samples/blob/master/src/main/java/org/example/FilesystemJsonSQL.java]. It appears that the json flink-SQL connector tries to read the compressed file as json leading to an error as binary is not text. So I checked in the doc I linked [above|https://issues.apache.org/jira/browse/FLINK-30314?focusedCommentId=17645294&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17645294]  (https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/dataset/overview/#read-compressed-files) and I realized that I linked the old Dataset doc. For SQL the filesystem connector does not seem to support compressed files (https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/filesystem/).;;;","15/Dec/22 08:39;dyaraev;[~echauchot] Yes, you are right about the docs. The page is related to the DataSet API. But I am a bit surprised that you couldn't reproduce the issue because in my case there were no errors. I will check it with your code sample. Could you also share data files that you used for testing?;;;","15/Dec/22 08:50;echauchot;[~dyaraev] the input file I used is already in the repo in _src/main/resources_. It is the one for the passing case (10 json entries successfully read and output to csv). I just updated the same file zip version that generates a reading issue (reading binary as json). You should get this stacktrace:

{code:java}
Caused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParseException: Invalid UTF-8 start byte 0x81
 at [Source: (byte[])""PK ��U        �  ""; line: 1, column: 13]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:710)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidInitial(UTF8StreamJsonParser.java:3606)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser._decodeCharForError(UTF8StreamJsonParser.java:3349)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3581)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2688)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:870)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:762)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4622)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:3056)
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserializeToJsonNode(JsonRowDataDeserializationSchema.java:127)
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:116)
	... 14 more
{code}
;;;","15/Dec/22 08:51;echauchot;And by the way I used the latest released version of Flink: 1.16.0;;;","16/Dec/22 10:22;martijnvisser;I've changed this to an Improvement because like the docs mention, JSON on FileSystem is only supported with uncompressed newline delimited JSON. 

Adding support for compressed files is fine, but then it shouldn't be done for JSON only, but I would expect that as a general capability for all formats. The FileSource implementation forRecordStreamFormat already supports de-compression https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/FileSource.html;;;","16/Dec/22 15:33;echauchot;Thanks [~martijnvisser] I wanted to change this label but I did not have the rights;;;","16/Dec/22 16:09;dyaraev;[~echauchot], [~martijnvisser] Thank you for your efforts in verifying the issue. It would be great to have this implemented (as a bug fix or a new feature). As promised, I ran several tests using the code from the above repository and I was able to reproduce the issue with version 1.16.0 as well. I changed the code a bit, so it could write output to the console:
{code:java|title=TestCompressedJson.java|borderStyle=solid}
package examples.flinktest;

import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;

public class TestCompressedJson {

    public static void main(final String[] args) {
        final String inputDir = args[0];

        // init Table Env
        final EnvironmentSettings environmentSettings = EnvironmentSettings
                .newInstance()
                .inBatchMode()
                .build();
        
        final TableEnvironment tableEnv = TableEnvironment.create(environmentSettings);

        // create source
        tableEnv.executeSql(String.format(
                ""CREATE TEMPORARY TABLE SourceTable (my_field1 BIGINT, my_field2 INT, my_field3 VARCHAR(2147483647)) "" +
                        ""WITH ('connector' = 'filesystem', 'path' = '%s', 'format' = 'json')"",
                        inputDir));

        // create sink
        tableEnv.executeSql(
                ""CREATE TEMPORARY TABLE SinkTable (my_field1 BIGINT, my_field2 INT, my_field3 VARCHAR(2147483647)) "" +
                        ""WITH ('connector' = 'print')"");

        final Table sourceTable = tableEnv.from(""SourceTable"");
        sourceTable.executeInsert(""SinkTable"");
    }
}{code}
Also, I added a few more lines to the data file so that it contained 30 lines. Then I ran the code with 3 different inputs (the data files can be found in attachments):
 # input.json (without compression)
 # input.json.gz
 # input.json.zip

There were no errors for any of the runs, and the outputs were as follows:
{code:java|title=input.json|borderStyle=solid}
4> +I[4646464, 654, test0]
4> +I[4646464, 654, test1]
4> +I[4646464, 654, test2]
4> +I[4646464, 654, test3]
4> +I[4646464, 654, test4]
4> +I[4646464, 654, test5]
4> +I[4646464, 654, test6]
4> +I[4646464, 654, test7]
4> +I[4646464, 654, test8]
4> +I[4646464, 654, test9]
4> +I[4646464, 654, test10]
4> +I[4646464, 654, test11]
4> +I[4646464, 654, test12]
4> +I[4646464, 654, test13]
4> +I[4646464, 654, test14]
4> +I[4646464, 654, test15]
4> +I[4646464, 654, test16]
4> +I[4646464, 654, test17]
4> +I[4646464, 654, test18]
4> +I[4646464, 654, test19]
4> +I[4646464, 654, test20]
4> +I[4646464, 654, test21]
4> +I[4646464, 654, test22]
4> +I[4646464, 654, test23]
4> +I[4646464, 654, test24]
4> +I[4646464, 654, test25]
4> +I[4646464, 654, test26]
4> +I[4646464, 654, test27]
4> +I[4646464, 654, test28]
4> +I[4646464, 654, test29]{code}
{code:java|title=input.json.gz|borderStyle=solid}
4> +I[4646464, 654, test0]
4> +I[4646464, 654, test1]
4> +I[4646464, 654, test2] {code}
There was no output for input.json.zip. It just silently exited with no output or errors. I would still keep this ticket as a bug.;;;","16/Dec/22 16:21;dyaraev;[~martijnvisser] I absolutely agree with you that the issue is not only about JSON. CSV and maybe some other formats are also affected. So the fix (or an improvement) should include support for all compressed formats. ;;;","16/Dec/22 17:04;martijnvisser;[~dyaraev] Am I understand it correctly that input.json.gz outputted 3 lines instead of 30? But it did generate output? ;;;","16/Dec/22 17:19;dyaraev;[~martijnvisser] Yes, that's exactly what happened. As I wrote in the ticket description, the problem is that Flink stops reading the file once the reading position exceeds the compressed file size, which is wrong. It should either rely on the uncompressed file size or read to the end of the stream. I think the latter would be better because we wouldn't have to calculate the uncompressed file size. 

As far as I understand this is not an easy fix as there are multiple dependencies, but I think in case of an unsupported format at least an exception should be thrown.;;;","16/Dec/22 17:24;martijnvisser;I'm really curious if this works as expected for DataStream use cases. But that's to check for later :);;;","16/Dec/22 17:30;echauchot;_> Martijn Visser I absolutely agree with you that the issue is not only about JSON. CSV and maybe some other formats are also affected. So the fix (or an improvement) should include support for all compressed formats._ 

Sure, it needs to be dealt with at the SQL Filesystem connector level.;;;","16/Dec/22 18:40;echauchot;[~dyaraev] I updated the code with the output to console and committed your input files. See https://github.com/echauchot/flink-samples
Here is what I observe:
* First, there is no silent fail while reading the zip version: I do see in the log the stacktrace I mentioned above
* Then, there is indeed a problem when reading the gz version: 3 records are output whereas there should be a similar error;;;","16/Dec/22 23:07;dyaraev;[~echauchot] Thanks. I fixed logging in my test app and now I can see the exception which is thrown when zipped JSON is read. The only problem is that the application itself doesn't fail. There are exceptions in the log but the application still exits with code 0, which is strange. Another confusing thing is that the exception says nothing about unsupported formats because it is an internal exception of the Jackson parser. I would say that in both cases (gz and zip) an appropriate error with an informative message about unsupported file format should be thrown.

Also, I checked why it comes to the JSON parser without trying to decompress the stream and found that ZIP is not on the list of supported compression codecs (can be found in debug mode on [this line|https://github.com/apache/flink/blob/af6eff873a53bbdc85a2b1018140754e65758e3e/flink-core/src/main/java/org/apache/flink/api/common/io/FileInputFormat.java#L153]). The list includes the following formats:
 * xz
 * deflate
 * gz
 * gzip
 * zst
 * bz2

As we can see, it shouldn't even try to read this ZIP file because it's not supported. But it does try to read it uncompressed and even decides that the file should be processed by the Jackson parser. Probably it is another bug.;;;","16/Dec/22 23:18;dyaraev;I decided also to try *{{bz2}}* and *{{xz}}* and in both cases only 3 out of 30 records were returned. Btw, *{{xz}}* requires an additional [dependency|https://mvnrepository.com/artifact/org.tukaani/xz/1.9]. I could not find information about it in the docs. ;;;","19/Dec/22 09:22;echauchot;Thanks [~dyaraev] for the further tests. Yes, indeed, there seems to be several strange things that we need to take care of.;;;","28/Dec/22 04:02;jkf6160@163.com;Hello, everyone. This problem also occurs in 1.14.3 when reading gz files.How is the progress ?  Thanks

 
CREATE TABLE MyUserTable (
     column_name1 STRING
) WITH (
     'connector'='filesystem',
     'path'='xxx.gz',
     'format'='raw'
);;;;","12/Jan/23 15:42;echauchot;[~martijnvisser] can you change the label to bug please as the behavior for gz, xz and bz2 is incorrect (output 3 out of 30 records instead of throwing an unsupported exception). Can you assign me the ticket ?;;;","17/Jan/23 09:21;martijnvisser;[~echauchot] Done :);;;","17/Jan/23 09:23;echauchot;[~martijnvisser] thanks. I finish the current work on operator state compression and then I tackle this other subject;;;","05/Sep/23 11:48;echauchot;[~dyaraev] Sorry for resuming this late. I have investigated the code. The enumerator creates a split per file which is fine then the reader reads the file as a single split. The compression is transparent to the user through a decorator that comes in play just before the actual reading process. What happens is what I thought, the boundaries of the split are evaluated based on the size in bytes of the *compressed* file therefore stopping the reading before the end of the uncompressed file. 

I'll look at a good way to fix it so that. It will be fixed for all file based readers no matter the format.;;;","07/Sep/23 08:47;echauchot;Made the ticket description more general (the problem is with delimited input format being splittable but not when it is compressed). Added tests on newer flink versions and reduced the module scope to just file-connector (the problem is on the file-connector in ganeral that is use for table but also for other APIs).

The problem is with DelimitedInputFormat as none of its subclasses call FileInputFormat#createSplits (that would detect that the file is non-splittable and deal with reading boundaries correctly), they all use FileSource in org.apache.flink.connector.file.src that creates its own splits;;;","07/Sep/23 10:13;echauchot;Ticket will be fixed with [this|https://issues.apache.org/jira/browse/FLINK-33059] feature;;;","10/Nov/23 10:11;echauchot;Fixed by [FLINK-33059|https://issues.apache.org/jira/browse/FLINK-33059];;;"
Flink Kubernetes Operator seems to rely on 1.15 k8s HA code but 1.16 config code,FLINK-30313,13510060,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,mbalassi,mapohl,mapohl,06/Dec/22 13:40,07/Dec/22 14:56,04/Jun/24 20:41,07/Dec/22 14:47,kubernetes-operator-1.2.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,,,,"Based on [this SO post|https://stackoverflow.com/questions/74599009/flink1-16-high-availability-job-manager], it looks like there can be a setup where we use different versions of Flink:
{code}
2022-11-28 08:57:56.032 [main] INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint  - Shutting KubernetesApplicationClusterEntrypoint down with application status FAILED. Diagnostics java.lang.NoSuchFieldError: USE_OLD_HA_SERVICES
        at org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.createHAServices(KubernetesHaServicesFactory.java:37)
        at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:296)
        at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:139)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:439)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:382)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
        at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.main(KubernetesApplicationClusterEntrypoint.java:86)
{code}

This field was removed in FLINK-25806 with Flink 1.16. I'd guess that it's caused by the Flink operator still depending on Flink 1.15.x whilest there is an option to use Flink 1.16 for deployments already.

Another SO user mentioned:
{quote}
I have the same issue with flink-kubernetes-operator. This field NoSuchFieldError: USE_OLD_HA_SERVICES was removed in flink 1.16, but error occurs with setting: flinkVersion: v1_15
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25806,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 14:56:42 UTC 2022,,,,,,,,,,"0|z1dfog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 13:53;mbalassi;Imho FLINK-30222 solves this, [~mapohl] please confirm.;;;","06/Dec/22 14:20;mapohl;Thanks for the pointer [~mbalassi_impala_2d74]. Can you elaborate a bit more how FLINK-30222 fixes this Jira issue? AFAIU, FLINK-30222 is about supporting the newly introduced {{kubernetes}} label to load k8s HA services. This Jira issue, though, is about different Flink modules ({{flink-core}} providing {{HighAvailabilityOptions.USE_OLD_HA_SERVICES}} and {{flink-kubernetes}} trying to access this field through {{KubernetesHaServicesFactory}}) being based on different Flink versions. Based on the error message, it appears that we have {{flink-core:1.16.0}} and {{flink-kubernetes:1.15.x}} on the classpath. ;;;","07/Dec/22 12:12;gyfora;At this point operator should not contain any 1.16 dependencies on the classpath so I don’t see how this error may occur. Especially not the 1.2.0 version which was released before Flink 1.16.0 I think.

could something in the user image cause this problem?;;;","07/Dec/22 12:19;gyfora;[~mapohl] coild you reproduce this locally?

maybe you could do a mvn dependency:tree check to see if there are any unwanted 1.16 deps .;;;","07/Dec/22 12:24;mapohl;Ok, thanks for clarification. I couldn't find any hints in the k8s operator code, either. The {{affectedVersion}} in this ticket was just a guess from my side based on the Flink versions being involved. I asked the SO user to share more context in this ticket about this issue.;;;","07/Dec/22 14:17;mbalassi;The operator depends on 1.15.3 of flink-core:
{code:java}
% mvn dependency:tree | grep flink-core     
[INFO] |  +- org.apache.flink:flink-core:jar:1.15.3:provided
[INFO] +- org.apache.flink:flink-core:jar:1.15.3:compile
[INFO] +- org.apache.flink:flink-core:jar:1.15.3:compile
[INFO] |  +- org.apache.flink:flink-core:test-jar:tests:1.15.3:test
[INFO] |  +- org.apache.flink:flink-core:jar:1.15.3:provided
[INFO] |  +- org.apache.flink:flink-core:jar:1.15.3:provided
[INFO] |  +- org.apache.flink:flink-core:jar:1.15.3:provided
[INFO]    +- org.apache.flink:flink-core:jar:1.15.3:compile
 {code}
That still has this config option:
[https://github.com/apache/flink/blob/release-1.15/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java#L229]

1.16 removes it via this:
[https://github.com/apache/flink/pull/19438/files#diff-3cf7dccc898a0b78b70f65ad51019e44d0221bc59aa9918d21260fd9c883d2b2L220-L238]

I did not have the bandwidth to try to reproduce it yet, [~gaborgsomogyi] please take a look.;;;","07/Dec/22 14:47;mbalassi;We jumped on a call to try to reproduce this with [~gyfora]. 

We could not reproduce the issue using the 1.16 flink image with the latest operator build and the [basic checkpointing example|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-checkpoint-ha.yaml]. We tried both the new and old HA [configs|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-checkpoint-ha.yaml#L30].

As an educated guess most probably the user packages flink-core 1.16 into their user jar and that is causing this.

 ;;;","07/Dec/22 14:56;mapohl;Thanks for looking into it. It might be worth it responding in the SO thread since it appears that there were two users running into the same problem.;;;",,,,,,,,,,,,,,,,,,,,,,,
Remove cassandra connector from master branch,FLINK-30312,13510034,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/Dec/22 10:27,07/Dec/22 08:18,04/Jun/24 20:41,07/Dec/22 08:18,,,,,,1.17.0,,,,,Connectors / Cassandra,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 08:18:09 UTC 2022,,,,,,,,,,"0|z1dfio:",9223372036854775807,The Cassandra connector has been externalized and is no longer released as part of the main Flink release.,,,,,,,,,,,,,,,,,,,"07/Dec/22 08:18;chesnay;master: 12cb2212f7836fe668edc0523f4f4e2efbeaf8de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CI error: Back-off pulling image ""flink:1.14""",FLINK-30311,13510028,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,pvary,pvary,06/Dec/22 09:43,22/Jan/24 13:52,04/Jun/24 20:41,22/Jan/24 13:52,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"CI failed with: {{Flink Deployment failed 2022-12-06T08:45:03.0244383Z org.apache.flink.kubernetes.operator.exception.DeploymentFailedException: Back-off pulling image ""flink:1.14""}}

We should find the root cause of this issue and try to mitigate it.

[https://github.com/apache/flink-kubernetes-operator/actions/runs/3627824632/jobs/6118131271]

 
{code:java}
2022-12-06T08:45:03.0243558Z [m[33m2022-12-06 08:41:44,716[m [36mo.a.f.k.o.c.FlinkDeploymentController[m [1;31m[ERROR][default/flink-example-statemachine] Flink Deployment failed
2022-12-06T08:45:03.0244383Z org.apache.flink.kubernetes.operator.exception.DeploymentFailedException: Back-off pulling image ""flink:1.14""
2022-12-06T08:45:03.0245385Z 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.checkContainerBackoff(AbstractFlinkDeploymentObserver.java:194)
2022-12-06T08:45:03.0246604Z 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeJmDeployment(AbstractFlinkDeploymentObserver.java:150)
2022-12-06T08:45:03.0247780Z 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:84)
2022-12-06T08:45:03.0248934Z 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:55)
2022-12-06T08:45:03.0249941Z 	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:56)
2022-12-06T08:45:03.0250844Z 	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:32)
2022-12-06T08:45:03.0252038Z 	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:113)
2022-12-06T08:45:03.0252936Z 	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
2022-12-06T08:45:03.0253850Z 	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
2022-12-06T08:45:03.0254412Z 	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
2022-12-06T08:45:03.0255322Z 	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
2022-12-06T08:45:03.0256081Z 	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
2022-12-06T08:45:03.0256872Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
2022-12-06T08:45:03.0257804Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
2022-12-06T08:45:03.0258720Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
2022-12-06T08:45:03.0259635Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
2022-12-06T08:45:03.0260448Z 	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
2022-12-06T08:45:03.0261070Z 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2022-12-06T08:45:03.0261595Z 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2022-12-06T08:45:03.0262005Z 	at java.base/java.lang.Thread.run(Unknown Source) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:52:05 UTC 2024,,,,,,,,,,"0|z1dfhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:52;gaborgsomogyi;See comment in FLINK-30310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-enable e2e test error check,FLINK-30310,13510008,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,06/Dec/22 07:02,22/Jan/24 13:49,04/Jun/24 20:41,22/Jan/24 13:49,,,,,,,,,,,Kubernetes Operator,,,,0,,,,In FLINK-30307 e2e test error check has been turned off temporarily. We must re-enable it after release.,,,,,,,,,,,,FLINK-30117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:48:46 UTC 2024,,,,,,,,,,"0|z1dfcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:48;gaborgsomogyi;There are too many false positives because of negative test cases and its not realistic to be so strict that error typed messages can't appear in the operator log so closing this with won't do and remove this dead code part.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the ability to supply custom SslContexts for netty client/servers.,FLINK-30309,13509989,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,SteveNiemitz,SteveNiemitz,06/Dec/22 03:40,20/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Network,,,,0,auto-deprioritized-major,pull-request-available,,"The existing flink configuration supports on simple SSL configuration via keystore/truststore configuration.  For more advanced configuration options it is desirable to be able to instead provide the entire pre-configured SslContext, as well as all them to be reloaded if needed (eg, the certificate material rotates while running).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:03 UTC 2023,,,,,,,,,,"0|z1df8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException: class java.io.ObjectStreamClass$Caches$1 cannot be cast to class java.util.Map is showing in the logging when the job shutdown,FLINK-30308,13509979,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,06/Dec/22 02:11,09/Jan/23 09:50,04/Jun/24 20:41,09/Jan/23 09:50,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,0,pull-request-available,,,"{code:java}
2022-12-05 18:26:40,229 WARN  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Failed to clean up the leaking objects.
java.lang.ClassCastException: class java.io.ObjectStreamClass$Caches$1 cannot be cast to class java.util.Map (java.io.ObjectStreamClass$Caches$1 and java.util.Map are in module java.base of loader 'bootstrap')
        at org.apache.flink.streaming.api.utils.ClassLeakCleaner.clearCache(ClassLeakCleaner.java:58) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.api.utils.ClassLeakCleaner.cleanUpLeakingClasses(ClassLeakCleaner.java:39) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.close(AbstractPythonFunctionOperator.java:142) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.api.operators.python.AbstractExternalPythonFunctionOperator.close(AbstractExternalPythonFunctionOperator.java:73) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930) [flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15.2.jar:1.15.2]
        at java.lang.Thread.run(Unknown Source) [?:?]{code}

Reported in Slack: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1670265131083639?thread_ts=1670265114.640369&cid=C03G7LJTS2G",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 09:50:27 UTC 2023,,,,,,,,,,"0|z1df6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 09:50;dianfu;Fixed in:
- master via 8aa446a8aa71e1d03986970585dacd94e1a4182d
- release-1.16 via bbd7b2c0d2c9be0c42b7c4e8dcb8200a4bd4a285
- release-1.15 via 01127bc254ccceb4ed32afe3c130fe86cc9b3d96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Turn off e2e test error check temporarily,FLINK-30307,13509910,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,05/Dec/22 16:31,06/Dec/22 09:09,04/Jun/24 20:41,06/Dec/22 09:09,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 09:09:32 UTC 2022,,,,,,,,,,"0|z1der4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 09:09;mbalassi;b95e3e41fbac940fa281b69d05df32ef02591691 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Audit utils can expose potentially sensitive information,FLINK-30306,13509909,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,asardaes,asardaes,05/Dec/22 16:27,22/Feb/23 13:06,04/Jun/24 20:41,,kubernetes-operator-1.2.0,,,,,,,,,,Kubernetes Operator,,,,0,,,,"I see events being logged by {{org.apache.flink.kubernetes.operator.listener.AuditUtils}} along the lines of "">>> Event  | Info    | SPECCHANGED     | UPGRADE change(s) detected"". This logs the entire new spec, which can contain sensitive information that has been injected from a Kubernetes secret.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 23 11:02:54 UTC 2023,,,,,,,,,,"0|z1deqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 17:40;gyfora;Cc @morhidi ;;;","05/Dec/22 17:43;gyfora;Why would the spec contain injected sensitive info? Wouldn’t those be generally injected at runtime to the Flink jobs instead? Also if the person has access to the operator logs then probably has access to the FLinkDeployment CRs too right?;;;","05/Dec/22 18:04;asardaes;In my case, there are some passwords that we encrypt and are then injected by Argo CD into the {{FlinkDeployment}} resource, and I can see them in the {{AuditUtils}} logs. Moreover, we forward logs from containers to other infrastructure to facilitate searching, so even though that remains internal, it increases the ""exposed area""; if someone could access the searchable logs, that doesn't mean they have access to the Kubernetes cluster.;;;","08/Dec/22 10:35;mbalassi;We should introduce sensible redaction to cover this.;;;","23/Jan/23 11:02;asardaes;A few more thoughts from me. I'm not sure if it's feasible to cover all scenarios with redaction. Take for example the name of the (potentially internal) repository used in the resource's {{image}}, should that be redacted? Should redaction rules be configurable?

For the scenario I initially reported, I did end up changing my resource to use {{valueFrom -> secretKeyRef}} since that covered my needs, but similar to the above, does that really cover all scenarios? Imagine someone wants to inject custom, sensitive values into the {{FlinkDeployment}}'s {{flinkConfiguration}} map --- if they are custom, it's not possible to have hardcoded rules in the operator.

As a side note, regarding this last point, I wonder if that's common (I personally don't do that). Maybe the operator could support a syntax similar to Kubernetes, along the lines of:

{noformat}
flinkConfiguration:
  my.custom.key:
    valueFrom:
      secretKeyRef: ...
{noformat}

In that case, the operator would inject the values, but I imagine it might not be possible to do it _after_ {{AuditUtils}} has logged the change. In any case, that's just me thinking aloud, I'm not sure anyone would actually need that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Operator deletes HA metadata during stateful upgrade, preventing potential manual rollback",FLINK-30305,13509908,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,asardaes,asardaes,05/Dec/22 16:24,06/Dec/22 23:03,04/Jun/24 20:41,06/Dec/22 23:03,kubernetes-operator-1.2.0,,,,,,,,,,Kubernetes Operator,,,,1,,,,"I was testing resiliency of jobs with Kubernetes-based HA enabled, upgrade mode = {{savepoint}}, and with _automatic_ rollback _disabled_ in the operator. After the job was running, I purposely created an erroneous spec by changing my pod template to include an entry in {{envFrom -> secretRef}} with a name that doesn't exist. Schema validation passed, so the operator tried to upgrade the job, but the new pod hangs with {{CreateContainerConfigError}}, and I see this in the operator logs:

{noformat}
>>> Status | Info    | UPGRADING       | The resource is being upgraded
Deleting deployment with terminated application before new deployment
Deleting JobManager deployment and HA metadata.
{noformat}

Afterwards, even if I remove the non-existing entry from my pod template, the operator can no longer propagate the new spec because ""Job is not running yet and HA metadata is not available, waiting for upgradeable state"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 23:02:17 UTC 2022,,,,,,,,,,"0|z1deqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 18:07;gyfora;I think there are a mix of things happening here. And it may not be a bug.

when you use savepoint upgrademode the job actually completes and HA metadata is automatically deleted by Flink itself. So the first upgrading log message is fine.

 

the problem with submitting incorrect pod template is that if the job never starts the operator does not know why HA metadata is not available and not sure if it’s safe to retry ;;;","05/Dec/22 18:09;gyfora;You could try doing the same flow but originally using the last-state upgradeMode, that should not result in the same error I believe just because that preserves the HA metadata during the upgrade because it simply deletes the Deployment object ;;;","05/Dec/22 18:17;asardaes;Since I saw ""Deleting JobManager deployment and HA metadata"" in the logs, I thought it had been the operator, but if that's done by Flink itself then I guess there's not much the operator can do. I prefer savepoint upgrades because it shuts down the job cleanly, but if it's impossible to detect these scenarios, I'd have to use an alternative. Nevertheless, couldn't the operator detect that there was a successful savepoint and the job hasn't started since, allowing further spec changes?;;;","05/Dec/22 18:22;gyfora;We haven’t yet found a completely reliable way to detect if the job/jobmanager started successfully (before failing) . This is definitely on our radar but if you have some ideas we are happy to hear. There are only a few types of errors that lead to this (when the JM fails to start completely) so currently we took the safer approach and throw an error;;;","05/Dec/22 21:49;asardaes;I understand why this is complex for all scenarios, but I'd say some concrete ones could be addressed. Even in the current version, the documentation states:

bq. When Kubernetes HA is enabled, the savepoint upgrade mode may fall back to the last-state behaviour in cases where the job is in an unhealthy state.

If Flink itself is deleting the HA CMs, how is this logic implemented in the operator? It would never be able to fall back if the CMs are missing, no?;;;","05/Dec/22 22:08;gyfora;I recommend checking the getAvailableUpgradeMode method of the ApplicationReconciler for details of the logic.

Flink only removes HA metadata for terminal jobs. Completed/finished/failed/cancelled, . A normally starting/running/failing/restarting job will have HA metadata so we can use it. However if you stopped with savepoint (that is a terminal state) and then the job doesn’t even get into starting state (the JM pod cannot be created) then you have the strange hard to recover scenario that you are experiencing.

Savepoint only works for running jobs but last state works for any non terminal running/failing/created/restarting etc states which is the large majority of the cases.

so the current behavior is actually much better than you think;;;","05/Dec/22 23:50;asardaes;I briefly looked through the code on the main branch, but I see that [this line in ApplicationReconciler|https://github.com/apache/flink-kubernetes-operator/blob/d382c74ea04fbe17ab41f42559d663d55d21763a/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java#L182] sets the {{deleteHaConfigMaps}} flag to true when calling {{deleteCluster}}.

Now, I'm not very familiar with Flink's internals, but a few lines above the one I linked I see that {{SavepointConfigOptions.SAVEPOINT_PATH}} is set in the deployment config if a savepoint is found. Would Flink still honor the configured {{SavepointConfigOptions.SAVEPOINT_PATH}} even if it finds HA metadata?

Regardless of the answer to that question, let's assume for a moment that we follow the scenario I described but the HA CMs are _not_ deleted. I could roll back my changes and the operator would detect said CMs, so even though the job didn't run successfully again, the operator would propagate the rolled-back spec and let the JM continue from the checkpoint stored in the HA metadata (savepoint information is not stored in HA metadata, right?). In this case the savepoint goes basically unused.

I imagine this is also suboptimal, since rolling back and using a checkpoint that is potentially older than the savepoint means some data could be re-processed and create duplicates. So I could agree that it's probably best to let the user clean up manually and maybe use the savepoint as initial savepoint, but I still don't understand why the operator deletes the HA CMs in this case; if they were kept, the user could still decide - either rollback and use the checkpoint, or do manual cleanup and set the latest savepoint as initial.;;;","06/Dec/22 01:07;gyfora;When you execute stop with savepoint operation in Flink, the job terminates and HA metadata is automatically removed By FLINK.

you could set the remove ha metadata flag to false in the operator it makes no difference in practice…

 

i think that flag confuses you a little but in reality you cannot have both HA metadata AND a stop with savepoint operation at the same time.;;;","06/Dec/22 15:16;asardaes;All right, understood. Then, forgetting that entirely, the scenario would be the following:

# {{upgradeMode=savepoint}}
# HA metadata unavailable
# Job in unhealthy state

The operator currently gets stuck ""waiting for upgradeable state"", so it's not possible to alter the {{FlinkDeployment}} resource anymore.

* What are the limitations that prevent the operator from allowing resource modifications in this case?
* If conditions 1 and 2 are given, is the root cause for 3 relevant?

I would imagine that the critical question for the operator would be: how to be sure that the job is unealthy and cannot recover on its own? Finding the answer could use sophisticated methods: we could check {{unavailableReplicas}} in the JM Deployment's status, maybe also check {{restartCount}} of {{flink-main-container}} in the JM pod(s), etc.

Alternatively, the method could be very simple: regardless of automatic-rollback configuration, wait {{kubernetes.operator.deployment.readiness.timeout}}. If the job doesn't reach a healthy state, allow further spec changes while still respecting {{lastSavepoint}} and {{savepointHistory}}.

Do you see big problems with these ideas?;;;","06/Dec/22 15:29;gyfora;i think you are missing the crucial problem here. You have to guarantee that a job always restores from the very latest checkpoint otherwise some sink/other specific mechanisms may fail for the user and can cause duplication or data loss.

 

it would be easy to implement some naive logic that restores based on the last observed savepoint but that’s dangerous and it’s going to break prod jobs. Maybe not for you specifically but we have to consider all use cases.

 

if we submitted a job from the operator as long as it’s running (or we cannot determine the exact state) the HA metadata is the only reliable source of information of the latest checkpoint. Going around it introduces all kinds of tricky cornercases.

 

as I said earlier the only real solution we need is to somehow figure out that the JM never started. That would solve all our problems. Relying on the HA metadata once the job is submitted is required for correctness.;;;","06/Dec/22 22:32;asardaes;So, for example, if HA is not even enabled for a job, the lack of HA metadata is not enough to determine whether the job ever ran or not, so it's not possible to know if the savepoint from the status is actually representative the newest state anymore.;;;","06/Dec/22 22:52;gyfora;That’s correct , the savepoint from the status is only guaranteed to be latest if the job was observed in a terminal state.;;;","06/Dec/22 23:02;asardaes;I see, I guess I'll close this since potential improvements would be unrelated anyway, thanks.;;;",,,,,,,,,,,,,,,,,,
Possible Deadlock in Kinesis/Firehose/DynamoDB Connector,FLINK-30304,13509907,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dannycranmer,dannycranmer,dannycranmer,05/Dec/22 16:12,16/Jan/23 08:58,04/Jun/24 20:41,16/Jan/23 08:58,1.15.3,1.16.0,aws-connector-3.0.0,aws-connector-4.0.0,,1.15.4,1.16.1,1.17.0,aws-connector-3.1.0,aws-connector-4.1.0,Connectors / DynamoDB,Connectors / Firehose,Connectors / Kinesis,,0,,,,"AWS Sinks based on Async Sink can enter a deadlock situation if the AWS async client throws error outside of the future. We observed this with a local application:
{code:java}
java.lang.NullPointerException
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.utils.NettyUtils.closedChannelMessage(NettyUtils.java:135)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.utils.NettyUtils.decorateException(NettyUtils.java:71)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.handleFailure(NettyRequestExecutor.java:310)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.makeRequestListener(NettyRequestExecutor.java:189)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.CancellableAcquireChannelPool.lambda$acquire$1(CancellableAcquireChannelPool.java:58)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.HealthCheckedChannelPool.ensureAcquiredChannelIsHealthy(HealthCheckedChannelPool.java:114)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.HealthCheckedChannelPool.lambda$tryAcquire$1(HealthCheckedChannelPool.java:97)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:502)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
at org.apache.flink.kinesis.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
at org.apache.flink.kinesis.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at java.base/java.lang.Thread.run(Thread.java:829){code}
Related AWS SDK issues that can cause this:
 * [https://github.com/aws/aws-sdk-java-v2/issues/3435]
 * [https://github.com/aws/aws-sdk-java-v2/issues/1812]

If an error is thrown and not handled by the future then the AsyncSink will never decrement {{{}inFlightRequestCount{}}}. the job will hang while trying flush for checkpoint

 

!sink-deadlock.png|width=736,height=374!",,,,,,,,,,,,,,,,,FLINK-30633,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/22 22:38;dannycranmer;sink-deadlock.png;https://issues.apache.org/jira/secure/attachment/13053546/sink-deadlock.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 08:58:51 UTC 2023,,,,,,,,,,"0|z1deqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 22:37;dannycranmer;I do not think we can do anything about this besides wait for the fix in the AWS SDK. Essentially there is an error in the Async client which results in the Future not being completed. This [issue](https://github.com/aws/aws-sdk-java-v2/pull/3574) should fix it once merged.

One way around this is to keep track of inflight request time, and fail the job (or retry) upon some timeout.;;;","06/Dec/22 17:06;chalixar;Thanks [~dannycranmer] for reporting the issue. The root cause seems correct to me.

{{- I do not think we can do anything about this besides wait for the fix in the AWS SDK.}}

I agree,I can't find a quick workaround from the concrete writer side.

- {{One way around this is to keep track of inflight request time, and fail the job (or retry) upon some timeout.}}

I think this might need more investigation as it might break some existing implementations, unless we enable sink implementer to override but that would not be backward compatible, wdyt?
;;;","06/Dec/22 17:36;dannycranmer;Thanks [~chalixar] ;;;","11/Jan/23 09:11;dannycranmer;This has been fixed in SDK v2.19.14, upgrade in FLINK-30633;;;","16/Jan/23 08:58;dannycranmer;Fixed with SDK upgrade FLINK-30633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support max column width in sql client,FLINK-30303,13509894,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,05/Dec/22 15:09,05/Dec/22 15:09,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Table SQL / Client,,,,0,,,,"Currently user can use `sql-client.display.max-column-width` to set column width in sql-client in streaming, this should be supported in batch too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-05 15:09:32.0,,,,,,,,,,"0|z1denk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WatermarkAssignerChangelogNormalizeTransposeRuleTest failed,FLINK-30302,13509861,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,05/Dec/22 11:19,14/Aug/23 13:31,04/Jun/24 20:41,14/Aug/23 13:31,1.17.0,,,,,,,,,,Table SQL / Planner,,,,0,stale-major,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43715&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11792

{code}
Dec 05 04:26:28 [ERROR] org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.testPushdownCalcNotAffectChangelogNormalizeKey  Time elapsed: 0.601 s  <<< FAILURE!
Dec 05 04:26:28 org.junit.ComparisonFailure: 
Dec 05 04:26:28 optimized rel plan expected:<...c(select=[a, b, f], [changelogMode=[I])
Dec 05 04:26:28 +- TemporalJoin(joinType=[InnerJoin], where=[AND(=(a, a0), __TEMPORAL_JOIN_CONDITION(ingestion_time, ingestion_time0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(a0), __TEMPORAL_JOIN_LEFT_KEY(a), __TEMPORAL_JOIN_RIGHT_KEY(a0)))], select=[ingestion_time, a, b, ingestion_time0, a0, f], changelogMode=[I])
Dec 05 04:26:28    :- Exchange(distribution=[hash[a]], changelogMode=[I])
Dec 05 04:26:28    :  +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time], changelogMode=[I])
Dec 05 04:26:28    :     +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, b], changelogMode=[I])
Dec 05 04:26:28    :        +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, ingestion_time], changelogMode=[I])
Dec 05 04:26:28    +- Exchange(distribution=[hash[a]], changelogMode=[I,UB,UA,D])
Dec 05 04:26:28       +- Calc(select=[ingestion_time, a, f], where=[f], changelogMode=[I,UB,UA,D])
Dec 05 04:26:28          +- ChangelogNormalize(key=[a], changelogMode=[I,UB,UA,D])
Dec 05 04:26:28             +- Exchange(distribution=[hash[a]], changelogMode=[I,UA,D])
Dec 05 04:26:28                +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time], changelogMode=[I,UA,D])
Dec 05 04:26:28                   +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, f], changelogMode=[I,UA,D])
Dec 05 04:26:28    ]                  +-...> but was:<...c(select=[a, b, f], [where=[f], changelogMode=[I])
Dec 05 04:26:28 +- TemporalJoin(joinType=[InnerJoin], where=[AND(=(a, a0), __TEMPORAL_JOIN_CONDITION(ingestion_time, ingestion_time0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(a0), __TEMPORAL_JOIN_LEFT_KEY(a), __TEMPORAL_JOIN_RIGHT_KEY(a0)))], select=[ingestion_time, a, b, ingestion_time0, a0, f], changelogMode=[I])
Dec 05 04:26:28    :- Exchange(distribution=[hash[a]], changelogMode=[I])
Dec 05 04:26:28    :  +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time], changelogMode=[I])
Dec 05 04:26:28    :     +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, b], changelogMode=[I])
Dec 05 04:26:28    :        +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, ingestion_time], changelogMode=[I])
Dec 05 04:26:28    +- Exchange(distribution=[hash[a]], changelogMode=[I,UA,D])
Dec 05 04:26:28       +- ChangelogNormalize(key=[a], changelogMode=[I,UA,D])
Dec 05 04:26:28          +- Exchange(distribution=[hash[a]], changelogMode=[I,UA,D])
Dec 05 04:26:28             +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time], changelogMode=[I,UA,D])
Dec 05 04:26:28                +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, f], changelogMode=[I,UA,D])
Dec 05 04:26:28 ]                  +-...>
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 13:31:15 UTC 2023,,,,,,,,,,"0|z1deg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 11:20;mapohl;cc'd [~snuyanzin] might this be related to the Calcite update?;;;","05/Dec/22 14:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43726&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12693;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 10:49;mapohl;[~snuyanzin] can this one be closed? It hasn't re-appeared for awhile.;;;","14/Aug/23 13:28;Sergey Nuyanzin;I think so, I've also haven't faced this for a long time
And it doesn't look like Calcite upgrade related thing... 
Also there were several fixes to Watermark related functionality over the past several months...;;;","14/Aug/23 13:31;mapohl;Thanks for the clarification, [~Sergey Nuyanzin]. I'm gonna go ahead and close this issue for the aformentioned reasons.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorTest.testSharedResourcesLifecycle failed with TaskException,FLINK-30301,13509855,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,05/Dec/22 11:04,25/Jan/23 01:59,04/Jun/24 20:41,25/Jan/23 01:59,1.17.0,,,,,1.17.0,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"This seems to be a follow-up of FLINK-30275. Same test but different test failure (2x in the same build):
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7479
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7852
{code}
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
Dec 05 03:59:18 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Dec 05 03:59:18 Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskException: Cannot find task to stop for execution 096b33c46c225fd4af41a9484b64c7fe_010f83ce510d70707aaf04c441173b70_0_0.
Dec 05 03:59:18 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.cancelTask(TaskExecutor.java:864)
Dec 05 03:59:18 	... 53 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 01:59:51 UTC 2023,,,,,,,,,,"0|z1deew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 11:05;mapohl;cc [~roman] aaand another round ;-) May you have a look at it?;;;","08/Dec/22 23:29;roman;Sorry for introducing the instability and thanks for reporting it.

 

I found locally that the existing heartbeat timeout of 1s can be too small when the machine is overloaded.

This timeout causes TM to release the connection with JM, cancelling all the tasks.

Then subsequent ""cancelTask"" calls will fail with the above exception.

 

This can be tested by increasing the number of tasks from 10 to e.g. 100 [here|https://github.com/apache/flink/blob/d86ae5d642fa578fb85118e81dd4140d504f818a/flink-runtime/src/test/java/org/apache/flink/runtime/taskexecutor/TaskExecutorTest.java#L3042]; or adding `Thread.sleep(1s)` right before `cancelTask` [here|https://github.com/apache/flink/blob/d86ae5d642fa578fb85118e81dd4140d504f818a/flink-runtime/src/test/java/org/apache/flink/runtime/taskexecutor/TaskExecutorTest.java#L3077].

 

I think simply increasing the timeouts to large values should be enough. Otherwise, a new `HeartbeatServices` has to be added.

I've created a [PR|https://github.com/apache/flink/pull/21467] for that - would you be able to take a look [~mapohl] ?;;;","25/Jan/23 01:59;roman;Merged into master as b4fe4a4288777c491019f8a59fb894bc9f60031b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building wheels on macos took longer than expected,FLINK-30300,13509851,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,05/Dec/22 10:38,14/Aug/23 11:33,04/Jun/24 20:41,14/Aug/23 11:33,1.17.0,,,,,,,,,,API / Python,Build System / Azure Pipelines,Test Infrastructure,,0,stale-major,test-stability,,"Looks like {{build_wheels_on_macos}} reached the Azure-specific limit of 1h runtime for this job:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43700&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=176",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 11:33:00 UTC 2023,,,,,,,,,,"0|z1dee0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 11:33;mapohl;Looks like this was a one time issue where the agent was slow for unknown reasons. It didn't appear again. I'm gonna close that issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerRunnerTest fails with 239 exit code (i.e. FatalExitExceptionHandler was called) NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1,FLINK-30299,13509846,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,mapohl,mapohl,mapohl,05/Dec/22 10:28,13/Jun/23 21:32,04/Jun/24 20:41,13/Jun/23 21:32,1.16.0,1.17.0,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"We're again experiencing 239 exit code being caused by {{FatalExitExceptionHandler}} due class loading issues:
{code}
04:53:03,365 [flink-akka.remote.default-remote-dispatcher-8] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-8' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1
        at akka.remote.transport.netty.NettyFutureBridge$.apply(NettyTransport.scala:65) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.remote.transport.netty.NettyFutureBridge$$anon$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 16 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43694&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8319

I created this as a follow-up of FLINK-26037 becasue we repurposed it and fixed a bug in FLINK-26037. But it looks like both are being caused by the same issue.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30140,,,,,,,,,,,,FLINK-26037,,,FLINK-31669,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 02 15:19:47 UTC 2023,,,,,,,,,,"0|z1decw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 10:30;mapohl;FYI: [~chesnay];;;","07/Dec/22 16:06;mapohl;I provided a PR that adds thread dump generation to the {{{}FatalExitExceptionHandler{}}}.;;;","19/Dec/22 08:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43871&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","21/Dec/22 14:02;mapohl;I added logging the thread dump in case of a fatal exit:
master: 49146cdec41467445de5fc81f100585142728bdf
1.16: f24b1816479a7ae9a1250f0e297e369008aa1666
1.15: 0f14477a3f2ff133ea209f166f2dcc8aab36ff1b

I couldn't find any other issue with the closing procedure of {{TaskManagerRunner}} so far. The only issue where we didn't wait for closing the metric system was fixed in FLINK-26037. Let's see whether the thread dump helps us with the investigation.;;;","02/Apr/23 15:19;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47796&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8321;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testStartFromGroupOffsetsNone failed due to timeout,FLINK-30298,13509841,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,05/Dec/22 09:58,16/Oct/23 07:08,04/Jun/24 20:41,16/Oct/23 07:08,1.16.0,1.17.0,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,"KafkaTableITCase.testStartFromGroupOffsetsNone failed due to timeout in the following build:
{code:java}
Dec 03 01:07:31 org.assertj.core.error.AssertJMultipleFailuresError: 
Dec 03 01:07:31 
Dec 03 01:07:31 Multiple Failures (1 failure)
Dec 03 01:07:31 -- failure 1 --
Dec 03 01:07:31 [Any cause is instance of class 'class org.apache.kafka.clients.consumer.NoOffsetForPartitionException'] 
Dec 03 01:07:31 Expecting any element of:
Dec 03 01:07:31   [java.lang.IllegalStateException: Fail to create topic [groupOffset_json_dc640086-d1f1-48b8-ad7a-f83d33b6a03c partitions: 4 replication factor: 1].
Dec 03 01:07:31 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
Dec 03 01:07:31 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.startFromGroupOffset(KafkaTableITCase.java:881)
Dec 03 01:07:31 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testStartFromGroupOffsetsWithNoneResetStrategy(KafkaTableITCase.java:981)
Dec 03 01:07:31 	...(64 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
Dec 03 01:07:31     java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
Dec 03 01:07:31 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
Dec 03 01:07:31 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
Dec 03 01:07:31 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Dec 03 01:07:31 	...(67 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
Dec 03 01:07:31     org.apache.kafka.common.errors.TimeoutException: The request timed out.
Dec 03 01:07:31 ]
Dec 03 01:07:31 to satisfy the given assertions requirements but none did:
[...] {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37708",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30879,FLINK-31756,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 23 08:10:32 UTC 2023,,,,,,,,,,"0|z1debs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/22 08:00;mapohl;Here's another one: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44191&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37378

This time, we see the test failure in {{KafkaTableITCase.testKafkaSourceSinkWithKeyAndFullValue}} which is also caused by a request timeout:
{code}
Dec 23 02:39:07 [ERROR] KafkaTableITCase.testKafkaSourceSinkWithKeyAndFullValue  Time elapsed: 31.81 s  <<< ERROR!
Dec 23 02:39:07 java.lang.IllegalStateException: Fail to create topic [key_full_value_topic_avro_814ad733-9fb0-4afe-866d-d5677d114196 partitions: 1 replication factor: 1].
Dec 23 02:39:07 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
Dec 23 02:39:07 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testKafkaSourceSinkWithKeyAndFullValue(KafkaTableITCase.java:453)
Dec 23 02:39:07 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Dec 23 02:39:07 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Dec 23 02:39:07 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Dec 23 02:39:07 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Dec 23 02:39:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Dec 23 02:39:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Dec 23 02:39:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Dec 23 02:39:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Dec 23 02:39:07 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Dec 23 02:39:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Dec 23 02:39:07 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Dec 23 02:39:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Dec 23 02:39:07 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Dec 23 02:39:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Dec 23 02:39:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Dec 23 02:39:07 	at org.junit.runners.Suite.runChild(Suite.java:128)
Dec 23 02:39:07 	at org.junit.runners.Suite.runChild(Suite.java:27)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Dec 23 02:39:07 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Dec 23 02:39:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Dec 23 02:39:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Dec 23 02:39:07 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Dec 23 02:39:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Dec 23 02:39:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Dec 23 02:39:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Dec 23 02:39:07 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Dec 23 02:39:07 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Dec 23 02:39:07 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Dec 23 02:39:07 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Dec 23 02:39:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Dec 23 02:39:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Dec 23 02:39:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
Dec 23 02:39:07 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Dec 23 02:39:07 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Dec 23 02:39:07 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Dec 23 02:39:07 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Dec 23 02:39:07 Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
Dec 23 02:39:07 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
Dec 23 02:39:07 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
Dec 23 02:39:07 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Dec 23 02:39:07 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:140)
Dec 23 02:39:07 	... 58 more
Dec 23 02:39:07 Caused by: org.apache.kafka.common.errors.TimeoutException: The request timed out.
{code};;;","23/Jan/23 08:10;mapohl;Similar stacktrace with an IllegalStateException being caused by a request timeout:
{code}
java.lang.IllegalStateException: Fail to create topic [word_count_json partitions: 4 replication factor: 1].
 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
[...]
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:140)
 	... 59 more
Caused by: org.apache.kafka.common.errors.TimeoutException: The request timed out.
{code}

* [KafkaTableITCase.testStartFromGroupOffsetsLatest|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45137&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=36917]
* [UpsertKafkaTableITCase.testAggregate|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45137&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=36992];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alibaba001 not responding,FLINK-30297,13509840,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,mapohl,mapohl,05/Dec/22 09:51,05/Dec/22 14:34,04/Jun/24 20:41,05/Dec/22 14:34,1.15.3,1.16.0,1.17.0,,,,,,,,Build System / Azure Pipelines,Test Infrastructure,,,0,test-stability,,,"{{Alibaba001}} seems to be corrupted causing build failures like [that one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5]:
{code}
##[error]We stopped hearing from agent AlibabaCI001-agent01. Verify the agent machine is running and has a healthy network connection. Anything that terminates an agent process, starves it for CPU, or blocks its network access can cause this error. For more information, see: https://go.microsoft.com/fwlink/?linkid=846610
Agent: AlibabaCI001-agent01
Started: Sat at 1:48 AM
Duration: 1h 3m 27s
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 14:34:12 UTC 2022,,,,,,,,,,"0|z1debk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 09:52;mapohl;[~wangyang0918] feel free to forward the assignee status to anyone who's able to handle the issue.;;;","05/Dec/22 10:50;mapohl;Looks like the issue was already resolved. Some of the new builds of the weekend execute jobs on {{Alibaba001}} without problems again (e.g. [this one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43701&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90]).;;;","05/Dec/22 10:51;mapohl;I'm lowering the priority to Major but keep this issue open to observe. Feel free to confirm that the issue is resolved, Yang.;;;","05/Dec/22 14:34;mapohl;Closing the issue again. It appears that the issue was fixed already. No failures due to {{Alibaba001}} not being responsive occurred anymore.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the situation of flink on yarn, how to control the number of TM in each nodemanager？",FLINK-30296,13509839,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Information Provided,,StarBoy1005,StarBoy1005,05/Dec/22 09:43,05/Dec/22 11:19,04/Jun/24 20:41,05/Dec/22 11:19,,,,,,,,,,,Table SQL / API,,,,0,,,,"Help, I use sql-client to execute batch query from hive and it's a on YARN job, parallelism=450 and taskmanager.numberOfTaskSlots=1,then the 450 tm is not average deploied on 15 nodemanagers (hosts),some nn have 28 and some have 34. I'm sure the load average or another resource of the 15 is totally same. 
I want to know is there exist a config can control the max tm num in a nodemanager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 11:19:26 UTC 2022,,,,,,,,,,"0|z1debc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 11:19;yunta;Hi [~StarBoy1005], JIRA is a place to report bug or contributing new features/improvements, not a place to ask questions. You can use Flink mailing list or slack channel to ask questions (you can refer to https://flink.apache.org/gettinghelp.html for more details).
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Develop MariaDbCatalog to connect Flink with MariaDb tables and ecosystem,FLINK-30295,13509827,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,samrat007,samrat007,samrat007,05/Dec/22 08:01,16/Dec/22 10:24,04/Jun/24 20:41,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,FLINK-15350,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 10:24:44 UTC 2022,,,,,,,,,,"0|z1de8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 12:08;samrat007;i will like to work on this. ;;;","16/Dec/22 10:24;martijnvisser;Please make sure that you create the PR in the flink-connector-jdbc repo;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change table property key 'log.scan' to 'startup.mode' and add a default startup mode in Table Store,FLINK-30294,13509819,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,05/Dec/22 07:08,06/Dec/22 07:46,04/Jun/24 20:41,06/Dec/22 07:46,table-store-0.3.0,,,,,,,,,,Table Store,,,,0,pull-request-available,,,"We're introducing time-travel reading of Table Store for batch jobs. However this reading mode is quite similar to the ""from-timestamp"" startup mode for streaming jobs, just that ""from-timestamp"" streaming jobs only consume incremental data but not history data.

We can support startup mode for both batch and streaming jobs. For batch jobs, ""from-timestamp"" startup mode will produce all records from the last snapshot before the specified timestamp. For streaming jobs the behavior doesn't change.

Previously, in order to use ""from-timestamp"" startup mode, users will have to specify ""log.scan"" and also ""log.scan.timestamp-millis"", which is a little inconvenient. We can introduce a ""default"" startup mode and its behavior will base on the execution environment and other configurations. In this way, to use ""from-timestamp"" startup mode, it is enough for users to specify just ""startup.timestamp-millis"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 07:46:58 UTC 2022,,,,,,,,,,"0|z1de6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 07:46;TsReaper;master: dadd735d362d355d60b737bf2c9669cb2fc880f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create an enumerator for static (batch),FLINK-30293,13509796,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lzljs3620320,lzljs3620320,05/Dec/22 02:18,08/Dec/22 11:54,04/Jun/24 20:41,08/Dec/22 11:54,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"In FLINK-30207, we have created enumerator for continuous.
We should have an enumerator for static (batch).
For example, for the current read-compacted, time traveling may specify the commit time to read snapshots in the future.
I think these capabilities need to be in the core, but should they be in scan? (It seems that it should not)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 11:54:07 UTC 2022,,,,,,,,,,"0|z1de1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 11:54;lzljs3620320;master: e7690d7c40c9a4371feafc170a147c5b03f70358;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better support for conversion between DataType and TypeInformation,FLINK-30292,13509790,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,05/Dec/22 01:21,11/Mar/24 12:43,04/Jun/24 20:41,,1.15.3,,,,,1.20.0,,,,,Table SQL / API,,,,0,,,,"In Flink 1.15, we have the following ways to convert a DataType to a TypeInformation. Each of them has some disadvantages.

* `TypeConversions.fromDataTypeToLegacyInfo`
It might lead to precision losses in face of some data types like timestamp.
It has been deprecated.
* `ExternalTypeInfo.of`
It cannot be used to get detailed type information like `RowTypeInfo`
It might bring some serialization overhead.

Given that the ways mentioned above are both not perfect,  Flink SQL should provide a better API to support DataType-TypeInformation conversions, and thus better support Table-DataStream conversions.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 10:46:20 UTC 2022,,,,,,,,,,"0|z1de0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 10:46;twalthr;Thanks for starting this discussion [~yunfengzhou]. I agree that the current approach using `ExternalTypeInfo` adds some serialization overhead. It was introduced to eliminate any friction during conversion of both type systems during conversion Table API->DataStream API. This purpose was successful. However, we can further evolve `ExternalTypeInfo` and use more specialized type info/serializers. In theory, if the `DataType` is of class `Row` with default conversions in every subfield, `ExternalTypeInfo.of` can also return `RowTypeInfo` with `BasicTypeInfo.*` etc. This specialized logic has just not been implemented yet. Since ListView and MapView in UDFs use ExternalTypeInfo as well we need to be careful for not introducing state incompatibilities. 

In any case, there were already discussion about introducing a dedicated `table-types` module to ease the development and expose common utilities including the data structure converters.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate flink-connector-aws into Flink docs,FLINK-30291,13509787,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,05/Dec/22 00:15,05/Dec/22 11:11,04/Jun/24 20:41,05/Dec/22 11:11,,,,,,1.16.1,1.17.0,,,,Connectors / AWS,Documentation,,,0,pull-request-available,,,"Update the docs render to integrate {{{}flink-connector-aws{}}}.

Add a new shortcode to handle rendering the SQL connector correctly",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 11:07:35 UTC 2022,,,,,,,,,,"0|z1ddzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 11:07;dannycranmer;Merged commit [{{fe88dcd}}|https://github.com/apache/flink/commit/fe88dcd8ef5fef3c54d64659e29c86f1719717a5] into apache:master (1.17)
Merged commit [{{da9e997}}|https://github.com/apache/flink/commit/da9e997752cc0b29aba5e0484726f89ddedfb7ce] into apache:release-1.16
Merged commit [{{acb8ff3}}|https://github.com/apache/flink/commit/acb8ff35fc40d15376072aece091cfc3d9ddda57] into apache:release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IteratorSourceReaderBase should report END_OF_INPUT sooner,FLINK-30290,13509779,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,04/Dec/22 20:37,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,1.20.0,,,,,API / Core,,,,0,,,,"The iterator source reader base does not report end_of_input when the last value was emitted, but instead requires an additional call to pollNext to be made.
This is fine functionality-wise, and allowed by the the source reader api contracts, but it's not intuitive behavior and leaks into tests for the datagen source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-04 20:37:28.0,,,,,,,,,,"0|z1ddy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RateLimitedSourceReader uses wrong signal for checkpoint rate-limiting,FLINK-30289,13509778,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,04/Dec/22 20:35,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,1.20.0,,,,,API / Core,,,,0,pull-request-available,stale-assigned,,"The checkpoint rate limiter is notified when the checkpoint is complete, but since this signal comes at some point in the future (or not at all) it can result in no records being emitted for a checkpoint, or more records than expected being emitted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 10:35:08 UTC 2023,,,,,,,,,,"0|z1ddxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use visitor to convert predicate for orc,FLINK-30288,13509757,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,04/Dec/22 11:54,07/Dec/22 03:32,04/Jun/24 20:41,07/Dec/22 03:32,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,Use `PredicateVisitor` to convert `Predicate` in table store for orc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 03:32:38 UTC 2022,,,,,,,,,,"0|z1ddt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 03:32;lzljs3620320;master: f32ce6e99f9ced265abbd57ce7af4c2e184c20cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configmaps get cleaned up when upgrading standalone Flink cluster,FLINK-30287,13509307,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,stevenpyzhang,stevenpyzhang,02/Dec/22 19:22,07/Dec/22 00:31,04/Jun/24 20:41,06/Dec/22 19:31,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"I started up a Standalone session Flink cluster and ran one job on it. I checked the configMaps and see the HA data.

 
{code:java}
kubectl get configmaps -n flink-operator
NAME                                                                            DATA   AGE
flink-config-sql-example-deployment-s3-testing                                  2      5m41s
flink-operator-config                                                           3      42h
kube-root-ca.crt                                                                1      42h
sql-example-deployment-s3-testing-000000003f57cd5f0000000000000002-config-map   0      11m
sql-example-deploymnt-s3-testing-cluster-config-map                             5      12m
{code}
 

I then update the FlinkDep image field and the Flink cluster gets restarted. The HA configmap for the job is now gone.
{code:java}
kubectl get configmaps -n flink-operator
NAME                                                   DATA   AGE
flink-config-sql-example-deployment-s3-testing         2      18m
flink-operator-config                                  3      43h
kube-root-ca.crt                                       1      43h
sql-example-deployment-s3-testing-cluster-config-map   3      31m {code}
 

I think this is due to a race condition where the TM first terminates which causes the JM to interpret the Job entering a failed state which causes it to clean up the configmaps.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-02 19:22:51.0,,,,,,,,,,"0|z1db1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run rat plugin in validate phase,FLINK-30286,13509286,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,02/Dec/22 17:45,07/Dec/22 14:01,04/Jun/24 20:41,07/Dec/22 14:01,,,,,,1.17.0,,,,,Build System,,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-30083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 14:01:07 UTC 2022,,,,,,,,,,"0|z1dawo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 14:01;chesnay;master: a798e52ab41ee438db52b0e272f984c8508fb395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inline checkstyle config into pom,FLINK-30285,13509285,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,02/Dec/22 17:45,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Build System,,,,0,,,,"Inline the checkstyle config into the pom, avoiding the need to reference a file path, solving one problems of FLINK-30083.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30083,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-02 17:45:13.0,,,,,,,,,,"0|z1dawg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make datadog reporter url configurable,FLINK-30284,13509284,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,02/Dec/22 17:44,15/May/24 16:50,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"Make the URL configurable to support agents or other services supporting the datadog protocol.

Deprecate the data center option since it is subsumed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 03 15:09:14 UTC 2023,,,,,,,,,,"0|z1daw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 15:09;Wosinsan;This basically makes it impossible to directly support DataDog Federal site with Flink at the moment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log entry: Error while patching status,FLINK-30283,13509272,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gaborgsomogyi,gaborgsomogyi,02/Dec/22 16:22,22/Jan/24 13:51,04/Jun/24 20:41,22/Jan/24 13:51,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:51:34 UTC 2024,,,,,,,,,,"0|z1datk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:51;gaborgsomogyi;See comment in FLINK-30310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logical type ROW lost inner field's nullability after convert to RelDataType,FLINK-30282,13509255,13228818,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,qingyue,qingyue,qingyue,02/Dec/22 14:29,13/Jun/23 09:43,04/Jun/24 20:41,09/Jun/23 07:46,1.16.0,1.16.1,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"h3. Issue History

This is not a new issue, FLINK-13604 has tracked it before, and FLINK-16344 spared efforts to fix it (but did not tweak the ut case mentioned in FLINK-13604, i.e. SqlToOperationConverterTest#testCreateTableWithFullDataTypes). Nevertheless, the FunctionITCase added by FLINK-16344, which validates the fix, has been removed in FLINK-16377. 

h3. How to Reproduce

 c.c2 lost nullability
{code:java}
Flink SQL> create table dummy (a array<int not null> not null, b array<string not null>, c row<c1 int, c2 double not null>) with ('connector' = 'datagen');
[INFO] Execute statement succeed.
Flink SQL> desc dummy;
+------+----------------------------+-------+-----+--------+-----------+
| name |                       type |  null | key | extras | watermark |
+------+----------------------------+-------+-----+--------+-----------+
|    a |        ARRAY<INT NOT NULL> | FALSE |     |        |           |
|    b |     ARRAY<STRING NOT NULL> |  TRUE |     |        |           |
|    c | ROW<`c1` INT, `c2` DOUBLE> |  TRUE |     |        |           |
+------+----------------------------+-------+-----+--------+-----------+
3 rows in set
{code}
h3. Root Cause

Two places are causing this problem in ExtendedSqlRowTypeNameSpec.

1. dt.deriveType should also pass dt's nullability as well. See [https://github.com/apache/flink/blob/fb27e6893506006b9a3b1ac3e9b878fb6cad061a/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/type/ExtendedSqlRowTypeNameSpec.java#L159]
 

2. StructKind should be PEEK_FIELDS_NO_EXPAND instead of FULLY_QUALIFIED(see [https://github.com/apache/calcite/blob/main/core/src/main/java/org/apache/calcite/rel/type/StructKind.java]), so that FlinkTypeFactory#createTypeWithNullability will not fall back to super implement. See [https://github.com/apache/flink/blob/fb27e6893506006b9a3b1ac3e9b878fb6cad061a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/calcite/FlinkTypeFactory.scala#L417]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31829,,,FLINK-31830,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 07:45:43 UTC 2023,,,,,,,,,,"0|z1daps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 14:43;qingyue;Hi [~twalthr],  [~shengkai] could you help to verify this ticket? Btw I'd like to help to fix it.;;;","09/Jun/23 07:45;qingyue;I rechecked the discussion CALCITE-2464 and assumed the current behavior is by design, and I'd like to close this issue.

 
{code:scala}
  @Test
  def testNullabilityForSql(): Unit = {
    // create temporary table t1 with r nullable
    tableEnv.executeSql(
      s""""""
         |create temporary table t1 (
         |  r row<f0 int not null, f1 double, f2 row<f20 int not null, f21 int> not null, f3 array<int not null> not null, f4 array<int> not null, f5 array<int not null>, f6 map<string not null, multiset<int> not null> not null>
         |) with (
         | 'connector' = 'datagen'
         |)
         |"""""".stripMargin)

    // create temporary table t2 with r not null
    tableEnv.executeSql(
      s""""""
         |create temporary table t2 (
         |  r row<f0 int not null, f1 double, f2 row<f20 int not null, f21 int> not null, f3 array<int not null> not null, f4 array<int> not null, f5 array<int not null>, f6 map<string not null, multiset<int> not null> not null> not null
         |) with (
         | 'connector' = 'datagen'
         |)
         |"""""".stripMargin)

    val catalogManager = tableEnv.asInstanceOf[StreamTableEnvironmentImpl].getCatalogManager
    val result1 =
      catalogManager.getTable(ObjectIdentifier.of(""default_catalog"", ""default_database"", ""t1""))
    val result2 =
      catalogManager.getTable(ObjectIdentifier.of(""default_catalog"", ""default_database"", ""t2""))
    println(result1.get().getResolvedSchema)
    println(result2.get().getResolvedSchema)
  }
{code}
{code:sql}
-- t1
-- the top-level nullability is set to true, and for the inner fields, the nullability is also set to true
-- note for type such as array, multiset or map, the element nullability is unchanged
-- for nested row type, the nullability is recursively set to true
(
  `r` ROW<`f0` INT, `f1` DOUBLE, `f2` ROW<`f20` INT, `f21` INT>, `f3` ARRAY<INT NOT NULL>, `f4` ARRAY<INT>, `f5` ARRAY<INT NOT NULL>, `f6` MAP<STRING NOT NULL, MULTISET<INT> NOT NULL>>
)

-- t2
-- the top-level nullability is set to false, and the inner nullability remains unchanged
(
  `r` ROW<`f0` INT NOT NULL, `f1` DOUBLE, `f2` ROW<`f20` INT NOT NULL, `f21` INT> NOT NULL, `f3` ARRAY<INT NOT NULL> NOT NULL, `f4` ARRAY<INT> NOT NULL, `f5` ARRAY<INT NOT NULL>, `f6` MAP<STRING NOT NULL, MULTISET<INT> NOT NULL> NOT NULL> NOT NULL
)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing for log4j error entry instead of plain error string in e2e tests,FLINK-30281,13509247,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,02/Dec/22 13:44,05/Dec/22 09:05,04/Jun/24 20:41,05/Dec/22 09:05,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Error which came up:

[https://github.com/gaborgsomogyi/flink-kubernetes-operator/actions/runs/3601102650/jobs/6066522997#step:9:121]
{code:java}
Found error in log files.

flink-kubernetes-operator-7456697448-mgw42.k8soperator.flink.flink-kubernetes-operator.namespace.default.FlinkDeployment.JmDeploymentStatus.ERROR.Count: 0
121 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 09:05:36 UTC 2022,,,,,,,,,,"0|z1dao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:49;morhidi;The metrics must be enabled, Gabor. The ERROR matching patterns should handle this.;;;","02/Dec/22 14:09;gaborgsomogyi;OK, then adding a whitelist entry.;;;","05/Dec/22 09:05;gaborgsomogyi;[{{31e2e10}}|https://github.com/apache/flink-kubernetes-operator/commit/31e2e10f112385120ac44378eaaa2e39b338ea61] on main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default operator logging configuration is broken,FLINK-30280,13509244,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,02/Dec/22 13:33,06/Dec/22 17:01,04/Jun/24 20:41,06/Dec/22 17:01,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The default logging configuration is set here: [https://github.com/apache/flink-kubernetes-operator/blob/ea01e294cf1b68d597244d0a11b3c81822a163e7/helm/flink-kubernetes-operator/templates/flink-operator.yaml#L89]

However, this file is not available in the official Docker image. It's best to not set this value and rely on the included logging configuration in the operator JAR. Users can define overrides on the deployment in [https://github.com/apache/flink-kubernetes-operator/blob/ea01e294cf1b68d597244d0a11b3c81822a163e7/helm/flink-kubernetes-operator/values.yaml#L135 |https://github.com/apache/flink-kubernetes-operator/blob/ea01e294cf1b68d597244d0a11b3c81822a163e7/helm/flink-kubernetes-operator/values.yaml#L135]or re-add the environment variable pointing to a valid file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 17:01:39 UTC 2022,,,,,,,,,,"0|z1danc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:43;morhidi;The Helm installs the default configs it refers to. Don't really see the issue here.;;;","05/Dec/22 16:18;morhidi;[~mxm] are you planning to change anything in the current release or can I remove this from 1.3?;;;","06/Dec/22 17:01;mbalassi;[{{41b9f4f}}|https://github.com/apache/flink-kubernetes-operator/commit/41b9f4fc199785168b9c31ef35d62cdef7f86561] in main, scope has been reduced to documentation for 1.3

[~mxm] feel free to open a follow up if you feel it is warranted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing RabbitMQ connector code from Flink repo to dedicated RabbitMQ repo,FLINK-30279,13509234,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,02/Dec/22 12:42,13/Dec/22 09:43,04/Jun/24 20:41,13/Dec/22 09:43,,,,,,rabbitmq-3.0.0,,,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 09:43:43 UTC 2022,,,,,,,,,,"0|z1dal4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 09:43;martijnvisser;Fixed in main: a80c0f47afd49b1cda8f1135f63a179766efb58d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected config mutation in SinkTransformationTranslator ,FLINK-30278,13509227,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,akalashnikov,akalashnikov,02/Dec/22 11:30,24/Jan/23 16:15,04/Jun/24 20:41,24/Jan/23 16:15,1.17.0,,,,,1.17.0,,,,,Runtime / Configuration,,,,0,pull-request-available,,,"If we forbid changing configuration programmatically(`execution.program-config.enabled`) and try to use `FileSink`. The following exception will occur:
{noformat}
  org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Not allowed configuration change(s) were detected:
 - Configuration parallelism.default:1 not allowed in the configuration object ExecutionConfig.
   at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:364)
   at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:225)
   at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98)
   at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:319)
   at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:262)
   at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
   at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
   at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41)
   at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
   at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
   at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
Caused by: org.apache.flink.client.program.MutatedConfigurationException: Not allowed configuration change(s) were detected:
 - Configuration parallelism.default:1 not allowed in the configuration object ExecutionConfig.
   at org.apache.flink.client.program.StreamContextEnvironment.checkNotAllowedConfigurations(StreamContextEnvironment.java:235)
   at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:175)
   at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:115)
   at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2049)
   at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:81)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
   at java.base/java.lang.reflect.Method.invoke(Unknown Source)
   at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:347)
   ... 16 more
{noformat}

It happens since inside of `SinkTransformationTranslator` we have following logic:
* Remeber the current parallelism
* Set parallelism to default
* Do transformation
* Set parallelism to remembered one

But if the initial prallelism is default we actually should do nothing but according current logic we explicitly set default value to the configuration which actually is the programmatic config mutation(which we want to avoid)

See org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.SinkExpander#executionEnvironment:341 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 16:15:42 UTC 2023,,,,,,,,,,"0|z1dajk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 16:15;pnowojski;Merged to master as f38c2370e85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow PYTHONPATH of Python Worker configurable,FLINK-30277,13509222,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,prabhujoseph,prabhujoseph,02/Dec/22 10:55,19/Jan/24 09:23,04/Jun/24 20:41,27/Feb/23 02:43,1.18.0,,,,,1.18.0,,,,,API / Python,,,,0,pull-request-available,,,"Currently, below are the ways Python Worker gets the Python Flink Dependencies.
 # Worker Node's System Python Path (/usr/local/lib64/python3.7/site-packages)
 # Client passes the python Dependencies through -pyfs and --pyarch which is localized into PYTHONPATH of Python Worker.
 # Client passes the requirements through -pyreq which gets installed on Worker Node and added into PYTHONPATH of Python Worker.

This Jira intends to allow PYTHONPATH of Python Worker configurable where Admin/Service provider can install the required python flink depencies on a custom path (/usr/lib/pyflink/lib/python3.7/site-packages) on all Worker Nodes and then set the path in the client machine configuration flink-conf.yaml. This way it works without any configurations from the Application Users and also without affecting any other components dependent on System Python Path.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 09:20:39 UTC 2024,,,,,,,,,,"0|z1daig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:33;dannycranmer;Thanks [~prabhujoseph] , this makes sense. I have assigned the Jira to you.;;;","02/Dec/22 14:20;prabhujoseph;Thanks [~dannycranmer] ;;;","31/Jan/23 17:20;prabhujoseph;[~dannycranmer] Could you assign this ticket to Samrat, who is working on the changes.;;;","01/Feb/23 17:55;dannycranmer;Done;;;","27/Feb/23 02:43;hxb;Merged into master via 2e0efe4e0723429e26ca04e2f61fcf89884dd077;;;","19/Jan/24 09:20;bo.yu;Hello [~samrat007] , hope this message finds you well. May I know can these options(python.pythonpath/-pypath/-pyPythonPath) be specified multiple times and work properly? For example, if my flink-conf.yaml looks like below
{code:java}
python.pythonpath: /custom_path1/xxx
python.pythonpath: /custom_path2/xxx{code}
Will PYTHONPATH be like ${other_sys_pythonpath};/custom_path1/xxx;/custom_path2/xxx?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
[umbrella] Flink free for table store core,FLINK-30276,13509216,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Dec/22 10:37,19/Mar/23 05:41,04/Jun/24 20:41,19/Mar/23 05:41,,,,,,table-store-0.4.0,,,,,Table Store,,,,0,,,,"In FLINK-30080, We need a core that does not rely on specific Flink versions to support flexible deployment and ecology.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30080,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-02 10:37:47.0,,,,,,,,,,"0|z1dah4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorTest.testSharedResourcesLifecycle fails,FLINK-30275,13509213,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,roman,mapohl,mapohl,02/Dec/22 10:33,12/Feb/24 08:08,04/Jun/24 20:41,,1.17.0,1.18.0,1.19.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,stale-assigned,test-stability,"We observe a test failure in {{TaskExecutorTest.testSharedResourcesLifecycle}}:
{code}
Dec 02 03:35:32 [ERROR] Tests run: 37, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.604 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.TaskExecutorTest
Dec 02 03:35:32 [ERROR] org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testSharedResourcesLifecycle  Time elapsed: 0.549 s  <<< FAILURE!
Dec 02 03:35:32 java.lang.AssertionError: expected:<0> but was:<1>
Dec 02 03:35:32 	at org.junit.Assert.fail(Assert.java:89)
Dec 02 03:35:32 	at org.junit.Assert.failNotEquals(Assert.java:835)
Dec 02 03:35:32 	at org.junit.Assert.assertEquals(Assert.java:647)
Dec 02 03:35:32 	at org.junit.Assert.assertEquals(Assert.java:633)
Dec 02 03:35:32 	at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testSharedResourcesLifecycle(TaskExecutorTest.java:3080)
[...]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43662&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7466",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29928,,,,,FLINK-30301,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 08:08:45 UTC 2024,,,,,,,,,,"0|z1dagg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 10:34;mapohl;[~roman] may you have a look at this. Looks like this test was just recently introduced.;;;","02/Dec/22 18:41;roman;Thanks for reporting [~mapohl] .

Fixed in 6b68bc6ccc159e980f8f77e1ab7520d79f035390.;;;","13/Mar/23 10:40;renqs;Looks like the issue pops up again on master branch: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47059&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8340] 

[~roman] could you take a look on this one? Thanks~ ;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","09/Feb/24 08:39;mapohl;https://github.com/apache/flink/actions/runs/7838691836/job/21390758520#step:10:8902;;;","12/Feb/24 08:08;mapohl;https://github.com/apache/flink/actions/runs/7838691836/job/21390758520#step:10:7737;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade commons-collections 3.x to commons-collections4,FLINK-30274,13509186,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,taoran,taoran,taoran,02/Dec/22 08:44,16/May/24 12:30,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Build System,,,,0,pull-request-available,stale-assigned,,"First, Apache commons-collections 3.x is a Java 1.3 compatible version, and it does not use Java 5 generics. Apache commons-collections4 4.4 is an upgraded version of commons-collections and it built by Java 8.

The Apache Spark has same issue: [https://github.com/apache/spark/pull/35257]",,,,,,,,,,FLINK-32522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/22 08:40;taoran;image-2022-12-02-16-40-22-172.png;https://issues.apache.org/jira/secure/attachment/13053433/image-2022-12-02-16-40-22-172.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 12:29:57 UTC 2024,,,,,,,,,,"0|z1daag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 09:01;taoran;[~martijnvisser] [~chesnay] Hi, can u take a look for this?;;;","02/Dec/22 09:07;martijnvisser;I've replied to the PR. You're referring to a vulnerability in commons-collections, but the link you've provided has no CVE identifier and also no CVSS score, this is just a vendor assessment. ;;;","02/Dec/22 09:15;taoran;[~martijnvisser] please see https://issues.apache.org/jira/browse/COLLECTIONS-701 . however i'm not very sure currently flink will cause this error. But we can not limit the others and later usages. 
IMHO, at least we need to recommend the developers use new version.;;;","02/Dec/22 09:41;martijnvisser;[~lemonjing] That's not a CVE. If anything, it's a bug at most. I agree that moving to a new version is good, but then that's what needs to happen: add the dependency, refactor existing usage to the new dependency and remove the old one. Else, you're not replacing anything. ;;;","02/Dec/22 10:06;taoran;[~martijnvisser] I think you are right. It's not a replacing. Thanks a lot.  Can we keep this issue open?  I will try to refactor some old usages so we can drop the old version and finally upgrade the commons-collections4. ;;;","02/Dec/22 12:44;martijnvisser;[~lemonjing] Yes the issue remains open, feel free to push an updated PR and we'll have a look;;;","13/Feb/23 02:59;taoran;[~martijnvisser] I have updated the pr, PTAL if your are free. thanks.;;;","14/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/May/24 12:29;alvarorod;Hello, 

Is there any plan to merge the [PR|https://github.com/apache/flink/pull/21442] in the near future? Or future versions?

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce RecordReaderUtils.transform to transform RecordReader,FLINK-30273,13509179,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Dec/22 08:19,06/Dec/22 02:27,04/Jun/24 20:41,06/Dec/22 02:27,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,Returns a RecordReader that applies function to each element of fromReader.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 02:27:18 UTC 2022,,,,,,,,,,"0|z1da8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 02:27;lzljs3620320;master: 7a1eeae236f39eeaccde76e46fcca6ac6f9d9469;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a Predicate Visitor,FLINK-30272,13509173,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Dec/22 07:58,02/Dec/22 13:58,04/Jun/24 20:41,02/Dec/22 13:58,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"At present, predicate is traversed in many places. We need a visitor mode, which can better traverse Predicate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 13:58:07 UTC 2022,,,,,,,,,,"0|z1da7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:58;lzljs3620320;master: 69ebaa2a861da54dd21e62778a9e9eab73f72c18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Table.copy from dynamic options,FLINK-30271,13509170,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Dec/22 07:40,06/Dec/22 02:27,04/Jun/24 20:41,06/Dec/22 02:27,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"At present, our processing of dynamic options is relatively independent. In FileStoreTableFactory, this is not conducive to other engines configuring dynamic options.

We should propose an interface on the Table, and dynamic options can be configured at any time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 02:27:56 UTC 2022,,,,,,,,,,"0|z1da6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 02:27;lzljs3620320;master: 925186e0904b1eec8d571168ab96706344201211;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkJoinToMultiJoinRule incorrectly combines Left/Right outer join to MultiJoin,FLINK-30270,13509165,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,02/Dec/22 06:50,27/Dec/22 06:39,04/Jun/24 20:41,27/Dec/22 06:39,1.16.0,,,,,1.17.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"FlinkJoinToMultiJoinRule incorrectly combines Left/Right outer join to MultiJoin.  In some complex cases, it need to consider join conditions, ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 06:39:01 UTC 2022,,,,,,,,,,"0|z1da5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 08:28;martijnvisser;[~337361684@qq.com] With this ticket being a subticket on optimizing TPC-DS queries, it's not clear immediately if this is a bug or an improvement. Based on the description I think it's a bug, but then I don't think it should be listed as a ticket under an improvement story. WDYT? ;;;","22/Dec/22 09:07;337361684@qq.com;Thanks, [~martijnvisser]. I agree with you. I will remove this issue out and list it as a independent bug fix issue.;;;","27/Dec/22 06:39;godfrey;Fixed in master:7590cb7d84774b0e8afd7b0af31cc0af762d4c6a

1.16.1: ca42695dba5ab72c6b9b895bb6553321c30d5074;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate table name for metadata table,FLINK-30269,13509157,13490538,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,02/Dec/22 05:39,02/Dec/22 10:33,04/Jun/24 20:41,02/Dec/22 10:33,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"Currently user can create tables `tablename` and `tablename$snapshots`, but can't insert into values into `tablename$snapshots` and execute query on it. 

At the same time, user can create table `tablename$aaa$bbb`, but cannot execute query on it and event cannot drop it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 10:33:51 UTC 2022,,,,,,,,,,"0|z1da40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 10:33;lzljs3620320;master: 2c8d043acc7e24e9b8f0cbf86beea62002351bf7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA metadata and other cluster submission related errors should not throw DeploymentFailedException,FLINK-30268,13508803,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pvary,gyfora,gyfora,01/Dec/22 22:11,06/Dec/22 13:42,04/Jun/24 20:41,06/Dec/22 13:42,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Currently most critical cluster submission errors , and checks that validate HA metadata before deployment, end up throwing DeploymentFailedException.

This causes the operator to go into a weird state and actually hide the error in subsequent loops:


{noformat}
flink-kubernetes-operator 2022-12-01 21:55:03,978 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Info    | UPGRADING       | The resource is being upgraded 
flink-kubernetes-operator 2022-12-01 21:55:03,992 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Event  | Info    | SUBMIT          | Starting deployment
flink-kubernetes-operator 2022-12-01 21:55:03,992 o.a.f.k.o.s.AbstractFlinkService [INFO ][default/basic-checkpoint-ha-example] Deploying application cluster requiring last-state from HA metadata
flink-kubernetes-operator 2022-12-01 21:55:03,997 o.a.f.k.o.c.FlinkDeploymentController [ERROR][default/basic-checkpoint-ha-example] Flink Deployment failed
flink-kubernetes-operator org.apache.flink.kubernetes.operator.exception.DeploymentFailedException: HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required.
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.validateHaMetadataExists(AbstractFlinkService.java:844)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.submitApplicationCluster(AbstractFlinkService.java:177)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:195)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:60)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:210)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:142)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:161)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:62)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:123)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
flink-kubernetes-operator     at java.base/java.lang.Thread.run(Unknown Source)
flink-kubernetes-operator 2022-12-01 21:55:04,034 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Event  | Warning | RESTOREFAILED   | HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required.
flink-kubernetes-operator 2022-12-01 21:55:04,034 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/basic-checkpoint-ha-example] End of reconciliation
flink-kubernetes-operator 2022-12-01 21:55:04,054 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Error   | UPGRADING       | {""type"":""org.apache.flink.kubernetes.operator.exception.DeploymentFailedException"",""message"":""HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required."",""additionalMetadata"":{""reason"":""RestoreFailed""},""throwableList"":[]} 
flink-kubernetes-operator 2022-12-01 21:55:19,056 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/basic-checkpoint-ha-example] Starting reconciliation
flink-kubernetes-operator 2022-12-01 21:55:19,058 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][default/basic-checkpoint-ha-example] UPGRADE change(s) detected (FlinkDeploymentSpec[job.state=RUNNING] differs from FlinkDeploymentSpec[job.state=SUSPENDED]), starting reconciliation.
flink-kubernetes-operator 2022-12-01 21:55:19,092 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Info    | UPGRADING       | The resource is being upgraded 
flink-kubernetes-operator 2022-12-01 21:55:19,119 o.a.f.k.o.r.d.ApplicationReconciler [ERROR][default/basic-checkpoint-ha-example] Invalid status for deployment: FlinkDeploymentStatus(super=CommonStatus(jobStatus=JobStatus(jobName=CarTopSpeedWindowingExample, jobId=8d5c59b7e960984cd845b9977754d2ef, state=RECONCILING, startTime=1669931677233, updateTime=1669931696153, savepointInfo=SavepointInfo(lastSavepoint=null, triggerId=null, triggerTimestamp=null, triggerType=null, formatType=null, savepointHistory=[], lastPeriodicSavepointTimestamp=0)), error=null), clusterInfo={flink-version=1.15.2, flink-revision=69e8126 @ 2022-08-17T14:58:06+02:00}, jobManagerDeploymentStatus=ERROR, reconciliationStatus=FlinkDeploymentReconciliationStatus(super=ReconciliationStatus(reconciliationTimestamp=1669931719059, lastReconciledSpec={""spec"":{""job"":{""jarURI"":""local:///opt/flink/examples/streaming/TopSpeedWindowing.jar"",""parallelism"":2,""entryClass"":null,""args"":[],""state"":""suspended"",""savepointTriggerNonce"":0,""initialSavepointPath"":null,""upgradeMode"":""last-state"",""allowNonRestoredState"":null},""restartNonce"":2,""flinkConfiguration"":{""high-availability"":""org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory"",""high-availability.storageDir"":""file:///flink-data/ha"",""state.checkpoints.dir"":""file:///flink-data/checkpoints"",""state.savepoints.dir"":""file:///flink-data/savepoints"",""taskmanager.numberOfTaskSlots"":""2""},""image"":""flink:1.15"",""imagePullPolicy"":null,""serviceAccount"":""flink"",""flinkVersion"":""v1_15"",""ingress"":null,""podTemplate"":{""apiVersion"":""v1"",""kind"":""Pod"",""spec"":{""containers"":[{""name"":""flink-main-container"",""volumeMounts"":[{""mountPath"":""/flink-data"",""name"":""flink-volume""}]}],""volumes"":[{""hostPath"":{""path"":""/tmp/flink"",""type"":""Directory""},""name"":""flink-volume""}]}},""jobManager"":{""resource"":{""cpu"":1.0,""memory"":""2048m""},""replicas"":1,""podTemplate"":null},""taskManager"":{""resource"":{""cpu"":1.0,""memory"":""2048m""},""replicas"":null,""podTemplate"":null},""logConfiguration"":null,""mode"":null},""resource_metadata"":{""apiVersion"":""flink.apache.org/v1beta1"",""metadata"":{""generation"":5},""firstDeployment"":false}}, lastStableSpec=null, state=UPGRADING)), taskManager=TaskManagerInfo(labelSelector=, replicas=0))
flink-kubernetes-operator 2022-12-01 21:55:19,133 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Event  | Warning | CLUSTERDEPLOYMENTEXCEPTION | This indicates a bug...
flink-kubernetes-operator 2022-12-01 21:55:19,136 o.a.f.k.o.r.ReconciliationUtils [WARN ][default/basic-checkpoint-ha-example] Attempt count: 0, last attempt: false
flink-kubernetes-operator 2022-12-01 21:55:19,163 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Error   | UPGRADING       | {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""java.lang.RuntimeException: This indicates a bug..."",""throwableList"":[{""type"":""java.lang.RuntimeException"",""message"":""This indicates a bug...""}]} 
flink-kubernetes-operator 2022-12-01 21:55:19,164 i.j.o.p.e.ReconciliationDispatcher [ERROR][default/basic-checkpoint-ha-example] Error during event processing ExecutionScope{ resource id: ResourceID{name='basic-checkpoint-ha-example', namespace='default'}, version: 350553} failed.
flink-kubernetes-operator org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.RuntimeException: This indicates a bug...
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:133)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
flink-kubernetes-operator     at java.base/java.lang.Thread.run(Unknown Source)
flink-kubernetes-operator Caused by: java.lang.RuntimeException: This indicates a bug...
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:180)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:60)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:210)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:142)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:161)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:62)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:123)
flink-kubernetes-operator     ... 13 more
{noformat}

The main cause here is that DeploymentFailedExceptions were originally created so that the observer could signal a JobManager deployment failure (after it was submitted). Thus the error handler logic in the controller actually updates the jmDeploymentStatus and the job state which causes the problem.

To avoid this we should introduce a new Exception type or use something more suitable. We should not touch touch the jobmanagerDeploymentStatus or the jobstatus in most of these cases and simply retrigger the reconciliation. This will keep the CR in an error loop triggering warnings etc but that is expected in these critical failure scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 13:42:00 UTC 2022,,,,,,,,,,"0|z1d7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 13:42;mbalassi;[{{310ff30}}|https://github.com/apache/flink-kubernetes-operator/commit/310ff3072cd6196202ac37a171a896d3359cfc56] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink YARN per-job on Docker test failed because it could not build hadoop image,FLINK-30267,13508052,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,01/Dec/22 15:39,19/Aug/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Deployment / YARN,,,,0,auto-deprioritized-critical,test-stability,,"{code:java}
Dec 01 15:10:48 E: Unable to locate package net-tools
Dec 01 15:10:48 E: Unable to locate package build-essential
The command '/bin/sh -c set -x     && apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y     curl tar sudo openssh-server openssh-client rsync unzip krb5-user net-tools build-essential' returned a non-zero code: 100
Service 'master' failed to build : Build failed
Dec 01 15:10:48 Command: build_image failed. Retrying...
Dec 01 15:10:50 Command: build_image failed 5 times.
Dec 01 15:10:50 ERROR: Could not build hadoop image. Aborting...
Dec 01 15:10:50 Debugging failed YARN Docker test:
Dec 01 15:10:50 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43650&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=8414",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:05 UTC 2023,,,,,,,,,,"0|z1d3ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 15:40;martijnvisser;[~gsomogyi] [~gaborgsomogyi] Can you take a look?;;;","01/Dec/22 16:13;gaborgsomogyi;Seems like there is a generic infra issue. In the operator we see magical docker failures too: https://github.com/apache/flink-kubernetes-operator/actions/runs/3594259953/jobs/6052303465#step:7:43;;;","01/Dec/22 16:22;gaborgsomogyi;It builds locally + the following error tells me that it's tmp issue:
{code:java}
Dec 01 15:10:48 W: Failed to fetch http://deb.debian.org/debian/dists/bullseye/InRelease  Could not connect to debian.map.fastlydns.net:80 (199.232.138.132), connection timed out Unable to connect to deb.debian.org:http:
Dec 01 15:10:48 W: Failed to fetch http://deb.debian.org/debian-security/dists/bullseye-security/InRelease  Unable to connect to deb.debian.org:http:
Dec 01 15:10:48 W: Failed to fetch http://deb.debian.org/debian/dists/bullseye-updates/InRelease  Unable to connect to deb.debian.org:http:
Dec 01 15:10:48 W: Some index files failed to download. They have been ignored, or old ones used instead.
{code}
;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Recovery reconciliation loop fails if no checkpoint has been created yet,FLINK-30266,13507876,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,mxm,mxm,01/Dec/22 14:42,05/Dec/22 22:59,04/Jun/24 20:41,05/Dec/22 22:59,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"When the upgradeMode is LAST-STATE, the operator fails to reconcile a failed application unless at least one checkpoint has already been created. The expected behavior would be that the job starts with empty state.
{noformat}
2022-12-01 10:58:35,596 o.a.f.k.o.l.AuditUtils         [INFO ] [app] >>> Status | Error   | UPGRADING       | {""type"":""org.apache.flink.kubernetes.operator.exception.DeploymentFailedException"",""message"":""HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required."",""additionalMetadata"":{""reason"":""RestoreFailed""},""throwableList"":[]} {noformat}
{noformat}
2022-12-01 10:44:49,480 i.j.o.p.e.ReconciliationDispatcher [ERROR] [app] Error during event processing ExecutionScope{ resource id: ResourceID{name='app', namespace='namespace'}, version: 216933301} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.RuntimeException: This indicates a bug...
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:133)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.RuntimeException: This indicates a bug...
	at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:180)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:61)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:212)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:144)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:167)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:64)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:123)
	... 13 more {noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 22:59:56 UTC 2022,,,,,,,,,,"0|z1d27k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 19:18;thw;I believe this was discussed before and the reason we decided to not allow this was that we cannot safely determine the reason why the HA metadata is missing. It could be because there was never any successful checkpoint or because it was removed by mistake? As long as we can ensure that we don't accidentally reset a job with prior state to empty state I would also prefer the solution that does not involve manual intervention.;;;","05/Dec/22 16:15;morhidi;[~gyfora], can this be closed?;;;","05/Dec/22 22:59;gyfora;Merged to main 72ad3639e60fbf27dd408dabbe69f46d69ff52f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Turn on debug logs for k8s operator e2e tests in CI when tests failing,FLINK-30265,13507866,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Won't Do,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,01/Dec/22 14:35,02/Dec/22 11:40,04/Jun/24 20:41,02/Dec/22 11:40,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,Now e2e tests are not providing any operator or job logs when failing which makes it impossible to debug them.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 11:40:24 UTC 2022,,,,,,,,,,"0|z1d25c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 11:40;gaborgsomogyi;FLINK-30199 has been rolled back which caused the issue together w/ this fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set session job status to FAILED if already have retried max attempts,FLINK-30264,13507846,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,haoxin,haoxin,01/Dec/22 13:02,10/Apr/23 12:47,04/Jun/24 20:41,10/Apr/23 12:47,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Sometimes, the session job deployment fails because of the user code.

There is nothing the Flink operator can do to fix the failure.

 

So can we add a new reconciliation state *FAILED* and set the status to this if the failure still exists after we have retried for the max attempts?

The reconciliation status will continuously be *UPGRADING* currently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 21:27:15 UTC 2023,,,,,,,,,,"0|z1d20w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 21:27;mason6345;+1, same request for flinkdeployments;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce schemas meta table,FLINK-30263,13507839,13490538,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,01/Dec/22 12:18,06/Dec/22 02:31,04/Jun/24 20:41,06/Dec/22 02:31,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"You can query the historical schemas of the table through SQL, for example, query the historical schemas of table ""T"" through the following SQL:

SELECT * FROM T$schemas;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 02:31:25 UTC 2022,,,,,,,,,,"0|z1d1zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 02:31;lzljs3620320;master: f103f33fbd17bb43113275211bc0667a1bf968c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertKafkaTableITCase failed when starting the container because waiting for a port timed out,FLINK-30262,13507830,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,mapohl,mapohl,01/Dec/22 11:30,16/Oct/23 12:15,04/Jun/24 20:41,16/Oct/23 12:15,1.16.0,,,,,,,,,,Build System,Connectors / Kafka,Test Infrastructure,,0,auto-deprioritized-major,test-stability,,"{code:java}
Dec 01 08:35:00 Caused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for container port to open (172.17.0.1 ports: [60109, 60110] should be listening)
Dec 01 08:35:00 	at org.testcontainers.containers.wait.strategy.HostPortWaitStrategy.waitUntilReady(HostPortWaitStrategy.java:90)
Dec 01 08:35:00 	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)
Dec 01 08:35:00 	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)
Dec 01 08:35:00 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)
Dec 01 08:35:00 	... 33 more
 {code}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43643&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37366",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:03 UTC 2023,,,,,,,,,,"0|z1d1xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 09:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46201&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=37086;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase.test timed out while waiting for tasks to finish,FLINK-30261,13507823,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,mapohl,mapohl,01/Dec/22 11:03,01/Dec/22 13:08,04/Jun/24 20:41,01/Dec/22 13:08,1.15.3,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,"{{PartiallyFinishedSourcesITCase.test}} timed out while waiting for tasks to finish:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007fe78800b800 nid=0x3a11f waiting on condition [0x00007fe78e6fb000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
[...]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43637&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=39302",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 13:08:28 UTC 2022,,,,,,,,,,"0|z1d1vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 13:08;mapohl;Closing this issue. I interpreted the thread dump in a wrong way. FLINK-28319 is still the proper issue for that build failure. {{PartiallyFinishedSourcesITCase}} only waits for a limited amount of time based on the thread dump. For {{ResumeCheckpointManuallyITCase}} instead, the threads are in {{WAITING}} state.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-271: Add initial Autoscaling implementation ,FLINK-30260,13507795,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,01/Dec/22 09:48,09/Feb/23 16:45,04/Jun/24 20:41,19/Dec/22 21:21,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,4,,,,https://cwiki.apache.org/confluence/display/FLINK/FLIP-271%3A+Autoscaling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30773,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 10:35:14 UTC 2022,,,,,,,,,,"0|z1d1pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 12:33;ConradJam;Hi [~mxm] I would like to know if there are related subtasks that I can volunteer to take on part of the development :);;;","20/Dec/22 06:36;Swathi Chandrashekar;Hi [~mxm] , [~gyfora] ,  I would like to volunteer if there are any subtasks where I can contribute;;;","20/Dec/22 10:35;gyfora;Thank you [~Swathi Chandrashekar] 
We will open Jira tickets as we discover more and more potential improvements. 
The biggest help would be at this stage if you could actually try the autoscaler on some Flink jobs and provide feedback based on what you see in your environment. That also makes it easier to open tickets to work on :) 

Please feel free to reach out with any feedback on the mailing lists or on slack;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use flink Preconditions Util instead of uncertain Assert keyword to do checking,FLINK-30259,13507793,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,taoran,taoran,01/Dec/22 09:42,30/Mar/23 10:03,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Deployment / Kubernetes,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Planner,,0,,,,"The code of some modules of the current Flink project uses the 'assert' keyword of java to do checking, which actually depends on the enablement of the -enableassertions (-ea) option (default is false, which means some assert code can not work), otherwise it may lead to unexpected behavior. In fact, flink already has a mature Preconditions tool, we can use it to replace 'assert' keyword. it is more clean and consistent with flink.

The following is an example of some snippets.

RowDataPrintFunction
{code:java}
@Override
        public void invoke(RowData value, Context context) {
            Object data = converter.toExternal(value);
            assert data != null;
            writer.write(data.toString());
        }
{code}
e.g. if assert not enable,data.toString() will cause NPE.

KubernetesUtils
{code:java}
    public static KubernetesConfigMap checkConfigMaps(
            List<KubernetesConfigMap> configMaps, String expectedConfigMapName) {
        assert (configMaps.size() == 1);
        assert (configMaps.get(0).getName().equals(expectedConfigMapName));
        return configMaps.get(0);
    }
{code}
e.g. if assert not enable,configMaps.get(0)will cause NPE.

RocksDBOperationUtils
{code:java}
if (memoryConfig.isUsingFixedMemoryPerSlot()) {
                assert memoryConfig.getFixedMemoryPerSlot() != null;

                logger.info(""Getting fixed-size shared cache for RocksDB."");
                return memoryManager.getExternalSharedMemoryResource(
                        FIXED_SLOT_MEMORY_RESOURCE_ID,
                        allocator,
                        // if assert not enable,  here will cause NPE.
                        memoryConfig.getFixedMemoryPerSlot().getBytes());
            } else {
                logger.info(""Getting managed memory shared cache for RocksDB."");
                return memoryManager.getSharedMemoryResourceForManagedMemory(
                        MANAGED_MEMORY_RESOURCE_ID, allocator, memoryFraction);
            }
{code}
e.g. if assert not enable, RocksDBOperationUtils#memoryConfig.getFixedMemoryPerSlot().getBytes()) will cause NPE.

*Note:* many calcite-releated classes such as flink hive parser(Most are located in flink-connector-hive) use assert many places. we will not fix these classes. 
we need to keep these classes consistent with the assert used in calcite.  We just fix flink classes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 09:39:09 UTC 2023,,,,,,,,,,"0|z1d1p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 09:51;taoran;Hi, [~martijnvisser] can u take a look for this?  i’m glad to improve it.;;;","01/Dec/22 09:55;martijnvisser;[~chesnay] You're better suited to answer this ;;;","14/Mar/23 08:51;Wencong Liu;cc [~chesnay] ;;;","30/Mar/23 09:39;taoran;Hi [~chesnay] and [~twalthr]  because currently some modules such as flink-table/table-planner are doing some efforts like technical debt or removing of deprecated code to meet the possible and new 2.0 version.  Can we do a clean about this assert usage. WDYT? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink supports closing loopback server,FLINK-30258,13507784,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,01/Dec/22 09:07,01/Dec/22 09:08,04/Jun/24 20:41,,1.16.0,,,,,,,,,,API / Python,,,,0,,,,"Currently, a loopback server will be started whenever a StreamExecutionEnvironment or StreamTableEnvironment is created. The loopback server can only be closed after the process exit. This might not be a problem for regular uses where only one environment object is used.

However, when running tests, such as the unit tests for PyFlink itself, as the environment objects are created, the process starts more and more loopback servers and takes more and more resources.

Therefore, we want to support closing the loopback server.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-01 09:07:23.0,,,,,,,,,,"0|z1d1n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClientITCase#testMatchRecognize failed,FLINK-30257,13507763,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,martijnvisser,martijnvisser,01/Dec/22 08:28,17/Jul/23 07:47,04/Jun/24 20:41,17/Jul/23 07:46,1.17.0,,,,,1.17.2,1.18.0,,,,Table SQL / Client,,,,0,pull-request-available,test-stability,,"{code:java}
Nov 30 21:54:41 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 224.683 s <<< FAILURE! - in SqlClientITCase
Nov 30 21:54:41 [ERROR] SqlClientITCase.testMatchRecognize  Time elapsed: 50.164 s  <<< FAILURE!
Nov 30 21:54:41 org.opentest4j.AssertionFailedError: 
Nov 30 21:54:41 
Nov 30 21:54:41 expected: 1
Nov 30 21:54:41  but was: 0
Nov 30 21:54:41 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Nov 30 21:54:41 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Nov 30 21:54:41 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Nov 30 21:54:41 	at SqlClientITCase.verifyNumberOfResultRecords(SqlClientITCase.java:297)
Nov 30 21:54:41 	at SqlClientITCase.testMatchRecognize(SqlClientITCase.java:255)
Nov 30 21:54:41 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 30 21:54:41 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 30 21:54:41 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 30 21:54:41 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 30 21:54:41 	at org.junit.platform.commons.util.ReflectionUtils.invokeMetho
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43635&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14817",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30655,,,,,,,,,,,,,,,,,,,,"29/Dec/22 13:47;Wencong Liu;image-2022-12-29-21-47-31-606.png;https://issues.apache.org/jira/secure/attachment/13054220/image-2022-12-29-21-47-31-606.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 07:47:01 UTC 2023,,,,,,,,,,"0|z1d1ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 11:19;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43633&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","29/Dec/22 13:50;Wencong Liu;Hello [~martijnvisser]. In the situation of resource shortage, this sleep delay may not be sufficient, and the data in the file may not be flushed. May be adding timeout could solve the problem?

!image-2022-12-29-21-47-31-606.png!;;;","10/Jan/23 07:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44615&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15026;;;","10/Jan/23 07:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44629&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14855;;;","10/Jan/23 07:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44630&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15195;;;","10/Jan/23 10:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=11187;;;","12/Jan/23 15:30;martijnvisser;[~Wencong Liu] Sorry for the late reply. Can we refactor the test to avoid timeouts in general, given that we already want to avoid them for JUnit tests? https://flink.apache.org/contributing/code-style-and-quality-common.html#avoid-timeouts-in-junit-tests;;;","13/Jan/23 10:28;Wencong Liu;Thanks [~martijnvisser]. I think we can refactor this test by depending on the global timeout in Azure. WDYT?;;;","16/Jan/23 06:03;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44871&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","16/Jan/23 15:24;martijnvisser;[~Wencong Liu] That would be great. I'll assign the ticket to you;;;","17/Jan/23 03:53;Wencong Liu;Thanks [~martijnvisser], I've opened a pull request.;;;","15/Feb/23 13:59;Wencong Liu;cc [~martijnvisser] ;;;","15/Feb/23 14:39;martijnvisser;Fixed in

master: 4595762f974f03bdf5a4c3d42e211c3a24604bd4
release-1.17: 5ebc4b1dc5fba6b18f48143f01f4059dc02f08b0;;;","12/Jul/23 10:57;nuafonso;Hello [~martijnvisser],

 

Is this issue resolved?

I can see that the PR was merged.

 

Thank you.;;;","17/Jul/23 07:47;martijnvisser;[~nuafonso] Thanks for the comment, it's indeed fixed and I've closed the ticket;;;",,,,,,,,,,,,,,,,
LogalWindowAgg can set the chaining Strategy to always,FLINK-30256,13507750,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhoujira86,zhoujira86,01/Dec/22 06:51,01/Dec/22 07:08,04/Jun/24 20:41,01/Dec/22 07:08,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-01 06:51:19.0,,,,,,,,,,"0|z1d1fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception for upper case fields are used in hive metastore,FLINK-30255,13507730,13501019,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,01/Dec/22 02:24,06/Jan/23 01:54,04/Jun/24 20:41,06/Jan/23 01:54,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"Currently there will be incompatible when user use upper case in hive metastore and table store, we should throw exception for it and find a more elegant compatibility mode later",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 01:54:50 UTC 2023,,,,,,,,,,"0|z1d1b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 01:54;lzljs3620320;master: cbdb78a37d520e2adc208867957ce8d5134d0a09
release-0.3: 947a8e2b9fe38f9d647c6ad13f39b294ff541aea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync Pulsar updates to external Pulsar connector repository,FLINK-30254,13507513,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,30/Nov/22 19:03,08/Feb/23 13:09,04/Jun/24 20:41,06/Dec/22 16:38,,,,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,Currently the external Pulsar repository contains the code from the {release-1.16} branch. This should be synced with the changes that are merged into {master} since. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 16:38:27 UTC 2022,,,,,,,,,,"0|z1czyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 06:12;syhily;[~martijnvisser] I want to migrate all the pending PRs to new repository. When can we get this resolved?;;;","05/Dec/22 06:20;martijnvisser;[~syhily] This will probably be done today or tomorrow at latest;;;","06/Dec/22 16:38;martijnvisser;Fixed in main: 319992293faaf1c678b37904c3996c990f6e279d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce io executor for standalone checkpoint store,FLINK-30253,13507466,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,yunta,yunta,30/Nov/22 13:10,13/Dec/22 05:40,04/Jun/24 20:41,13/Dec/22 05:40,,,,,,1.17.0,,,,,Runtime / Checkpointing,,,,0,,,,"Currently, only standalone checkpoint store does not have io executor to execute IO related operations, we can introduce the io executor to standalone checkpoint store to speed up such IO operations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 05:40:47 UTC 2022,,,,,,,,,,"0|z1czog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 07:18;masteryhx;IIUC, Standalone checkpoint store will also use io executor getting from JobManagerSharedServices currently?;;;","13/Dec/22 05:40;yunta;[~masteryhx] Thanks for your information, this ticket has been outdated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish flink-shaded pom,FLINK-30252,13507443,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,30/Nov/22 11:41,11/Jan/24 15:31,04/Jun/24 20:41,,,,,,,shaded-19.0,,,,,Build System,,,,0,,,,"Publish a bom for flink-shaded, such that downtream projects just select the flink-shaded version, with all other dependency versions being selected automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30578,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-30 11:41:12.0,,,,,,,,,,"0|z1czjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move the IO with DFS during abort checkpoint to an asynchronous thread.,FLINK-30251,13507439,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,30/Nov/22 11:12,15/Feb/23 08:43,04/Jun/24 20:41,15/Feb/23 08:41,1.15.2,1.16.0,,,,1.18.0,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Currently when the {{checkpoint}} fails, we process the abort message in the Task's {{{}mailbox{}}}. We will close the output stream and delete the file on DFS. 

 

However, when the {{checkpoint}} failure is caused by a DFS system failure (for example, the namenode failure of HDFS), this operation may take a long time or hang, and the task will not be able to process the data at this time.

 

So I think we can put the operation of deleting files in an asynchronous thread just like uploading checkpoint data asynchronously.

!image-2022-11-30-19-10-51-226.png|width=731,height=347!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/22 11:10;Ming Li;image-2022-11-30-19-10-51-226.png;https://issues.apache.org/jira/secure/attachment/13053328/image-2022-11-30-19-10-51-226.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 08:41:31 UTC 2023,,,,,,,,,,"0|z1czig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 06:11;Ming Li;hi, [~Yanfei Lei] , I made a simple implementation, can you help to see if it works?

https://github.com/liming30/flink/commit/2dce488abecf1ab68b230f28336e1e3173a4793c;;;","02/Dec/22 02:54;Yanfei Lei;Thanks for the proposal, this helps reduce the time blocked during abort checkpoint, but I'm not sure if this has any other impact, FLINK-28984 once pointed out that the shared file cannot be cleaned up during aborting, deleting files in an asynchronous thread maybe have the same issue.

[~gaoyunhaii] , [~Weijie Guo] would you like to take a look? ;;;","06/Dec/22 08:55;Ming Li;hi, [~Yanfei Lei]. I re-read FLINK-28984, it seems that it is a different problem from ours, can we first move the synchronous IO to the asynchronous thread to solve the problem of data not being processed? The problem of FLINK-28984 can be fixed in other issues.;;;","08/Dec/22 08:49;gaoyunhaii;Hi [~Ming Li] very thanks for reporting the issue and thanks [~Yanfei Lei] for tracking the issue! I also think this is indeed an issue and moving it to an asynchronous thread should be a reasonable solution. [~Ming Li]  Could you first open a formal PR?

There is also one concern:
 # The current `asyncOperationsThreadPool` is a cached thread pool, which do not have an upper limit of the number of threads, and it will create a new thread whenever there is not free thread when submitting tasks. Then if we have a large number of file to close, we might end up with a lot of threads, which might further cause a large number of memory consumption (1MB for each thread RSS region).
 # Thus we might change it to a thread pool with a limited maximum number of thread and one unbounded Blocking Queue. Also since the thread in this pool might be blocked, we might need to use a separate thread pool.

 ;;;","08/Dec/22 08:57;gaoyunhaii;[~Ming Li] Hi sorry I might also need to complement one thing: as an alternative option, could you also have a double check if it is possible to limit the maximum of time to wait for closing to finish? ;;;","09/Dec/22 09:10;Ming Li;[~gaoyunhaii] thanks for your reply.

We have tried setting timeout to limit the waiting time of these external IO operations, but if the timeout is too short, it will be easily affected by network, and if it is too long, it will cause no data processing for a long time. 

 
{quote}Thus we might change it to a thread pool with a limited maximum number of thread and one unbounded Blocking Queue. Also since the thread in this pool might be blocked, we might need to use a separate thread pool.
{quote}
I think this is a good idea, can you assign me this ticket so that I can make a formal PR based on this suggestion.;;;","15/Feb/23 08:41;pnowojski;Note, [we had a quick discussion in the PR|https://github.com/apache/flink/pull/21503#discussion_r1100231851] about which thread pool to actually use, and decided to use the existing async thread pool, but also to limit it's size.

merged commit d24760b into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,
The flame graph type is wrong,FLINK-30250,13507437,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,30/Nov/22 11:09,12/Dec/22 07:34,04/Jun/24 20:41,12/Dec/22 03:40,1.15.0,1.16.0,1.17.0,,,1.17.0,,,,,Runtime / REST,Runtime / Web Frontend,,,0,pull-request-available,,,"When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.
h2. Root cause:

When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type.

 [code link |https://github.com/apache/flink/blob/8bbf52688758bbede45df060a4c11e5fa228b6f0/flink-runtime-web/web-dashboard/src/app/pages/job/overview/flamegraph/job-overview-drawer-flamegraph.component.ts#L82]

!image-2022-11-30-19-08-42-067.png|width=1026,height=389!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13550,,,,,,,,,,,,,,,"30/Nov/22 11:08;fanrui;image-2022-11-30-19-08-42-067.png;https://issues.apache.org/jira/secure/attachment/13053327/image-2022-11-30-19-08-42-067.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 03:40:20 UTC 2022,,,,,,,,,,"0|z1czi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 08:15;xtsong;[~junhan], could you please take a look at this?;;;","09/Dec/22 02:21;junhan;[~fanrui] I left some comments in the PR :);;;","09/Dec/22 07:02;fanrui;[~xtsong] [~junhan] thanks for your help and review :);;;","12/Dec/22 03:40;junhan;master: 6af92a3eda4d243f765f6ea0c8c548ca70ca423a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
TableUtils.getRowTypeInfo() creating wrong TypeInformation,FLINK-30249,13507426,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,30/Nov/22 09:54,06/Jan/23 06:24,04/Jun/24 20:41,06/Jan/23 06:24,ml-2.0.0,ml-2.1.0,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-30 09:54:03.0,,,,,,,,,,"0|z1czfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark writer supports insert overwrite,FLINK-30248,13507412,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,30/Nov/22 08:59,29/Mar/23 01:53,04/Jun/24 20:41,29/Mar/23 01:53,,,,,,table-store-0.4.0,,,,,Table Store,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29614,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-30 08:59:53.0,,,,,,,,,,"0|z1czcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Time Travel reading for table store,FLINK-30247,13507405,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,30/Nov/22 08:11,26/Dec/22 09:00,04/Jun/24 20:41,26/Dec/22 09:00,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"For example:
- SELECT * FROM T /*+ OPTIONS('as-of-timestamp-mills'='121230')*/; Read snapshot specific by commit time.
- SELECT * FROM T /*+ OPTIONS('as-of-snapshot'='12')*/; Read snapshot specific by snapshot id.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 26 09:00:23 UTC 2022,,,,,,,,,,"0|z1czaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/22 09:00;TsReaper;master: 7e9cf0dd907c84b67850503ff61e27944363d06b
release-0.3: b7d7518e07cfdd0e57052e384fc56f50099b8a48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Registering the same metric object with different name and group will conflict in metric reporter,FLINK-30246,13507402,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiasun,renqs,renqs,30/Nov/22 07:51,16/Dec/22 01:32,04/Jun/24 20:41,,1.17.0,,,,,2.0.0,,,,,Runtime / Metrics,,,,0,,,,"Metrics are organized by maps in AbstractReporter and MetricQueryService, which uses metric object as key and metric name as value. This kind of implementation is problematic when the same metric object is registered multiple times, either with different names or under different metric groups, which will override each other and only one metric is kept finally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 07:55:47 UTC 2022,,,,,,,,,,"0|z1cza8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 04:01;xiasun;Hi, I would like to work on this issue, could you assign this ticket to me?;;;","08/Dec/22 04:03;zhuzh;Thanks for volunteering to take this ticket! I have assigned you the ticket. [~xiasun];;;","15/Dec/22 06:21;xiasun;I found that the access level of maps in AbstractReporter is protected, that means the user interface may change. I'm afraid this ticket can only be fixed in version 2.0 and avoid registering a same metric multiple times for now. [~zhuzh] ;;;","15/Dec/22 07:55;zhuzh;You are right. [~xiasun] Unfortunately, we cannot change the protected fields of AbstractReporter which is {{@Public}}.
Let's moving this ticket to Flink 2.0.
Thanks for looking into this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"NPE thrown when filtering decimal(18, 4) values after calling DecimalDataUtils.subtract method",FLINK-30245,13507394,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,30/Nov/22 07:16,06/Dec/22 07:54,04/Jun/24 20:41,06/Dec/22 07:53,1.13.6,1.17.0,,,,1.17.0,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"Reproduce code:
{code:java}
        TableEnvironment tableEnv = TableEnvironment.create(EnvironmentSettings.newInstance().build());

        tableEnv.executeSql(""create table datagen_source1 (disburse_amount int) with ('connector' = 'datagen')"");

        tableEnv.executeSql(""create table print_sink (disburse_amount Decimal(18,4)) with ('connector' = 'print')"");

        tableEnv.executeSql(""create view mid as select cast(disburse_amount as Decimal(18,4)) - cast(disburse_amount as Decimal(18,4)) as disburse_amount from datagen_source1"");

        tableEnv.executeSql(""insert into print_sink select * from mid where disburse_amount > 0 "").await();
{code}
Excpetion:
{code:java}
Exception in thread ""main"" java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
	at com.shopee.flink.BugExample2.main(BugExample2.java:21)
Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
	at akka.dispatch.OnComplete.internal(Future.scala:300)
	at akka.dispatch.OnComplete.internal(Future.scala:297)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.data.DecimalDataUtils.compare(DecimalDataUtils.java:217)
	at StreamExecCalc$17.processElement_split1(Unknown Source)
	at StreamExecCalc$17.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource.run(DataGeneratorSource.java:120)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}
Root cause:

For above sql, the generated StreamExecCalc has following code:
{code:java}
          isNull$299 = externalResult$298 == null;
          result$299 = null;
          if (!isNull$299) {
            result$299 = externalResult$298;
          }
          
          isNull$300 = isNull$296 || isNull$299;
          result$301 = null;
          if (!isNull$300) {
            
          
          result$301 = org.apache.flink.table.data.DecimalDataUtils.subtract(result$296, result$299, 19, 4);  // note the preciesion is 19
          
            isNull$300 = (result$301 == null);
          }
          
          
          isNull$302 = isNull$300 || false;
          result$303 = false;
          if (!isNull$302) {
            
          
          result$303 = org.apache.flink.table.data.DecimalDataUtils.compare(result$301, ((int) 0)) < 0;
          
            
          }
{code}
It seems the precision param of the DecimalDataUtils.subtract method is 19 rather than 18, but the precision of DecimalData value (result$296, result$299) is still 18. So the isCompact() method still returns true. Finally, this method will generate a problematic DecimalData:

!image-2022-11-30-15-11-03-706.png|width=559,height=277!

The returned DecimalData is not compacted (precision > MAX_LONG_DIGITS == 18). When comparing it with other int value, the decimalVal will be used, but for this value, the decimalVal is null. So the NPE thrown.

We found it on flink 1.13 and the latest master branch. Other versions of flink have not been tested, but there should be this bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:11;zhongwei;image-2022-11-30-15-11-03-706.png;https://issues.apache.org/jira/secure/attachment/13053312/image-2022-11-30-15-11-03-706.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 07:54:13 UTC 2022,,,,,,,,,,"0|z1cz8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 10:36;zhongwei;The precision change (18 -> 19) is caused by org.apache.calcite.rel.type.RelDataTypeSystem.deriveDecimalPlusType.;;;","30/Nov/22 11:01;zhongwei;[~jark] Could you help to take a look at this issue?;;;","30/Nov/22 11:41;jark;cc [~lsy] do you have time to have a look at this?;;;","06/Dec/22 07:53;jark;Fixed in 
 - master: bca57b7a222615869ddc38acdeb46c436b1bc0b3
 - release-1.16: TODO
 - release-1.15: TODO;;;","06/Dec/22 07:54;jark;[~zhongwei] could you open PRs for release-1.16 and release-1.15 branches? 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"When task using udf/udtf with jni, on k8s session the old TM will shut down and create new TM or the task will fail",FLINK-30244,13507385,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,AlexXXX,AlexXXX,30/Nov/22 06:41,20/Dec/22 16:47,04/Jun/24 20:41,20/Dec/22 16:47,1.15.3,,,,,,,,,,Deployment / Kubernetes,Kubernetes Operator,Runtime / Task,,0,,,,"We face a problem when we try to use flink on k8s to execute task with udf/udtf. When we finished or canceled a job and submit a same job, the old TM will be not reachable and restart. Why the TM have to be restart? In session mode, the TM should be reused by JM. Moreover, if we off restart strategy, this task will fail.

!image-2022-11-30-14-47-50-923.png!

 

First submit, the job will running:

!image-2022-11-30-15-00-06-710.png!

 

But, cancel it and submit the same:

!image-2022-11-30-15-04-45-696.png!

Internal server error, but in k8s the pod is running.

!image-2022-11-30-15-05-29-120.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/22 06:47;AlexXXX;image-2022-11-30-14-47-50-923.png;https://issues.apache.org/jira/secure/attachment/13053308/image-2022-11-30-14-47-50-923.png","30/Nov/22 07:00;AlexXXX;image-2022-11-30-15-00-06-710.png;https://issues.apache.org/jira/secure/attachment/13053311/image-2022-11-30-15-00-06-710.png","30/Nov/22 07:04;AlexXXX;image-2022-11-30-15-04-45-696.png;https://issues.apache.org/jira/secure/attachment/13053310/image-2022-11-30-15-04-45-696.png","30/Nov/22 07:05;AlexXXX;image-2022-11-30-15-05-29-120.png;https://issues.apache.org/jira/secure/attachment/13053309/image-2022-11-30-15-05-29-120.png",,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 09:20:44 UTC 2022,,,,,,,,,,"0|z1cz6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 06:46;gyfora;Could you please provide a proper title and description for the ticket?
Otherwise I would have to close it because it doesnt describe the problem.;;;","30/Nov/22 06:51;AlexXXX;Sorry, now I have given more details about my problem. ;;;","30/Nov/22 08:53;gyfora;When using the Native (active) kubernetes integration. Session cluster taskmanagers will be shut down after a certain timeout if the job running on it failed/finished.

So if you submit a new job within that time it should reuse the cluster, but after that it will create a new taskmanager;;;","15/Dec/22 09:20;pltbkd;[~AlexXXX]
Hi, do you have any question about this? If so you may post the log of JobManager which I think can help addressing the reason. While if there's no more question, maybe we can ask [~gyfora] to close the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove duplicate commas,FLINK-30243,13507384,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,slfan1989,slfan1989,30/Nov/22 06:37,30/Nov/22 08:34,04/Jun/24 20:41,30/Nov/22 07:51,1.16.0,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"When reading the code, I found that there are duplicate commas in the comments, I submit pr for improvement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 30 08:00:38 UTC 2022,,,,,,,,,,"0|z1cz68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:51;martijnvisser;Per https://flink.apache.org/contributing/code-style-and-quality-pull-requests.html there's no need to file a Jira for these issues. Please adhere to the contribution guide;;;","30/Nov/22 08:00;slfan1989;[~martijnvisser] Thanks for the documentation!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push localHashAggregate pass the union node,FLINK-30242,13507380,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,30/Nov/22 06:18,20/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,pull-request-available,,"The local hash aggregate after union will have an extra shuffle stage. We can swap it with the union node so the local hash aggregate can chain with the mapper stage saving the unnecessary shuffle, especially in the batch job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/22 03:17;aitozi;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13053357/screenshot-1.png","01/Dec/22 03:18;aitozi;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13053358/screenshot-2.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:03 UTC 2023,,,,,,,,,,"0|z1cz5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:09;Runking;I also find this problem in flink-1.11, and to meet this optimization in olap, we have implemented these related work internally and presented in FFA 2022, if you need this, we would like to consider to bring back it to the community, feel free to let me know if you want implement it by yourself.;;;","30/Nov/22 07:28;aitozi;Hi [~Runking], Sorry I have not noticed that. If you have already done, you can take this ticket :)

BTW,  can you share your presentation link, I'd like to take a look ;;;","01/Dec/22 02:24;Runking;[~aitozi] thanks, the link is here: [https://developer.aliyun.com/special/ffa2022?spm=ffa.ffa-home.0.0.35d843c9XkAp7b|https://developer.aliyun.com/special/ffa2022?spm=ffa.ffa-home.0.0.35d843c9XkAp7b,]

BTW, which scenario do you meet this problem, is it an urgent needs?;;;","01/Dec/22 03:18;aitozi;> which scenario do you meet this problem?

I met this when we ran the batch job with flink-1.15. There is an unnecessary shuffle between the map node and local hash aggregate, and this will impact the performance. 
It's not an urgent need. We have already added a rule to support the local hash aggregate and union transpose. And it's a common optimization rule. So, it would be nice to contribute it upstream.
If you have already done the work, you can take this ticket. If not, I can prepare a PR for this.

BTW, I post two images to describe our use case problem

Thanks.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML Iteration ConcurrentModificationException,FLINK-30241,13507352,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,yunfengzhou,yunfengzhou,30/Nov/22 01:40,10/Jan/23 04:56,04/Jun/24 20:41,10/Jan/23 04:56,ml-2.1.0,,,,,,,,,,Library / Machine Learning,,,,0,,,,"https://github.com/jiangxin369/flink-ml/actions/runs/3577811156/jobs/6017233847


{code}
___________________ LinearRegressionTest.test_get_model_data ___________________

self = <ml.lib.regression.tests.test_linearregression.LinearRegressionTest testMethod=test_get_model_data>

    def test_get_model_data(self):
        regression = LinearRegression().set_weight_col('weight')
        model = regression.fit(self.input_data_table)
        model_data = self.t_env.to_data_stream(
>           model.get_model_data()[0]).execute_and_collect().next()

pyflink/ml/lib/regression/tests/test_linearregression.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/pyflink/datastream/data_stream.py:1760: in next
    if not self._j_closeable_iterator.hasNext():
/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/py4j/java_gateway.py:1322: in __call__
    answer, self.gateway_client, self.target_id, self.name)
/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/pyflink/util/exceptions.py:146: in deco
    return f(*a, **kw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

answer = 'xro12236'
gateway_client = <py4j.java_gateway.GatewayClient object at 0x7fdb862ca190>
target_id = 'o12139', name = 'hasNext'

    def get_return_value(answer, gateway_client, target_id=None, name=None):
        """"""Converts an answer received from the Java gateway into a Python object.
    
        For example, string representation of integers are converted to Python
        integer, string representation of objects are converted to JavaObject
        instances, etc.
    
        :param answer: the string returned by the Java gateway
        :param gateway_client: the gateway client used to communicate with the Java
            Gateway. Only necessary if the answer is a reference (e.g., object,
            list, map)
        :param target_id: the name of the object from which the answer comes from
            (e.g., *object1* in `object1.hello()`). Optional.
        :param name: the name of the member from which the answer comes from
            (e.g., *hello* in `object1.hello()`). Optional.
        """"""
        if is_error(answer)[0]:
            if len(answer) > 1:
                type = answer[1]
                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
                if answer[1] == REFERENCE_TYPE:
                    raise Py4JJavaError(
                        ""An error occurred while calling {0}{1}{2}.\n"".
>                       format(target_id, ""."", name), value)
E                   py4j.protocol.Py4JJavaError: An error occurred while calling o12139.hasNext.
E                   : java.lang.RuntimeException: Failed to fetch next result
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
E                   	at sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)
E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E                   	at java.lang.reflect.Method.invoke(Method.java:498)
E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
E                   	at java.lang.Thread.run(Thread.java:750)
E                   Caused by: java.io.IOException: Failed to fetch job execution result
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
E                   	... 11 more
E                   Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
E                   	... 13 more
E                   Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
E                   	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
E                   	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
E                   	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
E                   	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
E                   	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
E                   	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
E                   	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
E                   	... 13 more
E                   Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
E                   	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
E                   	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
E                   	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
E                   	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E                   	at java.lang.reflect.Method.invoke(Method.java:498)
E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
E                   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
E                   	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
E                   	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
E                   	at akka.actor.Actor.aroundReceive(Actor.scala:537)
E                   	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
E                   	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
E                   	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
E                   	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
E                   	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
E                   Caused by: java.util.ConcurrentModificationException
E                   	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:648)
E                   	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
E                   	at org.apache.flink.iteration.operator.HeadOperator.parseInputChannelEvents(HeadOperator.java:463)
E                   	at org.apache.flink.iteration.operator.HeadOperator.endInput(HeadOperator.java:391)
E                   	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96)
E                   	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97)
E                   	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)
E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
E                   	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
E                   	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
E                   	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
E                   	at java.lang.Thread.run(Thread.java:750)

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-30 01:40:02.0,,,,,,,,,,"0|z1cyz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Doc error pointing to TableKind#MANAGED ,FLINK-30240,13507312,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,samrat007,samrat007,29/Nov/22 18:33,02/Dec/22 13:07,04/Jun/24 20:41,02/Dec/22 13:07,,,,,,1.17.0,,,,,Documentation,,,,0,pull-request-available,,,"TableKind#MANAGED is referenced in [Catalog.java|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/Catalog.java#L320]. 
but TableKind is an enum consisting of only 2 types 
`TABLE` and `VIEW`  [here|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/CatalogBaseTable.java#L40]. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 13:07:28 UTC 2022,,,,,,,,,,"0|z1cyq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:06;dannycranmer;[~samrat007] thanks for the contribution. For hotfixes like this a Jira is not usually required, see [1]:
""Note: trivial hot fixes such as typos or syntax errors can be opened as a [hotfix] pull request, without a Jira ticket.""

[1] https://flink.apache.org/contributing/contribute-code.html;;;","02/Dec/22 13:07;dannycranmer;Merged commit [{{fb27e68}}|https://github.com/apache/flink/commit/fb27e6893506006b9a3b1ac3e9b878fb6cad061a] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The flame graph doesn't work due to groupExecutionsByLocation has bug,FLINK-30239,13507290,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,29/Nov/22 16:17,06/Dec/22 08:57,04/Jun/24 20:41,06/Dec/22 08:06,1.16.0,1.17.0,,,,1.16.1,1.17.0,,,,Runtime / REST,Runtime / Web Frontend,,,0,pull-request-available,,,"The flame graph cannot be generated forever when multiple tasks in the same TM. It's caused by FLINK-26074

 
h1. Root cause:

A Set cannot be converted to an ImmutableSet during the aggregation of ExecutionAttemptIDs. It will cause only the first ExecutionAttemptID of the TM to be added to the set, the second ExecutionAttemptID will fail.

 

!image-2022-11-30-00-14-11-355.png!

 

 

 

!image-2022-11-30-00-11-09-728.png!

 

Exception Info: 

!image-2022-11-30-00-10-48-940.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26074,,,,,,,,,,,,,,,"29/Nov/22 16:10;fanrui;image-2022-11-30-00-10-48-940.png;https://issues.apache.org/jira/secure/attachment/13053286/image-2022-11-30-00-10-48-940.png","29/Nov/22 16:11;fanrui;image-2022-11-30-00-11-09-728.png;https://issues.apache.org/jira/secure/attachment/13053285/image-2022-11-30-00-11-09-728.png","29/Nov/22 16:14;fanrui;image-2022-11-30-00-14-11-355.png;https://issues.apache.org/jira/secure/attachment/13053284/image-2022-11-30-00-14-11-355.png",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 08:06:04 UTC 2022,,,,,,,,,,"0|z1cylc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 08:06;xtsong;- master (1.17): 95be1e5abfe78d7ed8d171a779f280277f51569c
- release-1.16: 0f98c6aad1f5bc8ad25f7608c419a5e396b8e8ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unified Sink committer does not clean up state on final savepoint,FLINK-30238,13507239,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Invalid,,fpaul,fpaul,29/Nov/22 11:01,11/Apr/24 21:54,04/Jun/24 20:41,11/Apr/24 21:54,1.15.3,1.16.1,1.17.0,,,,,,,,Connectors / Common,,,,4,,,,"During stop-with-savepoint the committer only commits the pending committables on notifyCheckpointComplete.

This has several downsides.
 * Last committableSummary has checkpoint id LONG.MAX and is never cleared from the state leading to that stop-with-savepoint does not work when the pipeline recovers from a savepoint 
 * While the committables are committed during stop-with-savepoint they are not forwarded to post-commit topology, potentially losing data and preventing to close open transactions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:27;agg_neha07;Screenshot 2023-03-09 at 1.47.11 PM.png;https://issues.apache.org/jira/secure/attachment/13056217/Screenshot+2023-03-09+at+1.47.11+PM.png","09/Mar/23 12:27;agg_neha07;image (8).png;https://issues.apache.org/jira/secure/attachment/13056218/image+%288%29.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 30 10:43:02 UTC 2023,,,,,,,,,,"0|z1cya0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 19:27;martijnvisser;[~gaoyunhaii] [~maguowei] Given your involvement with the Sink V2, what's your thought on this ticket?;;;","09/Feb/23 08:31;gaoyunhaii;Hi, I might first complement some background before continuing discussion:

Currently the sink topology could be simplified as  writer -> committer -> post-committer topology, and the rough process of two-phase commit is
 # Writer writes to temporary transaction or intermediate file. 
 # Writer emits Committables, which are the handles of the transactions or intermediate files, to the Committer on prepareSnapshotPreBarrier. 
 # Committer records these Committables on snapshotting. 
 # Committer commits these Committables on notifyCheckpointComplete. 
 # Committer emits the Committed Committables to the post-committer topology

If the job is bounded, then for the last piece of data, the process will be slightly different, based on the current implementation:
 # Writer received END_OF_DATA message and emits current pending Committables. Logically it should not emits new record since now, and also will start waiting for the final checkpoint. 
 # Committer then also received END_OF_DATA and waiting for the final checkpoint. 
 # Committer records the Committables on final checkpoint snapshotting. 
 # Committer commits these Committables on the final checkpoint notifyCheckpointComplete. Since it has already received END_OF_DATA, it could not emits these Committables to the post-committer topology at this time point. 

Thus It looks to me that the root issue is how we deal with the last batch of Committables. The original thought for this issue is some method like:
 # For Committer, it emit the un-committed Committables to the post-committer topology, with special tags. 
 # The post-commit topology might use connector specialized method to check if these Committables are committed, then wait until all the Committables to commit and do the cleanup in the final checkpoint notifyCheckpointComplete. 

What do you think about that ?

 ;;;","09/Feb/23 09:24;martijnvisser;[~pnowojski] WDYT?;;;","09/Feb/23 16:09;pnowojski;[~gaoyunhaii], thanks for the explanation, but I'm still a bit confused. Could you rephrase and clearly point where lies and what's the problem? I think I see some issues, but bounded input seems to be something else compared to what's mentioned in this ticket about {{stop-with-savepoint}}.;;;","16/Feb/23 07:46;gaoyunhaii;Hi [~pnowojski] , for this specific issue, based on my understanding, 
 # It looks to me the first issue might refer to the situation that on endOfInput(), the Writer will emit the summary message tag with \{checkpoint id = MAX_VALUE} to the Committer, followed by the remaining Committables. But with the current process, endOfInput() should not be called on stop-with-savepoint without draining, thus sorry I'm also not fully understand this part. 
 # For the second issue, it looks to me it refers to how the last piece of the committables should be passed to the post-committer topology, as posted in the above. Although here it refers to stop-with-savepoint, but it looks to me it should be stop-with-savepoint –-drain, since for stop-with-savepoint, the last piece of Committables should be still recorded in the last savepoint, and they will be re-committed after restarted. 

Perhaps [~fpaul] could give some more details?;;;","16/Feb/23 09:34;fpaul;Sorry for joining late I was on PTO.

[~gaoyunhaii] 
 # I think your analysis is not fully correct. endOfInput() is also called on stop-with-savepoint if the source, drain only advances the max watermark. I took one of our KafkaSink tests on 1.15 [1][2] and set a breakpoint at [3], and it is triggered on savepoint. So the special marker \{checkpoint id = MAX_VALUE} is emitted from the SinkWriters and stored in the Committer state (only committables with lower checkpoint ids are committed forwarded). The marker is initialized again if a new pipeline tries to recover from that state but is never committed. The problem starts if you try to take another savepoint because the committer will receive the marker again, and currently, we do not allow two summaries with the same checkpoint id.
 # The problem here is more about a possible migration story. So far, when users use the sink and want to close all pending transactions, we advise stopping the job with a savepoint that should finalize all open transactions (- some windows if drain is not used). In the case of sinks with a post-commit topology, that doesn't entirely work because it essentially needs two notifyCheckpointCompletes first to flush the committer and then the, for example, global committer.

 

We should concentrate first on the first issue because it atm blocks all sinks running on Flink 1.15+, and figure out how to improve the situation for the second one later.

 

[1] [https://github.com/apache/flink/blob/release-1.15/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaSinkITCase.java#L177] 

[2] [https://github.com/apache/flink/blob/c6b649bf937976038dbfcd00e59c51b8d886ad96/flink-test-utils-parent/flink-connector-test-utils/src/main/java/org/apache/flink/connector/testframe/testsuites/SinkTestSuiteBase.java#L174]

[3] [https://github.com/apache/flink/blob/c6b649bf937976038dbfcd00e59c51b8d886ad96/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/SinkWriterOperator.java#L183] ;;;","03/Mar/23 07:36;agg_neha07;This issue is not limited to stop-with-savepoint, It is the same for trigger savepoint as well.;;;","06/Mar/23 09:50;martijnvisser;[~agg_neha07] Given the difference between stop-with-savepoint and savepoints in general, can you explain the reproducible steps for your comment?;;;","06/Mar/23 10:31;agg_neha07;While migrating from Flink 1.13 to Flink 1.16, I found that flink with the delta sink pipeline is not writing data to the filesystem on savepoint while it does in checkpointing. Savepoint and checkpoint behaved the same in Flink 1.13 but in Flink 1.16 Savepoint is not acting the same as a checkpoint in the case of Delta sink.;;;","07/Mar/23 06:40;gaoyunhaii;Hi [~fpaul] sorry for the long delay for it took me some time to do the double confirmation:
 # Currently endOfInput() should only be called on there is no new data or stop-with-savepoint --drain. For these two cases, the job is logically ""terminated"", namely they should not be restarted again: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/cli/#stopping-a-job-gracefully-creating-a-final-savepoint
 # For normal savepoint or stop-with-savepoint without draining, the endOfInput() will not be called, and the job is logically ""suspended"" and could be restarted from the savepoint again. 
 # For this specific Kafka test case, I think it should not use stopWithSavepoint with advancedTimestamp = true (which is in fact translated to stop-with-savepont --drain finally), since it tries to create a savepoint that the job could be restarted from. 

 

Thus for the real-world case, if a job has ""endOfInput()"" called, it is expected to be not restarted any more. ;;;","07/Mar/23 06:50;gaoyunhaii;Hi [~agg_neha07] I think the change of intermediate savepoint (namely the ones not triggered by stop-with-savepoint) comes from the modification to skip savepoint for recovery: [https://cwiki.apache.org/confluence/display/FLINK/FLIP-193%3A+Snapshots+ownership,] which is merged in 1.15.  With this modification the intermediate savepoint will only do snapshots and will not commit side-effect. 

May I have a double confirmation about the detailed issue that the change caused issues for the delta sink pipeline? ;;;","09/Mar/23 12:31;agg_neha07;Attaching the screenshot of one Kafka source and Delta sink pipeline with Flink 1.16. When you say intermediate savepoint doesn't commit anything, why is a new temp file getting created after savepoint?
Flink UI shows savepoint time as 12:40:25 and the s3 ls command is showing 12:41:01 parquet file creation time.

 

!Screenshot 2023-03-09 at 1.47.11 PM.png!

 

 

 

!image (8).png|width=3502,height=2189!;;;","09/Mar/23 12:45;martijnvisser;[~agg_neha07] Let's stay on topic in this ticket; perhaps it better to move this question to the User mailing list to avoid that we are mixing two topics in this ticket;;;","04/Jul/23 12:08;jingge;[~gaoyunhaii] Do you have time to check this task again?;;;","05/Jul/23 10:15;ConradJam;I would like to report that I have the same problem with 1.16.2. I cannot use drain to empty a calculation with a window;;;","06/Jul/23 02:15;gaoyunhaii;Hi [~jingge]  may I have a double confirmation on the detailed issue now? Since in the above there seems some divergence on the cause of this issue. ;;;","11/Jul/23 16:27;jingge;[~ConradJam] would you like to describe the issue you got. We'd like to have the most up-to-date status of the issue. Thanks!;;;","31/Aug/23 17:38;tzulitai;hi all, I'd like to move this ticket forward and make a call if things are working as expected, or there's an actual bug.

First of all, lets narrow down the scope of this ticket to address only the following :
{{stop-with-savepoint}} stores last CommittableSummary with LONG_MAX checkpoint ID to be persisted in savepoints, which prevents future {{stop-with-savepoint}} to succeed after restore because CommittableSummaries should always have different checkpoint IDs.

The separate issue with post-commit topologies not receiving committed committables can be addressed separately.

Summarizing the facts we have so far, looking at the comments above and discussing this offline with [~martijnvisser] [~Gerrrr]:
 * If a LONG_MAX checkpoint ID special marker is propagated, it means {{endOfInput}} was called and it is expected behavior that the job will not function properly if restored from the generated savepoint. {{endOfInput}} should only ever be called when 1) reaching bounded end of input, or 2) stop-with-savepoint with the --drain option.
 * The reason why the Kafka ITCase referenced by [~fpaul] has {{endOfInput}} being called is because of the setup {{{}advanceTimestamp = true{}}}, which translates equivalently to stop-with-savepoint with --drain.

With the above, can we conclude that:
 * {{stop-with-savepoint}} without draining works as expected, such that {{endOfInput}} is not being called and the LONG_MAX CommittableSummary is not being sent and stored as committables state.
 * {{stop-with-savepoint}} _with draining_ is also working as expected - jobs restored from these savepoints are not expected to function properly in the first place.

 

I'm looking to verify that {{stop-with-savepoint}} without draining indeed doesn't call the {{endOfInput}} method - if that's the case, then I think we have a case closed? If that's the case - the only fix we need is to adjust the Kafka ITCase.

In parallel, just wanna jot these thoughts down to see if I'm missing anything obvious cc [~fpaul] [~gaoyunhaii].;;;","28/Sep/23 16:47;martijnvisser;[~tzulitai] Shall we move this topic to the Dev ML?;;;","29/Sep/23 16:00;tzulitai;[~martijnvisser] I've replied to your SinkFunction deprecation / SinkV2 public thread in the ML, and touched on this issue as an attempt to revive the discussion there. Lets move the discussion there and let me know what you think.;;;","30/Oct/23 10:43;martijnvisser;This ticket is pending to be closed, unless new feedback is provided in the discussion thread (see https://lists.apache.org/thread/25z3ld1ntzkonmp57joth174489g420y);;;",,,,,,,,,,
Only bundle a single Zookeeper version,FLINK-30237,13507225,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Nov/22 09:10,29/Nov/22 17:28,04/Jun/24 20:41,29/Nov/22 17:28,,,,,,1.17.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Way back when we added support for ZK 3.5 we started bundling 2 zookeeper clients, because of incompatibilities between 3.4 and 3.5. This is no longer required, and we could simplify things again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13417,,,,,,,FLINK-16350,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 17:28:19 UTC 2022,,,,,,,,,,"0|z1cy6w:",9223372036854775807,"The Flink distribution no longer bundles 2 different Zookeeper client jars (one in lib/opt respectively). Instead only 1 client will be bundled within the flink-dist jar.
This has no effect on the supported Zookeeper server versions.",,,,,,,,,,,,,,,,,,,"29/Nov/22 17:28;chesnay;master: 2aab91df5707d24259fcdcc37a413d89e19e14c1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertKafkaTableITCase failed due to timeout when creating a topic,FLINK-30236,13507202,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,29/Nov/22 07:26,16/Oct/23 07:09,04/Jun/24 20:41,16/Oct/23 07:09,1.17.0,,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,"The following build failed due to a timeout when creating the topic.
{code:java}
Nov 28 16:25:44 [ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 72.815 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase
Nov 28 16:25:44 [ERROR] UpsertKafkaTableITCase.testAggregate  Time elapsed: 31.629 s  <<< ERROR!
Nov 28 16:25:44 java.lang.IllegalStateException: Fail to create topic [word_count_json partitions: 4 replication factor: 1].
Nov 28 16:25:44 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
Nov 28 16:25:44 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.testAggregate(UpsertKafkaTableITCase.java:78)
Nov 28 16:25:44 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 28 16:25:44 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[...] {code}
I suspect this being a general issue with providing timeouts for tests.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43566&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37623",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 15:52:48 UTC 2023,,,,,,,,,,"0|z1cy1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 07:28;mapohl;There's [another stacktrace printed|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43566&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37698] indicating the same problem:
{code:java}
[...]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: org.apache.kafka.common.errors.TimeoutException: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000ms while awaiting InitProducerId
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000ms while awaiting InitProducerId{code}

But it appears that this test strangely didn't fail. [~ym] do you have some thoughts on that?;;;","29/Nov/22 08:32;gaborgsomogyi;Topic creation can fail because of the Kafka broker. This is hard to find out because such cases we need broker logs which is running in a different container. Double pain how to debug deadlock from logs.

The test most probably is passing because Kafka broker auto topic creation is enabled by default (auto.create.topics.enable). When it's enabled then manual topic creation is just additional code because any producer can create arbitrary topics :)
;;;","16/Mar/23 15:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comprehensive benchmarks on changelog checkpointing,FLINK-30235,13507198,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xiarui,xiarui,xiarui,29/Nov/22 07:13,20/Nov/23 07:16,04/Jun/24 20:41,20/Nov/23 07:16,,,,,,1.17.0,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,performance,,,"Changelog checkpointing is functionally usable right now. To make it as a productive feature, more comprehensive benchmarks are required. In this issue, I aim to answer the following two major concerns:
 * The expansion of full checkpoint size caused by changelog persistence;
 * The TPS regression caused by DTSL double-write;

By the way, I will also present other metrics related to checkpointing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 07:16:21 UTC 2023,,,,,,,,,,"0|z1cy0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:21;knaufk;I will mark this as ""Not finished"" for Flink 1.18 and remove the fixVersion from the ticket as the feature freeze has passed. Thanks, Konstantin (one of the release managers for Flink 1.18)

;;;","20/Nov/23 07:16;masteryhx;Resolved this in 1.17.0
See the [blog post|https://www.ververica.com/blog/generic-log-based-incremental-checkpoint] for more details about the benchmark results.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceReaderBase should provide an option to disable numRecordsIn metric registration,FLINK-30234,13507194,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,loserwang1024,renqs,renqs,29/Nov/22 06:55,17/Aug/23 07:55,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Connectors / Common,,,,1,,,,"Currently the numRecordsIn metric is pre-registered for all sources in SourceReaderBase. Considering different implementation of source reader, the definition of ""record"" might differ from the one we use in SourceReaderBase, hence numRecordsIn might be inaccurate.

We could introduce an option in SourceReader to disable the registration of numRecordsIn in SourceReaderBase and let the actual implementation to report the metric instead. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 06:30:32 UTC 2023,,,,,,,,,,"0|z1cy00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 14:06;Wencong Liu;Hello [~renqs] , I think this is a reasonable improvement. I'd like to take this ticket, could you assign this to me?;;;","03/Jan/23 03:16;renqs;Thanks [~Wencong Liu] ! I've assigned the ticket to you. Actually we need to introduce a new option for SourceReaderBase, which touches public API, so it's better to have a discussion in the ML.;;;","03/Jan/23 06:30;Wencong Liu;The discussion has been created. cc [~renqs] 

[[DISCUSS] Allow source readers extending SourceReaderBase to override numRecordsIn report logic-Apache Mail Archives|https://lists.apache.org/thread/pq7vrqc1vjbzj6of4wm5x8x7pkpdchox];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid full result partition should also work with speculative execution,FLINK-30233,13507178,13491747,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,29/Nov/22 04:26,22/Dec/22 09:36,04/Jun/24 20:41,22/Dec/22 09:36,1.17.0,,,,,1.17.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Hybrid full result partition is re-consumable now, and we have enabled `VertexwiseSchedulingStrategy` to support the scheduling of hybrid type edges in FLINK-29767. Now is the time to enable hybrid full result partition to support speculative execution.

In the case of speculative execution, downstream task can be scheduled only if all upstream task finished. In the next ticket, we will introduce a new mechanism to enable the downstream to be scheduled after partial upstream task is finished, thus improving the performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 09:36:48 UTC 2022,,,,,,,,,,"0|z1cxwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 09:36;xtsong;master (1.17): a56a4d38d1f750dce947e13f0593404dee6b69e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shading of netty epoll shared library does not account for ARM64 platform,FLINK-30232,13506892,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,cthomson,cthomson,28/Nov/22 22:05,19/Jan/23 09:13,04/Jun/24 20:41,19/Jan/23 09:13,1.15.2,,,,,shaded-17.0,,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,"While evaluating migration of Flink application to Graviton 2 based EC2 instances in a AWS managed Kubernetes service (EKS) using Kubernetes 1.23, found that the shaded Netty library renames the AMD64 version of the shared library as part of relocation of the Netty library but does not rename the matching ARM64 shared library. This results in the following error when `taskmanager.network.netty.transport: epoll` is used:



 

 

{{Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll in java.library.path}}
{{at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860) ~[?:1.8.0_352]}}
{{at java.lang.Runtime.loadLibrary0(Runtime.java:843) ~[?:1.8.0_352]}}
{{at java.lang.System.loadLibrary(System.java:1136) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]}}
{{at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]}}
{{at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:335) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:327) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:293) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:309) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.<clinit>(EpollEventLoop.java:51) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:185) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:36) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:60) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:49) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:113) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:100) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:77) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyClient.initEpollBootstrap(NettyClient.java:164) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyClient.init(NettyClient.java:79) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.start(NettyConnectionManager.java:87) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start(NettyShuffleEnvironment.java:329) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerServices.fromConfiguration(TaskManagerServices.java:293) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:623) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.createTaskExecutorService(TaskManagerRunner.java:559) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManagerRunnerServices(TaskManagerRunner.java:245) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.start(TaskManagerRunner.java:288) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:481) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$runTaskManagerProcessSecurely$5(TaskManagerRunner.java:525) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:525) [flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:505) [flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.kubernetes.taskmanager.KubernetesTaskExecutorRunner.main(KubernetesTaskExecutorRunner.java:39) [flink-dist-1.15.2.jar:1.15.2]}}
{{Caused by: java.io.FileNotFoundException: META-INF/native/liborg_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64.so}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:170) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:306) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{... 25 more}}

 

[https://github.com/apache/flink-shaded/blob/3082afc952e68366e9fefe4d1181c4666969ee67/flink-shaded-netty-4/pom.xml#L97] appears to be where the problem is, it only renames the x86_64 shared library, it doesn’t account for aarch_64 shared library.","Kubernetes 1.23 provided by AWS managed Kubernetes service (EKS) with Graviton 2 based EC2 instances (ARM64) using Flink 1.15.2, native epoll enabled (taskmanager.network.netty.transport: epoll)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 09:13:11 UTC 2023,,,,,,,,,,"0|z1cw4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 07:55;martijnvisser;[~cthomson] Thanks for the report! Do you want to open a PR to fix this for a future version?;;;","29/Nov/22 14:36;cthomson;Our team is trying to get approval to contribute the fix for this back to the Apache Flink project, I'll know in two weeks if that is an option.;;;","19/Dec/22 19:39;martijnvisser;[~cthomson] Have you heard anything? Else I can take a look myself;;;","20/Dec/22 14:07;cthomson;We haven't heard anything yet, more likely to be the latter part of January before we get feedback on one of our team members being able to contribute.;;;","21/Dec/22 11:33;martijnvisser;I've created https://github.com/apache/flink-shaded/pull/118 so that we can include it before the 1.17 feature freeze;;;","19/Jan/23 09:13;martijnvisser;Fixed in master: ba56570bbe395385c17240a2119c1c2cf8141675;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update to Fabric8 Kubernetes Client to a version that has automatic renewal of service account tokens,FLINK-30231,13506861,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ouyangwuli,cthomson,cthomson,28/Nov/22 21:50,11/Jan/23 10:25,04/Jun/24 20:41,05/Jan/23 18:31,1.15.2,,,,,1.17.0,,,,,Deployment / Kubernetes,,,,1,pull-request-available,,,"The Fabric8 Kubernetes Client library was updated to account for Kubernetes configuration changes that result in service account tokens becoming bounded in duration, needing to be renewed after an hour. The AWS managed Kubernetes service (AWS EKS) currently has a configuration change that extends the one hour bounded duration for the account to 90 days but this will eventually be removed by AWS and  produces warnings.

It appears that Fabric8 Kubernetes Client library version 5.12.4 is the closest version to 5.12.3 that is currently in use by the Apache Flink project to contain https://github.com/fabric8io/kubernetes-client/issues/2271.","Kubernetes 1.23 environment provided by Amazon Web Services managed Kubernetes service (EKS), using Flink 1.15.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 10:25:27 UTC 2023,,,,,,,,,,"0|z1cvy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jan/23 15:41;liadsh;Hey,

We need this feature too.

AWS are removing EKS version 1.21 soon and the grace period for token refresh with it.

Any chance to get this handled so we can upgrade our clusters? This is a big blocker for us.;;;","01/Jan/23 18:47;martijnvisser;[~liadsh] Can you open a PR for this upgrade?;;;","04/Jan/23 11:56;ouyangwuli;[~martijnvisser]  Can you help me check it out？;;;","05/Jan/23 18:31;martijnvisser;Fixed in master: 1028ee3285257d39312a2c9b0f91847cca1a2e68;;;","05/Jan/23 18:33;martijnvisser;[~cthomson] [~liadsh] This has been fixed for Flink 1.17. We don't backport these changes (since they introduce new functionality to patch versions of Flink) to Flink 1.15 or Flink 1.16. In case you need that, you can manually cherry-pick these changes and build a Flink version yourself with this patch. ;;;","11/Jan/23 10:05;liadsh;[~martijnvisser] 

I tried building flink myself with the instructions from the flink git repo and failed with an error:
{code:java}
[INFO] --- scala-maven-plugin:3.2.2:testCompile (scala-test-compile) @ flink-scala_2.12 ---
[INFO] /build/flink/flink-scala/src/test/java:-1: info: compiling
[INFO] /build/flink/flink-scala/src/test/scala:-1: info: compiling
[INFO] Compiling 62 source files to /build/flink/flink-scala/target/test-classes at 1673431091029
[ERROR] error: java.io.IOError: java.lang.RuntimeException: /packages cannot be represented as URI
[INFO]     at java.base/jdk.internal.jrtfs.JrtPath.toUri(JrtPath.java:175)
[INFO]     at scala.tools.nsc.classpath.JrtClassPath.asURLs(DirectoryClassPath.scala:204)
[INFO]     at scala.tools.nsc.classpath.AggregateClassPath.$anonfun$asURLs$1(AggregateClassPath.scala:55)
[INFO]     at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:240)
[INFO]     at scala.collection.Iterator.foreach(Iterator.scala:937)
[INFO]     at scala.collection.Iterator.foreach$(Iterator.scala:937)
[INFO]     at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
[INFO]     at scala.collection.IterableLike.foreach(IterableLike.scala:70)
[INFO]     at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
[INFO]     at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
[INFO]     at scala.collection.TraversableLike.flatMap(TraversableLike.scala:240)
[INFO]     at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:237)
[INFO]     at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
[INFO]     at scala.tools.nsc.classpath.AggregateClassPath.asURLs(AggregateClassPath.scala:55)
[INFO]     at scala.tools.nsc.typechecker.Macros.findMacroClassLoader(Macros.scala:66)
[INFO]     at scala.tools.nsc.typechecker.Macros.findMacroClassLoader$(Macros.scala:65)
[INFO]     at scala.tools.nsc.Global$$anon$1.findMacroClassLoader(Global.scala:469)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.$anonfun$defaultMacroClassloaderCache$2(MacroRuntimes.scala:56)
[INFO]     at scala.reflect.internal.SymbolTable$perRunCaches$$anon$1.apply(SymbolTable.scala:457)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.defaultMacroClassloader(MacroRuntimes.scala:58)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.defaultMacroClassloader$(MacroRuntimes.scala:58)
[INFO]     at scala.tools.nsc.Global$$anon$1.defaultMacroClassloader(Global.scala:469)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes$MacroRuntimeResolver.resolveRuntime(MacroRuntimes.scala:76)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.$anonfun$standardMacroRuntime$3(MacroRuntimes.scala:38)
[INFO]     at scala.collection.mutable.MapLike.getOrElseUpdate(MapLike.scala:206)
[INFO]     at scala.collection.mutable.MapLike.getOrElseUpdate$(MapLike.scala:203)
[INFO]     at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.standardMacroRuntime(MacroRuntimes.scala:38)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.standardMacroRuntime$(MacroRuntimes.scala:31)
[INFO]     at scala.tools.nsc.Global$$anon$1.standardMacroRuntime(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$14.default(AnalyzerPlugins.scala:448)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$14.default(AnalyzerPlugins.scala:445)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.invoke(AnalyzerPlugins.scala:401)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroRuntime(AnalyzerPlugins.scala:445)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroRuntime$(AnalyzerPlugins.scala:445)
[INFO]     at scala.tools.nsc.Global$$anon$1.pluginsMacroRuntime(Global.scala:469)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.macroRuntime(MacroRuntimes.scala:25)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.macroRuntime$(MacroRuntimes.scala:25)
[INFO]     at scala.tools.nsc.Global$$anon$1.macroRuntime(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Macros$MacroExpander.$anonfun$expand$1(Macros.scala:619)
[INFO]     at scala.tools.nsc.Global.withInfoLevel(Global.scala:219)
[INFO]     at scala.tools.nsc.typechecker.Macros$MacroExpander.expand(Macros.scala:613)
[INFO]     at scala.tools.nsc.typechecker.Macros$MacroExpander.apply(Macros.scala:600)
[INFO]     at scala.tools.nsc.typechecker.Macros.standardMacroExpand(Macros.scala:764)
[INFO]     at scala.tools.nsc.typechecker.Macros.standardMacroExpand$(Macros.scala:762)
[INFO]     at scala.tools.nsc.Global$$anon$1.standardMacroExpand(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$12.default(AnalyzerPlugins.scala:432)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$12.default(AnalyzerPlugins.scala:429)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.invoke(AnalyzerPlugins.scala:401)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroExpand(AnalyzerPlugins.scala:429)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroExpand$(AnalyzerPlugins.scala:429)
[INFO]     at scala.tools.nsc.Global$$anon$1.pluginsMacroExpand(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Macros.macroExpand(Macros.scala:757)
[INFO]     at scala.tools.nsc.typechecker.Macros.macroExpand$(Macros.scala:757)
[INFO]     at scala.tools.nsc.Global$$anon$1.macroExpand(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.vanillaAdapt$1(Typers.scala:1163)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1223)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1196)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1216)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.typedImplicit1(Implicits.scala:759)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.typedImplicit0(Implicits.scala:696)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.scala$tools$nsc$typechecker$Implicits$ImplicitSearch$$typedImplicit(Implicits.scala:513)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch$ImplicitComputation.rankImplicits(Implicits.scala:1019)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch$ImplicitComputation.findBest(Implicits.scala:1050)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.searchImplicit(Implicits.scala:1107)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.bestImplicit(Implicits.scala:1477)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicit(Implicits.scala:95)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicit$(Implicits.scala:82)
[INFO]     at scala.tools.nsc.Global$$anon$1.inferImplicit(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicitFor(Implicits.scala:40)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicitFor$(Implicits.scala:39)
[INFO]     at scala.tools.nsc.Global$$anon$1.inferImplicitFor(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$applyImplicitArgs$2(Typers.scala:243)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.applyImplicitArgs(Typers.scala:236)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$adapt$1(Typers.scala:881)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adaptToImplicitMethod$1(Typers.scala:471)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1219)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5631)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.computeType(Typers.scala:5692)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.assignTypeToTree(Namers.scala:1079)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1629)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.memberSig(Namers.scala:1798)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1763)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer$MonoTypeCompleter.completeImpl(Namers.scala:842)
[INFO]     at scala.tools.nsc.typechecker.Namers$LockingTypeCompleter.complete(Namers.scala:1960)
[INFO]     at scala.tools.nsc.typechecker.Namers$LockingTypeCompleter.complete$(Namers.scala:1958)
[INFO]     at scala.tools.nsc.typechecker.Namers$TypeCompleterBase.complete(Namers.scala:1953)
[INFO]     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1530)
[INFO]     at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1678)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5240)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStat$1(Typers.scala:5681)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typedStats$10(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedBlock(Typers.scala:2468)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typed1$98(Typers.scala:5546)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedOutsidePatternMode$1(Typers.scala:480)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5581)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedDefDef(Typers.scala:5833)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5536)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStat$1(Typers.scala:5681)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typedStats$10(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1999)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1821)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5537)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStat$1(Typers.scala:5681)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typedStats$10(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5247)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5540)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Analyzer$typerFactory$TyperPhase.apply(Analyzer.scala:106)
[INFO]     at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:441)
[INFO]     at scala.tools.nsc.typechecker.Analyzer$typerFactory$TyperPhase.run(Analyzer.scala:96)
[INFO]     at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1467)
[INFO]     at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1451)
[INFO]     at scala.tools.nsc.Global$Run.compileSources(Global.scala:1444)
[INFO]     at scala.tools.nsc.Global$Run.compile(Global.scala:1567)
[INFO]     at scala.tools.nsc.Driver.doCompile(Driver.scala:35)
[INFO]     at scala.tools.nsc.MainClass.doCompile(Main.scala:24)
[INFO]     at scala.tools.nsc.Driver.process(Driver.scala:55)
[INFO]     at scala.tools.nsc.Driver.main(Driver.scala:68)
[INFO]     at scala.tools.nsc.Main.main(Main.scala)
[INFO]     at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
[INFO]     at java.base/java.lang.reflect.Method.invoke(Method.java:577)
[INFO]     at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)
[INFO]     at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)
[INFO] Caused by: java.lang.RuntimeException: /packages cannot be represented as URI
[INFO]     ... 131 more
[INFO] java.lang.reflect.InvocationTargetException
[INFO]     at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:119)
[INFO]     at java.base/java.lang.reflect.Method.invoke(Method.java:577)
[INFO]     at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)
[INFO]     at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)
[ERROR] Caused by: java.io.IOError: java.lang.RuntimeException: /packages cannot be represented as URI
[INFO]     at java.base/jdk.internal.jrtfs.JrtPath.toUri(JrtPath.java:175)
[INFO]     at scala.tools.nsc.classpath.JrtClassPath.asURLs(DirectoryClassPath.scala:204)
[INFO]     at scala.tools.nsc.classpath.AggregateClassPath.$anonfun$asURLs$1(AggregateClassPath.scala:55)
[INFO]     at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:240)
[INFO]     at scala.collection.Iterator.foreach(Iterator.scala:937)
[INFO]     at scala.collection.Iterator.foreach$(Iterator.scala:937)
[INFO]     at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
[INFO]     at scala.collection.IterableLike.foreach(IterableLike.scala:70)
[INFO]     at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
[INFO]     at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
[INFO]     at scala.collection.TraversableLike.flatMap(TraversableLike.scala:240)
[INFO]     at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:237)
[INFO]     at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
[INFO]     at scala.tools.nsc.classpath.AggregateClassPath.asURLs(AggregateClassPath.scala:55)
[INFO]     at scala.tools.nsc.typechecker.Macros.findMacroClassLoader(Macros.scala:66)
[INFO]     at scala.tools.nsc.typechecker.Macros.findMacroClassLoader$(Macros.scala:65)
[INFO]     at scala.tools.nsc.Global$$anon$1.findMacroClassLoader(Global.scala:469)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.$anonfun$defaultMacroClassloaderCache$2(MacroRuntimes.scala:56)
[INFO]     at scala.reflect.internal.SymbolTable$perRunCaches$$anon$1.apply(SymbolTable.scala:457)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.defaultMacroClassloader(MacroRuntimes.scala:58)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.defaultMacroClassloader$(MacroRuntimes.scala:58)
[INFO]     at scala.tools.nsc.Global$$anon$1.defaultMacroClassloader(Global.scala:469)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes$MacroRuntimeResolver.resolveRuntime(MacroRuntimes.scala:76)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.$anonfun$standardMacroRuntime$3(MacroRuntimes.scala:38)
[INFO]     at scala.collection.mutable.MapLike.getOrElseUpdate(MapLike.scala:206)
[INFO]     at scala.collection.mutable.MapLike.getOrElseUpdate$(MapLike.scala:203)
[INFO]     at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.standardMacroRuntime(MacroRuntimes.scala:38)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.standardMacroRuntime$(MacroRuntimes.scala:31)
[INFO]     at scala.tools.nsc.Global$$anon$1.standardMacroRuntime(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$14.default(AnalyzerPlugins.scala:448)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$14.default(AnalyzerPlugins.scala:445)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.invoke(AnalyzerPlugins.scala:401)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroRuntime(AnalyzerPlugins.scala:445)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroRuntime$(AnalyzerPlugins.scala:445)
[INFO]     at scala.tools.nsc.Global$$anon$1.pluginsMacroRuntime(Global.scala:469)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.macroRuntime(MacroRuntimes.scala:25)
[INFO]     at scala.reflect.macros.runtime.MacroRuntimes.macroRuntime$(MacroRuntimes.scala:25)
[INFO]     at scala.tools.nsc.Global$$anon$1.macroRuntime(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Macros$MacroExpander.$anonfun$expand$1(Macros.scala:619)
[INFO]     at scala.tools.nsc.Global.withInfoLevel(Global.scala:219)
[INFO]     at scala.tools.nsc.typechecker.Macros$MacroExpander.expand(Macros.scala:613)
[INFO]     at scala.tools.nsc.typechecker.Macros$MacroExpander.apply(Macros.scala:600)
[INFO]     at scala.tools.nsc.typechecker.Macros.standardMacroExpand(Macros.scala:764)
[INFO]     at scala.tools.nsc.typechecker.Macros.standardMacroExpand$(Macros.scala:762)
[INFO]     at scala.tools.nsc.Global$$anon$1.standardMacroExpand(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$12.default(AnalyzerPlugins.scala:432)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins$$anon$12.default(AnalyzerPlugins.scala:429)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.invoke(AnalyzerPlugins.scala:401)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroExpand(AnalyzerPlugins.scala:429)
[INFO]     at scala.tools.nsc.typechecker.AnalyzerPlugins.pluginsMacroExpand$(AnalyzerPlugins.scala:429)
[INFO]     at scala.tools.nsc.Global$$anon$1.pluginsMacroExpand(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Macros.macroExpand(Macros.scala:757)
[INFO]     at scala.tools.nsc.typechecker.Macros.macroExpand$(Macros.scala:757)
[INFO]     at scala.tools.nsc.Global$$anon$1.macroExpand(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.vanillaAdapt$1(Typers.scala:1163)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1223)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1196)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1216)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.typedImplicit1(Implicits.scala:759)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.typedImplicit0(Implicits.scala:696)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.scala$tools$nsc$typechecker$Implicits$ImplicitSearch$$typedImplicit(Implicits.scala:513)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch$ImplicitComputation.rankImplicits(Implicits.scala:1019)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch$ImplicitComputation.findBest(Implicits.scala:1050)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.searchImplicit(Implicits.scala:1107)
[INFO]     at scala.tools.nsc.typechecker.Implicits$ImplicitSearch.bestImplicit(Implicits.scala:1477)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicit(Implicits.scala:95)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicit$(Implicits.scala:82)
[INFO]     at scala.tools.nsc.Global$$anon$1.inferImplicit(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicitFor(Implicits.scala:40)
[INFO]     at scala.tools.nsc.typechecker.Implicits.inferImplicitFor$(Implicits.scala:39)
[INFO]     at scala.tools.nsc.Global$$anon$1.inferImplicitFor(Global.scala:469)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$applyImplicitArgs$2(Typers.scala:243)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.applyImplicitArgs(Typers.scala:236)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$adapt$1(Typers.scala:881)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adaptToImplicitMethod$1(Typers.scala:471)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.adapt(Typers.scala:1219)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5631)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.computeType(Typers.scala:5692)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.assignTypeToTree(Namers.scala:1079)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1629)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.memberSig(Namers.scala:1798)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1763)
[INFO]     at scala.tools.nsc.typechecker.Namers$Namer$MonoTypeCompleter.completeImpl(Namers.scala:842)
[INFO]     at scala.tools.nsc.typechecker.Namers$LockingTypeCompleter.complete(Namers.scala:1960)
[INFO]     at scala.tools.nsc.typechecker.Namers$LockingTypeCompleter.complete$(Namers.scala:1958)
[INFO]     at scala.tools.nsc.typechecker.Namers$TypeCompleterBase.complete(Namers.scala:1953)
[INFO]     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1530)
[INFO]     at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1678)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5240)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStat$1(Typers.scala:5681)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typedStats$10(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedBlock(Typers.scala:2468)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typed1$98(Typers.scala:5546)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedOutsidePatternMode$1(Typers.scala:480)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5581)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedDefDef(Typers.scala:5833)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5536)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStat$1(Typers.scala:5681)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typedStats$10(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1999)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1821)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5537)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStat$1(Typers.scala:5681)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.$anonfun$typedStats$10(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3264)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5247)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5540)
[INFO]     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5617)
[INFO]     at scala.tools.nsc.typechecker.Analyzer$typerFactory$TyperPhase.apply(Analyzer.scala:106)
[INFO]     at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:441)
[INFO]     at scala.tools.nsc.typechecker.Analyzer$typerFactory$TyperPhase.run(Analyzer.scala:96)
[INFO]     at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1467)
[INFO]     at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1451)
[INFO]     at scala.tools.nsc.Global$Run.compileSources(Global.scala:1444)
[INFO]     at scala.tools.nsc.Global$Run.compile(Global.scala:1567)
[INFO]     at scala.tools.nsc.Driver.doCompile(Driver.scala:35)
[INFO]     at scala.tools.nsc.MainClass.doCompile(Main.scala:24)
[INFO]     at scala.tools.nsc.Driver.process(Driver.scala:55)
[INFO]     at scala.tools.nsc.Driver.main(Driver.scala:68)
[INFO]     at scala.tools.nsc.Main.main(Main.scala)
[INFO]     at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
[INFO]     ... 3 more
[INFO] Caused by: java.lang.RuntimeException: /packages cannot be represented as URI
[INFO]     ... 131 more

[INFO] Flink : Scala ...................................... FAILURE [ 42.107 s]


[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:17 min
[INFO] Finished at: 2023-01-11T09:58:13+00:00
[INFO] Final Memory: 208M/498M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:testCompile (scala-test-compile) on project flink-scala_2.12: wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 240 (Exit value: 240) -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-scala_2.12 {code}
I am running the process from a docker container that is based on ""maven:3-openjdk-18-slim""

 

any suggestions would be appreciated.;;;","11/Jan/23 10:25;martijnvisser;[~liadsh] In the future, it's better to reach out to Slack / the User mailing list for these type of questions. See https://flink.apache.org/community.html#how-do-i-get-help-from-apache-flink

Flink is not compatible with Java 18; please use Java 11. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Drop CI Support for Flink 1.15,FLINK-30230,13506666,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,28/Nov/22 16:53,28/Nov/22 17:12,04/Jun/24 20:41,28/Nov/22 17:12,,,,,,aws-connector-4.0.0,,,,,Connectors / AWS,,,,0,pull-request-available,,,"We are dropping support for Flink 1.15.x in AWS connectors version 4.x.

 

Update CI build to target solely Flink 1.16",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 17:12:47 UTC 2022,,,,,,,,,,"0|z1cuqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 17:12;dannycranmer;Merged commit [{{dd6b215}}|https://github.com/apache/flink-connector-aws/commit/dd6b215bcead352b08079ad340c8c0c06b17707d] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL IT case for DynamoDB Sink,FLINK-30229,13506435,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,dannycranmer,dannycranmer,28/Nov/22 14:10,18/Mar/23 08:04,04/Jun/24 20:41,18/Mar/23 08:02,,,,,,aws-connector-4.2.0,,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,Add SQL IT case(s) for DynamoDB sink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29895,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 18 08:04:13 UTC 2023,,,,,,,,,,"0|z1ctbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/23 08:04;dannycranmer;Merged commit [{{930d196}}|https://github.com/apache/flink-connector-aws/commit/930d196a005c52f23ada6acb6ff47c17a2f6d7d5] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RateLimitedSourceReaderITCase.testRateLimitingParallelExecution failed,FLINK-30228,13506306,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,28/Nov/22 13:08,29/Nov/22 14:46,04/Jun/24 20:41,29/Nov/22 08:52,1.17.0,,,,,1.17.0,,,,,Tests,,,,0,pull-request-available,test-stability,,"{{RateLimitedSourceReaderITCase.testRateLimitingParallelExecution}}  failed due to {{NullPointerException}}:
{code}
Nov 28 09:51:10 [ERROR] org.apache.flink.api.connector.source.lib.util.RateLimitedSourceReaderITCase.testRateLimitingParallelExecution  Time elapsed: 1.051 s  <<< ERROR!
Nov 28 09:51:10 java.lang.NullPointerException
Nov 28 09:51:10 	at java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:210)
Nov 28 09:51:10 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
Nov 28 09:51:10 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Nov 28 09:51:10 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Nov 28 09:51:10 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
Nov 28 09:51:10 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Nov 28 09:51:10 	at java.util.stream.IntPipeline.reduce(IntPipeline.java:479)
Nov 28 09:51:10 	at java.util.stream.IntPipeline.sum(IntPipeline.java:437)
Nov 28 09:51:10 	at org.apache.flink.api.connector.source.lib.util.RateLimitedSourceReaderITCase$MockRateLimiterStrategy.getRateLimitersCallCount(RateLimitedSourceReaderITCase.java:116)
Nov 28 09:51:10 	at org.apache.flink.api.connector.source.lib.util.RateLimitedSourceReaderITCase.testRateLimitingParallelExecution(RateLimitedSourceReaderITCase.java:77)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43541&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7016",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 08:52:37 UTC 2022,,,,,,,,,,"0|z1csiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 08:52;chesnay;master: 3130b9a907afb420f95a64a3cada19156fedc72a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add LeafEmptyFunction for predicate without fields,FLINK-30227,13506225,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,28/Nov/22 10:07,19/Mar/23 05:42,04/Jun/24 20:41,19/Mar/23 05:42,table-store-0.3.0,,,,,,,,,,Table Store,,,,0,,,,"In addition to `LeafBinaryFunction` and `LeafUnaryFunction`, we should add `LeafEmptyFunction` for predicate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-28 10:07:26.0,,,,,,,,,,"0|z1cs14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offer a Identity(De)SerializationSchema,FLINK-30226,13506224,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,twalthr,twalthr,28/Nov/22 10:03,28/Nov/22 10:03,04/Jun/24 20:41,,,,,,,,,,,,API / DataStream,,,,0,,,,"Sometimes it can be convenient if the API provides a ""Identity(De)SerializationSchema"" which simply takes and returns its input as {{byte[]}}. The Table API offers the {{raw}} format for this. DataStream API does not have equivalent functionality.

We could also name it `Raw(De)SerializationSchema` for consistency?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-28 10:03:37.0,,,,,,,,,,"0|z1cs0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Convergence test failed once after reference version upgrade to 1.15.3,FLINK-30225,13506221,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,28/Nov/22 09:34,29/Nov/22 06:39,04/Jun/24 20:41,29/Nov/22 06:11,1.15.4,,,,,,,,,,Build System,,,,0,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43505&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=1299] failed when doing the API convergence check for {{{}IteratorSourceReader:METHOD_ABSTRACT_ADDED_IN_IMPLEMENTED_INTERFACE{}}}. But [another build on the same commit|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb] ([041e038e|https://github.com/flink-ci/flink-mirror/commit/041e038e6434e7e5f7497d20fe14df9a962673fe]) succeeded. The only difference I could observe so far are the different kernel versions: The failed build was executed on {{Alibaba001}} where a kernel update was performed due to [FLINK-29315|https://issues.apache.org/jira/browse/FLINK-29315?focusedCommentId=17629591&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17629591] ran on [5.4.220-1.el7.elrepo.x86_64|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43505&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=23]. The successful build ran on agent {{Alibaba006}} and [3.10.0-1160.76.1.el7.x86_64|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=23].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 06:39:07 UTC 2022,,,,,,,,,,"0|z1cs08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 09:46;mapohl;I noticed that even on the successful build, there are failed job runs [cron_snapshot_deployment_maven|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=6952] (hosted on {{Hosted Agent}} which had kernel version [5.15.0-1023-azure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=189] installed) and [compile_cron_azure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=cc2c0b9a-de49-5517-79e5-b1df1cadc1e5&t=e46d8929-052e-573c-00f4-50453f4d6369&l=21] (hosted on {{Azure Pipelines 21}} which had kernel version [5.15.0-1023-azure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=cc2c0b9a-de49-5517-79e5-b1df1cadc1e5&t=e46d8929-052e-573c-00f4-50453f4d6369&l=21] installed) but with different classes causing the failure. This would support the fact that it's caused by having different kernel versions.;;;","28/Nov/22 10:10;chesnay;{code:java}
[INFO] --- japicmp-maven-plugin:0.16.0_m325:cmp (default) @ flink-core ---    
[WARNING] Could not find artifact org.apache.flink:flink-core:jar:1.15.3 in alicloud-mvn-mirror (http://172.17.0.1:8888/repository/maven-central/)   
[WARNING] Could not resolve dependency with descriptor 'org.apache.flink:flink-core:1.15.3'. 
[WARNING] Please provide at least one resolvable old version using one of the configuration elements <oldVersion/> or <oldVersions/>. 
{code}
 
This is from the failed build. Did we wait sufficiently long for the maven artifacts to be actually available?;;;","28/Nov/22 11:32;mapohl;Good pointer. Thanks for that. That sounds like a more reasonable cause.

I failed to find a way to retrieve the upload timestamps from Maven Central to verify that. But there's a [new CI run|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43550&view=results] triggered for {{release-1.15}} which we could use to verify that everything's working smoothly again.;;;","28/Nov/22 14:21;mapohl;I updated the Flink release guide accordingly (see [diff|https://cwiki.apache.org/confluence/pages/diffpagesbyversion.action?pageId=73631092&selectedPageVersions=183&selectedPageVersions=182]);;;","28/Nov/22 15:59;chesnay;But the guide already says to wait 24h after finalizing before promoting the release.;;;","29/Nov/22 06:11;mapohl;Closing issue. Even the [nightly build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43573&view=results] didn't run into issues anymore.;;;","29/Nov/22 06:39;mapohl;{quote}
But the guide already says to wait 24h after finalizing before promoting the release.
{quote}

Fair point. IMHO, the additional remark doesn't harm the documentation, though. It's generally good to know the prerequisites of a step to get a better understanding of what's happening. But generally speaking, you're right - the 24h remark should be enough to make sure we're not running into the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add IT Case for NPE in FlinkKinesisConsumer's close() method,FLINK-30224,13506218,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,astamur,astamur,astamur,28/Nov/22 09:28,21/Dec/22 10:33,04/Jun/24 20:41,21/Dec/22 10:33,,,,,,aws-connector-4.1.0,,,,,Connectors / Kinesis,,,,0,pull-request-available,,,"In our project we observed an issue during too early call of {{stop-with-savepoint}} operation when we're calling it after 4-6 seconds after a job was started.

We use Kinesis EFO consumer in our application and it looks like that it takes some time for the {{KinesisDataFetcher}} to be initialized and because of that {{FlinkKinesisConsumer#run(SourceContext<T>)}} method is slow. In some tests we call {{stop-with-savepoint}} after 4-6 seconds after a job was started and we have {{NullPointerException}} in {{{}FlinkKinesisConsumer#close(){}}}:
{code:java}
java.lang.NullPointerException: null
        at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.close(FlinkKinesisConsumer.java:421) ~[classes/:?]
        at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[classes/:?]
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[classes/:?]
        at org.apache.flink.streaming.api.operators.StreamSource.close(StreamSource.java:124) ~[classes/:?]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163) ~[classes/:?]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125) ~[classes/:?]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997) ~[classes/:?]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:876) ~[classes/:?]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:759) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568) ~[classes/:?]
        at java.lang.Thread.run(Thread.java:829) ~[?:?] {code}
We were able to reproduce this issue in {{FlinkKinesisITCase}} even for a standard KDS consumer. A fix is just an additional non-null check during consumer's close. We'll also add an IT-test for that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29324,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 10:33:01 UTC 2022,,,,,,,,,,"0|z1crzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 10:32;astamur;I see that issue was already fixed in FLINK-29324.
In this case, I'm only adding an IT-test;;;","28/Nov/22 11:02;dannycranmer;[~astamur] I have assigned this issue to you. Since this is already fixed, I will change issue type from bug and rename.;;;","28/Nov/22 11:12;astamur;Thank you Danny;;;","28/Nov/22 15:01;astamur;I see that this test failed in Azure build (which isn't relevant to my changes): {{{}org.apache.flink.connector.datagen.source.DataGeneratorSourceITCase.testGatedRateLimiter{}}}. I don't have privileges to re-run failed jobs [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43548&view=results] ;;;","05/Dec/22 10:16;astamur;PR checks passed, it's waiting for an approval and to be merge.;;;","21/Dec/22 10:33;dannycranmer;Merged commit [{{9ee0fe3}}|https://github.com/apache/flink-connector-aws/commit/9ee0fe32d1a9e6d62e514824ab553956fe88ee9d] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Refactor Lock to provide Lock.Factory,FLINK-30223,13506161,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,28/Nov/22 02:32,28/Nov/22 07:51,04/Jun/24 20:41,28/Nov/22 07:51,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"For the core, it should not see too many Flink Table concepts, such as database and tableName. It only needs to create a Lock.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 07:51:51 UTC 2022,,,,,,,,,,"0|z1crn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 07:51;TsReaper;master: 6886303b2f482f2f31c7b98221691a650c1e67d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suspended a job in last-state mode bug,FLINK-30222,13506106,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pvary,tanjialiang,tanjialiang,27/Nov/22 08:54,29/Nov/22 11:32,04/Jun/24 20:41,29/Nov/22 11:32,1.16.0,kubernetes-operator-1.2.0,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"In flink 1.16.0, it support set kubernetes HA with options 'kubernetes', such as 'high-availability: kubernetes'. But in kubernetes operator 1.2.0, I try to suspended a job in last-state mode, it validate fail, because of 'Job could not be upgraded with last-state while Kubernetes HA disabled'.

 

I try to use kubectl patch to supsended a job with last-state
{code:sh}
kubectl -nbigdata-flink patch flinkdeployments.flink.apache.org/streaming-638223bf650ac869689faa62-flink --type=merge -p '{""spec"": {""job"":
{""state"": ""suspended"", ""upgradeMode"": ""last-state""}{code}
it found an error, because my kubernetes HA is disabled
{code:java}
Error from server: admission webhook ""flinkoperator.flink.apache.org"" denied the request: Job could not be upgraded with last-state while Kubernetes HA disabled {code}
but i enabled kubernetes HA with this follow options:
{code:yaml}
kubernetes.cluster-id: <cluster-id>
high-availability: kubernetes
high-availability.storageDir: hdfs:///flink/recovery {code}
and i found flink kubernetes operator 1.2.0 validate the kubernetes HA in the old options:
{code:yaml}
high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory {code}
it may be in the org.apache.flink.kubernetes.operator.utils.FlinkUtils#isKubernetesHAActivated to judge.

!image-2022-11-27-16-48-08-445.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/22 08:48;tanjialiang;image-2022-11-27-16-48-08-445.png;https://issues.apache.org/jira/secure/attachment/13053166/image-2022-11-27-16-48-08-445.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 11:32:42 UTC 2022,,,,,,,,,,"0|z1craw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/22 11:03;gyfora;Good catch! Would you like to work on this ticket?

The current workaround is to set:
```
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
```
instead of simply `kubernetes`;;;","29/Nov/22 11:32;mbalassi;[{{458022d}}|https://github.com/apache/flink-kubernetes-operator/commit/458022d2e67247c9941f102fb39d9dda96bd8837] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of sum(try_cast(string as bigint)) return null when partial elements can't convert to bigint,FLINK-30221,13506022,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,lsy,lsy,26/Nov/22 09:54,30/Nov/22 13:02,04/Jun/24 20:41,30/Nov/22 13:01,1.17.0,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 30 13:01:52 UTC 2022,,,,,,,,,,"0|z1cqs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 07:03;tonyzhu;Could you assign to me and I'd like to try the fix.;;;","29/Nov/22 07:58;martijnvisser;Why is this a bug? Because the entire purpose of TRY_CAST is to return a NULL in case there's an error. ;;;","30/Nov/22 02:16;lsy;[~martijnvisser] I know TRY_CAST will return a NULL when cast occur error, this is as expected. However, when we use SUM with TRY_CAST, if some elements in the given column can't convert to target type, the result is NULL, I think this is not as expected. For the following example:

Regarding column a STRING, it has the following values ['1', '2', 'abc', '4', '5'], we query the column by the sql  `SELECT  SUM(TRY_CAST(a as BIGINT))`, the result return by flink is NULL. IMO, although the value 'abc' can't be converted to BIGINT, the query result should be 12 instead of NULL. WDYT?;;;","30/Nov/22 02:19;lsy;[~tonyzhu] you can try it, and pull the pr.;;;","30/Nov/22 12:14;tonyzhu;[~lsy] , I agree with [~martijnvisser]  that it is as expected to return null when some element got cast error.;;;","30/Nov/22 13:01;martijnvisser;[~lsy] No, the result should not be 12 from your example. From the documentation example it even states {{TRY_CAST(’non-number’ AS INT) returns NULL}}. This is intended behaviour. Failure in this TRY_CAST should result in the returning of a NULL value. ;;;","30/Nov/22 13:01;martijnvisser;Closing because this is not a bug, but intended design. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Secure user credentials in Flink SQL JDBC connector,FLINK-30220,13506018,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,qinjunjerry,qinjunjerry,26/Nov/22 08:34,29/Nov/22 09:38,04/Jun/24 20:41,29/Nov/22 08:00,,,,,,,,,,,,,,,1,,,,"Similar to FLINK-28028, when using Flink SQL JDBC connector, we should also have a way to secure the username and the password used in the DDL:
{code:java}
CREATE TABLE MyUserTable (
  id BIGINT,
  name STRING,
  age INT,
  status BOOLEAN,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://localhost:3306/mydatabase',
   'table-name' = 'users',
   'username' = 'a-username',
   'password' = 'a-password'
);
 {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 09:38:28 UTC 2022,,,,,,,,,,"0|z1cqrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 08:00;martijnvisser;[~qinjunjerry] I've closed it since the ticket you're linking to already talks about a credential mechanism for Flink SQL in general. ;;;","29/Nov/22 09:38;qinjunjerry;The same is needed for JDBC connector:
CREATE TABLE MyUserTable (
  id BIGINT,
  name STRING,
  age INT,
  status BOOLEAN,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (   'connector' = 'jdbc',   'url' = 'jdbc:mysql://localhost:3306/mydatabase',   'table-name' = 'users',   'username' = 'a-username',   'password' = 'a-password');;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch results api in sql gateway return error result.,FLINK-30219,13506000,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,Aiden Gong,Aiden Gong,Aiden Gong,26/Nov/22 02:38,03/Nov/23 08:55,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,stale-assigned,,!image-2022-11-26-10-38-02-270.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/22 02:38;Aiden Gong;image-2022-11-26-10-38-02-270.png;https://issues.apache.org/jira/secure/attachment/13053160/image-2022-11-26-10-38-02-270.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:03 UTC 2023,,,,,,,,,,"0|z1cqnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/22 02:41;Aiden Gong;Hi , [~Leonard] , I'm willing to fix this issue. Please assign to me.;;;","12/Dec/22 04:58;Wencong Liu;Hi, [~Aiden Gong] , could you please illustrate this error in detail? Is the returned format not as expected?;;;","13/Dec/22 10:22;JasonLee;hi [~Aiden Gong] Can you provide your query statement?;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kafka Connector] java.lang.OutOfMemoryError: Metaspace,FLINK-30218,13505961,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,com98,com98,25/Nov/22 14:56,20/Aug/23 10:35,04/Jun/24 20:41,,1.15.1,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-major,bug,,"Hello!

I'm running a Flink application on AWS EMR which consumes from a Kafka Topic using the official Flink Kafka consumer. I'm running the application as a Flink batch job every 30 minutes and I see that the JobManager's metaspace is increasing every time I submit a new job and doesn't reduce once a job has finished executing. Eventually the metaspace overflows and I get a OutOfMemory metaspace exception. I've tried increasing the metaspace to 512m, but this just delays the problem - hence it's definitely a classloading leak.

I debugged the issue by creating a simple Flink application with a Kafka consumer only and the issue still occurred, hence I suppose the issue is somewhere in the Kafka consumer. Only other third party plugin I was using when doing so was the AWS IAM Kafka dependency (software.amazon.msk:aws-msk-iam-auth:1.1.5). 

I also tried debugging the issue by generating a heap dump as described here ([https://cwiki.apache.org/confluence/display/FLINK/Debugging+ClassLoader+leak|[https://cwiki.apache.org/confluence/display/FLINK/Debugging+ClassLoader+leaks]),] but I wasn't really able to spot the origin of the memory leak. I can see references to org.apache.kafka.common.utils.AppInfoParser$AppInfo though:

!image-2022-11-25-15-55-43-559.png|width=860,height=359!

I have attached the heap dump - if you need any more information from my set up feel free to ask, I can provide anything you need.

Many thanks in advance! ","+*AWS EMR*+
 * Standard AWS EMR Cluster (1 master YARN node, 1 core node) - 2 vCore, 8 GB memory each
 * JDK 11 Coretto

+*Kafka Consumer Config*+
{code:java}
    acks = 1
    batch.size = 16384
    bootstrap.servers = [...]
    buffer.memory = 33554432
    client.dns.lookup = use_all_dns_ips
    client.id = producer-1
    compression.type = none
    connections.max.idle.ms = 540000
    delivery.timeout.ms = 120000
    enable.idempotence = false
    interceptor.classes = []
    internal.auto.downgrade.txn.commit = false
    key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
    linger.ms = 0
    max.block.ms = 60000
    max.in.flight.requests.per.connection = 5
    max.request.size = 1048576
    metadata.max.age.ms = 300000
    metadata.max.idle.ms = 300000
    metric.reporters = []
    metrics.num.samples = 2
    metrics.recording.level = INFO
    metrics.sample.window.ms = 30000
    partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
    receive.buffer.bytes = 32768
    reconnect.backoff.max.ms = 1000
    reconnect.backoff.ms = 50
    request.timeout.ms = 30000
    retries = 2147483647
    retry.backoff.ms = 100
    sasl.client.callback.handler.class = class software.amazon.msk.auth.iam.IAMClientCallbackHandler
    sasl.jaas.config = [hidden]
    sasl.kerberos.kinit.cmd = /usr/bin/kinit
    sasl.kerberos.min.time.before.relogin = 60000
    sasl.kerberos.service.name = null
    sasl.kerberos.ticket.renew.jitter = 0.05
    sasl.kerberos.ticket.renew.window.factor = 0.8
    sasl.login.callback.handler.class = null
    sasl.login.class = null
    sasl.login.refresh.buffer.seconds = 300
    sasl.login.refresh.min.period.seconds = 60
    sasl.login.refresh.window.factor = 0.8
    sasl.login.refresh.window.jitter = 0.05
    sasl.mechanism = AWS_MSK_IAM
    security.protocol = SASL_SSL
    security.providers = null
    send.buffer.bytes = 131072
    socket.connection.setup.timeout.max.ms = 30000
    socket.connection.setup.timeout.ms = 10000
    ssl.cipher.suites = null
    ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
    ssl.endpoint.identification.algorithm = https
    ssl.engine.factory.class = null
    ssl.key.password = null
    ssl.keymanager.algorithm = SunX509
    ssl.keystore.certificate.chain = null
    ssl.keystore.key = null
    ssl.keystore.location = null
    ssl.keystore.password = null
    ssl.keystore.type = JKS
    ssl.protocol = TLSv1.3
    ssl.provider = null
    ssl.secure.random.implementation = null
    ssl.trustmanager.algorithm = PKIX
    ssl.truststore.certificates = null
    ssl.truststore.location = null
    ssl.truststore.password = null
    ssl.truststore.type = JKS
    transaction.timeout.ms = 3600000
    transactional.id = null
    value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer{code}
+*Flink Config*+
{code:java}
taskmanager.memory.process.size=3g
taskmanager.memory.jvm-metaspace.size=512m
taskmanager.numberOfTaskSlots=2
jobmanager.memory.process.size=3g
jobmanager.memory.jvm-metaspace.size=256m
jobmanager.web.address0.0.0.0
env.java.opts.all-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${FLINK_LOG_PREFIX}.hprof {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/22 14:49;com98;dump.hprof.zip;https://issues.apache.org/jira/secure/attachment/13053146/dump.hprof.zip","25/Nov/22 14:55;com98;image-2022-11-25-15-55-43-559.png;https://issues.apache.org/jira/secure/attachment/13053145/image-2022-11-25-15-55-43-559.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:03 UTC 2023,,,,,,,,,,"0|z1cqeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/23 13:09;rmetzger;We had a pretty similar problem (also related to AWS auth) with the Kinesis connector: https://issues.apache.org/jira/browse/FLINK-19259. I guess the fix for Kafka is similar / the same.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use ListState#update() to replace clear + add mode.,FLINK-30217,13505959,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,xljtswf,xljtswf,25/Nov/22 14:43,08/Sep/23 04:20,04/Jun/24 20:41,08/Sep/23 04:20,,,,,,1.19.0,,,,,Runtime / State Backends,,,,0,,,,"When using listState, I found many times we need to clear current state, then add new values. This is especially common in CheckpointedFunction#snapshotState, which is slower than just use ListState#update().

Suppose we want to update the liststate to contain value1, value2, value3.
With current implementation, we first call Liststate#clear(). this updates the state 1 time.
then we add value1, value2, value3 to the state.
if we use heap state, we need to search the stateTable 3 times and add 3 values to the list.
this happens in memory and is not too bad.
if we use rocksdb. then we will call backend.db.merge() 3 times.
finally, we will  update the state 4 times.
The more values to be added, the more times we will update the state.

while if we use listState#update. then we just need to update the state 1 time. I think this can save a lot of time.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 04:20:38 UTC 2023,,,,,,,,,,"0|z1cqe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 12:55;xljtswf;For the KeyedState, I found this will almost always happen in org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet#persist. Every time, when it comes one element with timestamp later than every element in the Session Window, the mapping will change.;;;","12/Dec/22 05:32;masteryhx;Hi [~xljtswf]. Thanks a lot for reporting this.

I think it makes sense to me that we could replace clear + add with update for ListState in some cases.

It could reduce the call of StateBackend, and also reduce the count of serialization of key.
It's also related to implementation detail of some operators, so [~kevin.cyj] [~wanglijie] WDYT?;;;","30/Aug/23 07:42;zakelly;This makes sense.

Besides, I found many the list state of operator state also use clear + add, which could also be replaced for semantic considerations. I would like to work on this one.;;;","30/Aug/23 11:19;masteryhx;[~Zakelly] Thanks for volunteering, just assigned to you, please go ahead.;;;","08/Sep/23 04:20;masteryhx;merged 20c0acf60d137cd613914503f1b40e7b2adb86c1 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Address tabs/spaces in checkstyle/spotless,FLINK-30216,13505954,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,darenwkt,darenwkt,25/Nov/22 13:50,29/Nov/22 14:49,04/Jun/24 20:41,29/Nov/22 14:48,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The Flink Kubernetes Operator repo has a mix of tabs and spaces, the quality config does not seem to be enforcing anything. The code style guide says we should [use spaces|https://flink.apache.org/contributing/code-style-and-quality-formatting.html#whitespaces].
 - Update code to use spaces
 - Fix quality plugin to reject tabs

 

A similar effort has been done for Flink DynamoDB Connector repo and seems useful for the KubernetesOperator repo as well to maintain high code quality. Ref: https://issues.apache.org/jira/browse/FLINK-29906",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 14:48:29 UTC 2022,,,,,,,,,,"0|z1cqd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 14:48;mbalassi;[{{80e5f3f}}|https://github.com/apache/flink-kubernetes-operator/commit/80e5f3f4add50355de6964c1466ba558d46bc79d] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Cassandra explicit Scala support,FLINK-30215,13505933,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,25/Nov/22 13:11,13/Dec/22 11:54,04/Jun/24 20:41,,,,,,,cassandra-4.0.0,,,,,Connectors / Cassandra,,,,0,,,,"With the deprecation of the Scala APIs we should also deprecate the Scala-specific codepath in the cassandra connector.

I've marked this with a fixVersion of 4.0.0 because we don't know yet which version will exist at the time that 1.17 is released, which is ideally the time when the deprecation goes live.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-25 13:11:28.0,,,,,,,,,,"0|z1cq8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The resource is not enough,FLINK-30214,13505903,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,25/Nov/22 09:23,29/Nov/22 16:50,04/Jun/24 20:41,29/Nov/22 15:38,,,,,,1.17.0,,,,,Runtime / Configuration,,,,0,pull-request-available,,,"When turn up the parallelism, the resources isn't enough for MiniCluster. I'm not sure whether the flink on yarn or k8s is right? I can test later.

 

I guess the parallelism should be changed in a right place. And we should add more unit test or ITCase to check these cases. And it's my honor to make these improvements and bug fixes.

!image-2022-11-25-17-18-36-935.png!

 

!image-2022-11-25-17-23-14-208.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:18;fanrui;image-2022-11-25-17-18-36-935.png;https://issues.apache.org/jira/secure/attachment/13053116/image-2022-11-25-17-18-36-935.png","25/Nov/22 09:23;fanrui;image-2022-11-25-17-23-14-208.png;https://issues.apache.org/jira/secure/attachment/13053115/image-2022-11-25-17-23-14-208.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 15:38:04 UTC 2022,,,,,,,,,,"0|z1cq1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:24;fanrui;Hi [~gyfora] [~mxm] , please help take a look, thanks~:);;;","25/Nov/22 09:36;gyfora;Hi [~fanrui],
Thanks for jumping on this and testing it.
Probably the minicluster is started with fix resources and that is not enough after scaling up. You could try to allocate more tas slots in the minicluster than the job initially needs so you can scale up.;;;","25/Nov/22 10:22;fanrui;Hi [~gyfora] , thanks for your quick feedback.

I run a flink job in IDEA, I didn't set any conf for the minicluster. Flink helps user start the minicluster by maximumParallelism of all tasks.

This is my demo: [https://github.com/1996fanrui/flink/commit/52692249c86cbc366f3e5fb54d7744da75586972]

And I have updated the PR, it can fix this bug. In other words: minicluster should adapt {_}jobvertex-parallelism-overrides{_}.

 

And I will add some tests later.

 

 ;;;","29/Nov/22 15:38;gyfora;merged to master ee4d5b8d65768776f070ccc0c28e3fe3da8e10c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
The edge is wrong when the vertex parallelism is changed,FLINK-30213,13505895,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,fanrui,fanrui,25/Nov/22 09:08,17/Jan/24 13:26,04/Jun/24 20:41,02/Jan/23 13:07,1.17.0,,,,,1.17.0,,,,,Runtime / Configuration,,,,0,pull-request-available,,,"After FLINK-29501, flink allows overriding JobVertex parallelisms during job submission.

However, the edge should be changed as well. For example, the job has 4 vertex, and all shipStrategyName of all tasks are forward.

After the parallelism of the third task is changed to 1, the second and third edge should be changed from forward to rebalance. But they are still forward.

 

And from the second picture, the subtask_1 of sink cannot receive any data.

 

!image-2022-11-25-17-02-12-492.png|width=1318,height=356!

!image-2022-11-25-17-06-53-466.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29501,,,,,FLINK-33123,,,,,,,,,,"25/Nov/22 09:02;fanrui;image-2022-11-25-17-02-12-492.png;https://issues.apache.org/jira/secure/attachment/13053114/image-2022-11-25-17-02-12-492.png","25/Nov/22 09:06;fanrui;image-2022-11-25-17-06-53-466.png;https://issues.apache.org/jira/secure/attachment/13053113/image-2022-11-25-17-06-53-466.png","25/Nov/22 20:58;gyfora;image-2022-11-25-21-58-48-968.png;https://issues.apache.org/jira/secure/attachment/13053154/image-2022-11-25-21-58-48-968.png",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 15:24:12 UTC 2023,,,,,,,,,,"0|z1cq00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:13;fanrui;Hi [~gyfora] [~mxm] , please help take a look, and I can fix this bug. 

BTW, I am very interested in AutoScaling and hope to participate due to it's a very useful feature for our company.;;;","25/Nov/22 14:24;mxm;Thanks for reporting this [~fanrui]! I think we'll have to patch the forward partitioner in this scenario. All other partitioners should continue to work with new parallelisms. I'll have a look.;;;","25/Nov/22 20:59;gyfora;The same issue already existed Flink with the reactive scaling logic. 
Take the following pipeline as an example:


{code:java}
public class RescaleTest {

    public static void main(String[] args) throws Exception {
        var env = StreamExecutionEnvironment.getExecutionEnvironment();
        var source = env.fromSequence(0, 100000000L).setParallelism(1);
        source.getTransformation().setMaxParallelism(1);

        source.disableChaining()
                .filter(
                        val -> {
                            Thread.sleep(1000);
                            return true;
                        })
                .print();

        env.execute();
    }
} {code}

If you start this on a cluster with 1 taskslot (parallelism = 1), that will create forward partitioning between the source and the filter.
Scaling it up by adding one more taskmanager will increase the parallelism of the filter but not the source (due to the forced max parallelism 1)
leading to the same problem:

!image-2022-11-25-21-58-48-968.png!
Furthermore the UI reports incorrect parallelism numbers but thats just cosmetics :) ;;;","02/Jan/23 13:07;gyfora;Merged to main:
ded2df542fd5d585842e77d021fb84a92a5bea76..1c2f4eb4ba1d1b5ad403d7991170ca16cb71df56

I am sorry I accidentally rebased the PR instead of squashing the commits. This was a mistake from my side.;;;","12/Jan/23 15:24;gyfora;Added a fix for this: fb482fe39844efda33a4c05858903f5b64e158a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a TableStoreCompactJob class for users to submit compact jobs in Table Store,FLINK-30212,13505890,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:56,19/Dec/22 02:46,04/Jun/24 20:41,19/Dec/22 02:46,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,Currently Flink does not support SQL statements for compacting a table. So in this ticket we create a TableStoreCompactJob class for users to submit compact jobs in Table Store.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 02:46:48 UTC 2022,,,,,,,,,,"0|z1cpyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 02:46;TsReaper;master: 15af711a1bf50b2d612b439a27225c8ad42acb1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce CompactStoreSink for compact jobs in Table Store,FLINK-30211,13505887,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:55,14/Dec/22 06:55,04/Jun/24 20:41,14/Dec/22 06:55,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"This ticket introduces the {{CompactStoreSink}} for compact jobs in Table Store.

The behavior of {{CompactStoreSink}} is sketched as follows: This sink accepts records containing partitions and buckets to compact and perform compaction on these buckets.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 06:55:43 UTC 2022,,,,,,,,,,"0|z1cpy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 06:55;TsReaper;master: 422c409b36efcd0e75fdde61b0601f405b18e3ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor StoreCompactOperator to accept records containing partitions and buckets in Table Store,FLINK-30210,13505886,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:52,13/Dec/22 09:13,04/Jun/24 20:41,13/Dec/22 09:13,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,In this ticket we refactor StoreCompactOperator to accept records containing partitions and buckets in Table Store. The old {{ALTER TABLE COMPACT}} will also be disabled in this ticket due to various restrictions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 09:13:51 UTC 2022,,,,,,,,,,"0|z1cpy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 09:13;TsReaper;master: 2073bd66a4e41ac47a98b0c0370b0ccdeb516c62, 333bb8dad697d65e70a4d672748bc1e55d7ded03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce CompactFileStoreSource for compact jobs of Table Store,FLINK-30209,13505885,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:49,12/Dec/22 12:25,04/Jun/24 20:41,12/Dec/22 12:25,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"In this ticket we create the {{CompactFileStoreSource}} Flink source for separated compact jobs.

The behavior of this source is sketched as follows:
* For batch compact jobs, this source produces records containing all partitions and buckets of the current table.
* For streaming compact jobs, this source produces records containing all modified partitions and buckets of a snapshot. This source will also monitor on newly created snapshots.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 12:25:59 UTC 2022,,,,,,,,,,"0|z1cpxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 12:25;TsReaper;master: 20dd568f3c2a9b2afd4cb5eb512cd24976602e27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid unconditional state update in CountTrigger#onElement,FLINK-30208,13505883,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xljtswf,xljtswf,25/Nov/22 08:44,09/Feb/23 09:32,04/Jun/24 20:41,,,,,,,,,,,,API / DataStream,,,,0,,,,"In current CountTrigger#onElement, when one element is received, the state is updated unconditionally, and we then fetch the state again to check whether we need to clear the state. This implies we may update the state 2 times to process one element. I suppose to make following simplification:

    public TriggerResult onElement(Object element, long timestamp, W window, TriggerContext ctx)
            throws Exception {
        TriggerResult triggerResult;
        if (maxCount > 1) {
            ReducingState<Long> countState = ctx.getPartitionedState(stateDesc);
            Long currentCount = countState.get();
            if (currentCount == null || currentCount < maxCount - 1) {
                countState.add(1L);
                triggerResult = TriggerResult.CONTINUE;
            } else {
                countState.clear();
                triggerResult = TriggerResult.FIRE;
            }
        } else {
            triggerResult = TriggerResult.FIRE;
        }
        return triggerResult;
    }

If this is approved, I will make a pr then.
Thanks!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:32:22 UTC 2023,,,,,,,,,,"0|z1cpxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 08:32;kevin.cyj;After the change, for most cases, the state will be still accessed twice (get and update), am I understanding right?;;;","08/Dec/22 12:30;xljtswf;[~kevin.cyj] Yes, this is the most cases.
Indeed, there are 2 conditions we can improve:
1. when maxCount==1, i.e. we will trigger on every element. Thus we even do not need to use the state at all, there will be no get and update. And stateDesc do not need to be instantiated which can save the serialized memory. But this will change the serialized bytes of CountTrigger, I do not know whether it will change the back-compatibilaty.
2. when maxCount > 1. For the last element, we can save 1 add operation.;;;","22/Dec/22 08:36;kevin.cyj;[~xljtswf] Thanks for your explanation. I think there is no compatibility issue. But I am not sure about the performance gain. Do you have any test numbers?;;;","09/Feb/23 09:32;Weijie Guo;[~xljtswf] What's the progress of this ticket? Do you have the performance testing report?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Move split initialization and discovery logic fully into SnapshotEnumerator in Table Store,FLINK-30207,13505882,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:41,05/Dec/22 02:24,04/Jun/24 20:41,05/Dec/22 02:24,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"It is possible that the separated compact job is started long after the write jobs. so compact job sources need a special split initialization logic: we will find the latest COMPACT snapshot, and start compacting right after this snapshot.

However, split initialization logic are currently coded into {{FileStoreSource}}. We should extract these logic into {{SnapshotEnumerator}} so that we can create our special {{SnapshotEnumerator}} for compact sources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 02:24:20 UTC 2022,,,,,,,,,,"0|z1cpx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 02:24;lzljs3620320;master: 23dad789879735249651992ab9fe23b986ef1564;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow FileStoreScan to read incremental changes from OVERWRITE snapshot in Table Store,FLINK-30206,13505880,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:36,30/Nov/22 07:06,04/Jun/24 20:41,30/Nov/22 07:06,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"Currently {{AbstractFileStoreScan}} can only read incremental changes from APPEND snapshots. However in OVERWRITE snapshots, users will also append new records to table. These changes must be discovered by compact job source so that the overwritten partition can be compacted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 30 07:06:45 UTC 2022,,,,,,,,,,"0|z1cpwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:06;TsReaper;master: c7bc1fb9d1d656dd93fafe66dbef185b560a33c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify compact interface for TableWrite and FileStoreWrite to support normal compaction in Table Store,FLINK-30205,13505879,13505878,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:28,28/Nov/22 06:50,04/Jun/24 20:41,28/Nov/22 06:50,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"Currently the {{compact}} interface in {{TableWrite}} and {{FileStoreWrite}} can only trigger full compaction. However a separated compact job should not only perform full compaction, but also perform normal compaction once in a while, just like what the current Table Store sinks do.

We need to modify compact interface for TableWrite and FileStoreWrite to support normal compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 06:50:15 UTC 2022,,,,,,,,,,"0|z1cpwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 06:50;lzljs3620320;master: e6e62699ef42a90d9eb2ca73b10460eb2764a586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store support separated compact jobs,FLINK-30204,13505878,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,25/Nov/22 08:25,19/Dec/22 02:46,04/Jun/24 20:41,19/Dec/22 02:46,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,,,,"Currently table store sinks will write and compact data files from the same job. While this implementation is enough and more economical for most users, some user may expect higher or more steady write throughput.

We decided to support creating separated compact jobs for Table Store. This will bring us the following advantages:
* Write jobs can concentrate only on writing files. Their throughput will be higher and more steady.
* By creating only one compact job for each table, no commit conflicts will occur.

The structure of a separated compact job is sketched out as follows:
* There should be three vertices in a compact job. One source vertex, one sink (compactor) vertex and one commit vertex.
* The source vertex is responsible for generating records containing partitions and buckets to be compacted.
* The sink vertex accepts records containing partitions and buckets, and compact these buckets.
* The commit vertex commit the changes from the sink vertex. It is possible that the user mistakenly creates other compact jobs so commit conflicts may still occur. However as compact changes are optional, this commit vertex will commit changes in an at-most-once style.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-25 08:25:26.0,,,,,,,,,,"0|z1cpw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchPandasConversionTests.test_empty_to_pandas failed,FLINK-30203,13505874,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,25/Nov/22 08:06,14/Aug/23 11:16,04/Jun/24 20:41,14/Aug/23 11:16,1.15.3,1.16.0,,,,,,,,,API / Python,,,,0,stale-major,test-stability,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43484&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=29175] failed (not exclusively) because of {{{}BatchPandasConversionTests.test_empty_to_pandas{}}}. Looks like it's caused by a {{{}NullPointerException{}}}:
{code:java}
Nov 25 02:08:35 E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
Nov 25 02:08:35 E                   : java.lang.NullPointerException
Nov 25 02:08:35 E                   	at java.util.Objects.requireNonNull(Objects.java:203)
Nov 25 02:08:35 E                   	at org.apache.calcite.rel.metadata.RelMetadataQuery.<init>(RelMetadataQuery.java:144)
Nov 25 02:08:35 E                   	at org.apache.calcite.rel.metadata.RelMetadataQuery.<init>(RelMetadataQuery.java:108)
Nov 25 02:08:35 E                   	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.<init>(FlinkRelMetadataQuery.java:78)
Nov 25 02:08:35 E                   	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:59)
Nov 25 02:08:35 E                   	at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)
Nov 25 02:08:35 E                   	at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)
Nov 25 02:08:35 E                   	at org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)
Nov 25 02:08:35 E                   	at org.apache.calcite.rel.logical.LogicalFilter.create(LogicalFilter.java:127)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 11:16:57 UTC 2023,,,,,,,,,,"0|z1cpvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 08:15;mapohl;different test method (PyFlinkBatchUserDefinedFunctionTests.test_table_function ) but same stacktrace on a branch based on {{{}release-1.15{}}}: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43494&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27600];;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 11:16;mapohl;I'm closing this one as a duplicate of FLINK-30163. Both present the same {{NullPointerException}} stacktrace.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
RateLimitedSourceReader may emit one more record than permitted,FLINK-30202,13505858,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,25/Nov/22 04:44,04/Dec/22 20:32,04/Jun/24 20:41,04/Dec/22 20:32,1.17.0,,,,,1.17.0,,,,,Connectors / Common,,,,0,pull-request-available,test-stability,," [This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24747] failed due to a failed assertion in {{{}DataGeneratorSourceITCase.testGatedRateLimiter{}}}:
{code:java}
Nov 25 03:26:45 org.opentest4j.AssertionFailedError: 
Nov 25 03:26:45 
Nov 25 03:26:45 expected: 2
Nov 25 03:26:45  but was: 1 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 04 20:32:44 UTC 2022,,,,,,,,,,"0|z1cprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 10:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43519&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24747;;;","28/Nov/22 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43530&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24745;;;","30/Nov/22 06:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43604&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24751;;;","01/Dec/22 09:01;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43636&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24837;;;","02/Dec/22 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43662&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24837;;;","04/Dec/22 20:32;chesnay;master: 81ed6c649e1a219c457628c54a7165d75b803474;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Function ""unnest"" can't process nesting JSON properly",FLINK-30201,13505848,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mooonzhang,mooonzhang,25/Nov/22 02:06,25/Aug/23 07:03,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Planner,,,,0,,,,"Here is the CREATE TABLE DDL：
{code:java}
riskRuleEngineResultLevel2_3 = CREATE TABLE `riskRuleEngineResultLevel2_3`(\
        `data` ROW<\
            `flow_id` STRING, \
            `flow_name` STRING, \
            `flow_version` STRING, \
            `risk_id` BIGINT, \
            `uid` BIGINT, \
            `is_pass` INT, \
            `result` INT, \
            `country_id` INT, \
            `business` STRING, \
            `engine_scene_id` STRING, \
            `flow_type` STRING, \
            `source` STRING, \
            `rule_results` ARRAY<ROW<`rule_id` STRING, \
                                `rule_name` STRING, \
                                `rule_type` STRING, \
                                `rule_type_name` STRING, \
                                `node_id` STRING, \
                                `result` INT, \
                                `policy_name` STRING, \
                                `in_path` BOOLEAN>>\
            >,\
        proctime as proctime()\
        ) WITH (\
            'connector' = 'kafka',\
            'topic' = 'riskRuleEngineResultLevel2_3',\
            'scan.startup.mode' = '%s',\
            'properties.bootstrap.servers' = '%s',\
            'properties.group.id' = '%s',\
            'format' = 'json'\
        ) {code}
flink sql:
{code:java}
String executeSql = ""select data.flow_id as flow_id,t.rule_id,t.rule_name,t.rule_type,t.rule_type_name,t.node_id,t.`result` from riskRuleEngineResultLevel2_3, unnest(data.rule_results) as t (rule_id,rule_name,rule_type,rule_type_name,node_id,`result`,policy_name,in_path)""; {code}
  when the param in ""unnest"" Function is ""data.rule_results"" which is actually the right structure, the ERROR occurs as below. And when I use ""rule_results"" instead of ""data.rule_results"" in ""unnest"" Function ,It goes well. I think it is wired.
{code:java}
// Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 0, column 0 to line 1, column 149: Column 'data.data' not found in table 'riskRuleEngineResultLevel2_3'
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:723)
    at com.akulaku.flink_tasks_project.tasks.FlowsRuleResultRiskCalc.main(FlowsRuleResultRiskCalc.java:42)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 0, column 0 to line 1, column 149: Column 'data.data' not found in table 'riskRuleEngineResultLevel2_3'
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4867)
    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5839)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5823)
    at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:320)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5431)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3101)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3082)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3133)
    at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3076)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:182)
    ... 5 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'data.data' not found in table 'riskRuleEngineResultLevel2_3'
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
    ... 28 more {code}
if I change DDL by adding ""data2"" which is the same structure with ""data"" at the same level, no matter I use ""rule_results"" or ""data.rule_results"" in ""unnest"" FUNCTION the ERROR occurs:
{code:java}
riskRuleEngineResultLevel2_3 = CREATE TABLE `riskRuleEngineResultLevel2_3`(\
        `data` ROW<\
            `flow_id` STRING, \
            `flow_name` STRING, \
            `flow_version` STRING, \
            `risk_id` BIGINT, \
            `uid` BIGINT, \
            `is_pass` INT, \
            `result` INT, \
            `country_id` INT, \
            `business` STRING, \
            `engine_scene_id` STRING, \
            `flow_type` STRING, \
            `source` STRING, \
            `rule_results` ARRAY<ROW<`rule_id` STRING, \
                                `rule_name` STRING, \
                                `rule_type` STRING, \
                                `rule_type_name` STRING, \
                                `node_id` STRING, \
                                `result` INT, \
                                `policy_name` STRING, \
                                `in_path` BOOLEAN>>\
            >,\
        `data2` ROW<\
            `flow_id` STRING, \
            `flow_name` STRING, \
            `flow_version` STRING, \
            `risk_id` BIGINT, \
            `uid` BIGINT, \
            `is_pass` INT, \
            `result` INT, \
            `country_id` INT, \
            `business` STRING, \
            `engine_scene_id` STRING, \
            `flow_type` STRING, \
            `source` STRING, \
            `rule_results` ARRAY<ROW<`rule_id` STRING, \
                                `rule_name` STRING, \
                                `rule_type` STRING, \
                                `rule_type_name` STRING, \
                                `node_id` STRING, \
                                `result` INT, \
                                `policy_name` STRING, \
                                `in_path` BOOLEAN>>\
            >,\
        proctime as proctime()\
        ) WITH (\
            'connector' = 'kafka',\
            'topic' = 'riskRuleEngineResultLevel2_3',\
            'scan.startup.mode' = '%s',\
            'properties.bootstrap.servers' = '%s',\
            'properties.group.id' = '%s',\
            'format' = 'json'\
        ) {code}
ERROR when ""rule_results"" in ""unnest""：
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 146 to line 1, column 157: Column 'rule_results' is ambiguous
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:723)
    at com.akulaku.flink_tasks_project.tasks.FlowsRuleResultRiskCalc.main(FlowsRuleResultRiskCalc.java:42)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 146 to line 1, column 157: Column 'rule_results' is ambiguous
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4867)
    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:467)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateIdentifier(SqlValidatorImpl.java:2921)
    at org.apache.calcite.sql.SqlIdentifier.validateExpr(SqlIdentifier.java:300)
    at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:419)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5404)
    at org.apache.calcite.sql.validate.UnnestNamespace.validateImpl(UnnestNamespace.java:64)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.AbstractNamespace.getRowType(AbstractNamespace.java:115)
    at org.apache.calcite.sql.validate.AbstractNamespace.getRowTypeSansSystemColumns(AbstractNamespace.java:122)
    at org.apache.calcite.sql.validate.AliasNamespace.validateImpl(AliasNamespace.java:69)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.AbstractNamespace.getRowType(AbstractNamespace.java:115)
    at org.apache.calcite.sql.validate.AliasNamespace.getRowType(AliasNamespace.java:43)
    at org.apache.calcite.sql.validate.DelegatingScope.resolveInNamespace(DelegatingScope.java:101)
    at org.apache.calcite.sql.validate.ListScope.resolve(ListScope.java:190)
    at org.apache.calcite.sql.validate.ListScope.findQualifyingTableNames(ListScope.java:155)
    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:238)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5839)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5823)
    at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:320)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5431)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3101)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3082)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3133)
    at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3076)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:182)
    ... 5 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'rule_results' is ambiguous
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
    ... 46 more{code}
ERROR when ""data.rule_results"" in ""unnest""
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 0, column 0 to line 1, column 149: Column 'data.data' not found in table 'riskRuleEngineResultLevel2_3'
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:723)
    at com.akulaku.flink_tasks_project.tasks.FlowsRuleResultRiskCalc.main(FlowsRuleResultRiskCalc.java:42)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 0, column 0 to line 1, column 149: Column 'data.data' not found in table 'riskRuleEngineResultLevel2_3'
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4867)
    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5839)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5823)
    at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:320)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5431)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3101)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3082)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3133)
    at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3076)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:182)
    ... 5 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'data.data' not found in table 'riskRuleEngineResultLevel2_3'
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
    ... 28 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 07:03:10 UTC 2023,,,,,,,,,,"0|z1cppk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 07:03;337361684@qq.com;Hi, [~mooonzhang] . I think it's not a bug, you need to find a column use full name like 'TableName.ColumnName.SubColumnName' instead of 'ColumnName.SubColumnName'.  In your case, you need to use 'riskRuleEngineResultLevel2_3.data.rule_results'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimestampOffsetsInitializer allows configurable OffsetResetStrategy,FLINK-30200,13505818,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,24/Nov/22 20:26,24/Nov/22 20:27,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Connectors / Kafka,,,,0,,,,"Currently, the TimestampOffsetsInitializer defaults to `LATEST` and it would be beneficial to allow the user to customize this to `EARLIEST` or `NONE`. This was brought up in PR review: [https://github.com/apache/flink/pull/20370#discussion_r991928769]

NOTE: this is a public API change albeit small, so we need to discuss on the mailing list first.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-24 20:26:27.0,,,,,,,,,,"0|z1cpiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a script to run Kubernetes Operator e2e tests manually,FLINK-30199,13505805,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pvary,pvary,pvary,24/Nov/22 18:41,14/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,stale-assigned,,Currently it is very hard to run the Kubernetes Operator e2e tests manually. Especially on MAC. We need to improve upon this to ease of the development process.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:04 UTC 2023,,,,,,,,,,"0|z1cpg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 12:00;mbalassi;[{{f033976}}|https://github.com/apache/flink-kubernetes-operator/commit/f033976f2f225e8b5b869b0e523624230a2540eb] in main;;;","02/Dec/22 11:46;gaborgsomogyi;The change has been rolled back because it caused instabilities on CI.;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support AdaptiveBatchScheduler to set per-task size for reducer task ,FLINK-30198,13505779,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,24/Nov/22 15:15,09/Feb/23 09:52,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"When we use AdaptiveBatchScheduler in our case, we found that it can work well in most case, but there is a limit that, there is only one global parameter for per task data size by {{jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task}}. 

However, in a map-reduce architecture, the reducer tasks are usually have more complex computation logic such as aggregate/sort/join operators. So I think it will be nicer if we can set the reducer and mapper task's data size per task individually.

Then, how to distinguish the reducer task?
IMO, we can let the parallelism decider know whether the vertex have a hash edge inputs. If yes, it should be a reducer task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:52:23 UTC 2023,,,,,,,,,,"0|z1cpa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 15:16;aitozi;cc [~wanglijie] what do you think of this ?;;;","29/Nov/22 01:51;wanglijie;My main concern here is that Flink does not have the concept of map and reduce currently (which also makes us unable to strictly define what a reducer is).

I also think it is a reasonable requirement to define per-task-volume for each stage(job vertex) individually, but I have no idea for this at present.;;;","29/Nov/22 01:54;wanglijie;[~zhuzh] Would you like share your thoughts;;;","29/Nov/22 04:45;Weijie Guo;IMO, it seems too strange to identify the hash input and then define the reducer task. So +1 for [~wanglijie]'s proposal to define per-task-volume for each stage(job vertex). Maybe we can first introduce the support of setting consumed data volume in jobvertex granularity, then introduction the SQL/Planer level configuration to configure per task data volume for join/agg/sort operators. In this way, manual configuration for datastream jobs can also be supported. Of course, any other better proposals are also acceptable.;;;","14/Dec/22 06:49;zhuzh;In our experience, map tasks are also possible to be heavy, depending on the implementation, e.g. complex Calc.
Allowing to specify expected data to consume in vertex level can be flexible. However, it might be too complex for users to tune it for each vertex. Also, we will keep improving the adaptive batch in next versions. Therefore, I prefer to not add this feature to core at the moment. 
We may reconsider this once the adaptive batch becomes stable. Alternatives, in the future, we may make VertexParallelismDecider pluggable, so that users will be able to customize their own strategy, including vertex-wise configurable data size to consume.;;;","15/Dec/22 14:51;aitozi;Thanks, all guys, for your input. I agree that vertex-level tuning will be more complex. I think a pluggable {{VertexParallelismDecider}} is a good choice. Maybe we can also provide some information about the vertex, eg: the vertex type {{Calc, Join, Local/Global Aggregate...}} to the interface to let users do more suitable choices. ;;;","09/Feb/23 09:52;zhuzh;Flink scheduler currently is not aware of operators at the moment, i.e. it does not know whether a job vertex contains Join, Agg. A workaround is to provide the task name and the plugin strategy can infer the operators through the name. It's possible that in the future, we will improve the scheduler to be more aware of operators, to support advanced optimization.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Update slf4j from 1.7.32 to 1.7.36,FLINK-30197,13505774,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,24/Nov/22 14:42,30/Nov/22 18:50,04/Jun/24 20:41,30/Nov/22 18:50,,,,,,1.17.0,,,,,API / Core,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 30 18:50:25 UTC 2022,,,,,,,,,,"0|z1cp94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 18:50;martijnvisser;Fixed in master: 8b4cb7583d509898b09782bbd4fb06388a219efe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace usage of deprecated Mockito's methods with advised by Mockito,FLINK-30196,13505771,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,24/Nov/22 14:18,20/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,Tests,,,,0,auto-deprioritized-major,pull-request-available,,"Currently there is a number of deprecated Mockito's methods in use. Most of them will be removed in 4.* as stated at https://github.com/mockito/mockito/releases/tag/v4.0.0

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:03 UTC 2023,,,,,,,,,,"0|z1cp8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderElectionService should avoid potential deadlock with leaderContender,FLINK-30195,13505769,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,24/Nov/22 14:09,09/Feb/23 09:59,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"As discussed in [https://github.com/apache/flink/pull/21137|https://github.com/apache/flink/pull/21137,] , leader election service should not call `contender#grant/revokeLeadership` under a lock while the same lock can be accessed by the contender. One proposal to fix this issue is introduce a dedicated executor to get rid of the nested lock structure. This would affect all contenders and we need to carefully check that no existing contenders are relying on the current behavior that `grant/removeLeadership{{{}`{}}} are called under lock. We should also clean up things like `ResourceManagerServiceImpl.handleLeaderEventExecutor`. Any other better suggestions are also welcome.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-24 14:09:38.0,,,,,,,,,,"0|z1cp80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename DelegationTokenProvider to HadoopDelegationTokenProvider,FLINK-30194,13505750,13498461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,24/Nov/22 12:24,28/Nov/22 11:10,04/Jun/24 20:41,28/Nov/22 11:10,1.17.0,,,,,1.17.0,,,,,API / Core,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 11:10:45 UTC 2022,,,,,,,,,,"0|z1cp3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 11:10;mbalassi;[b4bad50|https://github.com/apache/flink/commit/b4bad50d77cb21abff697fc12f6586b67f6622ff] in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
could flink has dynamic rebalance  according to the load of downstream operators ,FLINK-30193,13505731,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,landlord,landlord,24/Nov/22 10:00,24/Nov/22 15:32,04/Jun/24 20:41,24/Nov/22 10:21,1.15.2,,,,,,,,,,API / DataStream,,,,0,,,,"Different records cause different loads to the downstream and different task slot has different compute power. So rebalancing according to the load of downstream operators will greatly reduce data skew and improve resource utilization.

I thaught the point is how to gain the load of downstream.  I have seen the 

StreamPartitioner , and it dosen't has any context or method to get the load information of downstream operators. So ordinary developers cannot easily implement such functions.

finally，Could flink provide such a function?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 15:32:17 UTC 2022,,,,,,,,,,"0|z1cozk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 10:21;martijnvisser;[~landlord] Please ask a question like this on the User mailing list or the Slack channel.;;;","24/Nov/22 15:32;pltbkd;HI [~landlord],
We did some work like this, which makes rebalance partitioner able to distribute the records according the load of downstream operators, and we are considering that maybe we would contribute the code to the community when we can make the time. Maybe we can discuss about the feature in the mail list if you'd like. The jira should be created after the discuss in the mail list reaches a consensus.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update python-dateutil from 2.8.0 to 2.8.2,FLINK-30192,13505724,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,martijnvisser,martijnvisser,martijnvisser,24/Nov/22 09:16,29/Jan/23 06:58,04/Jun/24 20:41,29/Jan/23 06:57,,,,,,,,,,,API / Python,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 06:57:42 UTC 2023,,,,,,,,,,"0|z1coy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 06:57;dianfu;The version of python-dateutil has been bumped to 2.8.0+ in FLINK-25874 and I'm closing this ticket~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update py4j from 0.10.9.3 to 0.10.9.7,FLINK-30191,13505723,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,yunfengzhou,martijnvisser,martijnvisser,24/Nov/22 09:16,28/Mar/23 10:02,04/Jun/24 20:41,19/Jan/23 03:50,,,,,,1.17.0,,,,,API / Python,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30756,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 10:01:53 UTC 2023,,,,,,,,,,"0|z1coxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 07:38;yunfengzhou;Hi [~martijnvisser], would you mind my asking about the progress of this ticket?

An ecosystem project of Flink I am participating in (FeatHub) expects that PyFlink could use py4j 0.10.9.7 to resolve a conflict in python dependency, and we are expecting that the solution of this ticket could be merged into Flink 1.17.

In case this ticket might be of low priority on your side, may I take this ticket and create the corresponding PR?;;;","16/Jan/23 14:00;martijnvisser;[~yunfengzhou] Feel free to take this one over, I have little time to work on this one unfortunately. I've assigned it to you. ;;;","19/Jan/23 03:50;hxbks2ks;Merged into master via 0bbc7b1e9fed89b8c3e8ec67b7b0dad5999c2c01;;;","01/Feb/23 10:17;mapohl;[~yunfengzhou] [~hxbks2ks] looks like there was a remote branch accidentally created in the Apache Flink repo for this issue?;;;","27/Feb/23 13:35;mapohl;[~yunfengzhou] [~hxbks2ks] could you clean the branch if it's not needed?;;;","28/Mar/23 10:01;mapohl;Looks like I was the one creating the remote branch when reverting a change (caused by [PR #21734|https://github.com/apache/flink/pull/21734]). I should have checked properly. Sorry for that :D I deleted the remote branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update Cython from 0.29.24 to 0.29.32,FLINK-30190,13505722,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,martijnvisser,martijnvisser,martijnvisser,24/Nov/22 09:15,12/Jan/23 09:20,04/Jun/24 20:41,12/Jan/23 09:20,,,,,,,,,,,API / Python,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 09:20:02 UTC 2023,,,,,,,,,,"0|z1coxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 09:20;martijnvisser;Is a breaking change, will be done later;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsSubpartitionFileReader may load data that has been consumed from memory,FLINK-30189,13505721,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,24/Nov/22 09:07,07/Dec/22 04:39,04/Jun/24 20:41,07/Dec/22 04:39,1.16.0,1.17.0,,,,1.16.1,1.17.0,,,,Runtime / Network,,,,0,pull-request-available,,,"In order to solve the problem that data cannot be read from the disk correctly after failover, we changed the calculation logical of the buffer's readable state in FLINK-29238.  Buffers that are greater than consumingOffset and have been released can be pre-load from file. However, the update of consumingOffset is asynchronous, If it lags behind the actual consumption progress, the buffer will have a chance to be load from the disk again. 

IMO, we can record the consumed status of buffer by each consumer in the InternalRegion. Only the buffers that have not been consumed and have been released will be considered as readable. In the case of failover, a new consumerId will be generated, so all buffers will be considered as unconsumed and can be correctly read from the disk too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29419,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 04:39:24 UTC 2022,,,,,,,,,,"0|z1coxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 04:39;xtsong;- master (1.17): 4c67f8fca529a72389d69990307bbf78fcd3d99d
- release-1.16: a9e65bc2377ee7a4b3599b58a58ff0301b79c5d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition finished state in ConsumedPartitionGroup is not updated correctly in dynamic graph,FLINK-30188,13505715,13491747,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,24/Nov/22 08:32,22/Dec/22 06:10,04/Jun/24 20:41,22/Dec/22 06:10,1.17.0,,,,,1.17.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"For dynamic graph, the downstream task's parallelism is determined only after the upstream task is finished, and then the connection relationship between the downstream and upstream `ExecutionVertex` will be established (including `ConsumedPartitionGroup`). Therefore, the upstream task did not correctly notify `ConsumedPartitionGroup` to update the partition finished status after it is finished.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 06:10:40 UTC 2022,,,,,,,,,,"0|z1cow0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 06:10;xtsong;master (1.17): 4ea67f63eb1c43d7adf07c37946b20b525fb015d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi session job test is silently not checking the operator log for errors,FLINK-30187,13505711,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,24/Nov/22 08:19,24/Nov/22 16:19,04/Jun/24 20:41,24/Nov/22 16:19,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"{code:java}
Checking for operator log errors...
error: You must provide one or more resources by argument or filename.
Example resource specifications include:
   '-f rsrc.yaml'
   '--filename=rsrc.json'
   '<resource> <name>'
   '<resource>'
No errors in log files.
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 16:19:10 UTC 2022,,,,,,,,,,"0|z1cov4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 16:19;gyfora;merged to main b8483822516c526b8da19e031a2c695c2c59f207;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Flink version to 1.15.3,FLINK-30186,13505707,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,highfei2011@126.com,highfei2011@126.com,highfei2011@126.com,24/Nov/22 07:58,04/Dec/22 17:47,04/Jun/24 20:41,04/Dec/22 17:47,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"A new stable version of flink has been released, Bump Flink version to 1.15.3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 04 17:47:56 UTC 2022,,,,,,,,,,"0|z1cou8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/22 17:47;gyfora;Merged to main 3c5ec6cd86de61752c708eba385646a7c1164880;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide the flame graph to the subtask level,FLINK-30185,13505694,13516999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,fanrui,fanrui,fanrui,24/Nov/22 06:52,06/Jan/23 07:13,04/Jun/24 20:41,05/Jan/23 11:25,,,,,,1.17.0,,,,,Runtime / REST,Runtime / Web Frontend,,,0,pull-request-available,,,"FLINK-13550 supported for CPU FlameGraphs in web UI.

As Flink doc mentioned:

https://nightlies.apache.org/flink/flink-docs-master/docs/ops/debugging/flame_graphs/#sampling-process
{code:java}
Note: Stack trace samples from all threads of an operator are combined together. If a method call consumes 100% of the resources in one of the parallel tasks but none in the others, the bottleneck might be obscured by being averaged out.

There are plans to address this limitation in the future by providing “drill down” visualizations to the task level. {code}
 

The flame graph at the subtask level is very useful when a small number of subtasks are bottlenecked. So we should provide the flame graph to the subtask level

 

!image-2022-11-24-14-49-42-845.png!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/22 06:49;fanrui;image-2022-11-24-14-49-42-845.png;https://issues.apache.org/jira/secure/attachment/13053058/image-2022-11-24-14-49-42-845.png","28/Nov/22 06:38;fanrui;image-2022-11-28-14-38-47-145.png;https://issues.apache.org/jira/secure/attachment/13053190/image-2022-11-28-14-38-47-145.png","28/Nov/22 06:48;fanrui;image-2022-11-28-14-48-20-462.png;https://issues.apache.org/jira/secure/attachment/13053191/image-2022-11-28-14-48-20-462.png",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 11:25:12 UTC 2023,,,,,,,,,,"0|z1corc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 06:55;fanrui;Hi [~xtsong]  , please help take a look in your free time. And if it makes sense, please assign it to me, thanks~
 ;;;","28/Nov/22 04:35;xtsong;I think this makes a nice improvement. Could you explain a bit more in detail how do you plan to do this?;;;","28/Nov/22 06:48;fanrui;Hi [~xtsong] , thanks for your reply.

The improvement mainly includes 2 parts: 
 # How the web frontend show the flame_graph for single subtask?
 # How the backend save or fetch the thread info sample for single subtask?

h2. Web Frontend

It's similar with Metrics, we need to add a select box that select subtaskIndex all or one subtaskIndex.

And pass the subtaskIndex to backend.

!image-2022-11-28-14-48-20-462.png!

!image-2022-11-28-14-38-47-145.png|width=783,height=286!

 
h2. Backend
h3. 1. Refactor the cache logic

Currently, the cache key of ThreadInfo is jobId + JobVertexId. The cache key should be changed to jobId + jobVertexId + subtaskIndex.
h3. 2. Add the subtaskIndex

Allow request threadInfo from single subtask.

 

If anything is wrong or missed, please let me know, thanks!;;;","28/Nov/22 07:34;xtsong;[~fanrui],

Sounds good to me. You are assigned. Please move ahead.;;;","05/Jan/23 11:25;xtsong;master (1.17): 898fe6ba6c882b82a266fb5a415d68e9c9113cb8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Save TM/JM thread stack periodically,FLINK-30184,13505692,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,fanrui,fanrui,24/Nov/22 06:42,28/Nov/22 05:52,04/Jun/24 20:41,28/Nov/22 05:52,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,0,,,,"After FLINK-14816 FLINK-25398 and FLINK-25372 , flink user can view the thread stack of TM/JM in Flink WebUI. 

It can help flink users to find out why the Flink job is stuck, or why the processing is slow. It is very useful for trouble shooting.

However, sometimes Flink tasks get stuck or process slowly, but when the user troubleshoots the problem, the job has resumed. It is difficult to find out what happened to the Flink job at the time and why is it slow?

 

So, could we periodically save the thread stack of TM or JM in the TM log directory?

Define some configurations:
cluster.thread-dump.interval=1min
cluster.thread-dump.cleanup-time=48 hours",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 05:51:51 UTC 2022,,,,,,,,,,"0|z1coqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:19;gaoyunhaii;Hi [~fanrui] perhaps [FlameGraph | https://nightlies.apache.org/flink/flink-docs-master/docs/ops/debugging/flame_graphs/] could provide the functionality? 

 ;;;","24/Nov/22 08:35;fanrui;Hi [~gaoyunhaii] , thanks for your reply. I'm sorry, I may not have expressed my thoughts clearly.

As I understand, Flame Graph can only display the current stack.

What I mean is: if it's 5am, the Flink job lag is huge. The Flink user did not troubleshoot the problem in time, and started to troubleshoot after working hours. However, Metric can only find out which Task is slow, and cannot analyze where the Task is stuck at 5 o'clock in the morning? Why are Tasks slow? The Thread stack at 5am can help users analyze where the task is stuck.

In general, the historical thread stack can tell the user what the flink job is doing every minute.;;;","24/Nov/22 08:45;gaoyunhaii;Thanks [~fanrui] for the explanation! Now I got the issue. ;;;","24/Nov/22 14:10;wangyang0918;I lean towards to make this be done outside of Flink.;;;","25/Nov/22 08:16;fanrui;Hi [~wangyang0918] , thanks for your feedback.

In fact, this feature is useful for trouble shooting, and I know that some companies do this with yarn.

However, the yarn of apache version doesn't have this feature. And too many companies don't maintain their internal yarn version. So I'm not sure if this should be done on the flink side.;;;","28/Nov/22 04:29;xtsong;[~fanrui], sorry for the late response.

I agree with [~wangyang0918] that this is probably more suitable for an external service that manages / monitors Flink.

Thread dumps are for debugging and should not be activated constantly given the performance impact. Flink already offers rest api for capturing thread stacks of [jobmanager|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobmanager-thread-dump] and [taskmanager|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#taskmanagers-taskmanagerid-thread-dump]. It should be easy for an external monitoring system to capture the dumps when the job is detected to be slow.;;;","28/Nov/22 05:51;fanrui;Hi [~xtsong] , thanks for your explanation.

It sounds reasonable, I will close this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,
We should add a proper error message in case the deprecated reflection-based instantiation of a reporter is triggered,FLINK-30183,13505682,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,24/Nov/22 05:01,29/Nov/22 06:13,04/Jun/24 20:41,29/Nov/22 06:13,1.16.0,,,,,1.16.1,,,,,Runtime / Metrics,,,,0,pull-request-available,starter,,"A [user reported an issue|https://lists.apache.org/thread/yjv0hof5qqnzq22xcjf3y2v61j48gqh4] with the {{PrometheusReporter}} instantiation. The {{NoSuchMethodException}} is caused by the {{PromethusReporter}} not having a default constructor implemented. Instead, the user should switch to the factory class approach.

We might want to -log a warning if the reflection-based approach still works but- wrap the {{NoSuchMethodException}} into a {{IllegalConfigurationException}} with a proper error message pointing to the factory-based approach",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 06:13:09 UTC 2022,,,,,,,,,,"0|z1cooo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:53;chesnay;We already have a log message for that.

https://github.com/apache/flink/blob/af6eff873a53bbdc85a2b1018140754e65758e3e/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/ReporterSetup.java#L380;;;","29/Nov/22 06:13;mapohl;1.16: e70336893d808c9e9759fbde1a772e39ec2f8b6c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create template for release announcement,FLINK-30182,13505677,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,24/Nov/22 03:41,24/Nov/22 05:41,04/Jun/24 20:41,24/Nov/22 05:41,1.17.0,,,,,,,,,,Documentation,,,,0,,,,"A question popped up during the 1.15.3 release (see [flink-web PR comment|https://github.com/apache/flink-web/pull/581#discussion_r1028894470]) about a template for the release announcement. We should provide something like that in the release docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 05:41:39 UTC 2022,,,,,,,,,,"0|z1conk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 05:41;mapohl;I linked the [1.15.3 blog post PR|https://github.com/apache/flink-web/pull/581] as an example to [Creating a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiInputCheckpointingTimeBenchmark.checkpointMultiInput benchmark failed,FLINK-30181,13505670,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Yanfei Lei,Yanfei Lei,24/Nov/22 02:16,15/Mar/23 02:55,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Benchmarks,Runtime / Checkpointing,,,0,,,,"{code:java}
08:30:14  # JMH version: 1.19
08:30:14  # VM version: JDK 11.0.16, VM 11.0.16+8-adhoc.root.jdk11u
08:30:14  # VM invoker: /usr/lib/jvm/openlogic-openjdk-11.0.16+8-linux-x64/bin/java
08:30:14  # VM options: -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.ssl
08:30:14  # Warmup: 10 iterations, 1 s each
08:30:14  # Measurement: 10 iterations, 1 s each
08:30:14  # Timeout: 10 min per iteration
08:30:14  # Threads: 1 thread, will synchronize iterations
08:30:14  # Benchmark mode: Throughput, ops/time
08:30:14  # Benchmark: org.apache.flink.benchmark.MultiInputCheckpointingTimeBenchmark.checkpointMultiInput
08:30:14  
08:30:14  # Run progress: 41.07% complete, ETA 01:16:40
08:30:14  # Fork: 1 of 3
08:30:32  # Warmup Iteration   1: 4.561 ops/s
08:30:32  # Warmup Iteration   2: 3.926 ops/s
08:30:33  # Warmup Iteration   3: 2.986 ops/s
08:30:34  # Warmup Iteration   4: 2.858 ops/s
08:30:35  # Warmup Iteration   5: 2.806 ops/s
08:30:36  # Warmup Iteration   6: 2.708 ops/s
08:30:37  # Warmup Iteration   7: 2.638 ops/s
08:30:39  # Warmup Iteration   8: 2.686 ops/s
08:30:40  # Warmup Iteration   9: 2.606 ops/s
08:30:41  # Warmup Iteration  10: 2.729 ops/s
08:30:42  Iteration   1: 2.665 ops/s
08:30:43  Iteration   2: 2.686 ops/s
08:30:44  Iteration   3: 2.708 ops/s
08:30:46  Iteration   4: 2.733 ops/s
08:30:47  Iteration   5: 2.759 ops/s
08:30:47  Iteration   6: 2.805 ops/s
08:30:48  Iteration   7: 2.791 ops/s
08:30:50  Iteration   8: 2.762 ops/s
08:30:51  Iteration   9: 2.666 ops/s
08:30:52  Iteration  10: 2.667 ops/s
08:30:52  
08:30:52  # Run progress: 41.67% complete, ETA 01:15:43
08:30:52  # Fork: 2 of 3
08:31:11  # Warmup Iteration   1: 4.756 ops/s
08:31:11  # Warmup Iteration   2: 4.163 ops/s
08:31:12  # Warmup Iteration   3: 2.977 ops/s
08:31:13  # Warmup Iteration   4: 2.628 ops/s
08:31:14  # Warmup Iteration   5: 2.614 ops/s
08:31:15  # Warmup Iteration   6: 2.623 ops/s
08:31:16  # Warmup Iteration   7: 2.625 ops/s
08:31:17  # Warmup Iteration   8: 2.538 ops/s
08:31:19  # Warmup Iteration   9: 2.754 ops/s
08:31:20  # Warmup Iteration  10: 2.701 ops/s
08:31:21  Iteration   1: 2.748 ops/s
08:31:22  Iteration   2: 2.691 ops/s
08:31:23  Iteration   3: 2.654 ops/s
08:31:24  Iteration   4: 2.683 ops/s
08:31:26  Iteration   5: 2.527 ops/s
08:31:27  Iteration   6: 2.729 ops/s
08:31:28  Iteration   7: 2.683 ops/s
08:31:29  Iteration   8: 2.642 ops/s
08:31:30  Iteration   9: 2.653 ops/s
08:36:37  01:36:31,596 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Fatal error occurred in TaskExecutor akka://flink/user/rpc/taskmanager_9.
08:36:37  org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
08:36:37  	at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1524) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
08:36:37  	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$17(TaskExecutor.java:1509) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
08:36:37  	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_8f0f0fd6-15b6-408d-9f86-28585d436b16.jar:1.17-SNAPSHOT]
08:36:37  	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) [?:?]
08:36:37  	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) [?:?]
08:36:37  	at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) [?:?]
08:36:37  	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) [?:?]
08:36:37  	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183) [?:?]
08:41:44  Iteration  10: (*interrupt*) <failure>
08:41:44  
08:41:44  java.lang.InterruptedException
08:41:44  	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:385)
08:41:44  	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
08:41:44  	at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:36)
08:41:44  	at org.apache.flink.benchmark.FlinkEnvironmentContext.tearDown(FlinkEnvironmentContext.java:87)
08:41:44  	at org.apache.flink.benchmark.generated.MultiInputCheckpointingTimeBenchmark_checkpointMultiInput_jmhTest.checkpointMultiInput_Throughput(MultiInputCheckpointingTimeBenchmark_checkpointMultiInput_jmhTest.java:99)
08:41:44  	at jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)
08:41:44  	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44  	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44  	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
08:41:44  	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
08:41:44  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
08:41:44  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
08:41:44  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
08:41:44  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
08:41:44  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:41:44  	at java.base/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  
08:41:44  <JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 24 seconds more...>
08:41:44  
08:41:44  Non-finished threads:
08:41:44  
08:41:44  Thread[co-map -> Sink: Unnamed (3/4)#0,5,Flink Task Threads]
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:44    at java.base@11.0.16/java.util.ArrayList.writeObject(ArrayList.java:897)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:44    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[flink-akka.actor.default-dispatcher-10,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:44  
08:41:44  Thread[flink-metrics-scheduler-1,5,main]
08:41:44    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:44    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[co-map -> Sink: Unnamed (2/4)#0,5,Flink Task Threads]
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:44    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[flink-metrics-akka.actor.internal-dispatcher-2,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:44  
08:41:44  Thread[flink-akka.actor.internal-dispatcher-4,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:44  
08:41:44  Thread[DestroyJavaVM,5,main]
08:41:44  
08:41:44  Thread[flink-scheduler-1,5,main]
08:41:44    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:44    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[pool-32-thread-1,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081)
08:41:44    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
08:41:44    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
08:41:44    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)
08:41:44    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)
08:41:44    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  
08:41:44  <JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 19 seconds more...>
08:41:44  
08:41:44  Non-finished threads:
08:41:44  
08:41:44  Thread[co-map -> Sink: Unnamed (3/4)#0,5,Flink Task Threads]
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:44    at java.base@11.0.16/java.util.ArrayList.writeObject(ArrayList.java:897)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:44    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[flink-akka.actor.default-dispatcher-10,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:44  
08:41:44  Thread[flink-metrics-scheduler-1,5,main]
08:41:44    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:44    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[co-map -> Sink: Unnamed (2/4)#0,5,Flink Task Threads]
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:44    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:44    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:44    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:44    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:44    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:44    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:44    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:44    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:44    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[flink-metrics-akka.actor.internal-dispatcher-2,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:44  
08:41:44  Thread[flink-akka.actor.internal-dispatcher-4,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:44    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:44  
08:41:44  Thread[DestroyJavaVM,5,main]
08:41:44  
08:41:44  Thread[flink-scheduler-1,5,main]
08:41:44    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:44    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:44    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  Thread[pool-32-thread-1,5,main]
08:41:44    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:44    at java.base@11.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081)
08:41:44    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
08:41:44    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
08:41:44    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)
08:41:44    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)
08:41:44    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:41:44    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:44  
08:41:44  
08:41:45  <JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 14 seconds more...>
08:41:45  
08:41:45  Non-finished threads:
08:41:45  
08:41:45  Thread[co-map -> Sink: Unnamed (3/4)#0,5,Flink Task Threads]
08:41:45    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:45    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:45    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:45    at java.base@11.0.16/java.util.ArrayList.writeObject(ArrayList.java:897)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:45    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:45    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:45    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:45    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:45    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:45    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:45    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:45    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:45    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:45    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:45    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:45  
08:41:45  Thread[flink-akka.actor.default-dispatcher-10,5,main]
08:41:45    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:45    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:45    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:45    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:45  
08:41:45  Thread[flink-metrics-scheduler-1,5,main]
08:41:45    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:45    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:45    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:45    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:45    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:45  
08:41:45  Thread[co-map -> Sink: Unnamed (2/4)#0,5,Flink Task Threads]
08:41:45    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:45    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:45    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:45    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:45    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:45    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:45    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:45    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:45    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:45    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:45    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:45    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:45    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:45    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:45    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:45    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:45  
08:41:45  Thread[flink-metrics-akka.actor.internal-dispatcher-2,5,main]
08:41:45    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:45    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:45    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:45    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:45  
08:41:45  Thread[flink-akka.actor.internal-dispatcher-4,5,main]
08:41:45    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:45    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:45    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:45    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:45  
08:41:45  Thread[DestroyJavaVM,5,main]
08:41:45  
08:41:45  Thread[flink-scheduler-1,5,main]
08:41:45    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:45    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:45    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:45    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:45    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:45  
08:41:45  Thread[pool-32-thread-1,5,main]
08:41:45    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:45    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:45    at java.base@11.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081)
08:41:45    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
08:41:45    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
08:41:45    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)
08:41:45    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)
08:41:45    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:41:45    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:45  
08:41:45  
08:41:50  <JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 9 seconds more...>
08:41:50  
08:41:50  Non-finished threads:
08:41:50  
08:41:50  Thread[co-map -> Sink: Unnamed (3/4)#0,5,Flink Task Threads]
08:41:50    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:50    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:50    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:50    at java.base@11.0.16/java.util.ArrayList.writeObject(ArrayList.java:897)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:50    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:50    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:50    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:50    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:50    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:50    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:50    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:50    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:50    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:50    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:50    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:50  
08:41:50  Thread[flink-akka.actor.default-dispatcher-10,5,main]
08:41:50    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:50    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:50    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:50    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:50  
08:41:50  Thread[flink-metrics-scheduler-1,5,main]
08:41:50    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:50    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:50    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:50    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:50    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:50  
08:41:50  Thread[co-map -> Sink: Unnamed (2/4)#0,5,Flink Task Threads]
08:41:50    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:50    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:50    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:50    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:50    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:50    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:50    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:50    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:50    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:50    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:50    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:50    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:50    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:50    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:50    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:50    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:50  
08:41:50  Thread[flink-metrics-akka.actor.internal-dispatcher-2,5,main]
08:41:50    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:50    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:50    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:50    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:50  
08:41:50  Thread[flink-akka.actor.internal-dispatcher-4,5,main]
08:41:50    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:50    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:50    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:50    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:50  
08:41:50  Thread[DestroyJavaVM,5,main]
08:41:50  
08:41:50  Thread[flink-scheduler-1,5,main]
08:41:50    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:50    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:50    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:50    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:50    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:50  
08:41:50  Thread[pool-32-thread-1,5,main]
08:41:50    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:50    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:50    at java.base@11.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081)
08:41:50    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
08:41:50    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
08:41:50    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)
08:41:50    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)
08:41:50    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:41:50    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:50  
08:41:50  
08:41:56  <JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 4 seconds more...>
08:41:56  
08:41:56  Non-finished threads:
08:41:56  
08:41:56  Thread[co-map -> Sink: Unnamed (3/4)#0,5,Flink Task Threads]
08:41:56    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:56    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:56    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:56    at java.base@11.0.16/java.util.ArrayList.writeObject(ArrayList.java:897)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:56    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:56    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:56    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:56    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:56    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:56    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:56    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:56    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:56    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:56    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:56    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:56  
08:41:56  Thread[flink-akka.actor.default-dispatcher-10,5,main]
08:41:56    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:56    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:56    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:56    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:56  
08:41:56  Thread[flink-metrics-scheduler-1,5,main]
08:41:56    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:56    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:56    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:56    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:56    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:56  
08:41:56  Thread[co-map -> Sink: Unnamed (2/4)#0,5,Flink Task Threads]
08:41:56    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:56    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:56    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:41:56    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:41:56    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:41:56    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:41:56    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:41:56    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:41:56    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:41:56    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:41:56    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:41:56    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:41:56    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:41:56    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:41:56    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:41:56    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:56  
08:41:56  Thread[flink-metrics-akka.actor.internal-dispatcher-2,5,main]
08:41:56    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:56    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:56    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:56    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:56  
08:41:56  Thread[flink-akka.actor.internal-dispatcher-4,5,main]
08:41:56    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:56    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:56    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:41:56    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:41:56  
08:41:56  Thread[DestroyJavaVM,5,main]
08:41:56  
08:41:56  Thread[flink-scheduler-1,5,main]
08:41:56    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:41:56    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:41:56    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:41:56    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:41:56    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:56  
08:41:56  Thread[pool-32-thread-1,5,main]
08:41:56    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:41:56    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:41:56    at java.base@11.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081)
08:41:56    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
08:41:56    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
08:41:56    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)
08:41:56    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)
08:41:56    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:41:56    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:41:56  
08:41:56  
08:42:00  <JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 0 seconds more...>
08:42:00  
08:42:00  Non-finished threads:
08:42:00  
08:42:00  Thread[co-map -> Sink: Unnamed (3/4)#0,5,Flink Task Threads]
08:42:00    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:42:00    at java.base@11.0.16/java.util.ArrayList.writeObject(ArrayList.java:897)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:42:00    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:42:00    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:42:00    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:42:00    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:42:00    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:42:00    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:42:00    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:42:00  
08:42:00  Thread[flink-akka.actor.default-dispatcher-10,5,main]
08:42:00    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:42:00    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:42:00    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:42:00    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:42:00  
08:42:00  Thread[flink-metrics-scheduler-1,5,main]
08:42:00    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:42:00    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:42:00    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:42:00    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:42:00    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:42:00  
08:42:00  Thread[co-map -> Sink: Unnamed (2/4)#0,5,Flink Task Threads]
08:42:00    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:990)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:438)
08:42:00    at java.base@11.0.16/java.lang.Throwable.writeObject(Throwable.java:996)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
08:42:00    at java.base@11.0.16/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00    at java.base@11.0.16/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00    at java.base@11.0.16/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)
08:42:00    at java.base@11.0.16/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)
08:42:00    at app//org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:548)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:72)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
08:42:00    at app//org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
08:42:00    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:96)
08:42:00    at app//org.apache.flink.runtime.taskmanager.TaskExecutionState.<init>(TaskExecutionState.java:70)
08:42:00    at app//org.apache.flink.runtime.taskmanager.Task.notifyFinalState(Task.java:1039)
08:42:00    at app//org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:837)
08:42:00    at app//org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
08:42:00    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:42:00  
08:42:00  Thread[flink-metrics-akka.actor.internal-dispatcher-2,5,main]
08:42:00    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:42:00    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:42:00    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:42:00    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:42:00  
08:42:00  Thread[flink-akka.actor.internal-dispatcher-4,5,main]
08:42:00    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:42:00    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:42:00    at java.base@11.0.16/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
08:42:00    at java.base@11.0.16/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
08:42:00  
08:42:00  Thread[DestroyJavaVM,5,main]
08:42:00  
08:42:00  Thread[flink-scheduler-1,5,main]
08:42:00    at java.base@11.0.16/java.lang.Thread.sleep(Native Method)
08:42:00    at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
08:42:00    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
08:42:00    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
08:42:00    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:42:00  
08:42:00  Thread[pool-32-thread-1,5,main]
08:42:00    at java.base@11.0.16/jdk.internal.misc.Unsafe.park(Native Method)
08:42:00    at java.base@11.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
08:42:00    at java.base@11.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081)
08:42:00    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
08:42:00    at java.base@11.0.16/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
08:42:00    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)
08:42:00    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)
08:42:00    at java.base@11.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:42:00    at java.base@11.0.16/java.lang.Thread.run(Thread.java:829)
08:42:00  
08:42:00  
08:42:00  <shutdown timeout of 30 seconds expired, forcing forked VM to exit>
08:42:00  Benchmark had encountered error, and fail on error was requested
08:42:00  ERROR: org.openjdk.jmh.runner.RunnerException: Benchmark caught the exception
08:42:00  	at org.openjdk.jmh.runner.Runner.runBenchmarks(Runner.java:570)
08:42:00  	at org.openjdk.jmh.runner.Runner.internalRun(Runner.java:313)
08:42:00  	at org.openjdk.jmh.runner.Runner.run(Runner.java:206)
08:42:00  	at org.openjdk.jmh.Main.main(Main.java:71)
08:42:00  Caused by: org.openjdk.jmh.runner.BenchmarkException: Benchmark error during the run
08:42:00  	at org.openjdk.jmh.runner.BenchmarkHandler.runIteration(BenchmarkHandler.java:428)
08:42:00  	at org.openjdk.jmh.runner.BaseRunner.runBenchmark(BaseRunner.java:282)
08:42:00  	at org.openjdk.jmh.runner.BaseRunner.runBenchmark(BaseRunner.java:234)
08:42:00  	at org.openjdk.jmh.runner.BaseRunner.doSingle(BaseRunner.java:139)
08:42:00  	at org.openjdk.jmh.runner.BaseRunner.runBenchmarksForked(BaseRunner.java:76)
08:42:00  	at org.openjdk.jmh.runner.ForkedRunner.run(ForkedRunner.java:72)
08:42:00  	at org.openjdk.jmh.runner.ForkedMain.main(ForkedMain.java:84)
08:42:00  	Suppressed: java.lang.InterruptedException
08:42:00  		at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:385)
08:42:00  		at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
08:42:00  		at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:36)
08:42:00  		at org.apache.flink.benchmark.FlinkEnvironmentContext.tearDown(FlinkEnvironmentContext.java:87)
08:42:00  		at org.apache.flink.benchmark.generated.MultiInputCheckpointingTimeBenchmark_checkpointMultiInput_jmhTest.checkpointMultiInput_Throughput(MultiInputCheckpointingTimeBenchmark_checkpointMultiInput_jmhTest.java:99)
08:42:00  		at jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)
08:42:00  		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00  		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00  		at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
08:42:00  		at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
08:42:00  		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
08:42:00  		at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
08:42:00  		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
08:42:00  		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
08:42:00  		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
08:42:00  		at java.base/java.lang.Thread.run(Thread.java:829)
08:42:00  [ERROR] Command execution failed.
08:42:00  org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
08:42:00  	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
08:42:00  	at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:166)
08:42:00  	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:804)
08:42:00  	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:751)
08:42:00  	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:313)
08:42:00  	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
08:42:00  	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
08:42:00  	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
08:42:00  	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
08:42:00  	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
08:42:00  	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
08:42:00  	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
08:42:00  	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
08:42:00  	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309)
08:42:00  	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194)
08:42:00  	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107)
08:42:00  	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993)
08:42:00  	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345)
08:42:00  	at org.apache.maven.cli.MavenCli.main(MavenCli.java:191)
08:42:00  	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
08:42:00  	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
08:42:00  	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
08:42:00  	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
08:42:00  	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
08:42:00  	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
08:42:00  	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
08:42:00  	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
08:42:00  [INFO] ------------------------------------------------------------------------
08:42:00  [INFO] BUILD FAILURE
08:42:00  [INFO] ------------------------------------------------------------------------
08:42:00  [INFO] Total time: 01:05 h
08:42:00  [INFO] Finished at: 2022-11-23T01:42:00+01:00
08:42:01  [INFO] Final Memory: 106M/416M
08:42:01  [INFO] ------------------------------------------------------------------------
08:42:01  [ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:exec (default-cli) on project benchmark: Command execution failed.: Process exited with an error: 1 (Exit value: 1) -> [Help 1]
08:42:01  [ERROR] 
08:42:01  [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
08:42:01  [ERROR] Re-run Maven using the -X switch to enable full debug logging.
08:42:01  [ERROR] 
08:42:01  [ERROR] For more information about the errors and possible solutions, please read the following articles:
08:42:01  [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[Pipeline] }

 {code}
[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/889/consoleFull]

http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java11/300/consoleFull",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/22 03:13;Yanfei Lei;java11-300.txt;https://issues.apache.org/jira/secure/attachment/13053047/java11-300.txt","24/Nov/22 03:11;Yanfei Lei;java8.txt;https://issues.apache.org/jira/secure/attachment/13053046/java8.txt",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 02:55:29 UTC 2023,,,,,,,,,,"0|z1com0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 03:04;leonard;Thanks [~Yanfei Lei] for reporting this issue, but the Jenkins links need authentication, you  can post the log files or post clickable links.;;;","24/Nov/22 13:21;Yanfei Lei;Hi [~gaborgsomogyi] , could you please take a look? maybe this is related to FLINK-30085

cc [~chesnay] ;;;","24/Nov/22 13:52;gaborgsomogyi;We don't have access to the CI so not able to check it in-depth.

Not sure why you think that FLINK-30085 caused this issue. There we've set the YARN client timeout to 5 seconds and in the log I see 5 minutes.
{code:java}
08:36:37  org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
{code}

or the other timeout which is 30 seconds:
{code:java}
08:42:00  <shutdown timeout of 30 seconds expired, forcing forked VM to exit>
{code}


I've had a look at https://github.com/apache/flink-benchmarks/blob/master/src/main/java/org/apache/flink/benchmark/MultiInputCheckpointingTimeBenchmark.java and at the first glance it hasn't much to do w/ YARN.

Please point to something to speak about details.
;;;","24/Nov/22 14:45;Yanfei Lei;[~gaborgsomogyi] Thanks for the investigation, I just took a look at commits near the benchmark fails, and found ""[FLINK-30085][tests] *Lower* YARN client *timeout* to speed-up negative tests"", so I'm guessing it might be related, sorry for bothering.;;;","15/Mar/23 02:55;Yanfei Lei;[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java11/400/console]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java11/405/console]

 

Because registering at the ResourceManager timeout, `MultiInputCheckpointingTimeBenchmark` failed again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the description of Filesystem supporting for the lookup in the Supported Connectors section,FLINK-30180,13505669,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yesorno,yesorno,yesorno,24/Nov/22 02:13,24/Nov/22 08:51,04/Jun/24 20:41,24/Nov/22 08:51,1.16.0,,,,,1.17.0,,,,,Documentation,,,,0,pull-request-available,,,"In the Supported Connectors [section|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/overview/#supported-connectors], the graph shows that the Filesystem connector supports the lookup source. Actually, it doesn't.

So I want to remove this description.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/22 02:16;yesorno;Supported connectors.jpg;https://issues.apache.org/jira/secure/attachment/13053041/Supported+connectors.jpg",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:51:39 UTC 2022,,,,,,,,,,"0|z1cols:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:51;martijnvisser;Fixed in master: 752321d31d1c43f5815f47927feb346e8c495498;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop stringifying exception for logging,FLINK-30179,13505664,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,24/Nov/22 01:27,25/Nov/22 01:40,04/Jun/24 20:41,25/Nov/22 01:40,1.16.0,,,,,1.17.0,,,,,Runtime / Task,,,,0,pull-request-available,,,"In {{org.apache.flink.runtime.taskmanager.Task}} there are multiple places where we stringify an exception before logging. According to Slf4j logging, we can just leave the last argument as exception (or throwable) without fitting them into log message string with placeholders ([doc|https://www.slf4j.org/faq.html]). Let Slf4j handles the last exception parameter is good as tooling can treat that differently. Our logging tool does not truncate the exception but truncates log message message.

So essentially:
{code:java}
LOG.debug(
        ""{} ({}) switched from {} to {} due to CancelTaskException: {}"",
        taskNameWithSubtask,
        executionId,
        currentState,
        newState,
        ExceptionUtils.stringifyException(cause));
{code}
should be
{code:java}
LOG.debug(
        ""{} ({}) switched from {} to {} due to CancelTaskException:"",
        taskNameWithSubtask,
        executionId,
        currentState,
        newState,
        cause);
{code}
Interestingly there was a [hot fix|https://github.com/apache/flink/commit/7eac5c62a10158ef210906deb161ac791f18d3ae] that deliberately changed the code from the latter to the former. This JIRA can track the discussion why that was needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 01:40:23 UTC 2022,,,,,,,,,,"0|z1coko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 01:40;mapohl;master: b77a9814a0769880d4dc9937a9dd1525cc16140a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kubernetes test failed with ""crictl: command not found""",FLINK-30178,13505656,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,23/Nov/22 22:22,24/Nov/22 08:26,04/Jun/24 20:41,24/Nov/22 02:43,1.17.0,,,,,,,,,,Deployment / Kubernetes,,,,0,test-stability,,,"{code:bash}
Nov 23 17:36:33 Starting minikube ...
Nov 23 17:36:33 * minikube v1.28.0 on Ubuntu 20.04
Nov 23 17:36:33 * Using the none driver based on user configuration
Nov 23 17:36:33 * Starting control plane node minikube in cluster minikube
Nov 23 17:36:33 * Running on localhost (CPUs=2, Memory=6944MB, Disk=85160MB) ...
Nov 23 17:36:34 * OS release is Ubuntu 20.04.5 LTS
Nov 23 17:37:35 
X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo crictl version: exit status 1
stdout:

stderr:
sudo: crictl: command not found
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43431&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4623",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:26:57 UTC 2022,,,,,,,,,,"0|z1coiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 02:43;mapohl;I'm closing this issue in favor of FLINK-29671. We see the same cause in line [line 4499 of the failed build reported in the description of this issue|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43431&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4499].
{code:java}
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/80172100/7186c302-3766-4ed5-920a-f85c9d6334ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221123T173439Z&X-Amz-Expires=300&X-Amz-Signature=526ac9e0564a28013e49b833e5df8722d3253611f288d9bb060df6a1d86d3e2a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=80172100&response-content-disposition=attachment%3B%20filename%3Dcrictl-v1.24.2-linux-amd64.tar.gz&response-content-type=application%2Foctet-stream [following]
--2022-11-23 17:34:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/80172100/7186c302-3766-4ed5-920a-f85c9d6334ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221123T173439Z&X-Amz-Expires=300&X-Amz-Signature=526ac9e0564a28013e49b833e5df8722d3253611f288d9bb060df6a1d86d3e2a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=80172100&response-content-disposition=attachment%3B%20filename%3Dcrictl-v1.24.2-linux-amd64.tar.gz&response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 503 Egress is over the account limit.
2022-11-23 17:34:40 ERROR 503: Egress is over the account limit..{code};;;","24/Nov/22 08:26;martijnvisser;Thanks :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update pgjdbc to fix CVE-2022-41946,FLINK-30177,13505652,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,23/Nov/22 22:03,14/Dec/22 12:40,04/Jun/24 20:41,24/Nov/22 08:31,,,,,,jdbc-3.1.0,,,,,Connectors / JDBC,,,,0,pull-request-available,,,"There is CVE-2022-41946 fixed in 42.5.1, 42.4.3 42.3.8, 42.2.27.jre7.
Also mentioned in released notes
https://jdbc.postgresql.org/changelogs/2022-11-23-42.5.1-release/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:31:43 UTC 2022,,,,,,,,,,"0|z1coi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:31;martijnvisser;Fixed in master: e08b251cf11e6d4a6922e9e5a3653f901a03257c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert_available_slots is passing even if curl is timing out,FLINK-30176,13505550,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,23/Nov/22 13:52,24/Nov/22 16:07,04/Jun/24 20:41,24/Nov/22 16:07,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 16:07:30 UTC 2022,,,,,,,,,,"0|z1cnvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 16:07;mbalassi;[4027fce|https://github.com/apache/flink-kubernetes-operator/commit/4027fcea70644552eb876d2b2f2db95c452ae5b7] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump snakeyaml from 1.31 to 1.33,FLINK-30175,13505549,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Nov/22 13:47,27/Mar/23 14:30,04/Jun/24 20:41,13/Dec/22 14:01,,,,,,1.17.0,pulsar-4.0.0,,,,Build System,BuildSystem / Shaded,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 18:54:52 UTC 2022,,,,,,,,,,"0|z1cnv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 14:01;martijnvisser;Fixed in master: 2c89283b87746398ac2be425f13cd587a9fa6228;;;","15/Dec/22 13:52;martijnvisser;Fixed for Pulsar in main: 6fe0a4466aba7fa2cdd4c306c2d95898b4d92de0;;;","15/Dec/22 15:39;syhily;Can we defined all the common dependencies in the {{io.github.zentol.flink:flink-connector-parent}}? We don't need to set it multiple times. [~chesnay] WDY?;;;","15/Dec/22 15:42;martijnvisser;[~syhily] None of the other connectors have needed snakeyaml, so I don't think we should in this case. ;;;","15/Dec/22 15:45;syhily;[~martijnvisser] Pulsar also don't need {{snakeyaml}}. But I don't know why we depend on it. I have created a ticket for removing all the uneed dependencies in {{flink-connector-pulsa}}. https://issues.apache.org/jira/browse/FLINK-30430;;;","15/Dec/22 15:53;martijnvisser;[~syhily] {{snakeyaml}} is a transitive dependency of {{io.swagger:swagger-core}} which is a dependency from {{org.apache.pulsar:pulsar-broker}} - The dependency tree for pulsar-broker is quite large in general. ;;;","15/Dec/22 15:56;syhily;I see. But I want to clarify that the {{pulsar-broker}} is only used for providing a runtime instance of Pulsar for test. And we will soon remove them because Pulsar will be compiled with JDK 17 in 2.11.0 release while we need to run the test in JDK 8.

All the tests will be executed with the Pulsar in Docker which works well on Github Action. So I prefer to remove them in the mean time.;;;","15/Dec/22 18:54;martijnvisser;[~syhily] I totally understand that, but perhaps it should be considered to lower the footprint of {{pulsar-broker}} so there's also less dependency convergence needed. Especially {{pulsar-broker}} is only used for testing and if these dependencies are not needed for the actual tests. ;;;",,,,,,,,,,,,,,,,,,,,,,,
Bump engine.io from 6.2.0 to 6.2.1,FLINK-30174,13505546,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Nov/22 13:33,24/Apr/23 12:36,04/Jun/24 20:41,24/Apr/23 12:36,,,,,,1.18.0,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"Bump engine.io from 6.2.0 to 6.2.1 to avoid false flag for CVE-2022-41940
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 12:36:23 UTC 2023,,,,,,,,,,"0|z1cnug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 12:36;martijnvisser;Fixed in master: a27e6fab46c820bd130cb993cdcfb649f95e007c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade @angular-devkit/build-angular from 14.2.6 to 15.0.0,FLINK-30173,13505544,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,junhan,martijnvisser,martijnvisser,23/Nov/22 13:28,23/Nov/22 13:28,04/Jun/24 20:41,,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,,"Upgrade @angular-devkit/build-angular from 14.2.6 to 15.0.0 because 14.2.6 uses loader-utils 3.2.0 which is vulnerable to CVE-2022-37599

While Flink is not affected, it's better to avoid the false flag",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-23 13:28:41.0,,,,,,,,,,"0|z1cnu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump loader-utils from 2.0.2 to 2.0.4,FLINK-30172,13505542,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Nov/22 13:22,29/Nov/22 14:46,04/Jun/24 20:41,29/Nov/22 14:46,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,0,,,,Dependabot created PR: https://github.com/apache/flink/pull/21327,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 14:46:30 UTC 2022,,,,,,,,,,"0|z1cntk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 14:46;martijnvisser;Fixed in master: 45d12dc840a8883af41112dea679d72788906125;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump socket.io-parser from 4.0.4 to 4.0.5,FLINK-30171,13505541,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Nov/22 13:20,29/Nov/22 14:47,04/Jun/24 20:41,29/Nov/22 14:47,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,0,,,,Dependabot created PR: https://github.com/apache/flink/pull/21279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 14:47:32 UTC 2022,,,,,,,,,,"0|z1cntc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 14:47;martijnvisser;Fixed in master: d2cf55ca062bcf7e4c13627fd4973d7a01cfd934;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Parquet version to 1.12.3,FLINK-30170,13505535,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Nov/22 12:59,24/Nov/22 08:23,04/Jun/24 20:41,24/Nov/22 08:23,,,,,,1.17.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"Flink currently uses version 1.12.2, which should be upgraded to 1.12.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:23:38 UTC 2022,,,,,,,,,,"0|z1cns0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:23;martijnvisser;Fixed in master: 9196791542926326caf28b5bb384b42135203fc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adds version switcher in PyFlink API doc,FLINK-30169,13505526,13476740,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxb,hxb,hxb,23/Nov/22 12:30,14/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,API / Python,Documentation,,,0,pull-request-available,stale-assigned,,Adds version switcher in PyFlink API doc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:04 UTC 2023,,,,,,,,,,"0|z1cnq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink Deserialization Error with Object Array,FLINK-30168,13505522,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,yunfengzhou,yunfengzhou,23/Nov/22 11:52,16/Jan/23 01:43,04/Jun/24 20:41,16/Jan/23 01:43,1.15.2,1.16.0,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,0,pull-request-available,,,"When it is attempted to collect object array records from a DataStream in PyFlink, an exception like follows would be thrown
{code:java}
data = 0, field_type = DenseVectorTypeInfo
def pickled_bytes_to_python_converter(data, field_type):if isinstance(field_type, RowTypeInfo):
row_kind = RowKind(int.from_bytes(data[0], 'little'))
data = zip(list(data[1:]), field_type.get_field_types())
fields = []for d, d_type in data:
fields.append(pickled_bytes_to_python_converter(d, d_type))
row = Row.of_kind(row_kind, *fields)return rowelse:
> data = pickle.loads(data)
E TypeError: a bytes-like object is required, not 'int'{code}
I found that this error is invoked because PyFlink deals with object arrays differently on Java side and Python side. 

 

On Java side (org.apache.flink.api.common.python.PythonBridgeUtils.getPickledBytesFromJavaObject)
{code:java}
...
else if (dataType instanceof BasicArrayTypeInfo || dataType instanceof PrimitiveArrayTypeInfo) {
# recursively deal with array elements
} ...
else {
# ObjectArrayTypeInfo is here
TypeSerializer serializer = dataType.createSerializer(null); ByteArrayOutputStreamWithPos baos = new ByteArrayOutputStreamWithPos(); DataOutputViewStreamWrapper baosWrapper = new DataOutputViewStreamWrapper(baos); serializer.serialize(obj, baosWrapper); return pickler.dumps(baos.toByteArray());
}
{code}
 

On python side(pyflink.datastream.utils.pickled_bytes_to_python_converter)
{code:java}
...
elif isinstance(field_type,
(BasicArrayTypeInfo, PrimitiveArrayTypeInfo, ObjectArrayTypeInfo)):
  element_type = field_type._element_type
  elements = []
  for element_bytes in data:
    elements.append(pickled_bytes_to_python_converter(element_bytes, element_type))
  return elements{code}
 

Thus a possible fix for this bug is to align PyFlink's behavior on Java side and Python side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 01:43:05 UTC 2023,,,,,,,,,,"0|z1cnp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 03:53;yunfengzhou;Apart from the proposed solution mentioned in the Description section, PyFlink needs to modify its behavior against None values as well. For example, the following code in PythonBridgeUtils.getPickledBytesFromJavaObject
{code:java}
if (obj == null) {
    return new byte[0];
} else {
{code}
might be modified into
{code:java}
if (obj == null) {
    return pickler.dumps(null);
} else {
{code}
 ;;;","11/Jan/23 05:26;dianfu;[~yunfengzhou] Could you share some code snippets which could reproduce the above issues?;;;","12/Jan/23 01:39;yunfengzhou;Thanks for the reminding. Here are the code snippets that could be used to reproduce the above issues. I reproduced the errors with these codes in Flink 1.15.

 
{code:java}
import unittest

from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment


class PyFlinkTest(unittest.TestCase):
    def setUp(self) -> None:
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.t_env = StreamTableEnvironment.create(self.env)

    def test_object_array(self):
        double_arrays = [
            ([0.0, 0.0],),
            ([0.0, 1.0],),
        ]

        input_table = self.t_env.from_data_stream(
            self.env.from_collection(
                double_arrays,
                type_info=Types.ROW_NAMED(
                    ['f0'],
                    [Types.OBJECT_ARRAY(Types.DOUBLE())]
                )
            )
        )

        input_table.print_schema()

        print([x for x in self.t_env.to_data_stream(input_table).execute_and_collect()])

    def test_none(self):
        string_array_with_none = [
            ([""test"", ""test""], ),
            ([None, ], )
        ]

        input_table = self.t_env.from_data_stream(
            self.env.from_collection(
                string_array_with_none,
                type_info=Types.ROW_NAMED(
                    ['f0', ],
                    [Types.OBJECT_ARRAY(Types.STRING()), ]
                )
            )
        )

        input_table.print_schema()

        print([x for x in self.t_env.to_data_stream(input_table).execute_and_collect()])

 {code};;;","12/Jan/23 02:08;dianfu;[~yunfengzhou] Thanks a lot (y);;;","16/Jan/23 01:43;dianfu;Fixed in:
- master via 46757739cf50c1e7b7305a4bc9cf779bb1945a1f
- release-1.16 via 46c91ed4bc22e2de3a662d52b11ade8ed64dba0b
- release-1.15 via cdecc21cad9f78b1555a0e2f5d7f1398949e7193;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade japicmp to 1.17.1,FLINK-30167,13505519,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Nov/22 11:36,23/Nov/22 15:31,04/Jun/24 20:41,23/Nov/22 15:31,,,,,,1.17.0,,,,,Build System,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 15:31:08 UTC 2022,,,,,,,,,,"0|z1cnog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 15:31;chesnay;master: 4722674329127f71c2ad50d2cf43b37f133f2068;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor tests that use the deprecated StreamingFileSink instead of FileSink,FLINK-30166,13505507,,Technical Debt,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,martijnvisser,martijnvisser,martijnvisser,23/Nov/22 10:52,11/Mar/24 13:14,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Connectors / FileSystem,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,FLINK-28641,,,,,,,,,,,,,,,,,,,,FLINK-30605,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:04 UTC 2023,,,,,,,,,,"0|z1cnls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 08:35;martijnvisser;[FLINK-30166][AVRO] Refactor deprecated StreamingFileSink usage with target FileSink: 24a133c7bda0b07bbae166db6e92435117071f84

[FLINK-30166][Tests] Remove no longer necessary test: 21c44c0863b52090a49da46ed228d3de18799506

[FLINK-30166][Tests] Remove StreamingFileSink as option for tests: 440a275cad1092be7245f0438b7e5b5b1691f708

[FLINK-30166][Parquet] Refactor deprecated StreamingFileSink usage with target FileSink: 3ef68ae2d3b0be58d9c015a260c668a3c4723d97

[FLINK-30166][SQL E2E Test] Refactor deprecated StreamingFileSink usage with target FileSink: 88c450b458e2f53f132d7f51cfd7771cc54fc072

[FLINK-30166][Hadoop Sequence Format] Refactor deprecated StreamingFileSink usage with target FileSink: 1a7a83b9639a07a7dd1dd324f669fb04a522c3e1

[FLINK-30166][Hadoop Compress] Refactor deprecated StreamingFileSink usage with target FileSink: cd54dfef4fc2c6f97f6e1ff3c0580a47641158a0

[FLINK-30166][ORC] Refactor deprecated StreamingFileSink usage with target FileSink: 1ac202dd8c21a9362611f45b6de2773699790acf;;;","09/Jan/23 12:35;mapohl;I reopen this issue after reverting the changes of this issue in 974f884021ea0587ffa029b997a76487e9911a36. They caused build failures (see FLINK-30605) on {{{}master{}}}.;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate unaligned checkpoint related tests under flink-runtime module to junit5,FLINK-30165,13505499,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,23/Nov/22 09:41,10/Dec/22 04:06,04/Jun/24 20:41,09/Dec/22 16:09,,,,,,1.17.0,,,,,Runtime / Checkpointing,Tests,,,0,pull-request-available,,,Migrate unaligned checkpoint related tests under flink-runtime module to junit5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 09 16:09:20 UTC 2022,,,,,,,,,,"0|z1cnk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 16:09;mapohl;master: 5f924bc84227a3a6c67b44e82c45fe444393f577;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose BucketComputer from SupportsWrite,FLINK-30164,13505497,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Nov/22 09:34,28/Nov/22 02:23,04/Jun/24 20:41,28/Nov/22 02:22,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"When other engines dock with Sink, they need to know the corresponding bucket rules before they can be correctly distributed to each bucket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 02:23:04 UTC 2022,,,,,,,,,,"0|z1cnjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 02:23;TsReaper;master: d8eb796f035f35e1ac85ff3f657452dd2a41e644;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamPandasUDFITTests.test_basic_functionality failed,FLINK-30163,13505492,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,23/Nov/22 09:12,14/Aug/23 14:45,04/Jun/24 20:41,14/Aug/23 14:45,1.15.2,,,,,,,,,,API / Python,,,,0,stale-major,test-stability,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42956&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=26331] failed due to {{StreamPandasUDFITTests.test_basic_functionality}}
{code:java}
ov 09 01:49:14 _______________ StreamPandasUDFITTests.test_basic_functionality ________________
Nov 09 01:49:14 
Nov 09 01:49:14 self = <pyflink.table.tests.test_pandas_udf.StreamPandasUDFITTests testMethod=test_basic_functionality>
Nov 09 01:49:14 
Nov 09 01:49:14     def test_basic_functionality(self):
Nov 09 01:49:14         # pandas UDF
Nov 09 01:49:14         add_one = udf(lambda i: i + 1, result_type=DataTypes.BIGINT(), func_type=""pandas"")
Nov 09 01:49:14     
Nov 09 01:49:14         # general Python UDF
Nov 09 01:49:14         subtract_one = udf(SubtractOne(), DataTypes.BIGINT(), DataTypes.BIGINT())
Nov 09 01:49:14     
Nov 09 01:49:14         table_sink = source_sink_utils.TestAppendSink(
Nov 09 01:49:14             ['a', 'b', 'c', 'd'],
Nov 09 01:49:14             [DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
Nov 09 01:49:14         self.t_env.register_table_sink(""Results"", table_sink)
Nov 09 01:49:14     
Nov 09 01:49:14         t = self.t_env.from_elements([(1, 2, 3), (2, 5, 6), (3, 1, 9)], ['a', 'b', 'c'])
Nov 09 01:49:14         t.where(add_one(t.b) <= 3) \
Nov 09 01:49:14             .select(t.a, t.b + 1, add(t.a + 1, subtract_one(t.c)) + 2, add(add_one(t.a), 1)) \
Nov 09 01:49:14 >           .execute_insert(""Results"") \
Nov 09 01:49:14             .wait()
 {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 14:45:42 UTC 2023,,,,,,,,,,"0|z1cnig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 10:44;mapohl;There's a {{NullPointerException}} that is triggered from within Calcite code:
{code}
Nov 09 01:49:14 E                   : java.lang.NullPointerException
Nov 09 01:49:14 E                   	at java.util.Objects.requireNonNull(Objects.java:203)
Nov 09 01:49:14 E                   	at org.apache.calcite.rel.metadata.RelMetadataQuery.<init>(RelMetadataQuery.java:144)
Nov 09 01:49:14 E                   	at org.apache.calcite.rel.metadata.RelMetadataQuery.<init>(RelMetadataQuery.java:108)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.<init>(FlinkRelMetadataQuery.java:78)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:59)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)
Nov 09 01:49:14 E                   	at org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)
Nov 09 01:49:14 E                   	at org.apache.calcite.rel.logical.LogicalFilter.create(LogicalFilter.java:108)
Nov 09 01:49:14 E                   	at org.apache.calcite.rel.core.RelFactories$FilterFactoryImpl.createFilter(RelFactories.java:344)
Nov 09 01:49:14 E                   	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:1250)
Nov 09 01:49:14 E                   	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:1208)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:276)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:158)
Nov 09 01:49:14 E                   	at org.apache.flink.table.operations.FilterQueryOperation.accept(FilterQueryOperation.java:67)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:155)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135)
Nov 09 01:49:14 E                   	at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:72)
Nov 09 01:49:14 E                   	at org.apache.flink.table.operations.FilterQueryOperation.accept(FilterQueryOperation.java:67)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter.lambda$defaultMethod$0(QueryOperationConverter.java:154)
Nov 09 01:49:14 E                   	at java.util.Collections$SingletonList.forEach(Collections.java:4824)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:154)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135)
Nov 09 01:49:14 E                   	at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:47)
Nov 09 01:49:14 E                   	at org.apache.flink.table.operations.ProjectQueryOperation.accept(ProjectQueryOperation.java:76)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.java:258)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:225)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:185)
Nov 09 01:49:14 E                   	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
Nov 09 01:49:14 E                   	at scala.collection.Iterator.foreach(Iterator.scala:937)
Nov 09 01:49:14 E                   	at scala.collection.Iterator.foreach$(Iterator.scala:937)
Nov 09 01:49:14 E                   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
Nov 09 01:49:14 E                   	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
Nov 09 01:49:14 E                   	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
Nov 09 01:49:14 E                   	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
Nov 09 01:49:14 E                   	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
Nov 09 01:49:14 E                   	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
Nov 09 01:49:14 E                   	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
Nov 09 01:49:14 E                   	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
Nov 09 01:49:14 E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
Nov 09 01:49:14 E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782)
Nov 09 01:49:14 E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861)
Nov 09 01:49:14 E                   	at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56)
Nov 09 01:49:14 E                   	at org.apache.flink.table.api.Table.executeInsert(Table.java:1470)
Nov 09 01:49:14 E                   	at sun.reflect.GeneratedMethodAccessor189.invoke(Unknown Source)
Nov 09 01:49:14 E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 09 01:49:14 E                   	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 09 01:49:14 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
Nov 09 01:49:14 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
Nov 09 01:49:14 E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
Nov 09 01:49:14 E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
Nov 09 01:49:14 E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
Nov 09 01:49:14 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
Nov 09 01:49:14 E                   	at java.lang.Thread.run(Thread.java:748)
{code}
[~snuyanzin] can you grasp based on the stacktrace whether we could close this Jira issue because the problem doesn't exist(, anymore)?;;;","14/Aug/23 13:30;Sergey Nuyanzin;I had a look through the jira and was not able to find anything like that, also tried to do it with ML search... There are only a couple of old issues closed much earlier this one;;;","14/Aug/23 14:45;mapohl;Ok, thanks. Let's close it then since it hasn't reappeared for awhile and all the Calcite updates that happened in between.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused arguments/methods/classes,FLINK-30162,13505486,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Nov/22 08:53,23/Nov/22 15:30,04/Jun/24 20:41,23/Nov/22 15:30,,,,,,elasticsearch-3.0.1,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 15:30:37 UTC 2022,,,,,,,,,,"0|z1cnh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 15:30;chesnay;main: b525081534bdaa0a95201e5e9e2e591f3a0a1275
v3.0: 4aff74e83860361e59f896279d571866f1fd5eb9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add TableSchema validation before it is commited,FLINK-30161,13505466,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,23/Nov/22 06:26,24/Nov/22 03:15,04/Jun/24 20:41,24/Nov/22 03:15,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"When creating or altering a table, there maybe some configuration or ddl conflicts, we need to check them before committing the table schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 03:15:56 UTC 2022,,,,,,,,,,"0|z1cnco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 03:15;lzljs3620320;master: 2e06ae5c81cf14f65ff7bb3619bed4a528fe8e68;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for FValueTest,FLINK-30160,13505451,13485850,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,23/Nov/22 02:47,06/Jan/23 06:05,04/Jun/24 20:41,06/Jan/23 06:05,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Add Transformer for FValueTest.

Its function would be at least equivalent to Spark's org.apache.spark.ml.stat.FValueTest. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-23 02:47:20.0,,,,,,,,,,"0|z1cn9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for ANOVATest,FLINK-30159,13505450,13485850,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,23/Nov/22 02:46,06/Jan/23 06:06,04/Jun/24 20:41,29/Nov/22 08:05,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Add Transformer for ANOVATest.

Its function would be at least equivalent to Spark's org.apache.spark.ml.stat.ANOVATest. The relevant PR should contain the following components:
 * Java implementation/test (Must include)
 * Python implementation/test ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 08:04:47 UTC 2022,,,,,,,,,,"0|z1cn94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 08:04;Jiang Xin;master: d6a5b42aae2a1a84789dfe0a35c7e97461b5d418;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink SQL][Protobuf] NullPointerException when querying Kafka topic using repeated or map attributes,FLINK-30158,13505438,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jamesmcguirepro,jamesmcguirepro,22/Nov/22 23:46,01/Dec/22 12:52,04/Jun/24 20:41,,1.16.0,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,0,,,,"I am encountering a {{java.lang.NullPointerException}} exception when trying to use Flink SQL to query a kafka topic that uses either {{repeated}} and/or {{map}} attributes.

 

{*}{*}{*}Replication{*} *steps*
 # Use a protobuf definition that either uses repeated and/or map.  This protobuf schema should cover a few of the problematic scenarios I ran into:

 
{code:java}
syntax = ""proto3"";
package example.message;


option java_package = ""com.example.message"";
option java_multiple_files = true;

message NestedType {
  int64 nested_first = 1;
  oneof nested_second {
    int64 one_of_first = 2;
    string one_of_second = 3;
  }
}

message Test {
  repeated int64 first = 1;
  map<string, NestedType> second = 2;
} {code}
2. Attempt query on topic, even excluding problematic columns:

 
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.formats.protobuf.PbCodegenException: java.lang.NullPointerException{code}
 

 

log file:

 
{code:java}
2022-11-22 15:33:59,510 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.org.apache.flink.table.client.gateway.SqlExecutionException: Error while retrieving result.    at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:79) ~[flink-sql-client-1.16.0.jar:1.16.0]Caused by: java.lang.RuntimeException: Failed to fetch next result    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222) ~[?:?]    at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:75) ~[flink-sql-client-1.16.0.jar:1.16.0]Caused by: java.io.IOException: Failed to fetch job execution result    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222) ~[?:?]    at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:75) ~[flink-sql-client-1.16.0.jar:1.16.0]Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: bc869097009a92d0601add881a6b920c)    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) ~[?:?]    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222) ~[?:?]    at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:75) ~[flink-sql-client-1.16.0.jar:1.16.0]Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: bc869097009a92d0601add881a6b920c)    at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]    at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]    at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$31(RestClusterClient.java:772) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]    at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610) ~[?:?]    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085) ~[?:?]    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]    at java.lang.Thread.run(Thread.java:829) ~[?:?]Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]    at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]    at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$31(RestClusterClient.java:772) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073) ~[?:?]    at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:301) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]    at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610) ~[?:?]    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085) ~[?:?]    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]    at java.lang.Thread.run(Thread.java:829) ~[?:?]Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477) ~[flink-dist-1.16.0.jar:1.16.0]    at jdk.internal.reflect.GeneratedMethodAccessor79.invoke(Unknown Source) ~[?:?]    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]    at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-scala_2.12-1.16.0.jar:1.16.0]    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-scala_2.12-1.16.0.jar:1.16.0]    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-scala_2.12-1.16.0.jar:1.16.0]    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-scala_2.12-1.16.0.jar:1.16.0]    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-scala_2.12-1.16.0.jar:1.16.0]    at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]    at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]    at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]    at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]    at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) ~[?:?]    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) ~[?:?]    at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) ~[?:?]    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) ~[?:?]    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183) ~[?:?]Caused by: org.apache.flink.formats.protobuf.PbCodegenException: java.lang.NullPointerException    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:126) ~[flink-sql-protobuf-1.16.0.jar:1.16.0]    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.open(PbRowDataDeserializationSchema.java:64) ~[flink-sql-protobuf-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.open(DynamicKafkaDeserializationSchema.java:94) ~[?:?]    at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.open(KafkaDeserializationSchemaWrapper.java:47) ~[?:?]    at org.apache.flink.connector.kafka.source.KafkaSource.createReader(KafkaSource.java:144) ~[?:?]    at org.apache.flink.connector.kafka.source.KafkaSource.createReader(KafkaSource.java:135) ~[?:?]    at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:286) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:94) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:692) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[flink-dist-1.16.0.jar:1.16.0]    at java.lang.Thread.run(Thread.java:829) ~[?:?]Caused by: java.lang.NullPointerException    at org.apache.flink.formats.protobuf.deserialize.PbCodegenRowDeserializer.pbGetMessageElementCode(PbCodegenRowDeserializer.java:106) ~[flink-sql-protobuf-1.16.0.jar:1.16.0]    at org.apache.flink.formats.protobuf.deserialize.PbCodegenRowDeserializer.codegen(PbCodegenRowDeserializer.java:84) ~[flink-sql-protobuf-1.16.0.jar:1.16.0]    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:109) ~[flink-sql-protobuf-1.16.0.jar:1.16.0]    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.open(PbRowDataDeserializationSchema.java:64) ~[flink-sql-protobuf-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.open(DynamicKafkaDeserializationSchema.java:94) ~[?:?]    at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.open(KafkaDeserializationSchemaWrapper.java:47) ~[?:?]    at org.apache.flink.connector.kafka.source.KafkaSource.createReader(KafkaSource.java:144) ~[?:?]    at org.apache.flink.connector.kafka.source.KafkaSource.createReader(KafkaSource.java:135) ~[?:?]    at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:286) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:94) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:692) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[flink-dist-1.16.0.jar:1.16.0]    at java.lang.Thread.run(Thread.java:829) ~[?:?]{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 12:52:23 UTC 2022,,,,,,,,,,"0|z1cn6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 01:42;luoyuxia;I have no much knowledge about protobuf format. But I had a quick overview about the related code.
{code:java}
FieldDescriptor elementFd = descriptor.findFieldByName(fieldName);
....

String pbGetMessageElementCode =
        pbGetMessageElementCode(
                pbMessageVar,
                strongCamelFieldName,
                elementFd,
                PbFormatUtils.isArrayType(subType)); // NPE is thrown in calling this method 


{code}
I think the reason is that the `elementFd` we got is null, so NPE happens.

So, how do you set the option `message-class-name` for Pb format? Does it contains all fieldNames defined in Flink SQL.;;;","23/Nov/22 10:32;martijnvisser;[~maosuhan] Could you also take a look at this issue?;;;","23/Nov/22 17:06;jamesmcguirepro;[~luoyuxia] ,

{{message-class-name }}is set to {{com.example.message.Test}}

If I remove the {{repeated}} and {{map}} fields from the table, the query will complete successfully.  I am also able to use {{{}kcat{}}}+{{{}protoc{}}} to confirm protobuf is able to deserialize the topic. See https://issues.apache.org/jira/browse/FLINK-30093 for further details on how I did that.;;;","26/Nov/22 14:41;libenchao;[~jamesmcguirepro] Currently Protobuf format requires users to provide the schema (Flink schema), and make sure it is aligned with the actual schema (protobuf schema). Could you provide us the full DDL you used for this case?;;;","28/Nov/22 07:02;tonyzhu;[~jamesmcguirepro]  could you provide more info? I'd like to take a look the detail.;;;","28/Nov/22 18:30;jamesmcguirepro;Sorry about that, I meant to provide the Flink schema!

It would look something like this for the above protobuf schema:

 
{code:java}
CREATE TABLE TestMessages (
  first array<BIGINT>,
  second array<row<nested_first BIGINT, one_of_first BIGINT, one_of_second STRING>>
)
COMMENT ''
WITH (
  'connector' = 'kafka',
  'format' = 'protobuf',
  'protobuf.message-class-name' = 'com.example.message.Test',
  'properties.auto.offset.reset' = 'earliest',
  'properties.bootstrap.servers' = 'host.docker.internal:9092',
  'properties.group.id' = 'drawings-1',
  'topic' = 'development.integration_events.connect'
);{code}
 

 ;;;","29/Nov/22 00:56;libenchao;[~jamesmcguirepro] The schema for 'second' seems not correct. It should be:

{code:sql}
CREATE TABLE TestMessages (
  first array<BIGINT>,
  second map<VARCHAR, row<nested_first BIGINT, nested_second row<one_of_first BIGINT, one_of_second STRING>>>
)
COMMENT ''
WITH (
  'connector' = 'kafka',
  'format' = 'protobuf',
  'protobuf.message-class-name' = 'com.example.message.Test',
  'properties.auto.offset.reset' = 'earliest',
  'properties.bootstrap.servers' = 'host.docker.internal:9092',
  'properties.group.id' = 'drawings-1',
  'topic' = 'development.integration_events.connect'
);
{code}
;;;","01/Dec/22 00:06;jamesmcguirepro;You are correct, I was trying to recreate the schema from memory.

 

Even if I commented out the second column in the table schema, I still got the same NullPointerException;;;","01/Dec/22 12:52;libenchao;bq. Even if I commented out the second column in the table schema, I still got the same NullPointerException

I'm still confused about current situation. If you only use
{code:sql}
CREATE TABLE TestMessages (
  `first` array<BIGINT>
) ...
{code}
I believe this is a very common case and [tested well|https://github.com/apache/flink/blob/c65591d4109f39dfa6a5b5f945c46f97dc5d967c/flink-formats/flink-protobuf/src/test/proto/test_repeated.proto#L26].

Could you give us a minimal reproducible test, including the following things:
1. the full DDL and query
2. the jar which includes your compiled protobuf classes
3. the version of protoc you are using
4. the protobuf schema;;;",,,,,,,,,,,,,,,,,,,,,,
Trigger Events Before JM Recovery and Unhealthy Job Restarts,FLINK-30157,13505431,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,22/Nov/22 22:20,24/Nov/22 01:01,04/Jun/24 20:41,23/Nov/22 17:57,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"We should emit specific events for the following cases:
 * JM recovery
 * Unhealthy Job Restarts",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 17:57:56 UTC 2022,,,,,,,,,,"0|z1cn4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 17:57;morhidi;67fc9d0c4567cbaf94f1af8c90761325dfc501ad merged to main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-242] Blogpost about the customisable RateLimitingStrategy in the AsyncSinkBase,FLINK-30156,13505401,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,22/Nov/22 17:23,25/Nov/22 10:58,04/Jun/24 20:41,25/Nov/22 09:45,,,,,,,,,,,,,,,0,pull-request-available,,,Create a blogpost to explain the customisability of the RateLimitingStrategy in the AsyncSinkBase. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 08:33:19 UTC 2022,,,,,,,,,,"0|z1cmy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 08:33;rmetzger;Merged in https://github.com/apache/flink-web/commit/0e4b8a20ac88202c8ff5c671014d4c464c86ec8d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pretty print MutatedConfigurationException,FLINK-30155,13505394,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,22/Nov/22 16:58,23/Nov/22 09:17,04/Jun/24 20:41,23/Nov/22 09:17,1.17.0,,,,,1.17.0,,,,,API / DataStream,,,,0,pull-request-available,,,"Currently MutatedConfigurationException is printed as:

{noformat}
    org.apache.flink.client.program.MutatedConfigurationException: Configuration execution.sorted-inputs.enabled:true not allowed.
    Configuration execution.runtime-mode was changed from STREAMING to BATCH.
    Configuration execution.checkpointing.interval:500 ms not allowed in the configuration object CheckpointConfig.
    Configuration execution.checkpointing.mode:EXACTLY_ONCE not allowed in the configuration object CheckpointConfig.
    Configuration pipeline.max-parallelism:1024 not allowed in the configuration object ExecutionConfig.
    Configuration parallelism.default:25 not allowed in the configuration object ExecutionConfig.
            at org.apache.flink.client.program.StreamContextEnvironment.checkNotAllowedConfigurations(StreamContextEnvironment.java:235)
            at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:175)
            at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:115)
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2049)
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2027)
{noformat}
Which is slightly confusing. First not allowed configuration is listed in the same line as the exception name, which (especially if wrapped) can make it more difficult than necessary for user to understand that this is a list of violations. I'm proposing to change it to:
{noformat}
    org.apache.flink.client.program.MutatedConfigurationException: Not allowed configuration change(s) were detected:
     - Configuration execution.sorted-inputs.enabled:true not allowed.
     - Configuration execution.runtime-mode was changed from STREAMING to BATCH.
     - Configuration execution.checkpointing.interval:500 ms not allowed in the configuration object CheckpointConfig.
     - Configuration execution.checkpointing.mode:EXACTLY_ONCE not allowed in the configuration object CheckpointConfig.
     - Configuration pipeline.max-parallelism:1024 not allowed in the configuration object ExecutionConfig.
     - Configuration parallelism.default:25 not allowed in the configuration object ExecutionConfig.
            at org.apache.flink.client.program.StreamContextEnvironment.checkNotAllowedConfigurations(StreamContextEnvironment.java:235)
            at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:175)
            at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:115)
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2049)
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2027)
{noformat} 
To make it more clear.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29888,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 09:17:20 UTC 2022,,,,,,,,,,"0|z1cmwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 09:17;pnowojski;merged commit 011819c into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flinkConfiguration in SessionJob is not accepted,FLINK-30154,13505353,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gsomogyi,eleroy,eleroy,22/Nov/22 13:42,06/Dec/22 13:45,04/Jun/24 20:41,06/Dec/22 13:45,kubernetes-operator-1.2.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"when declaring extra config in the FlinkSessionJob like in :

```yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: flink-state
  namespace: flink
spec:
  deploymentName: flink-session
  flinkConfiguration:
    kubernetes.operator.periodic.savepoint.interval: 30m
    kubernetes.operator.savepoint.history.max.age: 24h
    kubernetes.operator.savepoint.history.max.count: ""25""
  job:
    jarURI: 
[https://myendpoint/myjar.jar]

    parallelism: 2
    entryClass: com.example.MyClass
    upgradeMode: savepoint
Getting the error:
Invalid session job flinkConfiguration key: kubernetes.operator.periodic.savepoint.interval. Allowed keys are [kubernetes.operator.user.artifacts.http.header]
 ","kubernetes operator 1.2.0

kubernetes v1.24.1

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 13:45:54 UTC 2022,,,,,,,,,,"0|z1cmnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 13:45;mbalassi; [{{2c207ed}}|https://github.com/apache/flink-kubernetes-operator/commit/2c207edf5915f077b92c5b2bc2f0e6cac54daa9d] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase fails with NoClassDefFoundError  org/jboss/netty/channel/ExceptionEvent,FLINK-30153,13505315,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,22/Nov/22 10:58,13/Jun/23 21:33,04/Jun/24 20:41,13/Jun/23 21:33,1.15.2,1.16.3,,,,,,,,,Connectors / Hadoop Compatibility,,,,0,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43215&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199] failed due to {{{}NoClassDefFoundError{}}}:
{code:java}
Nov 16 05:35:52 2022-11-16 05:32:43,285 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-8' produced an uncaught exception. Stopping the process...
Nov 16 05:35:52 java.lang.NoClassDefFoundError: org/jboss/netty/channel/ExceptionEvent
Nov 16 05:35:52 	at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:640) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:599) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at org.jboss.netty.channel.Channels.disconnect(Channels.java:781) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at org.jboss.netty.channel.AbstractChannel.disconnect(AbstractChannel.java:219) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at akka.remote.transport.netty.NettyTransport$.$anonfun$gracefulClose$1(NettyTransport.scala:314) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at akka.remote.transport.netty.NettyTransport$.$anonfun$gracefulClose$1$adapted(NettyTransport.scala:313) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at scala.util.Success.foreach(Try.scala:249) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at scala.concurrent.Future.$anonfun$foreach$1$adapted(Future.scala:225) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) ~[flink-rpc-akka_84c96a75-ef18-4118-b435-fae30528a9e4.jar:1.15-SNAPSHOT]
Nov 16 05:35:52 ] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26037,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 15:23:32 UTC 2023,,,,,,,,,,"0|z1cmf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 14:36;mapohl;This issue might be caused by the same bug that caused the {{NoClassDefFoundErrors}} in FLINK-26037 because the error also happens after the shutdown is triggered. I'm leaving this issue open for now to see whether we run into such a test failure again.;;;","06/Jun/23 15:23;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49533&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=8566;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure in setting up the CI agent,FLINK-30152,13505314,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,22/Nov/22 10:54,02/Mar/23 15:20,04/Jun/24 20:41,24/Nov/22 06:03,1.15.2,1.16.0,1.17.0,,,,,,,,Test Infrastructure,,,,0,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43214&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&t=ba24ad14-6ea3-5ee3-c4ec-9e7cd2c9e754&l=44] failed due to an error while installing deb packages:
{code:java}
The following additional packages will be installed:
  libio-pty-perl libipc-run-perl libtime-duration-perl libtimedate-perl
The following NEW packages will be installed:
  libio-pty-perl libipc-run-perl libtime-duration-perl libtimedate-perl
  moreutils
0 upgraded, 5 newly installed, 0 to remove and 29 not upgraded.
Need to get 230 kB of archives.
After this operation, 771 kB of additional disk space will be used.
Err:1 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libio-pty-perl amd64 1:1.12-1
  Could not connect to azure.archive.ubuntu.com:80 (20.54.144.51), connection timed out
Err:2 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libipc-run-perl all 20180523.0-2
  Unable to connect to azure.archive.ubuntu.com:http:
Err:3 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 libtime-duration-perl all 1.21-1
  Unable to connect to azure.archive.ubuntu.com:http:
Err:4 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libtimedate-perl all 2.3200-1
  Unable to connect to azure.archive.ubuntu.com:http:
Err:5 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 moreutils amd64 0.63-1
  Unable to connect to azure.archive.ubuntu.com:http:
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.12-1_amd64.deb  Could not connect to azure.archive.ubuntu.com:80 (20.54.144.51), connection timed out
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_20180523.0-2_all.deb  Unable to connect to azure.archive.ubuntu.com:http:
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/libt/libtime-duration-perl/libtime-duration-perl_1.21-1_all.deb  Unable to connect to azure.archive.ubuntu.com:http:
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libt/libtimedate-perl/libtimedate-perl_2.3200-1_all.deb  Unable to connect to azure.archive.ubuntu.com:http:
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.63-1_amd64.deb  Unable to connect to azure.archive.ubuntu.com:http:
E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
 {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30921,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 10:23:59 UTC 2022,,,,,,,,,,"0|z1cmew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 11:07;martijnvisser;This mostly occurs when there's a connection issue at Azure;;;","22/Nov/22 11:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43216&view=logs&j=b31992a1-93b0-59f3-2c17-4a9deb43d11c&t=3a444376-fbe9-578d-2efd-f711b2558a25&l=44;;;","22/Nov/22 11:11;mapohl;yeah, I created the ticket for documentation purposes. It's quite likely a temporary issue. Both build failures happened on Nov 16, 2022 in the morning (~0:43am UTC & ~3:43am UTC);;;","22/Nov/22 11:19;martijnvisser;I'm suspecting it was https://status.dev.azure.com/_event/349325591 and/or https://status.dev.azure.com/_event/349377709;;;","24/Nov/22 06:03;mapohl;Could be. Thanks for looking into it. I'm going to close the issue again since we didn't see any similar issues anymore.;;;","02/Dec/22 10:23;mapohl;Adding this one for documentation purposes: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43214&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&t=ba24ad14-6ea3-5ee3-c4ec-9e7cd2c9e754&l=44;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: AuditUtils,FLINK-30151,13505312,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:50,06/Dec/22 12:49,04/Jun/24 20:41,06/Dec/22 12:49,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 12:49:42 UTC 2022,,,,,,,,,,"0|z1cmeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 12:49;mbalassi;e04797321f398e12f16cd0c6a7059b5037863bdf in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: REST service in session cluster is bad now,FLINK-30150,13505311,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:50,15/Dec/22 15:14,04/Jun/24 20:41,15/Dec/22 15:14,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 15:14:04 UTC 2022,,,,,,,,,,"0|z1cme8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 12:08;pvary;This is the exception in the logs:
{code:java}
2022-12-05T11:40:59.2665289Z [m[33m2022-12-05 11:40:26,746[m [36mo.a.f.k.o.o.d.SessionObserver [m [1;31m[ERROR][default/session-cluster-1] REST service in session cluster is bad now
2022-12-05T11:40:59.2665851Z java.util.concurrent.TimeoutException
2022-12-05T11:40:59.2666258Z 	at java.base/java.util.concurrent.CompletableFuture.timedGet(Unknown Source)
2022-12-05T11:40:59.2666841Z 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
2022-12-05T11:40:59.2667549Z 	at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.listJobs(AbstractFlinkService.java:231)
2022-12-05T11:40:59.2668462Z 	at org.apache.flink.kubernetes.operator.observer.deployment.SessionObserver.observeFlinkCluster(SessionObserver.java:48)
2022-12-05T11:40:59.2669809Z 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:89)
2022-12-05T11:40:59.2671385Z 	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:55)
2022-12-05T11:40:59.2672514Z 	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:56)
2022-12-05T11:40:59.2673507Z 	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:32)
2022-12-05T11:40:59.2674466Z 	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:113)
2022-12-05T11:40:59.2675692Z 	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
2022-12-05T11:40:59.2676509Z 	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
2022-12-05T11:40:59.2677043Z 	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
2022-12-05T11:40:59.2677741Z 	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
2022-12-05T11:40:59.2678451Z 	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
2022-12-05T11:40:59.2679180Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
2022-12-05T11:40:59.2680055Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
2022-12-05T11:40:59.2681621Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
2022-12-05T11:40:59.2682478Z 	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
2022-12-05T11:40:59.2683241Z 	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
2022-12-05T11:40:59.2683817Z 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2022-12-05T11:40:59.2684294Z 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2022-12-05T11:40:59.2684676Z 	at java.base/java.lang.Thread.run(Unknown Source) {code}
The log line show 2022-12-05 11:40:26,746 as the timestamp.

This is happening when we manually kill the job to test the recovery:
{code:java}
2022-12-05T11:40:12.8330378Z Successfully verified that sessionjob/flink-example-statemachine.status.jobStatus.state is in RUNNING state.
2022-12-05T11:40:12.9711940Z Kill the session-cluster-1-7bc5b4d7cb-t5hgq
2022-12-05T11:40:13.3083721Z Waiting for log ""Restoring job ffffffff9b85cb750000000000000001 from Checkpoint""...
2022-12-05T11:40:35.8208688Z Log ""Restoring job ffffffff9b85cb750000000000000001 from Checkpoint"" shows up. {code}
I would say that this is expected.;;;","05/Dec/22 12:14;gaborgsomogyi;cc [~morhidi] ;;;","15/Dec/22 15:14;gyfora;merged to main ec9d60113a4dda072d97e48462a059c6a3c84151;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: Error during event processing,FLINK-30149,13505310,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:50,22/Jan/24 13:51,04/Jun/24 20:41,22/Jan/24 13:51,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:51:10 UTC 2024,,,,,,,,,,"0|z1cme0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:51;gaborgsomogyi;See comment in FLINK-30310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: Failed to submit job to session cluster,FLINK-30148,13505309,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:50,22/Jan/24 13:50,04/Jun/24 20:41,22/Jan/24 13:50,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:50:40 UTC 2024,,,,,,,,,,"0|z1cmds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:50;gaborgsomogyi;See comment in FLINK-30310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: Failed to submit a listener notification task,FLINK-30147,13505308,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:49,22/Jan/24 13:50,04/Jun/24 20:41,22/Jan/24 13:50,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:50:14 UTC 2024,,,,,,,,,,"0|z1cmdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:50;gaborgsomogyi;See comment in FLINK-30310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: Exception while listing jobs,FLINK-30146,13505307,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:49,12/Dec/22 15:07,04/Jun/24 20:41,12/Dec/22 15:07,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 15:07:18 UTC 2022,,,,,,,,,,"0|z1cmdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 15:07;mbalassi;1598352 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entry: JmDeploymentStatus.ERROR.Count,FLINK-30145,13505306,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Nov/22 10:48,22/Nov/22 15:20,04/Jun/24 20:41,22/Nov/22 15:20,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 15:20:40 UTC 2022,,,,,,,,,,"0|z1cmd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 15:20;gaborgsomogyi;No action needed since this is just a metric;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Guarantee Flink ML operators function correctly with object-reuse enabled,FLINK-30144,13505290,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,22/Nov/22 09:50,10/Jan/23 05:17,04/Jun/24 20:41,23/Nov/22 06:54,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Flink ML operators are supposed to function correctly when object-reuse is enabled, as a part of Flink ML's performance improvement. Thus we need to add this configuration to Flink ML test cases and fix any possible bugs discovered along.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 09:50:17.0,,,,,,,,,,"0|z1cm9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store fails when temporary directory is a symlink,FLINK-30143,13505286,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,TsReaper,TsReaper,TsReaper,22/Nov/22 09:43,23/Nov/22 02:29,04/Jun/24 20:41,23/Nov/22 02:29,table-store-0.2.2,table-store-0.3.0,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,0,,,,"When {{java.io.tmpdir}} points to a symbolic link, the following exception will be thrown:

{code}
java.lang.ExceptionInInitializerError
	at org.apache.flink.table.store.codegen.CodeGenLoader.getInstance(CodeGenLoader.java:118)
	at org.apache.flink.table.store.codegen.CodeGenUtils.newProjection(CodeGenUtils.java:47)
	at org.apache.flink.table.store.table.sink.SinkRecordConverter.<init>(SinkRecordConverter.java:68)
	at org.apache.flink.table.store.table.sink.SinkRecordConverter.<init>(SinkRecordConverter.java:50)
	at org.apache.flink.table.store.table.sink.SinkRecordConverterTest.converter(SinkRecordConverterTest.java:99)
	at org.apache.flink.table.store.table.sink.SinkRecordConverterTest.converter(SinkRecordConverterTest.java:75)
	at org.apache.flink.table.store.table.sink.SinkRecordConverterTest.testBucket(SinkRecordConverterTest.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1255)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1255)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.RuntimeException: Could not initialize the flink-table-store-codegen loader.
	at org.apache.flink.table.store.codegen.CodeGenLoader.<init>(CodeGenLoader.java:89)
	at org.apache.flink.table.store.codegen.CodeGenLoader.<init>(CodeGenLoader.java:40)
	at org.apache.flink.table.store.codegen.CodeGenLoader$CodegenLoaderHolder.<clinit>(CodeGenLoader.java:114)
	... 74 more
Caused by: java.nio.file.FileAlreadyExistsException: /tmp/mytest
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:727)
	at org.apache.flink.table.store.codegen.CodeGenLoader.<init>(CodeGenLoader.java:73)
	... 76 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30139,,,,,,,,,,FLINK-28102,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 09:43:48.0,,,,,,,,,,"0|z1cm8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML operators lose table watermark after transform(),FLINK-30142,13505281,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yunfengzhou,yunfengzhou,22/Nov/22 09:17,24/Nov/22 11:39,04/Jun/24 20:41,,ml-2.1.0,,,,,,,,,,Library / Machine Learning,,,,0,,,,"Given FLINK-30130 and the datastream-table conversion process usually does not propagate watermarks, current Flink ML operators would lose the input table's watermark after transformation. This problem should be fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 09:17:39.0,,,,,,,,,,"0|z1cm7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MinioTestContainerTest failed due to IllegalStateException in container startup,FLINK-30141,13505278,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,mapohl,mapohl,22/Nov/22 08:53,14/Aug/23 13:14,04/Jun/24 20:41,03/Jul/23 14:01,1.17.0,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,Connectors / FileSystem,Tests,,,0,pull-request-available,test-stability,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43182&view=logs&j=a1ac4ce4-9a4f-5fdb-3290-7e163fba19dc&t=3a8f44aa-4415-5b14-37d5-5fecc568b139&l=15531] failed due to an {{IllegalStateException}} during container startup:
{code:java}
Nov 15 02:34:04 [ERROR] org.apache.flink.fs.s3.common.MinioTestContainerTest.testBucketCreation  Time elapsed: 120.874 s  <<< ERROR!
Nov 15 02:34:04 org.testcontainers.containers.ContainerLaunchException: Container startup failed
Nov 15 02:34:04 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:345)
Nov 15 02:34:04 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)
Nov 15 02:34:04 	at org.apache.flink.core.testutils.TestContainerExtension.instantiateTestContainer(TestContainerExtension.java:59)
Nov 15 02:34:04 	at org.apache.flink.core.testutils.TestContainerExtension.before(TestContainerExtension.java:70)
Nov 15 02:34:04 	at org.apache.flink.core.testutils.EachCallbackWrapper.beforeEach(EachCallbackWrapper.java:45)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeEachCallbacks$2(TestMethodTestDescriptor.java:166)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeMethodsOrCallbacksUntilExceptionOccurs$6(TestMethodTestDescriptor.java:202)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeMethodsOrCallbacksUntilExceptionOccurs(TestMethodTestDescriptor.java:202)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeEachCallbacks(TestMethodTestDescriptor.java:165)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:132)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-26402,,,,,,,,,,,,,,,FLINK-26402,FLINK-26907,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 14:01:26 UTC 2023,,,,,,,,,,"0|z1cm6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 09:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24&l=16775;;;","16/Mar/23 07:16;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47137&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","26/Apr/23 09:44;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48394&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=14049;;;","21/Jun/23 15:27;rskraba;I took a look at this and the related issue FLINK-26402 -- it looks like the *503 Service Unavailable* statuses are not rare: they occur about 1 in a couple hundred API calls to Minio on container startup.  On the other hand, the retry mechanism built into Amazon API clients _usually_ try again correctly until they succeed.  Sometimes, the Minio container doesn't move to the correct state to service API calls quickly enough, the default retry strategy fails eventually and we see the error here.

I can reproduce this pretty reliably by running a unit test somewhere between 1K-10K times.  At first I assumed it occurred when the system was loaded while running the test, but that doesn't appear to be the case.

Attempting to start up the container more than once might be the right thing to do here.  If the call to Minio fails while creating the default bucket, the container should be discarded and tried again.  This should have no overhead on the daily CI runs.;;;","03/Jul/23 14:01;mapohl;master: 972e15348c461d960b43919150480a0333e26ff8
1.17: 9344732e37469225ffc935e424fac0aac8434981
1.16: 8e6c586fc5f6596f4c8ae4ad338220b7202ea7e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
flink-runtime intergration tests fail due to Process Exit Code: 239 (because of NoClassDefFoundError akka.remote.transport.netty.NettyFutureBridge$$anon$1),FLINK-30140,13505247,13537639,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,22/Nov/22 08:25,25/May/23 12:40,04/Jun/24 20:41,25/May/23 12:40,1.16.0,1.17.0,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43124&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8177] failed with a 239 exit code in {{flink-runtime}} without any specific test being revealed:
{code:java}
Nov 14 11:40:46 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-runtime: There are test failures.
Nov 14 11:40:46 [ERROR] 
Nov 14 11:40:46 [ERROR] Please refer to /__w/2/s/flink-runtime/target/surefire-reports for the individual test results.
Nov 14 11:40:46 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Nov 14 11:40:46 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 14 11:40:46 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/2/s/flink-runtime/target/surefire/surefirebooter1852368249697011408.jar /__w/2/s/flink-runtime/target/surefire 2022-11-14T11-36-53_855-jvmRun4 surefire376251412927954214tmp surefire_22793960051744290914tmp
Nov 14 11:40:46 [ERROR] Error occurred in starting fork, check output in log
Nov 14 11:40:46 [ERROR] Process Exit Code: 239
Nov 14 11:40:46 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 14 11:40:46 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/2/s/flink-runtime/target/surefire/surefirebooter1852368249697011408.jar /__w/2/s/flink-runtime/target/surefire 2022-11-14T11-36-53_855-jvmRun4 surefire376251412927954214tmp surefire_22793960051744290914tmp
Nov 14 11:40:46 [ERROR] Error occurred in starting fork, check output in log
Nov 14 11:40:46 [ERROR] Process Exit Code: 239
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Nov 14 11:40:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Nov 14 11:40:46 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Nov 14 11:40:46 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Nov 14 11:40:46 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Nov 14 11:40:46 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
Nov 14 11:40:46 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Nov 14 11:40:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 14 11:40:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 14 11:40:46 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 14 11:40:46 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
Nov 14 11:40:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Nov 14 11:40:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Nov 14 11:40:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Nov 14 11:40:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Nov 14 11:40:46 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 14 11:40:46 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/2/s/flink-runtime/target/surefire/surefirebooter1852368249697011408.jar /__w/2/s/flink-runtime/target/surefire 2022-11-14T11-36-53_855-jvmRun4 surefire376251412927954214tmp surefire_22793960051744290914tmp
Nov 14 11:40:46 [ERROR] Error occurred in starting fork, check output in log
Nov 14 11:40:46 [ERROR] Process Exit Code: 239
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Nov 14 11:40:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Nov 14 11:40:46 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 14 11:40:46 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Nov 14 11:40:46 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Nov 14 11:40:46 [ERROR] at java.lang.Thread.run(Thread.java:748)
Nov 14 11:40:46 [ERROR] -> [Help 1]
Nov 14 11:40:46 [ERROR] 
Nov 14 11:40:46 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Nov 14 11:40:46 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Nov 14 11:40:46 [ERROR] 
Nov 14 11:40:46 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Nov 14 11:40:46 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
Nov 14 11:40:47 Process exited with EXIT CODE: 1 {code}
 The corresponding build logs do not reveal anything. I attached the logs

UPD: in logs there is
{noformat}
01:51:05,743 [flink-akka.remote.default-remote-dispatcher-14] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-14' produced an uncaught exception. Stopping 
the process...
java.lang.NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1
        at akka.remote.transport.netty.NettyFutureBridge$.apply(NettyTransport.scala:65) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.remote.transport.netty.NettyFutureBridge$$anon$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 16 more

{noformat}

also thread dump
{noformat}
01:51:05,830 [flink-akka.remote.default-remote-dispatcher-14] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - Thread dump: 
""flink-akka.actor.internal-dispatcher-24"" Id=1143 RUNNABLE
        at java.util.concurrent.ForkJoinPool$WorkQueue.growArray(ForkJoinPool.java:886)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1687)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.remote.default-remote-dispatcher-23"" Id=1141 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.remote.default-remote-dispatcher-22"" Id=1140 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.remote.default-remote-dispatcher-21"" Id=1139 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.remote.default-remote-dispatcher-20"" Id=1138 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.actor.default-dispatcher-16"" Id=1130 RUNNABLE
        at java.net.URLClassLoader.close(URLClassLoader.java:297)
        -  locked java.util.WeakHashMap@12cb0ba1
        at org.apache.flink.runtime.rpc.akka.CleanupOnCloseRpcSystem.close(CleanupOnCloseRpcSystem.java:56)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$$Lambda$2657/1030695720.run(Unknown Source)
        at org.apache.flink.util.concurrent.FutureUtils.lambda$runAfterwardsAsync$15(FutureUtils.java:594)
        at org.apache.flink.util.concurrent.FutureUtils$$Lambda$806/1864236240.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
        ...

""Flink Netty Client (0) Thread 0"" Id=1129 TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:787)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:402)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)

""Flink Netty Server (0) Thread 0"" Id=1125 TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:787)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:402)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)

""flink-akka.remote.default-remote-dispatcher-14"" Id=1090 RUNNABLE
        at sun.management.ThreadImpl.dumpThreads0(Native Method)
        at sun.management.ThreadImpl.dumpAllThreads(ThreadImpl.java:496)
        at sun.management.ThreadImpl.dumpAllThreads(ThreadImpl.java:484)
        at org.apache.flink.util.concurrent.ThreadUtils.errorLogThreadDump(ThreadUtils.java:33)
        at org.apache.flink.util.FatalExitExceptionHandler.uncaughtException(FatalExitExceptionHandler.java:47)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        ...

""flink-akka.remote.default-remote-dispatcher-6"" Id=1082 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.remote.default-remote-dispatcher-5"" Id=1081 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@58f74e6a
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""flink-akka.actor.default-dispatcher-4"" Id=1080 TIMED_WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@53883352
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@53883352
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""Time-limited test"" Id=1074 WAITING on java.util.concurrent.CompletableFuture$Signaller@31eea357
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.CompletableFuture$Signaller@31eea357
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
        at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
        at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.close(TaskManagerRunner.java:295)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.testWorkingDirIsSetupWhenStartingTaskManagerRunner(TaskManagerRunnerTest.java:226)
        ...

""ForkJoinPool.commonPool-worker-22"" Id=1060 WAITING on java.util.concurrent.ForkJoinPool@7bb220c8
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.ForkJoinPool@7bb220c8
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""IOManager reader thread #1"" Id=564 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2244da23
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2244da23
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)

""IOManager writer thread #1"" Id=563 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30446f9a
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30446f9a
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)

""pool-249-thread-1"" Id=538 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@747486de
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@747486de
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

""ForkJoinPool.commonPool-worker-29"" Id=439 WAITING on java.util.concurrent.ForkJoinPool@7bb220c8
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.ForkJoinPool@7bb220c8
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""ForkJoinPool.commonPool-worker-4"" Id=438 WAITING on java.util.concurrent.ForkJoinPool@7bb220c8
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.ForkJoinPool@7bb220c8
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""ForkJoinPool.commonPool-worker-11"" Id=437 WAITING on java.util.concurrent.ForkJoinPool@7bb220c8
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.ForkJoinPool@7bb220c8
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""Flink Netty Server (0) Thread 0"" Id=243 RUNNABLE (in native)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:193)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:186)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:290)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:347)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)

""Flink Netty Server (0) Thread 0"" Id=242 RUNNABLE (in native)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:193)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:186)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:290)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:347)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)

""ForkJoinPool.commonPool-worker-18"" Id=117 WAITING on java.util.concurrent.ForkJoinPool@7bb220c8
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.ForkJoinPool@7bb220c8
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

""ForkJoinPool.commonPool-worker-25"" Id=45 WAITING on java.lang.Object@79c876e1
        at java.lang.Object.wait(Native Method)
        -  waiting on java.lang.Object@79c876e1
        at java.lang.Object.wait(Object.java:502)
        at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
        at org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunnerTest$HaltingCheckpointRecoveryFactory.createRecoveredCompletedCheckpointStore(CheckpointResourcesCleanupRunnerTest.java:629)
        at org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunner.createCompletedCheckpointStore(CheckpointResourcesCleanupRunner.java:144)
        at org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunner.cleanupCheckpoints(CheckpointResourcesCleanupRunner.java:122)
        at org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunner.lambda$runCleanupAsync$1(CheckpointResourcesCleanupRunner.java:101)
        at org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunner$$Lambda$1087/627029889.run(Unknown Source)
        ...

""FlinkCompletableFutureDelayScheduler-thread-1"" Id=31 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e2a4cf8
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e2a4cf8
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ...

""Attach Listener"" Id=29 RUNNABLE

""surefire-forkedjvm-command-thread"" Id=25 WAITING on java.util.concurrent.CountDownLatch$Sync@5982a934
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.CountDownLatch$Sync@5982a934
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at sun.nio.ch.PendingFuture.get(PendingFuture.java:180)
        at org.apache.maven.surefire.api.util.internal.Channels$2.read(Channels.java:157)
        -  locked org.apache.maven.surefire.api.util.internal.Channels$2@1abb18dc
        ...

""pool-1-thread-2"" Id=23 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@15836370
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@15836370
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
        at sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:262)
        at sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:112)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ...

        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@2aafb23c

""pool-1-thread-1"" Id=22 RUNNABLE (in native)
        at sun.nio.ch.EPoll.epollWait(Native Method)
        at sun.nio.ch.EPollPort$EventHandlerTask.poll(EPollPort.java:194)
        at sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:268)
        at sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:112)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@34340fab

""Signal Dispatcher"" Id=5 RUNNABLE

""Finalizer"" Id=3 WAITING on java.lang.ref.ReferenceQueue$Lock@573ce4e4
        at java.lang.Object.wait(Native Method)
        -  waiting on java.lang.ref.ReferenceQueue$Lock@573ce4e4
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)

""Reference Handler"" Id=2 WAITING on java.lang.ref.Reference$Lock@67fbc138
        at java.lang.Object.wait(Native Method)
        -  waiting on java.lang.ref.Reference$Lock@67fbc138
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

""main"" Id=1 TIMED_WAITING on java.util.concurrent.FutureTask@5ac46741
        at sun.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.FutureTask@5ac46741
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:426)
        at java.util.concurrent.FutureTask.get(FutureTask.java:204)
        at org.junit.internal.runners.statements.FailOnTimeout.getResult(FailOnTimeout.java:167)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:128)
        at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        ...

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-32095,,FLINK-30299,,,,,,,,,,,,,,,,,,"22/Nov/22 08:25;mapohl;logs-ci-test_ci_finegrained_resource_management-1668425271.zip;https://issues.apache.org/jira/secure/attachment/13052964/logs-ci-test_ci_finegrained_resource_management-1668425271.zip",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 12:40:08 UTC 2023,,,,,,,,,,"0|z1cm00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 08:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43398&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8377;;;","24/Nov/22 08:19;mapohl;Closed in favor of FLINK-26037. I verified that all the builds failed due to a \{{NoClassDefFoundError}}.;;;","14/Apr/23 07:12;Sergey Nuyanzin;I reopened that issue since the reason in FLINK-26037 was {{java.lang.NoClassDefFoundError: scala/collection/immutable/Map$Map4$$anon$7}}

and based on logs from description here the reason is a bit different and moreover it still continues failing on ci

from logs
{noformat}
01:51:05,743 [flink-akka.remote.default-remote-dispatcher-14] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-14' produced an uncaught exception. Stopping 
the process...
java.lang.NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1
        at akka.remote.transport.netty.NettyFutureBridge$.apply(NettyTransport.scala:65) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_08034fee-ff0a-42c1-98f7-a4be5fc3db63.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.remote.transport.netty.NettyFutureBridge$$anon$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 16 more

{noformat};;;","14/Apr/23 07:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47933&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8743;;;","14/Apr/23 09:53;Sergey Nuyanzin;// cc [~chesnay], since your were involved in similar issue  FLINK-26037;;;","25/May/23 12:40;Sergey Nuyanzin;it looks like this is the same issue as https://issues.apache.org/jira/browse/FLINK-30299
so close it as duplicate;;;",,,,,,,,,,,,,,,,,,,,,,,,,
CodeGenLoader fails when temporary directory is a symlink,FLINK-30139,13505245,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Nov/22 08:24,23/Nov/22 02:43,04/Jun/24 20:41,23/Nov/22 02:43,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,Same to FLINK-28102 ,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30143,,,,,,,,,,,,,,,FLINK-28102,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 02:43:03 UTC 2022,,,,,,,,,,"0|z1clzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 02:43;lzljs3620320;master: ae96b335e470a068a7e87f3d1a22c58bf2d82256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobServerCleanupTest utilizes failsWithin with a quite low timeout which might result in test instabilities,FLINK-30138,13505237,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Nov/22 08:07,01/Dec/22 07:41,04/Jun/24 20:41,01/Dec/22 07:41,1.15.2,1.16.0,1.17.0,,,1.15.4,1.16.1,1.17.0,,,Runtime / Coordination,Tests,,,0,pull-request-available,starter,,"{{BlobServerCleanupTest}} utilizes {{failsWithin}} using a low timeout threshold (100ms) in several locations. This might result in test instabilities (e.g. [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43221&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8801]). We shouldn't rely on timeouts as stated in the Flink coding guidelines.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 07:41:06 UTC 2022,,,,,,,,,,"0|z1clxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43606&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8389;;;","01/Dec/22 07:41;mapohl;master: c65591d4109f39dfa6a5b5f945c46f97dc5d967c
1.16: 8f7c35ec0e434d788ea0984b8918f1be3354596b
1.15: 738c861024a534cdd92c96f5a35c1eb13a8d992f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory failed due to NoSuchFileException,FLINK-30137,13505235,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,22/Nov/22 07:54,22/Nov/22 07:55,04/Jun/24 20:41,22/Nov/22 07:55,1.15.2,1.16.0,1.17.0,,,,,,,,Test Infrastructure,Tests,,,0,test-stability,,,"{{LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory}} failed in [this build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43366&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10718] due to a {{NoSuchFileException}} during cleanup.
{code:java}
java.io.IOException: Failed to delete temp directory /tmp/junit2010448393472419340. The following paths could not be deleted (see suppressed exceptions for details): tm_taskManager_2/localState/aid_21c128b018cc61989c323cda6e36b0b1/jid_e5dbf7bc4ebb72baf20387e555083439/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_1
        at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.createIOExceptionWithAttachedFailures(TempDirectory.java:280)
        at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:188)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$closeAllStoredCloseableValues$3(ExtensionValuesStore.java:68)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.ArrayList.forEach(ArrayList.java:1259)
        at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
        at java.util.stream.Sink$ChainedReference.end(Sink.java:258)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
        at org.junit.jupiter.engine.execution.ExtensionValuesStore.closeAllStoredCloseableValues(ExtensionValuesStore.java:68)
        at org.junit.jupiter.engine.descriptor.AbstractExtensionContext.close(AbstractExtensionContext.java:77)
        at org.junit.jupiter.engine.execution.JupiterEngineExecutionContext.close(JupiterEngineExecutionContext.java:53)
        at org.junit.jupiter.engine.descriptor.JupiterTestDescriptor.cleanUp(JupiterTestDescriptor.java:222)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$cleanUp$1(TestMethodTestDescriptor.java:152)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.cleanUp(TestMethodTestDescriptor.java:152)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.cleanUp(TestMethodTestDescriptor.java:66)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$cleanUp$10(NodeTestTask.java:167)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.cleanUp(NodeTestTask.java:167)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:98)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
        Suppressed: java.nio.file.NoSuchFileException: /tmp/junit2010448393472419340/tm_taskManager_2/localState/aid_21c128b018cc61989c323cda6e36b0b1/jid_e5dbf7bc4ebb72baf20387e555083439/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_1
                at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
                at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
                at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
                at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
                at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
                at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
                at java.nio.file.Files.readAttributes(Files.java:1737)
                at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
                at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
                at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
                at java.nio.file.Files.walkFileTree(Files.java:2706)
                at java.nio.file.Files.walkFileTree(Files.java:2742)
                at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:199)
                at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:186)
                ... 51 more
                Suppressed: java.nio.file.NoSuchFileException: /tmp/junit2010448393472419340/tm_taskManager_2/localState/aid_21c128b018cc61989c323cda6e36b0b1/jid_e5dbf7bc4ebb72baf20387e555083439/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_1
                        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
                        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
                        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
                        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
                        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
                        at java.nio.file.Files.delete(Files.java:1126)
                        at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.resetPermissionsAndTryToDeleteAgain(TempDirectory.java:250)
                        at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:212)
                        at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:199)
                        at java.nio.file.Files.walkFileTree(Files.java:2672)
                        ... 54 more {code}
Wild guess: This might be related to some resource leakage based on this [SO thread|https://stackoverflow.com/a/56294611/11118339].",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30132,,,,,,,,,,,,,,,,,,"22/Nov/22 07:54;mapohl;LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory.log;https://issues.apache.org/jira/secure/attachment/13052962/LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory.log",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 07:54:25.0,,,,,,,,,,"0|z1clxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticsearchSinkITCase.testElasticsearchSink failed on azure due to ES service unavailable,FLINK-30136,13505234,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,leonard,leonard,22/Nov/22 07:54,21/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,elasticsearch-3.0.0,,,,,,,,,Connectors / ElasticSearch,,,,0,auto-deprioritized-major,test-stability,,"{noformat}
[...]
Nov 22 02:24:55 Caused by: org.elasticsearch.ElasticsearchStatusException: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://172.17.0.1:58600], URI [/], status line [HTTP/1.1 503 Service Unavailable]
Nov 22 02:24:55 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2028)
Nov 22 02:24:55 	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1778)
Nov 22 02:24:55 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1735)
Nov 22 02:24:55 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1718)
Nov 22 02:24:55 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:705)
Nov 22 02:24:55 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:140)
Nov 22 02:24:55 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:45)
Nov 22 02:24:55 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:317)
Nov 22 02:24:55 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
Nov 22 02:24:55 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
Nov 22 02:24:55 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
Nov 22 02:24:55 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
Nov 22 02:24:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
Nov 22 02:24:55 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
Nov 22 02:24:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
Nov 22 02:24:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
Nov 22 02:24:55 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
Nov 22 02:24:55 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
Nov 22 02:24:55 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
Nov 22 02:24:55 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
Nov 22 02:24:55 	at java.lang.Thread.run(Thread.java:748)
Nov 22 02:24:55 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:58600], URI [/], status line [HTTP/1.1 503 Service Unavailable]
Nov 22 02:24:55 	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:936)
Nov 22 02:24:55 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:233)
Nov 22 02:24:55 	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1765)
Nov 22 02:24:55 	... 19 more
Nov 22 02:24:55 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:58600], URI [/], status line [HTTP/1.1 503 Service Unavailable]
Nov 22 02:24:55 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:552)
Nov 22 02:24:55 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:537)
Nov 22 02:24:55 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122)
Nov 22 02:24:55 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
Nov 22 02:24:55 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
Nov 22 02:24:55 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
Nov 22 02:24:55 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
Nov 22 02:24:55 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
Nov 22 02:24:55 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
Nov 22 02:24:55 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
Nov 22 02:24:55 	... 1 more
{noformat}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43369&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 22:35:20 UTC 2023,,,,,,,,,,"0|z1clx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 14:44;martijnvisser;Not sure it's worthwhile to look into this, given that the ES connector is already externalized;;;","30/Jan/23 08:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45302&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=15431;;;","30/Jan/23 08:12;mapohl;[~martijnvisser] this issue will still affect as as long as we have to maintain 1.16. But I agree that, for now, it doesn't appear that often which makes the priority {{Major}} reasonable. I added the externalized connector as an affected version, though, assuming that the issue wasn't resolved, yet, and the code was simply moved.;;;","13/Feb/23 09:11;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46022&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=15976;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
RMQSourceITCase failed on azure with exit code 143,FLINK-30135,13505231,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leonard,leonard,22/Nov/22 07:44,22/Nov/22 07:44,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Connectors/ RabbitMQ,,,,0,,,,"
{noformat}
 Nov 22 02:38:20 JNI global refs: 29, weak refs: 0
Nov 22 02:38:20 
Nov 22 02:38:20 Killing process with pid=985 and all descendants
/__w/1/s/tools/ci/watchdog.sh: line 113:   985 Terminated              $cmd
Nov 22 02:38:21 Process exited with EXIT CODE: 143.
Nov 22 02:38:21 Trying to KILL watchdog (981).
Nov 22 02:38:21 Searching for .dump, .dumpstream and related files in '/__w/1/s'
Nov 22 02:38:26 Moving '/__w/1/s/flink-connectors/flink-connector-rabbitmq/target/surefire-reports/2022-11-22T02-07-07_451-jvmRun4.dumpstream' to target directory ('/__w/_temp/debug_files')
Nov 22 02:38:26 Moving '/__w/1/s/flink-connectors/flink-connector-rabbitmq/target/surefire-reports/2022-11-22T02-07-07_451-jvmRun4.dump' to target directory ('/__w/_temp/debug_files')
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '143'.
Finishing: Test - connect_2
{noformat}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43367&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 07:44:24.0,,,,,,,,,,"0|z1clwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotate all the public classes for the delegation token framework,FLINK-30134,13505230,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Nov/22 07:36,24/Nov/22 17:32,04/Jun/24 20:41,24/Nov/22 17:31,1.17.0,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 17:31:49 UTC 2022,,,,,,,,,,"0|z1clw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 17:31;gaborgsomogyi;master: a698232947ed778790d08bcb1fd00d2b77c2eddd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopModuleFactory creates error if the security module cannot be loaded,FLINK-30133,13505229,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,22/Nov/22 07:35,25/Nov/22 02:57,04/Jun/24 20:41,25/Nov/22 02:57,1.15.2,1.16.0,1.17.0,,,1.15.4,1.16.1,1.17.0,,,Connectors / Hadoop Compatibility,,,,0,pull-request-available,starter,,"[HadoopModuleFactory|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L51] tries to load the {{{}HadoopModule{}}}. If it fails to load the module, it will log an error an return {{null}} which is going to be handled properly. The resulting error log is, therefore, confusing. We might want to lower the log level to warning since the error doesn't affect the Flink cluster in a fatal way.

We might want to make the cluster fail fatally if we consider this a sever usability problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 02:57:50 UTC 2022,,,,,,,,,,"0|z1clw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 07:38;mapohl;[~gsomogyi] I'm curious what you think of that. Do you agree that warning the user through a log message is good enough or should we fail fatally if the hadoop security module failed to be loaded because the user might assume certain security measures being in place if he/she misses this log warning.;;;","22/Nov/22 09:40;gaborgsomogyi;I think this area is just conceptually not consistent so not sure what we can do about it w/o breaking change.

Here is my understanding:
 * All other factory classes make the workload finally fail if something bad happens
 * `security.module.factory.classes` contains `HadoopModuleFactory` by default which is fine
 * When no hadoop-common is on classpath then it silently prints an info and not loading the module. We can consider it [best effort behavior|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L41].
 * Then it tries to load the hadoop configuration in the [mentioned place|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L51], but this is just bad in general. It uses `flink-hadoop-fs` area code where [HdfsConfiguration|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/util/HadoopUtils.java#L59] is instantiated. This implicitly means one MUST have HDFS jars on classpath to run Flink securely. I'm constantly thinking about this to refactor but this is definitely a breaking change ( though I would support full rewrite of hadoop config loading since Flink has at least 6-7 different Hadoop config loading implementations which makes this area hell :) ).

Considering the actual situation we can decrease the error level to warning since the workload is going forward.
If you ask me then now/later on I would do the following for the clean solution:
 * Remove `HdfsConfiguration` from Flink Hadoop config loading since HDFS configs are not needed for Kerberos authentication.
 * Make the workload finally fail if module was not able to be loaded/installed (hadoop-common is on classpath so the user has intention to install the module)

I know that my clean solution would be a drastic change but that would be clear to the users.
;;;","24/Nov/22 06:20;mapohl;Thanks for your thorough analysis, Gabor. I went ahead and created PRs for that minor issue. It would be great if you could create a follow up ticket that covers the cleanup as this sounds like a reasonable refactoring.;;;","25/Nov/22 02:57;mapohl;master: 5086a33f41058e3a87bae4bf0107032fd0194b67
1.16: 622ed691c3e7f563d82f3e06273e7d2647dec1c1
1.15: 17b569f4c75064e4a5e76289bd598d9bc1140953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test LocalRecoveryITCase#testRecoverLocallyFromProcessCrashWithWorkingDirectory failed on azure due to File not exists,FLINK-30132,13505228,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,22/Nov/22 07:34,29/Nov/23 07:57,04/Jun/24 20:41,29/Nov/23 07:57,1.16.0,1.19.0,,,,,,,,,Tests,,,,0,test-stability,,,"
{noformat}

		at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
		at java.nio.file.Files.readAttributes(Files.java:1737)
		at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
		at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
		at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
		at java.nio.file.Files.walkFileTree(Files.java:2706)
		at java.nio.file.Files.walkFileTree(Files.java:2742)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:199)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:186)
		... 51 more
		Suppressed: java.nio.file.NoSuchFileException: /tmp/junit2010448393472419340/tm_taskManager_2/localState/aid_21c128b018cc61989c323cda6e36b0b1/jid_e5dbf7bc4ebb72baf20387e555083439/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_1
			at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
			at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
			at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
			at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
			at java.nio.file.Files.delete(Files.java:1126)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.resetPermissionsAndTryToDeleteAgain(TempDirectory.java:250)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:212)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:199)
			at java.nio.file.Files.walkFileTree(Files.java:2672)
			... 54 more

Nov 21 19:52:57 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 15.971 s <<< FAILURE! - in org.apache.flink.test.recovery.LocalRecoveryITCase
Nov 21 19:52:57 [ERROR] org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory  Time elapsed: 15.942 s  <<< ERROR!

{noformat}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43366&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30137,,FLINK-33641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 07:57:19 UTC 2023,,,,,,,,,,"0|z1clvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 09:04;mapohl;Same issue in {{JoinITCase.testLeftOuterJoin}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54925&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11391;;;","29/Nov/23 07:57;mapohl;Closing FLINK-30132 in favor of FLINK-33641 because there's more discussion going on but it seems to cover the same issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink iterate will suspend when record is a bit large,FLINK-30131,13505223,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,landlord,landlord,22/Nov/22 07:07,09/Feb/23 09:47,04/Jun/24 20:41,09/Feb/23 09:47,1.15.2,,,,,,,,,,API / DataStream,,,,0,,,," 
{code:java}
//代码占位符
Configuration configuration = new Configuration();
configuration.setInteger(RestOptions.PORT, 8082);
configuration.setInteger(NETWORK_MAX_BUFFERS_PER_CHANNEL, 10000000);
configuration.set(TaskManagerOptions.NETWORK_MEMORY_MAX, MemorySize.parse(""4g""));
configuration.setInteger(""taskmanager.network.memory.buffers-per-channel"", 10000000);
configuration.setInteger(""taskmanager.network.memory.floating-buffers-per-gate"", 10000000);
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(configuration);
env.setParallelism(1);
List<Integer> list = new ArrayList<>(10);
for (int i = 1; i < 10000; i++) {
    list.add(i);
}
DataStreamSource<Integer> integerDataStreamSource = env.fromCollection(list);
DataStream<byte[]> map = integerDataStreamSource.map(i -> new byte[10000000]).setParallelism(1).name(""map to byte[]"").shuffle();
IterativeStream<byte[]> iterate = map.iterate();
DataStream<byte[]> map1 = iterate.process(new ProcessFunction<byte[], byte[]>() {
    @Override
    public void processElement(byte[] value, ProcessFunction<byte[], byte[]>.Context ctx, Collector<byte[]> out) throws Exception {
        out.collect(value);
    }
}).name(""multi collect"");
DataStream<byte[]> filter = map1.filter(i -> true ).setParallelism(1).name(""feedback"");
iterate.closeWith(filter);
map1.map(bytes -> bytes.length).name(""map to length"").print();
env.execute(); {code}
my code is above.

 

when i use iterate with big record ,  the iterate will suspend at a random place. when i saw the stack, it has a suspicious thread

!image-2022-11-22-14-59-08-272.png|width=751,height=328!

it seems like a network related problem. so i increse the network buffer memory and num. but it only delay the suspend point,  it will still suspend after iterate a little more times than before.

i want to know if this is a bug or i have some error in my code or configuration.

looking forward to your reply. thanks in advance.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/22 06:59;landlord;image-2022-11-22-14-59-08-272.png;https://issues.apache.org/jira/secure/attachment/13052961/image-2022-11-22-14-59-08-272.png","24/Nov/22 09:10;landlord;image-2022-11-24-17-10-45-651.png;https://issues.apache.org/jira/secure/attachment/13053073/image-2022-11-24-17-10-45-651.png","24/Nov/22 09:12;landlord;image-2022-11-24-17-12-02-129.png;https://issues.apache.org/jira/secure/attachment/13053074/image-2022-11-24-17-12-02-129.png","24/Nov/22 09:12;landlord;image-2022-11-24-17-12-47-024.png;https://issues.apache.org/jira/secure/attachment/13053075/image-2022-11-24-17-12-47-024.png",,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:47:38 UTC 2023,,,,,,,,,,"0|z1cluo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 10:53;Weijie Guo;[~landlord] Thanks for reporting this.

At present, I have not investigated this issue in depth, but only made further understanding:
1. It can be seen that the thread is stuck in the `requestMemorySegmentBlocking` when the data shuffle is written. Theoretically, this will not be stuck all the time, and will be reused with downstream consumption. So, Is your job hanging forever?
2. What environment did you run this job, local or YARN/K8S, which helps to reproduce the problem. And can you also provide your flink-conf.yaml?
3. If you need to increase the number of network buffers, you only need to increase the total TM memory and network memory, adjusting `taskmanager.network.memory.buffers-per-channel` and `taskmanager.network.memory.floating-buffers-per-gate` will not help solve this problem, but will also make the buffer request more competitive.

TBH, I am not familiar with the implementation of iteration, cc [~gaoyunhaii]  for more professional advice.

 ;;;","23/Nov/22 01:39;landlord;[~Weijie Guo] , Thanks for your reply.
 # yes, my job hanging forever
 # i run this job in local env. you can reproduce the problem by the above code without config file.
 # ok, got it. but it doesn't work if i only increase TM memory and network memory for this problem. i trace the source code for LocalBufferPool, and find the other tow config is related.  But as said above, this does not solve the fundamental problem.;;;","23/Nov/22 01:42;landlord;[~gaoyunhaii] ，I heard from [~Weijie Guo] that you are very professional in this field, can you give me some advice? Thanks a lot.;;;","24/Nov/22 08:15;gaoyunhaii;Hi [~landlord]  sorry for the later response, first may I have a double confirmation that have you check the memory usage of the process?  Sine currently for the backward edge, the legacy iteration is using an in-memory ArrayDeque for that. Perhaps the processing is stuck for doing GC ?;;;","24/Nov/22 09:16;landlord;[~gaoyunhaii]  thanks for your apply. the pictures below show us the heap and direct memory is enough when the iterate is hanging.

  !image-2022-11-24-17-10-45-651.png|width=657,height=349!

taskmanager status when the iterate is hanging

!image-2022-11-24-17-12-02-129.png|width=768,height=370!

job status  when the iterate is hanging

 

!image-2022-11-24-17-12-47-024.png|width=762,height=357!

job manager status when the iterate is hanging;;;","01/Dec/22 08:34;gaoyunhaii;Hi [~landlord] from the attached image, it seems the iteration sink now has a busy = 100%, which seems to back-pressure previous tasks. Could you also have a check if the sink is a bottleneck?;;;","15/Dec/22 08:55;gaoyunhaii;Hi [~landlord] do you still have problem on this issue ? ;;;","29/Dec/22 08:25;zhuzh;[~landlord] do you have any updates for this issue?;;;","09/Feb/23 09:47;Weijie Guo;There was no reply for a long time, closed.;;;",,,,,,,,,,,,,,,,,,,,,,
Table.select lose watermark,FLINK-30130,13505222,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,yunfengzhou,yunfengzhou,22/Nov/22 07:03,16/Jan/23 03:29,04/Jun/24 20:41,16/Jan/23 03:29,1.15.1,,,,,,,,,,Table SQL / API,,,,0,,,,"Trying to execute the following program
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

DataStream<Long> stream = env.fromSequence(0, 1000);

Schema schema = Schema.newBuilder()
.column(""f0"", DataTypes.BIGINT())
.columnByExpression(""time_ltz"", ""TO_TIMESTAMP_LTZ(f0 * 1000, 3)"")
.watermark(""time_ltz"", ""time_ltz - INTERVAL '5' SECOND"")
.build();
Table table = tEnv.fromDataStream(stream, schema);

table.printSchema();

table = table.select($(""*""));

table.printSchema();{code}

Would get the following result


{code:java}
(
  `f0` BIGINT,
  `time_ltz` TIMESTAMP_LTZ(3) *ROWTIME* AS TO_TIMESTAMP_LTZ(f0 * 1000, 3),
  WATERMARK FOR `time_ltz`: TIMESTAMP_LTZ(3) AS time_ltz - INTERVAL '5' SECOND
)
(
  `f0` BIGINT,
  `time_ltz` TIMESTAMP_LTZ(3) *ROWTIME*
)
{code}

This result shows that the watermark property of a Table is lost during select operation.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 03:29:18 UTC 2023,,,,,,,,,,"0|z1clug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 14:45;kamalesh0420;[~yunfengzhou] I am interested in fixing this bug, I am new to the library. Can I be assigned this Jira task?;;;","16/Jan/23 03:29;jark;Sorry [~yunfengzhou] [~kamalesh0420], I don't think this is a problem or bug. This is by design that watermark **definition** can't be propagated as the referenced columns might not be selected in the current view/table. But the printed schema shows the ""ROWTIME"" attribute, which users can use to do windowing. You can also use ""TUMBLE_ROWTIME"" to generate a new rowtime attribute (and watermark implicitly) for the next rowtime-based operations. Therefore, from the perspective of users, there is enough information in the printed schema, and this doesn't affect users using rowtime-based operations. 

Please reopen the issue if you still have questions. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push projection through ChangelogNormalize,FLINK-30129,13505217,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,22/Nov/22 06:34,22/Jun/23 07:03,04/Jun/24 20:41,15/Mar/23 02:56,,,,,,1.18.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Currently, the ChangelogNormalize node is generated during the physical optimization phase. That means the projection is not pushed through ChangelogNormalize if the {{TableSource}} doesn't support {{SupportsProjectionPushDown}}. We can implement such optimization to reduce the state size (fewer columns in state value) and better throughput (only changes on the selected columns will be emitted). 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 22 07:03:51 UTC 2023,,,,,,,,,,"0|z1cltc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 08:20;qinjunjerry;Do we have a plan for this fix? ;;;","15/Mar/23 02:56;jark;Fixed in master: 3f53ef1fad8f45083cbad34eaaf2a67fb7fa9df4...617fcaa60743cf48afb43c867450ec5e5aa1d846;;;","22/Jun/23 07:03;qinjunjerry;Thanks [~jark] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Azure Data Lake Gen2 APIs in the Hadoop Recoverable path,FLINK-30128,13505211,13316143,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,ram_krish,ram_krish,ram_krish,22/Nov/22 04:42,17/Jan/23 09:37,04/Jun/24 20:41,10/Jan/23 02:16,1.13.1,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,"Currently the HadoopRecoverableWriter assumes that the underlying FS is Hadoop and so it checks for DistributedFileSystem. It also tries to do a truncate and ensure the lease is recovered before the 'rename' operation is done.
In the Azure Data lake gen 2 world, the driver does not support truncate and lease recovery API. We should be able to get the last committed size and if it matches go for the rename. Will be back with more details here. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 09:37:18 UTC 2023,,,,,,,,,,"0|z1cls0:",9223372036854775807,FileSink now supports Azure Data Lake Storage Gen2 APIs (`abfs://` and `abfss://`).,,,,,,,,,,,,,,,,,,,"02/Dec/22 05:37;ram_krish;Attaching a simple diagram that talks about how the class diagram will look like. 
Basically HadoopFileSystem will be extended to create AzureBlobFileSystem.
Internally it would create the AzureBlobRecoverableWriter which will work with AzureBlobFsRecoverableDataOutputStream. Will raise a PR for this after some more testing. ;;;","06/Dec/22 04:34;ram_krish;Trying to add tests for Azure fs, seems some of the IT tests are already not running in the CI AzureFileSystemBehaviorITCase. The others that run are AzureBlobStorageFSFactoryTest and AzureDataLakeStoreGen2FSFactoryTest. 
Any idea on how we should be adding those tests here?
For reference
{code}
Dec 05 17:06:36 [INFO] Running org.apache.flink.fs.azurefs.AzureFileSystemBehaviorITCase
Dec 05 17:06:37 [INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.792 s - in org.apache.flink.fs.azurefs.AzureFileSystemBehaviorITCase
Dec 05 17:06:37 [INFO] 
Dec 05 17:06:37 [INFO] Results:
Dec 05 17:06:37 [INFO] 
Dec 05 17:06:37 [INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
Dec 05 17:06:37 [INFO] 
Dec 05 17:06:37 [INFO] 
Dec 05 17:06:37 [INFO] --- japicmp-maven-plugin:0.17.1.1_m325:cmp (default) @ flink-azure-fs-hadoop ---
Dec 05 17:06:37 [INFO] Skipping execution because parameter 'skip' was set to true.
{code};;;","10/Jan/23 02:16;xtsong;master (1.17): c8ecfadfb968ed805c033911c8a68d30f35f9eee;;;","17/Jan/23 09:37;xtsong;Documentation updated:
- master (1.17): 06435cec2e510d0592e8ad868a1d0ea3e83c1b35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the packaging of HadoopRecoverableWriter and related issues,FLINK-30127,13505210,13316143,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ram_krish,ram_krish,22/Nov/22 04:39,09/Jan/23 17:25,04/Jun/24 20:41,09/Jan/23 17:25,1.13.1,,,,,,,,,,,,,,0,,,,The first issue here is that the HadoopRecoverableWriter that creates the RecoverableWriter classes are not found in the azure-fs package. We need to fix them so that we can use the hadoop-common libraries to create the ABFS wrappers for the Streaming sink. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 04:39:40.0,,,,,,,,,,"0|z1clrs:",9223372036854775807,Fixed as part of https://issues.apache.org/jira/browse/FLINK-30128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Delay registration of the catalog, register the catalog as needed",FLINK-30126,13505206,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,melin,melin,22/Nov/22 03:55,25/Nov/22 07:52,04/Jun/24 20:41,25/Nov/22 07:52,,,,,,,,,,,,,,,0,,,,"Data platform has registered many relational database data sources such as mysql, data source code is used as the catalog name, we are not sure which data source needs to register the catalog in flink, we hope that the required catalog can be dynamically loaded when sql is executed, flink provides the interface. Users can customize the registration catalog",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/22 01:13;melin;image-2022-11-25-09-12-59-477.png;https://issues.apache.org/jira/secure/attachment/13053095/image-2022-11-25-09-12-59-477.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 07:52:18 UTC 2022,,,,,,,,,,"0|z1clqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 18:08;hackergin;We are currently facing a similar problem. Our current solution: In version 1.12, we will analyze SQL node tree to find which catalogs are used in the SQL  and then register the catalogs in tableEnv. In 1.14, the catalog will be verified in the grammar parser stage. We must use other means to detect the used catalogs, such as through string matching, and then register the required catalog in advance before running SQL.   

 

My initial thoughts, is it possible to add some interface to CatalogManager to extend the behavior of catalog  ? ;;;","25/Nov/22 01:13;melin;Spark also does not support dynamic catalog, which is created through aop interception. Can flink do the same? flink does not have a spark.yarn.dist.jars similar parameter. In the yarn environment, it is convenient to load aspectjweaver.jar.
{code:java}
@Around(""execution(public * org.apache.spark.sql.connector.catalog.CatalogManager.catalog(..)) && args(catalogName)"")
public Object aroundCatalog(ProceedingJoinPoint pjp, String catalogName) throws Throwable {
    synchronized (SparkCatalogAspectj.class) {
        if (catalogName.equalsIgnoreCase(SESSION_CATALOG_NAME)) {
            return pjp.proceed();
        }

        CatalogPlugin catalog = catalogs.get(catalogName);
        if (catalog != null) {
            return catalog;
        }

        DataSourceEntity dataSource = dataSourceService.queryDataSource(catalogName);

        if (dataSource != null) {
            LOG.info(""init catalog {}"", catalogName);

            catalog = CatalogUtils.buildJdbcCatalogPlugin(dataSource, tableService, catalogName);
            catalogs.put(catalogName, catalog);
            return catalog;
        } else {
            throw new CatalogNotFoundException(""catalog not exists: "" + catalogName);
        }
    }
} {code}
 

!image-2022-11-25-09-12-59-477.png!

[https://github.com/melin/spark-jobserver/blob/master/jobserver-admin/src/main/java/io/github/melin/spark/jobserver/deployment/AbstractDriverDeployer.java]

 ;;;","25/Nov/22 07:52;martijnvisser;[~melin] [~hackergin] Please open a discussion thread on the Dev mailing list on this topic. It would be great if that would result in a FLIP https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals with a concrete proposal. When that has completed, we can re-open the FLIP. We should not have discussions in Jira, that's what the Dev mailing list is meant for. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projection pushdown is not work for partial update,FLINK-30125,13505201,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Nov/22 02:55,24/Nov/22 08:05,04/Jun/24 20:41,24/Nov/22 08:05,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"We did not properly process the project in MergeFunction, which resulted in subsequent reading position errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:05:51 UTC 2022,,,,,,,,,,"0|z1clps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:05;lzljs3620320;master: 2b8fd32140ed0bf05aa5f8e42f561b19904be69b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericType<java.util.Map> is not supported in PyFlink currently,FLINK-30124,13505177,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,22/Nov/22 01:31,29/Nov/22 09:18,04/Jun/24 20:41,29/Nov/22 09:18,ml-2.1.0,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"When we add and execute the following test case to flink-ml-python/pyflink/ml/lib/classification/tests/test_naivebayes.py of the Flink ML repository,
{code:java}
def test_get_model_data(self):
model_data = self.estimator.fit(self.train_data).get_model_data()[0]
self.t_env.to_data_stream(model_data).execute_and_collect().next(){code}
The following exception would be thrown.

 
{code:java}
j_type_info = JavaObject id=o698
    def _from_java_type(j_type_info: JavaObject) -> TypeInformation:
        gateway = get_gateway()
        JBasicTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo.BasicTypeInfo
    
        if _is_instance_of(j_type_info, JBasicTypeInfo.STRING_TYPE_INFO):
            return Types.STRING()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BOOLEAN_TYPE_INFO):
            return Types.BOOLEAN()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BYTE_TYPE_INFO):
            return Types.BYTE()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.SHORT_TYPE_INFO):
            return Types.SHORT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.INT_TYPE_INFO):
            return Types.INT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.LONG_TYPE_INFO):
            return Types.LONG()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.FLOAT_TYPE_INFO):
            return Types.FLOAT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.DOUBLE_TYPE_INFO):
            return Types.DOUBLE()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.CHAR_TYPE_INFO):
            return Types.CHAR()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BIG_INT_TYPE_INFO):
            return Types.BIG_INT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BIG_DEC_TYPE_INFO):
            return Types.BIG_DEC()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.INSTANT_TYPE_INFO):
            return Types.INSTANT()
    
        JSqlTimeTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo
        if _is_instance_of(j_type_info, JSqlTimeTypeInfo.DATE):
            return Types.SQL_DATE()
        elif _is_instance_of(j_type_info, JSqlTimeTypeInfo.TIME):
            return Types.SQL_TIME()
        elif _is_instance_of(j_type_info, JSqlTimeTypeInfo.TIMESTAMP):
            return Types.SQL_TIMESTAMP()
    
        JPrimitiveArrayTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo \
            .PrimitiveArrayTypeInfo
    
        if _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.BOOLEAN_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.BOOLEAN())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.BYTE())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.SHORT_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.SHORT())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.INT_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.INT())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.LONG_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.LONG())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.FLOAT_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.FLOAT())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.DOUBLE_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.DOUBLE())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.CHAR_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.CHAR())
    
        JBasicArrayTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo
    
        if _is_instance_of(j_type_info, JBasicArrayTypeInfo.BOOLEAN_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.BOOLEAN())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.BYTE_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.BYTE())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.SHORT_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.SHORT())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.INT_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.INT())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.LONG_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.LONG())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.FLOAT_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.FLOAT())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.DOUBLE_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.DOUBLE())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.CHAR_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.CHAR())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.STRING_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.STRING())
    
        JObjectArrayTypeInfo = gateway.jvm.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo
        if _is_instance_of(j_type_info, JObjectArrayTypeInfo):
            return Types.OBJECT_ARRAY(_from_java_type(j_type_info.getComponentInfo()))
    
        JPickledBytesTypeInfo = gateway.jvm \
            .org.apache.flink.streaming.api.typeinfo.python.PickledByteArrayTypeInfo\
            .PICKLED_BYTE_ARRAY_TYPE_INFO
        if _is_instance_of(j_type_info, JPickledBytesTypeInfo):
            return Types.PICKLED_BYTE_ARRAY()
    
        JRowTypeInfo = gateway.jvm.org.apache.flink.api.java.typeutils.RowTypeInfo
        if _is_instance_of(j_type_info, JRowTypeInfo):
            j_row_field_names = j_type_info.getFieldNames()
            j_row_field_types = j_type_info.getFieldTypes()
            row_field_types = [_from_java_type(j_row_field_type) for j_row_field_type in
                               j_row_field_types]
            row_field_names = [field_name for field_name in j_row_field_names]
            return Types.ROW_NAMED(row_field_names, row_field_types)
    
        JTupleTypeInfo = gateway.jvm.org.apache.flink.api.java.typeutils.TupleTypeInfo
        if _is_instance_of(j_type_info, JTupleTypeInfo):
            j_field_types = []
            for i in range(j_type_info.getArity()):
                j_field_types.append(j_type_info.getTypeAt(i))
            field_types = [_from_java_type(j_field_type) for j_field_type in j_field_types]
            return TupleTypeInfo(field_types)
    
        JMapTypeInfo = get_gateway().jvm.org.apache.flink.api.java.typeutils.MapTypeInfo
        if _is_instance_of(j_type_info, JMapTypeInfo):
            j_key_type_info = j_type_info.getKeyTypeInfo()
            j_value_type_info = j_type_info.getValueTypeInfo()
            return MapTypeInfo(_from_java_type(j_key_type_info), _from_java_type(j_value_type_info))
    
        JListTypeInfo = get_gateway().jvm.org.apache.flink.api.java.typeutils.ListTypeInfo
        if _is_instance_of(j_type_info, JListTypeInfo):
            j_element_type_info = j_type_info.getElementTypeInfo()
            return ListTypeInfo(_from_java_type(j_element_type_info))
    
        JExternalTypeInfo = gateway.jvm.org.apache.flink.table.runtime.typeutils.ExternalTypeInfo
        if _is_instance_of(j_type_info, JExternalTypeInfo):
            TypeInfoDataTypeConverter = \
                gateway.jvm.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter
            return ExternalTypeInfo(_from_java_type(
                TypeInfoDataTypeConverter.toLegacyTypeInfo(j_type_info.getDataType())))
    
>       raise TypeError(""The java type info: %s is not supported in PyFlink currently."" % j_type_info)
E       TypeError: The java type info: GenericType<java.util.Map> is not supported in PyFlink currently.
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 01:31:47.0,,,,,,,,,,"0|z1clkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize ApplicationDeployer API design,FLINK-30123,13505176,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,melin,melin,22/Nov/22 01:29,28/Nov/22 13:11,04/Jun/24 20:41,28/Nov/22 13:11,,,,,,,,,,,,,,,0,,,,"The task is submitted by ApplicationDeployer api, and the run is synchronous and waiting for the submission to be completed. If the task is submitted to yarn, it is probably accepted and the yarn applicationID is not obtained at this time. It is difficult to cancel the task.Recommended to org. apache. spark.launcher.SparkLauncher design, asynchronous submission tasks, can obtain applicationId as soon as possible, if you want to delete the task ahead of time, direct yarn application - kill XXX;

!image-2022-11-22-09-28-38-631.png!

!image-2022-11-22-09-28-54-660.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/22 01:28;melin;image-2022-11-22-09-28-38-631.png;https://issues.apache.org/jira/secure/attachment/13052949/image-2022-11-22-09-28-38-631.png","22/Nov/22 01:28;melin;image-2022-11-22-09-28-54-660.png;https://issues.apache.org/jira/secure/attachment/13052948/image-2022-11-22-09-28-54-660.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 13:11:10 UTC 2022,,,,,,,,,,"0|z1clk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 01:48;luoyuxia;[~melin] Thanks for reaching here.  But could you please translate it to English? All issues in Flink should be written in English, otherwise, it'll be closed.;;;","28/Nov/22 13:11;martijnvisser;Please first have a discussion on the Dev mailing list before creating Jira tickets. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML KMeans getting model data throws TypeError,FLINK-30122,13505175,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,22/Nov/22 01:25,10/Jan/23 04:11,04/Jun/24 20:41,10/Jan/23 04:11,ml-2.1.0,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,,,,"When the following test case is added to flink-ml-python/pyflink/ml/lib/clustering/tests/test_kmeans.py，
{code:java}
def test_get_model_data(self):
kmeans = KMeans().set_max_iter(2).set_k(2)
model = kmeans.fit(self.data_table)
model_data = model.get_model_data()[0]
expected_field_names = ['centroids', 'weights']
self.assertEqual(expected_field_names, model_data.get_schema().get_field_names())self.t_env.to_data_stream(model_data).execute_and_collect().next(){code}
The following exception would be thrown.
{code:java}
data = 0, field_type = DenseVectorTypeInfo
def pickled_bytes_to_python_converter(data, field_type):
if isinstance(field_type, RowTypeInfo):
row_kind = RowKind(int.from_bytes(data[0], 'little'))
data = zip(list(data[1:]), field_type.get_field_types())
fields = []
for d, d_type in data:
fields.append(pickled_bytes_to_python_converter(d, d_type))
row = Row.of_kind(row_kind, *fields)
return row
else:
> data = pickle.loads(data)
E TypeError: a bytes-like object is required, not 'int'{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 12:17:40 UTC 2022,,,,,,,,,,"0|z1clk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 12:17;yunfengzhou;It has been discovered that PyFlink has a bug dealing with object arrays.

https://issues.apache.org/jira/browse/FLINK-30168

Flink ML has worked around this problem with a temporary solution as shown in this PR.

[https://github.com/apache/flink-ml/pull/181];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Stored procedures,FLINK-30121,13505170,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,melin,melin,22/Nov/22 01:09,28/Nov/22 13:11,04/Jun/24 20:41,28/Nov/22 13:11,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"Supports operations like hudi/iceberg calls, such as savepoint
[https://hudi.apache.org/docs/procedures/]
 
CALL system.procedure_name(arg_1, arg_2, ... arg_n)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 13:11:21 UTC 2022,,,,,,,,,,"0|z1cliw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 14:59;martijnvisser;[~melin] To be honest, I don't see much value in creating tickets like these without a discussion on the Dev mailing list and a FLIP. Without a discussion or a design, I don't see anything happening with this ticket. This is also applicable to other tickets you've created like FLINK-30123 and FLINK-30126;;;","28/Nov/22 12:56;twalthr;I agree with [~martijnvisser], we all have hundreds of feature wishes but only the very frequent ones deserve a JIRA ticket. Or when there are actual resources for implementing it.;;;","28/Nov/22 13:11;martijnvisser;Please first have a discussion on the Dev mailing list before creating Jira tickets. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Statefun Task Failure causes Restart of all Tasks with Regional Failover Strategy,FLINK-30120,13505168,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stepweiwu,stepweiwu,21/Nov/22 23:49,23/Nov/22 23:44,04/Jun/24 20:41,,statefun-3.0.0,statefun-3.1.1,statefun-3.2.0,,,,,,,,Stateful Functions,,,,0,,,,"Hey all,

We've noticed that a single task failure causes all of the Statefun tasks to be restarted.

For example, a single task fails because of some Statefun Endpoint unavailability or if one of our Kuberentes TaskManager pods go down. 
Flink then determines that the _region_ failover strategy requires all tasks to be restarted so we see this in the logs:

 
{code:java}
Nov 17 10:20:30 org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 31284d56d1e2112b0f20099ee448a6a9_11.
Nov 17 10:20:30 org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 5650 tasks should be restarted to recover the failed task 31284d56d1e2112b0f20099ee448a6a9_11. {code}
Our tasks are all fully independent so I would like that only the one failed task to get restarted or moved to a different TaskManager slot.

Is there a way to tell Flink to only restart the failed task? Or is there a specific reason why the region failover strategy decides to restart all tasks?

If not, we'd really appreciate a way to enable individual task failovers.

Thanks in advance!
Stephan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-21 23:49:44.0,,,,,,,,,,"0|z1clig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Breaking change: Flink Kubernetes Operator should store last savepoint in the SavepointInfo.lastSavepoint field whether it is completed or pending,FLINK-30119,13505018,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,claraxiong,claraxiong,claraxiong,21/Nov/22 17:36,31/Aug/23 15:13,04/Jun/24 20:41,31/Aug/23 15:13,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,stale-assigned,,"End user experience proposal:

Users can see the properties of last savepoint pending or completed and can get status in one of three states for the status: PENDING, SUCCEEDED and FAILED. If there is never savepoint taken or attempted, it is empty. Completed savepoints (manual, periodic and upgrade) are included Savepoint history, merged with savepoints form Flink job.

Users can see this savepoint with PENDING status once one is trigger. Once completed, users can see last savepoint status changed to SUCCEEDED and included in savepoint history, or FAILED and not in savepoint history. If there is other savepoint triggered after completion before user checks, user cannot see the status of the one they triggered but they can check if the savepoint is in the history.

Currently lastSavepoint only stores the last completed one, duplicate with savepoint history. To expose the properties of the currently pending savepoint or last savepoint that failed, we need to expose those info in separate fields in SavepointInfo. The internal logic of Operator uses those fields for triggering and retries and creates compatibility issues with client. It also use more space for etcd size limit.

Code change proposal:

Use lastSavepoint to store the last completed/attempted one and deprecate SavepointInfo.triggerTimstamp, SavepointInfo.triggerType and SavepointInfo.formatType. This will simplify the CRD and logic.

Add SavepointInfo::retrieveLastSavepoint method to return the last succeeded one.

Update getLastSavepointStatus to simplify the logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29695,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 15:12:20 UTC 2023,,,,,,,,,,"0|z1ckl4:",9223372036854775807,"Given the impact vs benefit of the change, we can revisit next time we get a change to make major design changes.",,,,,,,,,,,,,,,,,,,"01/Dec/22 20:48;claraxiong;Updating the title to reflect the fact this could break existing application relying on some fields of SavepointInfo although there is potential benefits to simplying logic and reduce the size of etcd object which is hitting upper limit.;;;","01/Dec/22 21:05;claraxiong;This is a breaking change:
 * Existing application using lastSavepoint or trigger related fields in Savepoint info except triggerId may be broken.
 * For triggerId dependence, better use SavepointUtils.savepointInProgress which is guaranteed to work.
 * savepointHistory is not changed and still keeps the completed savepoints only. Unit tests for savepoint history also cover a few scenarios.;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","31/Aug/23 15:12;claraxiong;Given the impact and benefit of the change, we can revisit next time we get a change to make major design changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate DDB connector Integration Tests/ITCase to E2E module,FLINK-30118,13505013,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,darenwkt,darenwkt,21/Nov/22 16:45,24/Mar/23 15:27,04/Jun/24 20:41,24/Mar/23 15:27,,,,,,aws-connector-4.2.0,,,,,Connectors / DynamoDB,,,,0,,,,"Currently DDB connector [ITCase|https://github.com/apache/flink-connector-aws/blob/53ea41008910237073804dc090d67a1e0852163d/flink-connector-dynamodb/src/test/java/org/apache/flink/connector/dynamodb/sink/DynamoDbSinkITCase.java#L77] is implemented whereby it starts a [DDB docker image|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-dynamodb/src/test/java/org/apache/flink/connector/dynamodb/testutils/DynamoDbContainer.java] and run through several test scenarios on it.

The proposal is to move this ITCase to an e2e test that will be run as part of Github Action. This will help speed up Maven builds without sacrificing integration/e2e test to ensure quality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-21 16:45:15.0,,,,,,,,,,"0|z1ckk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log whitelist entries added in FLINK-29475,FLINK-30117,13505006,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gaborgsomogyi,gaborgsomogyi,21/Nov/22 16:08,22/Jan/24 13:52,04/Jun/24 20:41,22/Jan/24 13:52,1.17.0,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,FLINK-30310,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:52:30 UTC 2024,,,,,,,,,,"0|z1ckig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 08:38;Wencong Liu;Hello [~gaborgsomogyi] , I'm quite interested in this issue, but I can't understand it completely. Could you please illustrate it in detail ?;;;","30/Dec/22 15:50;gaborgsomogyi;[~Wencong Liu] Please have a look at the following code part: https://github.com/apache/flink-kubernetes-operator/blob/6ff02908b254c0f8e7328d495870d4bd562d54b1/e2e-tests/utils.sh#L144-L150

The aim is to analyze why these error messages are coming in the operator log during e2e test execution. When it's understood then we must add a proper solution to eliminate them (either give in-depth explanation why the log level can be lowered or provide a code fix). The solution is depending on why the error message is coming.
;;;","31/Dec/22 06:47;Wencong Liu;Thanks [~gaborgsomogyi] . According to the code part, the error message of opening subtasks in this issue have been eliminated by ""grep -v"". So what's the next step?;;;","31/Dec/22 11:52;gaborgsomogyi;[~Wencong Liu] The aim is not to have any exclusion unless there is a valid justification.;;;","22/Jan/24 13:52;gaborgsomogyi;See comment in FLINK-30310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Don't Show Env Vars in Web UI,FLINK-30116,13504830,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ConradJam,knaufk,knaufk,21/Nov/22 14:02,12/Dec/22 10:17,04/Jun/24 20:41,12/Dec/22 10:15,1.16.0,,,,,1.16.1,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"As discussed and agreed upon in [1], we'd like to revert [2] and not show any environment variables in the Web UI for security reasons. 

[1] https://lists.apache.org/thread/rjgk15bqttvblp60zry4n5pw4xjw7q9k 
[2] https://issues.apache.org/jira/browse/FLINK-28311","In 1.16.0 a new REST endpoint was added that exposed JVM/classpath information and all environment variables of the jobmanager process.

For security reasons environment variables are no longer exposed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28311,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 10:15:24 UTC 2022,,,,,,,,,,"0|z1cjfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 03:55;ConradJam;Hi [~knaufk] Can I take this ticket?;;;","29/Nov/22 13:35;chesnay;[~ConradJam] Have you made any progress?;;;","01/Dec/22 11:44;ConradJam;I was separated by something. It's done this week [~chesnay] ;;;","03/Dec/22 13:48;ConradJam;I was wondering does the name *ConfigurationInfo* have to be restored to the original as well?And *JobManagerJobConfigurationHandler* also need to remove?[~chesnay] ;;;","12/Dec/22 10:15;chesnay;master:
842ef5d2e820dcaf666410f67c855167a922c9cf
cfc2ec841257eba37f49f6ed96ea8937a98bf535
1.16:
cf027891b3ab51e8203af39e387f5badd4020a9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Include log4j-layout-template-json jar in the default operator image,FLINK-30115,13504818,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,asardaes,asardaes,21/Nov/22 12:55,27/Feb/23 10:00,04/Jun/24 20:41,,kubernetes-operator-1.2.0,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Ussing logging processors/forwarders in Kubernetes is pretty common, and using JSON format in logs can facilitate parsing, so it would be nice if the image included the {{log4j-layout-template-json}} jar by default.

Alternatively, if users must create their own images for this, the classpath for the operator must allow some customization.

It might even be valuable to separate the logging jars from the fat jar as it's done for the Flink distribution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31140,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-21 12:55:36.0,,,,,,,,,,"0|z1cjco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce PyFlink example for table store,FLINK-30114,13504803,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,lzljs3620320,lzljs3620320,21/Nov/22 11:25,04/Jan/23 11:29,04/Jun/24 20:41,04/Jan/23 11:29,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-21 11:25:56.0,,,,,,,,,,"0|z1cj9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression for operator state,FLINK-30113,13504782,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,echauchot,dwysakowicz,dwysakowicz,21/Nov/22 09:23,15/Jan/24 18:05,04/Jun/24 20:41,08/Mar/23 10:29,,,,,,1.18.0,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"It has been requested in the ML to be able to enable compression for broadcast state

https://lists.apache.org/thread/2kylgj0fdmn21jk7x63696mgdvd1csxo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34063,FLINK-33863,FLINK-30112,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 10:29:29 UTC 2023,,,,,,,,,,"0|z1cj4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 15:46;echauchot;[~dwysakowicz] I have a PR almost ready. I thought that even if the thread mentions only broadcast state, it will be good to add compression on both regular and broadcast operator states, so that is what I did. Do you agree ?;;;","10/Jan/23 15:48;echauchot;[~dwysakowicz] I have submitted the PR. Could you review it ?;;;","08/Mar/23 10:29;dwysakowicz;Implemented in d04d727958b641c041d593618de6a40fd62d5338;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve docs re state compression,FLINK-30112,13504778,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dwysakowicz,dwysakowicz,21/Nov/22 08:57,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Documentation,,,,0,,,,"Documentation should state explicitly state compression is supported only for KeyedState as of now.

https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/#compression",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30113,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 18:40:28 UTC 2023,,,,,,,,,,"0|z1cj3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 18:40;kkrugler;As of https://issues.apache.org/jira/browse/FLINK-30113, I think the docs should now be updated to note that both keyed and operator state compression is supported.

It would be good to clarify that `execution.checkpointing.snapshot-compression` is used for both keyed and operator state compression.

And that there's a change to snapshot format which isn't backwards compatible (but is forward compatible), or so I assume?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CacheRead fails with Intermediate data set with ID not found,FLINK-30111,13504774,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,prabhujoseph,prabhujoseph,21/Nov/22 08:34,05/Dec/22 08:49,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,0,,,,"CacheRead fails with below exception when running multiple parallel jobs in detached mode which all reads from a same CacheDataStream. The same application runs fine when either running in Attached Mode or when not using Cache.
{code:java}
2022-11-21 08:19:31,762 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CacheRead -> Map -> Sink: Writer (1/1) (8002916773ad489098a05e6835288f29_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED on container_1668960408356_0009_01_000009 @ ip-172-31-38-144.us-west-2.compute.internal (dataPort=38433).
java.lang.IllegalArgumentException: Intermediate data set with ID f0d8150945d3e396b8c0a4f6a527a8ce not found.
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.requestPartitionState(ExecutionGraphHandler.java:173) ~[flink-dist-1.16.0.jar:1.16.0]
        at org.apache.flink.runtime.scheduler.SchedulerBase.requestPartitionState(SchedulerBase.java:763) ~[flink-dist-1.16.0.jar:1.16.0]
        at org.apache.flink.runtime.jobmaster.JobMaster.requestPartitionState(JobMaster.java:515) ~[flink-dist-1.16.0.jar:1.16.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_e220e3c9-e81e-4259-9655-37a1f83e8a36.jar:1.16.0]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_342]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_342]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_342]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_342]
{code}
*Repro:*
{code:java}
        import org.apache.flink.api.common.RuntimeExecutionMode;
        import org.apache.flink.api.common.eventtime.WatermarkStrategy;
        import org.apache.flink.api.common.serialization.SimpleStringEncoder;
        import org.apache.flink.api.java.tuple.Tuple2;
        import org.apache.flink.connector.file.sink.FileSink;
        import org.apache.flink.connector.file.src.FileSource;
        import org.apache.flink.connector.file.src.reader.TextLineInputFormat;
        import org.apache.flink.streaming.api.datastream.CachedDataStream;
        import org.apache.flink.streaming.api.datastream.DataStreamSource;
        import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
        import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

        import java.io.File;
        import java.io.FileWriter;
        import java.util.UUID;

        public class CacheTester {

            public static void main(String[] args) throws Exception {
                StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
                env.setRuntimeMode(RuntimeExecutionMode.BATCH);

                File datafile1 = new File(""/tmp/FLINK/input1"");

                FileSource<String> source =
                        FileSource.forRecordStreamFormat(
                                        new TextLineInputFormat(),
                                        new org.apache.flink.core.fs.Path(datafile1.getPath()))
                                .build();
                final CachedDataStream<Integer> cachedDataStream =
                        env.fromSource(source,WatermarkStrategy.noWatermarks(), ""source"").map(i -> Integer.parseInt(i) + 1).cache();

                for (int i=0; i<100; i++) {
                    SingleOutputStreamOperator out2 = cachedDataStream.map(j -> j + 1);
                    File outputFile = new File(""/tmp/FLINK/output"" + (i));
                    FileSink fileSink = FileSink.forRowFormat(
                                    new org.apache.flink.core.fs.Path(outputFile.getPath()),
                                    new SimpleStringEncoder<>())
                            .build();
                    out2.sinkTo(fileSink);
                    env.execute();
        	    try { Thread.sleep(2); } catch(Exception e) {}
                }

            }
        }


        [root@ip-172-31-38-144 container_1668960408356_0008_01_000001]# cat /tmp/FLINK/input1 
        1
        2
        3

        flink-yarn-session -d

        flink run -d -c CacheTester cachetester.jar
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 08:49:36 UTC 2022,,,,,,,,,,"0|z1cj2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 08:49;dannycranmer;Assigning to [~samrat007] , as requested offline;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable from-timestamp log scan when timestamp-millis is configured,FLINK-30110,13504768,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,21/Nov/22 07:25,01/Dec/22 07:12,04/Jun/24 20:41,01/Dec/22 07:12,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 07:12:47 UTC 2022,,,,,,,,,,"0|z1cj1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 07:12;lzljs3620320;master: a9b15d678991a48a58e85b068c91bd01983a282d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checked exceptions are sneakingly transformed into unchecked exceptions in the Pulsar,FLINK-30109,13504751,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,mapohl,mapohl,21/Nov/22 05:25,17/Feb/23 07:20,04/Jun/24 20:41,17/Feb/23 07:20,1.15.2,1.16.0,pulsar-3.0.0,,,pulsar-4.0.0,,,,,Connectors / Pulsar,Documentation,,,0,pull-request-available,,,"[PulsarExceptionUtils|https://github.com/apache/flink/blob/c675f786c51038801161e861826d1c54654f0dde/flink-connectors/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/common/utils/PulsarExceptionUtils.java#L33] provides {{sneaky*}} utility methods for hiding checked exceptions. This is rather unusual coding. Based on what's provided in the code I would have concerns as a reader that we're not handling errors properly in calling code.

Either, we remove these methods and add proper exception handling or we add proper documentation on why this workaround is necessary.

[~syhily] already hinted in his [FLINK-29830 PR comment|https://github.com/apache/flink/pull/21252#discussion_r1019822514] that this is related to flaws of the Pulsar API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20731,,,,,,,,,,,,,FLINK-29830,FLINK-31014,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 07:20:18 UTC 2023,,,,,,,,,,"0|z1cixs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 13:11;syhily;I will try to drop most sneaky* code. Please assign this issue to me.;;;","09/Jan/23 15:55;mapohl;Thanks, [~syhily]. I assigned the ticket to you.;;;","13/Feb/23 13:47;tison;master via 2278653d67a8ddf171c88d538a288e503221625a

I'm not sure whether other versions should be picked.;;;","13/Feb/23 14:22;syhily;I think it's ok to fix the issue only on master branch. This is a  technical debt which doesn't affect any users.;;;","16/Feb/23 16:31;syhily;[~mapohl] Can we close this issue now?;;;","17/Feb/23 07:20;mapohl;You can resolve this issue if you're happy with the outcome. I will do so since you asked. It didn't cause any instabilities as far as I can see in Flink core. Therefore, I'm fine with closing this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionConnectionHandlingTest.testLoseLeadershipOnLostConnectionIfTolerateSuspendedConnectionsIsEnabled times out,FLINK-30108,13504746,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,leonard,leonard,21/Nov/22 04:49,31/Aug/23 10:19,04/Jun/24 20:41,,1.17.0,,,,,,,,,,Runtime / Coordination,Tests,,,0,auto-deprioritized-major,test-stability,,"
{noformat}

Nov 18 01:02:58 [INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 109.22 s - in org.apache.flink.runtime.operators.hash.InPlaceMutableHashTableTest
Nov 18 01:18:09 ==============================================================================
Nov 18 01:18:09 Process produced no output for 900 seconds.
Nov 18 01:18:09 ==============================================================================
Nov 18 01:18:09 ==============================================================================
Nov 18 01:18:09 The following Java processes are running (JPS)
Nov 18 01:18:09 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Nov 18 01:18:09 924 Launcher
Nov 18 01:18:09 23421 surefirebooter1178962604207099497.jar
Nov 18 01:18:09 11885 Jps
Nov 18 01:18:09 ==============================================================================
Nov 18 01:18:09 Printing stack trace of Java process 924
Nov 18 01:18:09 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Nov 18 01:18:09 2022-11-18 01:18:09
Nov 18 01:18:09 Full thread dump OpenJDK 64-Bit Server VM (25.292-b10 mixed mode):

...
...
...
Nov 18 01:18:09 ==============================================================================
Nov 18 01:18:09 Printing stack trace of Java process 11885
Nov 18 01:18:09 ==============================================================================
11885: No such process
Nov 18 01:18:09 Killing process with pid=923 and all descendants
/__w/2/s/tools/ci/watchdog.sh: line 113:   923 Terminated              $cmd
Nov 18 01:18:10 Process exited with EXIT CODE: 143.
Nov 18 01:18:10 Trying to KILL watchdog (919).
Nov 18 01:18:10 Searching for .dump, .dumpstream and related files in '/__w/2/s'
Nov 18 01:18:16 Moving '/__w/2/s/flink-runtime/target/surefire-reports/2022-11-18T00-55-55_041-jvmRun3.dumpstream' to target directory ('/__w/_temp/debug_files')
Nov 18 01:18:16 Moving '/__w/2/s/flink-runtime/target/surefire-reports/2022-11-18T00-55-55_041-jvmRun3.dump' to target directory ('/__w/_temp/debug_files')
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '143'.
Finishing: Test - core
{noformat}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43277&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33012,,,,,,,,,,"09/Jan/23 15:32;mapohl;FLINK-30108.tar.gz;https://issues.apache.org/jira/secure/attachment/13054451/FLINK-30108.tar.gz","05/Jan/23 10:05;mapohl;zookeeper-server.FLINK-30108.log;https://issues.apache.org/jira/secure/attachment/13054373/zookeeper-server.FLINK-30108.log",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:04 UTC 2023,,,,,,,,,,"0|z1ciwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 08:57;zhuzh;The stuck case is ZooKeeperLeaderElectionConnectionHandlingTest.
Seems it got stuck in waiting for leadership granting. I will take another look.

{code:java}
Nov 18 01:18:09 ""main"" #1 prio=5 os_prio=0 tid=0x00007fcf7400b800 nid=0x5b90 in Object.wait() [0x00007fcf7d29b000]
Nov 18 01:18:09    java.lang.Thread.State: WAITING (on object monitor)
Nov 18 01:18:09 	at java.lang.Object.wait(Native Method)
Nov 18 01:18:09 	at java.lang.Object.wait(Object.java:502)
Nov 18 01:18:09 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
Nov 18 01:18:09 	- locked <0x00000000e01a7510> (a java.lang.Object)
Nov 18 01:18:09 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest$TestingContender.awaitGrantLeadership(ZooKeeperLeaderElectionConnectionHandlingTest.java:199)
Nov 18 01:18:09 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.runTestWithZooKeeperConnectionProblem(ZooKeeperLeaderElectionConnectionHandlingTest.java:147)
Nov 18 01:18:09 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.runTestWithLostZooKeeperConnection(ZooKeeperLeaderElectionConnectionHandlingTest.java:106)
Nov 18 01:18:09 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.testLoseLeadershipOnLostConnectionIfTolerateSuspendedConnectionsIsEnabled(ZooKeeperLeaderElectionConnectionHandlingTest.java:93)
Nov 18 01:18:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
{code}
;;;","05/Jan/23 09:50;zhuzh;The case seems to be the {{grantLeadership}} did not succeed in seconds after Flink connected to ZK.
Then the connection was lost later due to some unexpected reason. Usually it should be the test itself to stop the zk server and cause the connection loss. However, the case was stuck in an earlier code line and did not do this.

I'm not pretty sure whether this problem is related to FLINK-30484. 
cc [~mapohl];;;","05/Jan/23 10:05;mapohl;[~zhuzh] it's not the same: FLINK-30484 has a reoccurring pattern in the ZooKeeper server logs (which is symptomatic for CURATOR-645) as described in [this FLINK-30484 comment|https://issues.apache.org/jira/browse/FLINK-30484?focusedCommentId=17651651&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17651651]. In contrast, this issue provides the following ZK server logs:
{code:java}
[        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x1011510c1180001 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
[        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x1011510c1180002 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a{code}
I added the ZK Server logs for this test case to this Jira issue ([^zookeeper-server.FLINK-30108.log]);;;","06/Jan/23 04:56;zhuzh;Thanks for the input! [~mapohl]
I can see tens of thousands occurring of ""type:ping XXX txntype:unknown reqpath:n/a"" logs in the zk server of this problematic case, which never happened for other tests.

There are such kind of ZK client logs which does not happen in other cases

{code:java}
00:59:34,271 [main-SendThread(127.0.0.1:42967)] WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Client session timed out, have not heard from server in 1001ms for sessionid 0x0
{code}

What's unexpected here is that the leader election is pretty slow in this case. For other cases, it takes tens of milli-seconds to get leadership after connected to ZK. However, in this case, the process did not finish after several seconds. Another unexpected point is the leader election did not succeed after reconnected to ZK.

I have no idea of the root cause. But I suspect it is the session timeout which triggers this problem. The session timeout happens due to the small timeout (1000ms) of the case {{testLoseLeadershipOnLostConnectionIfTolerateSuspendedConnectionsIsEnabled}}.;;;","09/Jan/23 10:51;mapohl;-But that's odd - based on the file names, I would conclude that these logs are independent from each other. The ID in the filenames works as a primary key of the JUnit process, i.e. {{{}mvn-1.log{}}}, {{zookeeper-client-1.log}} and {{zookeeper-server-3.log}} should belong together. The client session time out logs you see are in {{zookeeper-client-1.log}} whereas the test failure occurred in {{{}mvn-3.log{}}}. The servier-side log snippet I shared was coming from {{{}zookeeper-server-3.log{}}}.-

I mixed up logs. Your comment is, indeed, correct with the log message appearing in {{zookeeper-client-3.log}};;;","09/Jan/23 12:57;mapohl;The following line at {{00:59:34,533}} in {{zookeeper-server-3.log}} might give us some indication:
{code}
00:59:34,533 [        SyncThread:0] WARN  org.apache.zookeeper.server.persistence.FileTxnLog           [] - fsync-ing the write ahead log in SyncThread:0 took 1261ms which will adversely effect operation latency. File size is 67108880 bytes. See the ZooKeeper troubleshooting guide
{code}

It looks like writing to disk was kind of slow and took longer (1261ms) than the negotiated timeout of 1000ms. My initial thought is to increase the minSessionTimeout to reduce the risk of running into this. But it's weird that we have this timeout of the application/test happening rather than observing a test failure;;;","09/Jan/23 15:32;mapohl;The test itself gets stuck in [contender.awaitGrantLeadership()|https://github.com/apache/flink/blob/c60eb0c3b4bf7dc045dd7a1da2080c7befebb8dc/flink-runtime/src/test/java/org/apache/flink/runtime/leaderelection/ZooKeeperLeaderElectionConnectionHandlingTest.java#L147] according to the [thread dump|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43277&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9944]. I still don't understand, why we don't pickup leadership anymore.

I extract the relevant logs from each of the files (zookeeper-server-3.log, zookeeper-client-3.log, mvn-3.log) and merged it all into one sorting it based on its timestamp to get a better understanding of what's happening when. I used the following command (for reproducibility):
{code}
$ cat <(cat zookeeper-server.FLINK-30108.log| xargs -I'{}' echo 'server # {}') <(cat zookeeper-client.FLINK-30108.log | xargs -I'{}' echo 'client # {}') <(cat mvn.FLINK-30108.log| xargs -I'{}' echo 'test   # {}') | sort -t'#' -k2,2
{code}
...but the resulting file {{all.F LINK-30108.log}} is also to attached archive. (some of the lines might be in wrong order, e.g. for the lines without timestamp or with the same timestamp, but it's good enough to get an understanding of what's going on).

I'm still haven't figured out what the last line at 00:59:45,201 means:
{code}
est   # 00:59:45,201 [ Curator-Framework-0] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.ProtectedMode [] - Session has changed during protected mode with ephemeral. old: 72362230717874177 new: 72362230717874178
{code}

[~zhuzh] can you get something out of it?;;;","10/Jan/23 08:46;zhuzh;In the test logs, I can see that

{code:java}
00:59:45,201 [ Curator-Framework-0] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.ProtectedMode [] - Deleted old session's found node: /flink/default/path/latch/_c_1ffe4ebf-99ce-490c-bfda-05cd462f5b7d-latch-0000000000
{code}

In the server logs, around ""00:59:45,203"", between several {{delete}} actions I can see some {{create2}} actions. And later there were no more {{create2}} actions. 
Not sure if there is a race condition of the deletion and creation actions.;;;","11/Jan/23 12:45;wangyang0918;It is a little strange that leaderLatch did not retry to create a new ephemeral node when state changed to RECONNECTED. Otherwise, the leader will be eventually elected. And this test should not block at {{{}TestingContender.awaitGrantLeadership{}}}.;;;","11/Jan/23 14:31;mapohl;yes, I thought the same. It might be some bug in curator. But I haven't had the chance to look into it more deeply, yet.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Unstable test ChangelogRecoveryITCase#testMaterialization failed on azure,FLINK-30107,13504745,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,21/Nov/22 04:33,11/Jan/23 10:38,04/Jun/24 20:41,11/Jan/23 10:38,1.16.0,,,,,,,,,,Runtime / Checkpointing,,,,0,test-stability,,,"
{noformat}

Nov 18 06:18:39 [ERROR] Errors: 
Nov 18 06:18:39 [ERROR] ChangelogRecoveryITCase.testMaterialization
Nov 18 06:18:39 [INFO]   Run 1: PASS
Nov 18 06:18:39 [ERROR]   Run 2: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0)
Nov 18 06:18:39 [INFO]   Run 3: PASS
Nov 18 06:18:39 [INFO] 
Nov 18 06:18:39 [INFO] 
{noformat}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43279&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,,,,,,,,,FLINK-28898,FLINK-28766,FLINK-30561,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 10:38:58 UTC 2023,,,,,,,,,,"0|z1ciwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 09:14;masteryhx;{code:java}
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit7849331111491208454/junit6019736675453245587/8e0bcf531bdfb49f9cc23855d307e0ea/dstl/aee1f583-55a5-4f33-88ba-b50a5bd86386 (No such file or directory) 
Nov 18 06:00:11 at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321) 
Nov 18 06:00:11 at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87) 
Nov 18 06:00:11 at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69) 
Nov 18 06:00:11 at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107) 
Nov 18 06:00:11 at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78) 
Nov 18 06:00:11 at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94) 
Nov 18 06:00:11 at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136) 
Nov 18 06:00:11 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336) 
Nov 18 06:00:11 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) 
Nov 18 06:00:11 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) 
Nov 18 06:00:11 ... 13 more 
Nov 18 06:00:11 Caused by: java.io.FileNotFoundException: /tmp/junit7849331111491208454/junit6019736675453245587/8e0bcf531bdfb49f9cc23855d307e0ea/dstl/aee1f583-55a5-4f33-88ba-b50a5bd86386 (No such file or directory) 
Nov 18 06:00:11 at java.io.FileInputStream.open0(Native Method) 
Nov 18 06:00:11 at java.io.FileInputStream.open(FileInputStream.java:195) 
Nov 18 06:00:11 at java.io.FileInputStream.<init>(FileInputStream.java:138) 
Nov 18 06:00:11 at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) 
Nov 18 06:00:11 at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) 
Nov 18 06:00:11 at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87) 
Nov 18 06:00:11 at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69) 
Nov 18 06:00:11 at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89) 
Nov 18 06:00:11 at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42) 
Nov 18 06:00:11 at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85) 
Nov 18 06:00:11 ... 21 more 
 {code};;;","05/Dec/22 09:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9793;;;","05/Dec/22 09:49;mapohl;Linking FLINK-28898 and FLINK-28766 here because all of them are caused by {{FileNotFoundException}}.;;;","08/Dec/22 11:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7789;;;","11/Jan/23 10:38;mapohl;Closing this issue in favor of FLINK-28440 after verifying that all test instabilities reported in this issue were caused by the {{FileNotFoundException}} causing the start of {{StreamOperatorStateContext}} to fail.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Build python wheels on macs failed due to install for crcmod error,FLINK-30106,13504744,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leonard,leonard,21/Nov/22 04:30,19/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,,,,API / Python,Build System / CI,,,0,auto-deprioritized-critical,test-stability,,"

{noformat}
        note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed building wheel for crcmod
        Running setup.py clean for crcmod
        error: subprocess-exited-with-error
      
        × python setup.py clean did not run successfully.
        │ exit code: 1
        ╰─> [14 lines of output]
            error: Multiple top-level packages discovered in a flat-layout: ['python3', 'python2'].
      
            To avoid accidental inclusion of unwanted files or directories,
            setuptools will not proceed with this build.
      
            If you are trying to create a single distribution with multiple packages
            on purpose, you should not rely on automatic discovery.
            Instead, consider the following options:
      
            1. set up custom discovery (`find` directive with `include` or `exclude`)
            2. use a `src-layout`
            3. explicitly set `py_modules` or `packages` with a list of names
      
            To find more information, look for ""package discovery"" on setuptools docs.
            [end of output]
      
        note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed cleaning build dir for crcmod
        Building wheel for dill (setup.py): started
        Building wheel for dill (setup.py): finished with status 'done'
        Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=9ce160b3c3e2e1dcd24e136c59fb84ef9bd072b4715c6b34536d3ff9c39a5962
{noformat}




https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43279&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:05 UTC 2023,,,,,,,,,,"0|z1ciw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46087&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=319;;;","14/Feb/23 10:45;mapohl;[~hxbks2ks] Any idea on that one?;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SubtaskExecutionAttemptAccumulatorsInfoTest failed with JVM exit code 239 ,FLINK-30105,13504734,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,renqs,renqs,21/Nov/22 03:47,24/Nov/22 08:20,04/Jun/24 20:41,24/Nov/22 08:20,1.17.0,,,,,,,,,,Runtime / REST,,,,0,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43124&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8164]
{code:java}
Nov 14 08:08:38 [ERROR] org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptAccumulatorsInfoTest
Nov 14 08:08:38 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 14 08:08:38 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter285048696362473297.jar /__w/1/s/flink-runtime/target/surefire 2022-11-14T08-03-00_124-jvmRun3 surefire3531696916359342131tmp surefire_26906836589585616499tmp
Nov 14 08:08:38 [ERROR] Error occurred in starting fork, check output in log
Nov 14 08:08:38 [ERROR] Process Exit Code: 239
Nov 14 08:08:38 [ERROR] Crashed tests:
Nov 14 08:08:38 [ERROR] org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptAccumulatorsInfoTest
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Nov 14 08:08:38 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Nov 14 08:08:38 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Nov 14 08:08:38 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Nov 14 08:08:38 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Nov 14 08:08:38 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
Nov 14 08:08:38 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Nov 14 08:08:38 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 14 08:08:38 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 14 08:08:38 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 14 08:08:38 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
Nov 14 08:08:38 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Nov 14 08:08:38 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Nov 14 08:08:38 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Nov 14 08:08:38 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Nov 14 08:08:38 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 14 08:08:38 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter285048696362473297.jar /__w/1/s/flink-runtime/target/surefire 2022-11-14T08-03-00_124-jvmRun3 surefire3531696916359342131tmp surefire_26906836589585616499tmp
Nov 14 08:08:38 [ERROR] Error occurred in starting fork, check output in log
Nov 14 08:08:38 [ERROR] Process Exit Code: 239
Nov 14 08:08:38 [ERROR] Crashed tests:
Nov 14 08:08:38 [ERROR] org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptAccumulatorsInfoTest
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Nov 14 08:08:38 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Nov 14 08:08:38 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 14 08:08:38 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Nov 14 08:08:38 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Nov 14 08:08:38 [ERROR] at java.lang.Thread.run(Thread.java:748)
Nov 14 08:08:38 [ERROR] -> [Help 1] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:20:10 UTC 2022,,,,,,,,,,"0|z1ciu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 09:52;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43252&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","24/Nov/22 08:20;mapohl;Closed in favor of FLINK-26037. I verified that all the builds failed due to a \{{NoClassDefFoundError}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot deployment failed due to could not transfer artifact org.apache.flink:flink-runtime-web:jar:1.15-20221119.010836-325 from/to apache.snapshots,FLINK-30104,13504699,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,leonard,leonard,21/Nov/22 03:29,21/Aug/23 09:00,04/Jun/24 20:41,21/Aug/23 09:00,1.15.2,,,,,,,,,,Build System / CI,,,,0,,,,"
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.8.2:deploy (default-deploy) on project flink-runtime-web: Failed to deploy artifacts: Could not transfer artifact org.apache.flink:flink-runtime-web:jar:1.15-20221119.010836-325 from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): Failed to transfer file: https://repository.apache.org/content/repositories/snapshots/org/apache/flink/flink-runtime-web/1.15-SNAPSHOT/flink-runtime-web-1.15-20221119.010836-325.jar. Return code is: 502, ReasonPhrase: Proxy Error. -> [Help 1]
{noformat}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43308&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31034,FLINK-31241,FLINK-25168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 09:00:28 UTC 2023,,,,,,,,,,"0|z1cim8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/23 09:00;mapohl;As pointed out in FLINK-25168, this is an infrastructure issue that's out-of-scope for Flink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test InputFormatCacheLoaderTest.checkCounter failed due to unexpected value on azure,FLINK-30103,13504696,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,21/Nov/22 03:19,23/Nov/22 05:19,04/Jun/24 20:41,23/Nov/22 05:19,1.16.0,1.17.0,,,,,,,,,Table SQL / Runtime,,,,0,test-stability,,,"
{noformat}
Nov 20 02:43:43 [ERROR] Failures: 
Nov 20 02:43:43 [ERROR]   InputFormatCacheLoaderTest.checkCounter:74 
Nov 20 02:43:43 Expecting AtomicInteger(0) to have value:
Nov 20 02:43:43   0
Nov 20 02:43:43 but did not
{noformat}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43319&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94
",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29405,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 05:19:21 UTC 2022,,,,,,,,,,"0|z1cilk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 04:24;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43279&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","22/Nov/22 08:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43167&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11163;;;","22/Nov/22 09:14;pnowojski;[~smiralex], are those two issues related? Error message is the same, but stack traces seem different.;;;","23/Nov/22 05:19;mapohl;I guess, [~pnowojski] is right. I went through the failed builds of FLINK-29405 and FLINK-30103. They are all failing for the same error. There is a slight diff between the stacktraces as Piotr pointed out which is neglectable.

I'm closing this issue in favor of FLINK-29405.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test EventTimeWindowCheckpointingITCase.testPreAggregatedSlidingTimeWindow failed runs on azure,FLINK-30102,13504695,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leonard,leonard,21/Nov/22 03:16,11/Jan/23 10:34,04/Jun/24 20:41,11/Jan/23 10:33,1.16.0,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,Tests,,0,test-stability,,,"
 
{noformat}
 Nov 20 06:26:32 [ERROR] Failures: 
Nov 20 06:26:32 [ERROR] EventTimeWindowCheckpointingITCase.testPreAggregatedSlidingTimeWindow
Nov 20 06:26:32 [INFO]   Run 1: PASS
Nov 20 06:26:32 [ERROR]   Run 2: Job execution failed.
Nov 20 06:26:32 [INFO]   Run 3: PASS
Nov 20 06:26:32 [INFO]   Run 4: PASS
Nov 20 06:26:32 [INFO]   Run 5: PASS
Nov 20 06:26:32 [INFO]   Run 6: PASS
Nov 20 06:26:32 [INFO]   Run 7: PASS
Nov 20 06:26:32 [INFO]   Run 8: PASS
Nov 20 06:26:32 [INFO]   Run 9: PASS
Nov 20 06:26:32 [INFO]   Run 10: PASS
{noformat}



https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43319&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10358",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 10:33:46 UTC 2023,,,,,,,,,,"0|z1cilc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 08:45;zhuzh;I can see exception below:

{code:java}
2022-11-20T06:01:49.7205648Z Caused by: java.io.FileNotFoundException: /tmp/junit8961509909544404950/junit8348284241715243278/3478dfeb7b5a21f68e56a89681b18358/dstl/cccb396e-3953-4aea-aba2-4e27e4c85a64 (No such file or directory)
2022-11-20T06:01:49.7206259Z 	at java.io.FileInputStream.open0(Native Method)
2022-11-20T06:01:49.7206663Z 	at java.io.FileInputStream.open(FileInputStream.java:195)
2022-11-20T06:01:49.7207114Z 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
2022-11-20T06:01:49.7207627Z 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
2022-11-20T06:01:49.7208181Z 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
2022-11-20T06:01:49.7208738Z 	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
2022-11-20T06:01:49.7209325Z 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
2022-11-20T06:01:49.7209999Z 	at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89)
2022-11-20T06:01:49.7210650Z 	at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
2022-11-20T06:01:49.7211325Z 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
2022-11-20T06:01:49.7211821Z 	... 21 more
{code}



Seems the root cause is the state file was missing. 
cc [~ym];;;","02/Jan/23 10:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44359&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8385;;;","05/Jan/23 08:31;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44441&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8288;;;","10/Jan/23 10:37;mapohl;This is also caused by the FileNotFoundException:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=9022;;;","10/Jan/23 15:21;mapohl;I verified that the four previously mentioned builds all have the following stacktrace as a cause:
{code}
[...]
Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_0a448493b4782967b150582570326227_(1/4) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
        ... 11 more
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit8961509909544404950/junit8348284241715243278/3478dfeb7b5a21f68e56a89681b18358/dstl/cccb396e-3953-4aea-aba2-4e27e4c85a64 (No such file or directory)
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
        at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94)
        at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        ... 13 more
Caused by: java.io.FileNotFoundException: /tmp/junit8961509909544404950/junit8348284241715243278/3478dfeb7b5a21f68e56a89681b18358/dstl/cccb396e-3953-4aea-aba2-4e27e4c85a64 (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
        at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
        at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
        at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
        at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89)
        at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
        ... 21 more
{code};;;","11/Jan/23 10:33;mapohl;Closing this issue as a duplicate of FLINK-28440 as all the reported builds fail due to a problem in starting the {{StreamOperatorStateContext}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Always use StandaloneClientHAServices to create RestClusterClient when retriving a Flink on YARN cluster client ,FLINK-30101,13504694,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,Zhanghao Chen,Zhanghao Chen,21/Nov/22 03:10,23/Nov/22 02:24,04/Jun/24 20:41,23/Nov/22 02:24,1.16.0,,,,,1.17.0,,,,,Client / Job Submission,,,,0,,,,"*Problem*

Currently, the procedure of retrieving a Flink on YARN cluster client is as follows (in YarnClusterDescriptor#retrieve method):
 # Get application report from YARN
 # Set rest.address & rest.port using the info from application report
 # Create a new RestClusterClient using the updated configuration, will use client HA serivce to fetch the rest.address & rest.port if HA is enabled

Here, we can see that the usage of client HA in step 3 is redundant, as we've already got the rest.address & rest.port from YARN application report. When ZK HA is enabled, this would take ~1.5 s to initialize client HA services and fetch the rest IP & port. 

1.5 s can mean a lot for latency-sensitive client operations.  In my company, we use Flink client to submit short-running session jobs and e2e latency is critical. The job submission time is around 10 s on average, and 1.5s would mean a 15% time saving. 

*Proposal*

When retrieving a Flink on YARN cluster client, use StandaloneClientHAServices to
create RestClusterClient instead as we have pre-fetched rest.address & rest.port from YARN application report. This is also what we did in KubernetesClusterDescriptor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 10:32:45 UTC 2022,,,,,,,,,,"0|z1cil4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 03:12;Zhanghao Chen;[~xtsong] Looking forwarding to your opinions on the proposal~ Much thanks;;;","22/Nov/22 02:17;xtsong;I'm not sure about the proposed changes. {{StandaloneClientHAServices}} and {{StandaloneLeaderRetrievalService}} assumes there's only one contender, which should always be the leader. There's no such guarantee when running a Yarn deployment. It is possible that the leadership changes after getting the application report, and ZK HA makes sure the rest client always connects to the latest leader address in such cases.

For short sql jobs, you may want to consider sql-gateway, which does not fetch leader address for every submitted job. Unfortunately, there's no such thing for DataStream / Table API jobs. Besides, you may also consider a non-HA cluster, if the end-to-end latency is cared mostly.;;;","22/Nov/22 10:32;Zhanghao Chen;Thanks, you made a point. I did not take the case where leadership changes after getting the application report into consideration.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the unused CheckpointFailureReason,FLINK-30100,13504618,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fanrui,fanrui,fanrui,20/Nov/22 13:32,25/Nov/22 13:33,04/Jun/24 20:41,25/Nov/22 13:13,1.16.0,,,,,1.17.0,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Some CheckpointFailureReasons have not been used for a long time and should be removed. E.g: 

TOO_MANY_CONCURRENT_CHECKPOINTS

CHECKPOINT_DECLINED_TASK_NOT_CHECKPOINTING

CHECKPOINT_DECLINED_ALIGNMENT_LIMIT_EXCEEDED

JOB_FAILURE",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 13:13:07 UTC 2022,,,,,,,,,,"0|z1ci4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 13:13;pnowojski;Merged to master as 5e0e0fde630..aa0cd873709;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests to cover model data APIs for all existing algorithms,FLINK-30099,13504194,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,lindong,lindong,20/Nov/22 02:43,10/Jan/23 04:08,04/Jun/24 20:41,10/Jan/23 04:08,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"test_linearsvc.py should be updated to cover the get_model_data() and set_model_data() usage. Same for other existing algorithms in Flink ML.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-20 02:43:52.0,,,,,,,,,,"0|z1cfig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update DynamoDb Sink unit tests to JUnit 5,FLINK-30098,13503955,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,19/Nov/22 23:42,21/Nov/22 16:17,04/Jun/24 20:41,21/Nov/22 16:17,,,,,,aws-connector-3.0.0,,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,"Use JUnit 5 in all unit tests for dynamoDB sink.

 

[1] [[DISCUSS] Conventions on assertions to use in tests|https://lists.apache.org/thread/33t7hz8w873p1bc5msppk65792z08rgt]

[2] [JUnit5 migration guide|https://docs.google.com/document/d/1514Wa_aNB9bJUen4xm5uiuXOooOJTtXqS_Jqk9KJitU/edit]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 16:17:25 UTC 2022,,,,,,,,,,"0|z1ce1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 16:17;dannycranmer;Merged commit [{{53ea410}}|https://github.com/apache/flink-connector-aws/commit/53ea41008910237073804dc090d67a1e0852163d] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CachedDataStream java example in the document is not correct,FLINK-30097,13503749,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xuannan,prabhujoseph,prabhujoseph,19/Nov/22 13:44,14/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Documentation,,,,0,pull-request-available,stale-assigned,,"CachedDataStream java example in the document is not correct - [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/#datastream-rarr-cacheddatastream]

 
{code:java}
DataStream<Integer> dataStream = //...
CachedDataStream<Integer> cachedDataStream = dataStream.cache();{code}

The example shows to invoke cache() on a DataStream instance but DataStream class does not have cache() method. The right usage is to call cache() on an instance of DataStreamSource/SideOutputDataStream/SingleOutputStreamOperator. 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 22:35:05 UTC 2023,,,,,,,,,,"0|z1ccrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename DynamoDB config destinationTableName to tableName,FLINK-30096,13503745,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,19/Nov/22 12:52,21/Nov/22 17:14,04/Jun/24 20:41,21/Nov/22 17:14,,,,,,aws-connector-3.0.0,,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,"The word destination is redundant since it is part of a DDB table sink.

 

Renaming destinationTableName to tableName in all places",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 17:14:46 UTC 2022,,,,,,,,,,"0|z1ccqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 17:14;dannycranmer;Merged commit [{{7dfebdd}}|https://github.com/apache/flink-connector-aws/commit/7dfebdd3bdcc5b2012ea912e633c2eb3f80fec33] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink's JobCluster ResourceManager should throw an exception when the failure number of starting worker reaches the maximum failure rate,FLINK-30095,13503737,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhanglu153,zhanglu153,19/Nov/22 09:38,19/Nov/22 09:48,04/Jun/24 20:41,,1.13.0,1.14.0,1.15.0,1.16.0,,,,,,,,,,,0,,,,"As shown in https://issues.apache.org/jira/browse/FLINK-10868，although resourcemanager.start-worker.max-failure-rate and resourcemanager.start-worker.retry-interval are set, in a worse case, when new start containers consistently fail, YarnResourceManager will goes into an infinite resource acquirement process without failing the job. Resources on Yarn are continuously occupied and released after a period of time, affecting other tasks.

It should be considered that when the failure number of starting worker reaches the maximum failure rate, Flink JobCluster ResourceManager will directly throw an exception instead of sending a new request to start new worker after a period of time. This task does not fail but is always in the running state. Users may not be aware that tasks occupy resources on yarn in a timely manner, which affects other tasks' failure to obtain resources on yarn.",,,,,,,,,,,FLINK-10868,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-19 09:38:17.0,,,,,,,,,,"0|z1ccow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table Sotre's Lookup Join document may needs left table DDL example.,FLINK-30094,13503726,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,StarBoy1005,StarBoy1005,StarBoy1005,19/Nov/22 05:21,21/Nov/22 02:13,04/Jun/24 20:41,21/Nov/22 02:13,1.16.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"In this “Lookup Join” character, official document don't give a DDL example about the left table. Maybe some first time users who using lookup join with right table which in table store will get some confusions about ""Processing Time Temporal Join"". ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 02:13:53 UTC 2022,,,,,,,,,,"0|z1ccmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 02:13;lzljs3620320;master: fe780cf70aea008fead49a14ac0b71cd403db97b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink SQL][Protobuf] CompileException when querying Kafka topic using google.protobuf.Timestamp ,FLINK-30093,13503687,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,laughingman7743,jamesmcguirepro,jamesmcguirepro,18/Nov/22 18:36,06/Jul/23 05:25,04/Jun/24 20:41,19/Jan/23 12:56,1.16.0,,,,,1.17.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,0,pull-request-available,,,"I am encountering an issue when trying to use Flink SQL to query a Kafka topic that uses {{{}google.protobuf.Timestamp{}}}.

 

When attempting to use Flink SQL to query a protobuf serialized Kafka topic that uses  {{{}google.protobuf.Timestamp{}}}, a {{org.codehaus.commons.compiler.CompileException: Line 23, Column 5: Cannot determine simple type name ""com"" }}error occurs when trying to query the table.

 

*Replication steps:*

1. Use a protobuf definition that contains a {{{}google.protobuf.Timestamp{}}}:
{noformat}
syntax = ""proto3"";
package example.message;

import ""google/protobuf/timestamp.proto"";

option java_package = ""com.example.message"";
option java_multiple_files = true;

message Test {
  int64 id = 1;
  google.protobuf.Timestamp created_at = 5;
}{noformat}
2. Use protobuf definition to produce message to topic

3. Confirm message is deserializable by protoc:
{code:java}
kcat -C -t development.example.message -b localhost:9092 -o -1 -e -q -D """" | protoc --decode=example.message.Test --proto_path=/Users/jamesmcguire/repos/flink-proto-example/schemas/ example/message/test.proto 
id: 123
created_at {
  seconds: 456
  nanos: 789
}{code}
4. Create table in Flink SQL using kafka connector and protobuf format
{code:java}
CREATE TABLE tests (
  id BIGINT,
  created_at row<seconds BIGINT, nanos INT>
)
COMMENT ''
WITH (
  'connector' = 'kafka',
  'format' = 'protobuf',
  'protobuf.message-class-name' = 'com.example.message.Test',
  'properties.auto.offset.reset' = 'earliest',
  'properties.bootstrap.servers' = 'host.docker.internal:9092',
  'properties.group.id' = 'test-1',
  'topic' = 'development.example.message'
);{code}
5. Run query in Flink SQL and encounter error:
{code:java}
Flink SQL> select * from tests;
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 23, Column 5: Cannot determine simple type name ""com"" {code}
{*}NOTE{*}: If you repeat steps 4-5 without {{created_at row<seconds BIGINT, nanos INT>}} in the table, step 5 will complete successfully.

6. Observe in attached log file, Flink appears to be using the incorrect namespace (should be {{google.protobuf.Timestamp):}}
{code:java}
com.example.message.Timestamp message3 = message0.getCreatedAt(); {code}",Mac OS Ventura,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32547,,,,,"18/Nov/22 18:36;jamesmcguirepro;taskmanager_172.22.0 (1).4_46291-40eec2_log;https://issues.apache.org/jira/secure/attachment/13052385/taskmanager_172.22.0+%281%29.4_46291-40eec2_log",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 04:37:15 UTC 2023,,,,,,,,,,"0|z1cce0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 19:02;rmetzger;[~maosuhan] Sorry for pinging you directly, could you take a look at this?;;;","21/Nov/22 02:50;maosuhan;[~rmetzger] Thanks for your findings!

I did not consider that user will use *google.protobuf.Timestamp* in the proto file. So importing definition is not supported in current flink version, but it definitely should be supported in the future.

`;;;","28/Nov/22 17:12;hdulay;I may have found the solution to this. Basically the generated code doesn't compile and the error message originating from codehaus.janino is a terrible one. The line below is the culprit

[https://github.com/apache/flink/blob/master/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/util/PbFormatUtils.java#L39] 

 

Changing it from:
{code:java}
return outerProtoName + descriptor.getName();{code}
{{to}}
{code:java}
return descriptor.getFile().getOptions().getJavaPackage() + ""."" + descriptor.getName();{code}
 

Seemed to do the trick but I'm still working through it.;;;","24/Dec/22 16:20;laughingman7743;[~hdulay] 

I am using Flink 1.14.x by porting the early version of the branch where the protobuf format feature was developed, and google.protobuf.Timestamp can be handled as row<seconds BIGINT, nanos INT> without problems.

https://github.com/apache/flink/pull/14376

Perhaps you mey need to make sure that multiple_files is set and outer classes are defined as in the early version.

https://github.com/maosuhan/flink/blob/888696df9507a0129c894b913b7828c836a95985/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/PbFormatUtils.java#L55-L90;;;","29/Dec/22 04:14;maosuhan;[~rmetzger]  Since this issue can be solved by the way [~laughingman7743]  suggested. Can we close this issue?;;;","29/Dec/22 04:28;hdulay;I think that would be best. Thanks


;;;","29/Dec/22 04:45;libenchao;[~maosuhan] If this is the suggested way to handle {{google.protobuf.Timestamp}} type, how about adding it to the document explicitly?;;;","29/Dec/22 13:28;laughingman7743;I am not sure if 1.16 can handle the google.protobuf.Timestamp type. Since I am still using an early implementation of this Protobuf formatter.
If it is recommended to set multiple_files and define an outer class, it would help a lot of people if there is a description in the documentation.

It would also be better if a unit test of type google.protobuf.Timestamp could be added. It is possible that the google.protobuf.Timestamp type may no longer be handled since the implementation seems to have been changed from earlier versions.;;;","02/Jan/23 10:19;rmetzger;I'm not a protobuf expert, so I can not decide what's best from a user experience perspective.

Using {{row<seconds BIGINT, nanos INT>}} sounds like a workaround to me? Wouldn't it be nicer if the Timestamp-type is handled out of the box by Flink? If you think this is an ok user experience, then I agree with Benchao, that we should at least document how how to use the timestamp type with Flink.;;;","03/Jan/23 18:08;jamesmcguirepro;Using  {{row<seconds BIGINT, nanos INT>}} did not work with 1.16.0 for me.   If you review the ticket, you can see that is how I had the table schema defined.

 

+1 on updating the documentation no matter what.;;;","04/Jan/23 06:30;maosuhan;[~jamesmcguirepro] Thanks for your feedback, I'll update the documentation and try to find a better way to support this requirement.;;;","04/Jan/23 14:24;laughingman7743;Unfortunately, Flink 1.16 can no longer handle the google.protobuf.Timestamp type. I have added a test case for the google.protobuf.Timestamp type to confirm this.
https://github.com/laughingman7743/flink/pull/1/files

{code:java}
org.apache.flink.formats.protobuf.PbCodegenException: org.apache.flink.api.common.InvalidProgramException: Program cannot be compiled. This is a bug. Please file an issue.

	at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:126)
	at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.open(PbRowDataDeserializationSchema.java:64)
	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:118)
	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:102)
	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:97)
	at org.apache.flink.formats.protobuf.TimestampToRowTest.testSimple(TimestampToRowTest.java:22)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.api.common.InvalidProgramException: Program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.formats.protobuf.util.PbCodegenUtils.compileClass(PbCodegenUtils.java:265)
	at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:118)
	... 32 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 19, Column 5: Cannot determine simple type name ""org""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
	at org.codehaus.janino.UnitCompiler.getLocalVariable(UnitCompiler.java:2616)
	at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3662)
	at org.codehaus.janino.UnitCompiler.access$5800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3545)
	at org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3513)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3522)
	at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3512)
	at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3501)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3322)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.formats.protobuf.util.PbCodegenUtils.compileClass(PbCodegenUtils.java:262)
	... 33 more
{code}

I will try to check the difference of the implementation in Flink 1.16 from the earlier version.;;;","04/Jan/23 17:39;laughingman7743;I fixed it in the following branch.
https://github.com/laughingman7743/flink/pull/2
https://github.com/laughingman7743/flink/pull/2/commits/730b374b1056b5af9f39f21e2f83e8371e044948
Regardless of the user-defined protobuf option, the google.protobuf.Timestamp type always has multiple_files=true, in which case the best choice is to use the descriptor getJavaPackage or getPackage as the full name of the java class.
In other cases, it is best to use outerProtoName. A simple test has confirmed that this is not a problem.;;;","05/Jan/23 07:30;laughingman7743;I have investigated the Flink 1.16 implementation in detail.
The current implementation does not take into consideration the case where another package is imported and used for a message, so it seems that the way getOuterProtoPrefix is determined needs to be improved.

I made a fix in the following branch to try it out.
https://github.com/laughingman7743/flink/pull/3
https://github.com/laughingman7743/flink/pull/3/commits/f8ad68c401279b5911687473940918465d797692

It also takes into consideration cases where the multiple_files and java_outer_classname options are not specified and the suffix OuterClass is added when the file name and message name are the same.
This should probably cover most cases of protobuf definitions.

If this change is acceptable I will create a pull request in the Flink main repository.;;;","07/Jan/23 02:51;maosuhan;[~laughingman7743] Thanks for your code. It looks to me. You can go ahead and I will review the code when you're done.

I think the core change is to use a new getOuterProtoPrefix method which handle different cases. I appreciate that you also consider OuterClass suffix in a special case.  Maybe the reason it works in older version is that I wrote more codes to handle different options but deleted later for simplicity getting outer prefix name. But in fact this is an issue after probobuf is released, I think we should improve this logic.;;;","07/Jan/23 05:42;laughingman7743;[~maosuhan]
I have created a pull request in the Flink main repository.
https://github.com/apache/flink/pull/21613
Please take a look at it when you have time.;;;","19/Jan/23 12:56;libenchao;Fixed via https://github.com/apache/flink/commit/7ea4476c0544e17798cbb1e39609827954f6c266 (master)

[~laughingman7743] Thanks for your PR, and [~maosuhan] thanks for the review.;;;","09/Jun/23 16:17;baugarten;Hey, thanks for the contribution. I'm running into this same issue on Flink 1.16 and would like to backport the fix to the release-1.16 branch for a future patch release. I think this is a good candidate for backporting, and the commit against master applied cleanly to the release-1.16 branch.

I opened up [this PR |https://github.com/apache/flink/pull/22752] based on the original PR: https://github.com/apache/flink/pull/21613

;;;","10/Jun/23 02:28;libenchao;[~baugarten] Thanks for your work! 

However, I think this is more like a feature instead of bug, that's why we only merged it to ""master"" branch.;;;","15/Jun/23 19:49;baugarten;Thanks [~libenchao], at first I thought that this ticket might have been considered a bug.

I couldn't find guidance in the [contributing|https://flink.apache.org/how-to-contribute/contribute-code/] [guidelines|https://cwiki.apache.org/confluence/display/FLINK/Apache+Flink+Home] on what changes are eligible for backporting to patch releases. In the past, I know some features (ie HybridSource) ended up getting backported to a previous release;;;","16/Jun/23 02:30;libenchao;Usually the ""bugfix"" which fixes the functionality which is advertised to users by documentation/release note, is a good candidate to back port. For this one, in the original implementation, {{Timestamp}} is not considered as a supported type, and it's not tested/documented, hence supporting {{Timestamp}} is more like an improvement to me. What do you guys think?;;;","16/Jun/23 04:37;laughingman7743;I would like to see this change backported to 1.16, because without applying this change, we will not be able to use the imported portobuf definitions as well as google.protobuf.Timestamp.;;;",,,,,,,,,
Improve Table API experience for Flink DOUBLE type,FLINK-30092,13503618,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,18/Nov/22 16:13,19/Nov/22 11:06,04/Jun/24 20:41,,,,,,,,,,,,Connectors / DynamoDB,,,,0,,,,"Currently, the DynamoDB sink's Table API allows users to input records with Flink type of `DOUBLE`. However, the limitations on Double on Flink/Java side [1] is more permissive than DynamoDB's limitations on the Number type [2].

 

Java Double limitations:

MIN_VALUE: −1.7976931348623157×10{^}308{^} 

MAX_VALUE:  1.7976931348623157×10{^}308{^}

DynamoDB number type limitations:

MIN_VALUE: -9.9999999999999999999999999999999999999E+125 to -1E-130

MAX_VALUE: 1E-130 to 9.9999999999999999999999999999999999999E+125

 

This means that a user can write values of type `DOUBLE` into the DDB sink, as long as they are within the limitations of DynamoDB. Once they exceed the limitation, the record will fail to write.

 

We should consider improving this experience by adding a configuration to allow users to map DOUBLE to a specified DDB type (e.g. DDB String, DDB number) instead. We will be able to do this easily once FLINK-29267 is implemented.

 

 

 [1] [https://docs.oracle.com/javase/7/docs/api/java/lang/Double.html#MAX_VALUE]

 [2] [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.NamingRulesDataTypes.html#:~:text=Boolean%2C%20and%20null.-,Number,-Numbers%20can%20be] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29267,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 19 11:05:27 UTC 2022,,,,,,,,,,"0|z1cbyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 22:02;martijnvisser;[~liangtl] I think this is directly related to FLINK-29267;;;","19/Nov/22 11:05;liangtl;[~martijnvisser], indeed! Thanks for pointing that out. That solution is a more generic way forwards for other connectors as well. Updated the overview to point to using that framework when it has been implemented;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"mvn package verify fails with ""Error creating shaded jar: duplicate entry""",FLINK-30091,13503616,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Gerrrr,Gerrrr,Gerrrr,18/Nov/22 16:00,21/Nov/22 07:33,04/Jun/24 20:41,21/Nov/22 07:33,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"{{mvn package verify}} fails with

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.1.1:shade (shade-flink) on project flink-table-store-dist: Error creating shaded jar: duplicate entry: META-INF/services/org.apache.flink.table.store.shaded.org.apache.kafka.common.config.provider.ConfigProvider -> [Help 1]
{noformat}

The fix is to update {{maven-shade-plugin}} version to 3.4.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 07:33:06 UTC 2022,,,,,,,,,,"0|z1cby8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 16:19;Gerrrr;PR - [https://github.com/apache/flink-table-store/pull/391]

 ;;;","21/Nov/22 07:33;lzljs3620320;master: 0a31766290c36a0c9033da3850146c684a90468d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support timespan for TimerGauges,FLINK-30090,13503610,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,18/Nov/22 15:14,29/Nov/22 18:45,04/Jun/24 20:41,29/Nov/22 18:45,,,,,,1.17.0,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"Unlike MeterViews, TimerGauges currently do not support timespan settings and fix the timestamp on the ViewUpdater interval which is hardcoded to 5 seconds.

This means that all metrics that use TimerGauge like backpressure, idle, busytime etc. always show the last 5 second information.

We should support timespan simiar to meterview with the same 60sec default to make these metrics more stable and useful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 18:45:40 UTC 2022,,,,,,,,,,"0|z1cbww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 18:45;gyfora;merged to master a7fdab8b23cddf568fa32ee7eb804d7c3eb23a35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove dependency promotion from kinesis connector,FLINK-30089,13503600,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Nov/22 13:48,21/Nov/22 09:38,04/Jun/24 20:41,21/Nov/22 09:38,,,,,,1.17.0,,,,,Build System,Connectors / Kinesis,,,0,pull-request-available,,,"The shade-plugin in the kinesis connector is configured to promote transitive dependencies.
This adds a special case in our shading setup, breaks the dependency tree structure (since _all_ transitive dependencies are moved to the top) and it makes the sql-kinesis packaging overly complicated.

Get rid of the dependency promotion and explicitly depend on anything that we pull in transitively but don't shade in the kinesis connector itself.",,,,,,,,,,,,,,FLINK-28016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 09:38:15 UTC 2022,,,,,,,,,,"0|z1cbuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 09:38;chesnay;master: 8680caea9ad111c264aadf303c46b396956590f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excessive state updates for TtlMapState and TtlListState,FLINK-30088,13503595,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,rovboyko,rovboyko,rovboyko,18/Nov/22 13:28,01/Apr/24 10:04,04/Jun/24 20:41,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"After merging the FLINK-21413 every ttl check for cleanup for TtlMapState and TtlListState (even without expired elements) leads to whole state update.

This is because:

- comparison by link inside `TtlIncrementalCleanup`:

!image-2022-11-18-20-25-14-466.png|width=450,height=288!

- and creating new map or list inside TtlMapState or TtlListState:

!image-2022-11-18-20-27-24-054.png|width=477,height=365!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 13:25;rovboyko;image-2022-11-18-20-25-14-466.png;https://issues.apache.org/jira/secure/attachment/13052383/image-2022-11-18-20-25-14-466.png","18/Nov/22 13:27;rovboyko;image-2022-11-18-20-27-24-054.png;https://issues.apache.org/jira/secure/attachment/13052382/image-2022-11-18-20-27-24-054.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 10:04:04 UTC 2024,,,,,,,,,,"0|z1cbtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 13:45;rovboyko;The fix is ready - [https://github.com/rovboyko/flink/commit/cf8b7c1de5e3dfbe83106a4de89423e17e36e50e]

Waiting for task to be approved and assigned to me. Thanks.;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","01/Apr/24 10:04;rovboyko;I've updated the PR ([https://github.com/apache/flink/pull/21406]) to optimize only TtlMapState and added benchmark results there. 

The total performance improvement is above 17%.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql-gateway-api is bundled twice in the distribution,FLINK-30087,13503590,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,18/Nov/22 12:52,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,1.20.0,,,,,Build System,Table SQL / Gateway,,,0,,,,"The sql-gateway-api is bundled both in flink-sql-gateway (put into opt/) and flink-table-uber (put into lib/).
It probably(?) shouldn't be part of flink-table-uber.

In any case there's no reason to bundle it twice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 03:55:53 UTC 2023,,,,,,,,,,"0|z1cbsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 08:54;Wencong Liu;Hello [~chesnay] , it the module flink-table-uber named ""flink-table-api-java-uber"" ?;;;","12/Jan/23 14:07;chesnay;[~Wencong Liu] I think that's the jar I'm referring to.;;;","13/Jan/23 03:53;fsk119;Hi, [~chesnay] . The sql-gateway-api belongs to the api, so I add it into the table-api-java-uber. But I think you are right, we should not bundle it twice. I need some time to estimate whether it influences hive-connector if we remove it from the uber jar. ;;;","13/Jan/23 03:55;fsk119;Because the code froze in on the deadline and the Chinese Festival is coming, can we do this later?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Include classifier in DependencyTree tree,FLINK-30086,13503545,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Nov/22 10:57,21/Nov/22 11:01,04/Jun/24 20:41,21/Nov/22 11:01,,,,,,1.17.0,,,,,Build System / CI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 11:01:42 UTC 2022,,,,,,,,,,"0|z1cbig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 11:01;chesnay;master: 4ad7a955d58db01a502abded0871a4564609cffc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVM hanging after executing YARNSessionCapacitySchedulerITCase.testNonexistingQueueWARNmessage,FLINK-30085,13503457,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,18/Nov/22 08:51,24/Nov/22 14:41,04/Jun/24 20:41,21/Nov/22 14:20,1.17.0,,,,,1.17.0,,,,,Deployment / YARN,Tests,,,0,pull-request-available,,,After some analysis I've concluded the following: the test submitted an app into a wrong queue so fails. At submit time YARN added a shutdown hook named DeploymentFailureHook. The mentioned hook is executed after the YARN cluster is down and tries to kill the application. The kill call is hanging for 15 minutes blocking the JVM to shut down.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 14:20:48 UTC 2022,,,,,,,,,,"0|z1cayw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 08:52;gaborgsomogyi;Honestly I've no idea how this can work in CI. I'm pretty sure this can cause CI timeouts though.;;;","18/Nov/22 09:55;gaborgsomogyi;After some in-depth look we've found that after 15 minutes it stops. Still we think wasting 15 minutes per failed YARN test suite is huge so lowering down the YARN timout for all the YARN tests.;;;","21/Nov/22 14:20;chesnay;master: 0634d0cc0a401d268cc1b3788c8ee63a91ff33a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused ResourceActions.notifyAllocationFailure,FLINK-30084,13503402,13352415,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,18/Nov/22 08:23,05/Jan/23 01:54,04/Jun/24 20:41,28/Nov/22 09:06,,,,,,1.17.0,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"as commented in  https://github.com/apache/flink/pull/21233

ResourceActions.notifyAllocationFailure is not used, these related API should be removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 09:06:47 UTC 2022,,,,,,,,,,"0|z1camo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 09:06;xtsong;master (1.17): c79b60cb115f11bffa27e7a7ec91f568cabb8db4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump maven-shade-plugin to 3.4.1,FLINK-30083,13503396,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,18/Nov/22 08:21,16/Jan/23 13:22,04/Jun/24 20:41,16/Jan/23 13:06,1.17.0,,,,,1.17.0,,,,,Build System,,,,0,pull-request-available,,,"FLINK-24273 proposes to relocate the io.fabric8 dependencies of flink-kubernetes.
This is not possible because of a problem with the maven shade plugin (""mvn install"" doesn't work, it needs to be ""mvn clean install"").
MSHADE-425 solves this issue, and has been released with maven-shade-plugin 3.4.0.

Upgrading the shade plugin will solve the problem, unblocking the K8s relocation.",,,,,,,,,,,,FLINK-30286,FLINK-30350,FLINK-30380,,,,,,,,FLINK-20092,,,,,,,,,,,,,,FLINK-20092,MSHADE-434,,FLINK-30285,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 13:06:30 UTC 2023,,,,,,,,,,"0|z1calc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 13:06;chesnay;master: 0c4fdb6cb563ba50fe83af491e872e3aa5ce455c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable write-buffer-spillable by default only for object storage,FLINK-30082,13503368,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,18/Nov/22 06:38,22/Nov/22 02:53,04/Jun/24 20:41,22/Nov/22 02:53,,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"After a lot of tests, it is found that the participation of spillable does not improve HDFS greatly, but will bring some jitters.
In this jira, spillable is enabled only when the object is stored by default, so that the performance can be improved without affecting hdfs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 02:53:45 UTC 2022,,,,,,,,,,"0|z1caf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 02:53;lzljs3620320;master: 039f2ff5d2e25154b5ab4b04f4c36086ce51d9c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local executor can not accept different jvm-overhead.min/max values,FLINK-30081,13503365,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuml07,liuml07,liuml07,18/Nov/22 06:30,16/Apr/24 06:25,04/Jun/24 20:41,16/Apr/24 06:25,1.16.0,,,,,1.20.0,,,,,Runtime / Configuration,,,,0,auto-deprioritized-major,pull-request-available,,"In local executor, it's not possible to set different values for {{taskmanager.memory.jvm-overhead.max}} and {{{}taskmanager.memory.jvm-overhead.min{}}}. The same problem for {{taskmanager.memory.network.max}} and {{{}taskmanager.memory.network.min{}}}.

Sample code to reproduce:
{code:java}
Configuration conf = new Configuration();
conf.setString(TaskManagerOptions.JVM_OVERHEAD_MIN.key(), ""1GB"");
conf.setString(TaskManagerOptions.JVM_OVERHEAD_MAX.key(), ""2GB"");

StreamExecutionEnvironment.createLocalEnvironment(conf)
    .fromElements(""Hello"", ""World"")
    .executeAndCollect()
    .forEachRemaining(System.out::println); {code}
The failing exception is something like:
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException
  at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
  at org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils.calculateTotalProcessMemoryFromComponents(TaskExecutorResourceUtils.java:182)
  at org.apache.flink.runtime.taskexecutor.TaskExecutorMemoryConfiguration.create(TaskExecutorMemoryConfiguration.java:119)
{code}
I think the problem was that we expect the max and min to equal, but local executor did not reset them correctly?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 06:25:05 UTC 2024,,,,,,,,,,"0|z1caeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 08:27;gaoyunhaii;Hi [~liuml07]  the mini-cluster does not start new processes, it just executes different components in different threads. Thus I think it do not have chance to change memory settings, and you might increase the memory of the tests on startup directly. ;;;","01/Dec/22 18:17;liuml07;Thanks [~gaoyunhaii] It makes perfect sense to me now those options are not honored by the mini-cluster.

But it is still not ideal to fail the local job instead of just ignoring those options. In my day job, many users develop, test and debug locally based on the same configurations. They override only if required (e.g. parallelism). We pass the configurations via Spring Boot environments, but I guess using flink-conf.yaml file would have similar issues. Suppose the base flink-conf.yaml file has different min/max values, it is a burden for users to override those config values for local testing which turns out to be ignored.

We can either relax the check in the configuration, or just remove them from configuration in mini-cluster environment. Thoughts?;;;","08/Dec/22 08:38;xtsong;[~liuml07] [~gaoyunhaii],

I think this is indeed a nice improvement. We have a unified calculation logic which is used in many different places (RM, TM, etc.). These components are not aware of whether they are running in a MiniCluster or not. Thus, to avoid potential configuration errors, we set the memory configurations that are not supported by MiniCluster to some safe values, if they are not configured, in `TaskExecutorResourceUtils#adjustForLocalExecution`. Here we probably should override the configurations anyway, regardless they are set or not.

WDYT? And [~liuml07] would you like to work on this?;;;","08/Dec/22 11:14;chesnay;I think it's fine to fail in this case.

The user told us to adhere to these memory limits. This isn't possible, so we fail.

IMO we should interpret this either as
a) the user didn't intend to set these values but did anyway, in which case they should stop setting them,
b) the user set these value believing they'd have an effect, in which case we should inform the user that they don't,
c) the user set these value believing the environment they run in provides this many resources, in which case they also want to be informed about that.;;;","09/Dec/22 05:16;liuml07;[~xtsong] Thanks for providing more context. I'd love to provide a patch by improving {{adjustForLocalExecution}} method. The pointer here is very helpful!

[~chesnay] I think it is an option to just fail the MiniCluster. However, if we choose to fail the MiniCluster, it's inconsistent to just fail when the min and max value do not match. For example, if you modify the sample code above and set the min/max to the exactly same value, it will not fail or throw exceptions.

In the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#local-execution], we have listed all supported memory settings for local execution. I'd assume they would check that before tuning the parameters for local mode if they intend to do that. However, in my daily job, I don't met users who intend to test memory setting for local mode. But some of them still see the problem because their job inherit the default configs that are tuned for production - either via config files or Spring Boot properties. If the inherited min/max settings differ, the local job fails; if the min/max are the same, the job just runs locally. That makes me think MiniCluster failing would be intrusive and I prefer an ERROR logging and just ignore as suggested above. Thoughts? Thanks!;;;","08/May/23 21:56;liuml07;I checked again and found we have already been logging the unused memory configuration options at WARN level. However there is a small bug in the logging message. We also adjust memory configuration options for local execution, but for JVM overhead min/max, the logic is just for missing configuration options. I filed [PR #22547|https://github.com/apache/flink/pull/22547] to improve logging message and reset those unused options so local execution will not fail. This essentially takes [~xtsong]'s suggestion. I added a unit test and the same snippet in the Jira summary now also pass.;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","16/Apr/24 06:25;xtsong;master (1.20): 9cc5ab9caf368ef336599e7d48f679c8c9750f49;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce public programming api and dependency jar for table store,FLINK-30080,13503363,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,18/Nov/22 06:26,19/Mar/23 05:42,04/Jun/24 20:41,19/Mar/23 05:42,,,,,,,,,,,Table Store,,,,0,,,,"Users need to access tablestore through programming interfaces, but do not want to use a computing engine such as Flink or Spark.
We can expose the programming api to read and write the tablestore, and also need to expose the corresponding dependency jar. Note that this dependency may not conflict with multiple versions of Flink, which is conducive to the integration of third-party systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30031,FLINK-30276,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-18 06:26:15.0,,,,,,,,,,"0|z1cae0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop using deprecated TM options in doc,FLINK-30079,13503361,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,18/Nov/22 06:04,21/Nov/22 09:36,04/Jun/24 20:41,21/Nov/22 09:36,1.16.0,,,,,1.17.0,,,,,Documentation,,,,0,pull-request-available,,,"The option {{ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY}} was deprecated and configuring it should have no effect now. However, in the [documentation|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/dataset/local_execution/#local-environment] we still reference it and show in example code. This can be replaced with {{TaskManagerOptions.MANAGED_MEMORY_FRACTION}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 09:36:54 UTC 2022,,,,,,,,,,"0|z1cadk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 09:36;chesnay;master: 8d17a84123189cf3851aff85231cdb480adf34dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temporal join should finish when the left table is bounded and finished,FLINK-30078,13503343,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,18/Nov/22 03:27,18/Nov/22 03:27,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"Currently, temporal join with a bounded left table and an unbounded right table keeps running even when the left table is finished.

From the user's perspective, when the left table is finished, the result of the temporal join is finalized so the temporal join should finish and the job may also finish depending on the job topology.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-18 03:27:01.0,,,,,,,,,,"0|z1caa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
k8s jobmanager pod repeated restart,FLINK-30077,13503325,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,baibaiwuchang,baibaiwuchang,18/Nov/22 03:12,14/Mar/23 09:02,04/Jun/24 20:41,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,"We use Flink K8S. When task exist bug, jobmanager pod repeated start.

For example：

xxxx-88b95598d-rlzxg                  0/1     CrashLoopBackOff   215        19h

 

then, i learned that k8s deployment could only set ""restartPolicy:Always"". 

([https://github.com/kubernetes/kubernetes/issues/24725)]

When  jobmanager always restart, I don't think that's understandable。

Flink kuberne moudle watch taskmanager pod. We always watch jobmanager pod status and flink cancel deployment In my particular situation.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 09:02:41 UTC 2023,,,,,,,,,,"0|z1ca60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 09:02;Wencong Liu;Hello [~baibaiwuchang] . Thanks for your proposal! Actually I can't understand your statement completely. Could you please illustrate this sentence in detail ?
{code:java}
Flink kuberne moudle watch taskmanager pod. We always watch jobmanager pod status and flink cancel deployment In my particular situation. {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive join mysql error,FLINK-30076,13503323,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kcz,kcz,18/Nov/22 02:59,19/Nov/22 05:58,04/Jun/24 20:41,,1.15.2,1.16.0,,,,,,,,,Connectors / Hive,Table SQL / Runtime,,,0,,,,"There is no problem when reading hive table parquet alone, but after doing join with MySQL table, the above error appears, but there is no error when hive format is ORC.

 

this is my hive ddl.use orc is ok,but parquet error.

create table national_standard
(

message_time timestamp comment 'times',
vin string comment 'vin',
data string
) PARTITIONED BY (date_id STRING)
STORED AS parquet
TBLPROPERTIES (
'parquet.compression' = 'snappy',
'partition.time-extractor.timestamp-pattern' = '$date_id',
'sink.partition-commit.trigger' = 'partition-time',
'sink.partition-commit.delay' = '10 min',
'sink.partition-commit.watermark-time-zone' = 'Asia/Shanghai',
'sink.partition-commit.policy.kind' = 'metastore,success-file'
);

 

 

org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247) at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240) at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477) at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225) at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169) at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130) at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385) at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ... 1 more Caused by: java.lang.IllegalStateException: All of the offsets in the split should be found in the file. expected: [2234158329] found: [] at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:171) at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140) at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:95) at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:60) at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:93) at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.<init>(HiveMapredSplitReader.java:116) at org.apache.flink.connectors.hive.read.HiveInputFormat$HiveReader.<init>(HiveInputFormat.java:326) at org.apache.flink.connectors.hive.read.HiveInputFormat$HiveReader.<init>(HiveInputFormat.java:305) at org.apache.flink.connectors.hive.read.HiveInputFormat$HiveMapRedBulkFormat.createReader(HiveInputFormat.java:282) at org.apache.flink.connectors.hive.read.HiveInputFormat$HiveMapRedBulkFormat.createReader(HiveInputFormat.java:275) at org.apache.flink.connectors.hive.read.HiveInputFormat.createReader(HiveInputFormat.java:110) at org.apache.flink.connectors.hive.read.HiveInputFormat.createReader(HiveInputFormat.java:65) at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:112) at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:65) at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)","flink:1.15.2 1.16.0

hive:3.1.0

mysql:5.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 08:57:00 UTC 2022,,,,,,,,,,"0|z1ca5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 07:26;luoyuxia;[~kcz] Thanks for reporting.

I try to reproduce your problem, but can't reproduce it in my local env.

But I have the feelings that it may be a bug of parquet like something of PARQUET-2078

And in Flink 1.15, we upgrade parquet to 1.12.2. Maybe there still are some other bugs in 1.12.2.

Could you please try with Flink 1.14 or downgrade  the parquet version to 1.11.1 to build Flink and then to see whether this problem still exist?

 ;;;","18/Nov/22 08:57;kcz;use flink-1.14.4 not ok ,Same mistake;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to load data to the cache after the hive lookup join task is restarted,FLINK-30075,13503320,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhanglu153,zhanglu153,18/Nov/22 02:54,08/Dec/22 13:12,04/Jun/24 20:41,,1.12.2,,,,,,,,,,Connectors / Hive,,,,0,,,,"When I test kafka and hive lookup join, only the connection between tm and zookeeper is disconnected after the data is successfully loaded into the cache. In this case, the task will be restarted because the flink task restart policy is configured (note that the task process does not change at this time, and the task restarts in the same container).

The task sql is as follows:
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);
tableEnv.executeSql(""CREATE TABLE kafka_table_1 (\n"" +
""  a string,\n"" +
""  b int,\n"" +
""  c string,\n"" +
""  log_ts TIMESTAMP(0),\n"" +
"") WITH (\n"" +
""    'connector' = 'kafka',\n"" +
""    'topic' = 'test',\n"" +
""    'scan.startup.mode' = 'latest-offset',\n"" +
""    'properties.bootstrap.servers' = '172.16.144.208:9092',\n"" +
""    'format' = 'csv'\n"" +
"")"");
String name = ""hive_catalog"";
String hiveConfDir = ""/cloud/service/flink/conf"";
HiveCatalog hive = new HiveCatalog(name, null, hiveConfDir);
hive.open();
tableEnv.registerCatalog(name, hive);
tableEnv.useCatalog(name);
tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);
tableEnv.executeSql(""create table if not exists ttt(a string,b int,c string)"");
tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);
tableEnv.executeSql(""select * from default_catalog.default_database.kafka_table_1 as u join ttt for system_time as of u.proctime as o on u.a = o.a"").print();{code}
After the task is successfully restarted, kafka produces data. When some data comes in, an error message is displayed indicating that loading data into the cache fails. The error message is as follows:
{code:java}
2022-11-17 14:09:16,041 INFO  org.apache.flink.table.filesystem.FileSystemLookupFunction   [] - Populating lookup join cache
2022-11-17 14:09:16,051 INFO  org.apache.hadoop.conf.Configuration                         [] - getProps 1230612033
2022-11-17 14:09:16,053 INFO  org.apache.hadoop.io.retry.RetryInvocationHandler            [] - java.io.IOException: org.apache.flink.shaded.hadoop2.com.google.protobuf.ServiceException: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'., while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over hdp-hadoop-hdp-namenode-1.hdp-hadoop-hdp-namenode.hdp-dev-env-3.svc.cluster.local/192.168.6.85:9000. Trying to failover immediately.
2022-11-17 14:09:16,053 DEBUG org.apache.hadoop.ipc.Client                                 [] - The ping interval is 60000 ms.
2022-11-17 14:09:16,053 DEBUG org.apache.hadoop.ipc.Client                                 [] - Connecting to hdp-hadoop-hdp-namenode-0.hdp-hadoop-hdp-namenode.hdp-dev-env-3.svc.cluster.local/192.168.0.8:9000
2022-11-17 14:09:16,055 DEBUG org.apache.hadoop.security.UserGroupInformation              [] - PrivilegedAction as:hadoop/hdp-flinkhistory-hdp-flink-history-6754db4f9c-8b8nd.hdp-flinkhistory-hdp-flink-history.hdp-dev-env-3.svc.cluster.local@DAHUA.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:825)
2022-11-17 14:09:16,056 DEBUG org.apache.hadoop.security.SaslRpcClient                     [] - Sending sasl message state: NEGOTIATE

2022-11-17 14:09:16,057 DEBUG org.apache.hadoop.security.SaslRpcClient                     [] - Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2022-11-17 14:09:16,057 INFO  org.apache.hadoop.security.SaslRpcClient                     [] - getServerPrincipal 1230612033
2022-11-17 14:09:16,057 DEBUG org.apache.hadoop.security.SaslRpcClient                     [] - Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
2022-11-17 14:09:16,059 DEBUG org.apache.hadoop.security.SaslRpcClient                     [] - getting serverKey: dfs.namenode.kerberos.principal conf value: null principal: null
2022-11-17 14:09:16,059 DEBUG org.apache.hadoop.ipc.Client                                 [] - closing ipc connection to hdp-hadoop-hdp-namenode-0.hdp-hadoop-hdp-namenode.hdp-dev-env-3.svc.cluster.local/192.168.0.8:9000: Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name
java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:891) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:423) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1615) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client.call(Client.java:1446) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client.call(Client.java:1399) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at com.sun.proxy.$Proxy27.getFileInfo(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:800) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at com.sun.proxy.$Proxy28.getFileInfo(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1673) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1524) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1521) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1536) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1632) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.flink.connectors.hive.read.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:334) ~[flink-connector-hive_2.11-1.12.2-HDP-22.10.28.jar:1.12.2-HDP-22.10.28]
	at org.apache.flink.connectors.hive.read.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:318) ~[flink-connector-hive_2.11-1.12.2-HDP-22.10.28.jar:1.12.2-HDP-22.10.28]
	at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:83) ~[flink-connector-hive_2.11-1.12.2-HDP-22.10.28.jar:1.12.2-HDP-22.10.28]
	at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.table.filesystem.security.HdpFileSystemLookupFunction.lambda$eval$1(HdpFileSystemLookupFunction.java:59) ~[flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.runtime.security.SecurityUtils.runAtOneContext(SecurityUtils.java:100) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.table.filesystem.security.HdpFileSystemLookupFunction.eval(HdpFileSystemLookupFunction.java:58) [flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at LookupFunction$10.flatMap(Unknown Source) [flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:?]
	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:81) [flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34) [flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at StreamExecCalc$7.processElement(Unknown Source) [flink-table-blink_2.11-1.12.2-HDP-22.10.09.jar:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:322) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:426) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:365) [flink-sql-connector-kafka_2.11-1.12.2-HDP-22.09.20.jar:1.12.2-HDP-22.09.20]
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:183) [flink-sql-connector-kafka_2.11-1.12.2-HDP-22.09.20.jar:1.12.2-HDP-22.09.20]
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:142) [flink-sql-connector-kafka_2.11-1.12.2-HDP-22.09.20.jar:1.12.2-HDP-22.09.20]
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826) [flink-sql-connector-kafka_2.11-1.12.2-HDP-22.09.20.jar:1.12.2-HDP-22.09.20]
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:263) [flink-dist_2.11-1.12.2-HDP-22.10.09.jar:1.12.2-HDP-22.10.09]
Caused by: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name
	at org.apache.hadoop.security.SaslRpcClient.getServerPrincipal(SaslRpcClient.java:326) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:231) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:159) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:391) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:423) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:829) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:825) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_342]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_342]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:2012) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:825) ~[flink-shaded-hadoop-2-uber-2.10.1-HDP-22.09.1-release-10.0.jar:2.10.1-HDP-22.09.1-release-10.0]
	... 56 more{code}
The task did not fail, but hive failed to load data into the cache.

The hadoop version used in the test is 2.10.1. During the test, it was found that changing the hadoop version to 2.8.5 did not throw an exception. Only after hive data is loaded to the cache for the first time and the task is restarted, the restarted task cannot properly load hive data to the cache; The test found that if the nth time (n>=2) load the hive data into the cache and the task is restarted, the restarted task can normally load the hive data into the cache.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 07:17;zhanglu153;Set the properties object in the Configuration object of DFSClient#namenode proxy in the DistributedFileSystem object to null.png;https://issues.apache.org/jira/secure/attachment/13052366/Set+the+properties+object+in+the+Configuration+object+of+DFSClient%23namenode+proxy+in+the+DistributedFileSystem+object+to+null.png","18/Nov/22 08:33;zhanglu153;The error thrown when calling the getProps method after properties is set to null.png;https://issues.apache.org/jira/secure/attachment/13052373/The+error+thrown+when+calling+the+getProps+method+after+properties+is+set+to+null.png","18/Nov/22 08:15;zhanglu153;call the org.apache.hadoop.conf.Configuration#getProps method to create the Properties object.png;https://issues.apache.org/jira/secure/attachment/13052371/call+the+org.apache.hadoop.conf.Configuration%23getProps+method+to+create+the+Properties+object.png","18/Nov/22 08:31;zhanglu153;hadoop2.10.1 org.apache.hadoop.conf.Configuration#getProps.png;https://issues.apache.org/jira/secure/attachment/13052372/hadoop2.10.1+org.apache.hadoop.conf.Configuration%23getProps.png","18/Nov/22 07:41;zhanglu153;hadoop2.10.1 org.apache.hadoop.mapred.Master#getMasterPrincipal.png;https://issues.apache.org/jira/secure/attachment/13052367/hadoop2.10.1+org.apache.hadoop.mapred.Master%23getMasterPrincipal.png","18/Nov/22 07:07;zhanglu153;hadoop2.10.1 org.apache.hadoop.security.SaslRpcClient#conf.png;https://issues.apache.org/jira/secure/attachment/13052365/hadoop2.10.1+org.apache.hadoop.security.SaslRpcClient%23conf.png","18/Nov/22 07:42;zhanglu153;hadoop2.10.1 org.apache.hadoop.yarn.client.util.YarnClientUtils#getYarnConfWithRmHaId.png;https://issues.apache.org/jira/secure/attachment/13052368/hadoop2.10.1+org.apache.hadoop.yarn.client.util.YarnClientUtils%23getYarnConfWithRmHaId.png","18/Nov/22 07:44;zhanglu153;hadoop2.10.1 org.apache.hadoop.yarn.conf.YarnConfiguration static code block.png;https://issues.apache.org/jira/secure/attachment/13052369/hadoop2.10.1+org.apache.hadoop.yarn.conf.YarnConfiguration+static+code+block.png","18/Nov/22 09:05;zhanglu153;hadoop2.8.5 org.apache.hadoop.mapred.Master#getMasterPrincipal.png;https://issues.apache.org/jira/secure/attachment/13052376/hadoop2.8.5+org.apache.hadoop.mapred.Master%23getMasterPrincipal.png","18/Nov/22 06:49;zhanglu153;hadoop2.8.5 org.apache.hadoop.security.SaslRpcClient#conf.png;https://issues.apache.org/jira/secure/attachment/13052362/hadoop2.8.5+org.apache.hadoop.security.SaslRpcClient%23conf.png","18/Nov/22 06:50;zhanglu153;org.apache.flink.connectors.hive.read.HiveTableInputFormat#createInputSplits.png;https://issues.apache.org/jira/secure/attachment/13052363/org.apache.flink.connectors.hive.read.HiveTableInputFormat%23createInputSplits.png","18/Nov/22 07:46;zhanglu153;org.apache.hadoop.conf.Configuration#addDefaultResource.png;https://issues.apache.org/jira/secure/attachment/13052370/org.apache.hadoop.conf.Configuration%23addDefaultResource.png","18/Nov/22 06:48;zhanglu153;org.apache.hadoop.security.SaslRpcClient.png;https://issues.apache.org/jira/secure/attachment/13052361/org.apache.hadoop.security.SaslRpcClient.png",,13.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 13:12:13 UTC 2022,,,,,,,,,,"0|z1ca4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 03:32;zhanglu153;After the ChildFirstClassLoader is created, the default will be SafetyNetWrapperClassLoader class wrapping, SafetyNetWrapperClassLoader class allows ChildFirstClassLoader is closed and make ChildFirstClassLoader set to null, making ChildFirstClassLoader can be recycling, This is to prevent classloaders from leaking. SafetyNetWrapperClassLoader class wrapping ChildFirstClassLoader is the default behavior, which is controlled by the classloader.check-leaked-classloader option, and the default value is true. When the job fails or terminates, Flink will call the org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.SafetyNetWrapperClassLoader#close method to close ChildFirstClassLoader and set it to null. When loadClass or getResource, ChildFirstClassLoader will be used to load the class or resource, and org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.SafetyNetWrapperClassLoader#ensureInner method will be called in advance to ensure that ChildFirstClassLoader is not null. If some threads use the closed ChildFirstClassLoader of Flink, the following exception will be thrown:
{code:java}
Failed to load table into cache, will retry in 10 seconds
java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'. {code}
After the task is restarted, Flink will create a new ChildFirstClassLoader object and a new SafetyNetWrapperClassLoader object to wrap the new ChildFirstClassLoader. All tasks will use the new ChildFirstClassLoader to load classes and resources. However, because the fs.hdfs.impl.disable.cache option is false by default when creating a FileSystem object in the hadoop package, the FileSystem object cached in FileSystem.Cache will be used. The Configuration adapted to the hadoop proxy in the FileSystem object is the Configuration object before the task restart. The classloader stored in this Configuration object is the ChildFirstClassLoader previously closed by Flink, which causes an exception to be thrown when calling the closed ChildFirstClassLoader of Flink.;;;","18/Nov/22 07:07;zhanglu153;{code:java}
Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name {code}
This error occurs in the org.apache.hadoop.security.SaslRpcClient class. This problem is caused by the loss of configuration parameters in properties in the this.conf object. Configuration parameters such as dfs.namenode.kerberos.principal.pattern cannot be found.

!org.apache.hadoop.security.SaslRpcClient.png|width=689,height=285!

Using the 2.8.5 version of hadoop test, there was no error after the task failed and restarted, and the data of hive could be loaded into the cache normally. After the task restarts, the DistributedFileSystem object obtained by calling the org.apache.hadoop.fs.Path#getFileSystem method is the DistributedFileSystem object in FileSystem.Cache. The properties object in the Configuration object of this DistributedFileSystem object contains all configuration parameters. The properties object in the Configuration object in the DFSClient#namenode proxy in the DistributedFileSystem object also contains all the configuration parameters(The Configuration object will be passed to the org.apache.hadoop.security.SaslRpcClient class eventually use to read the Configuration parameters), the classloader retained in this Configuration object is the ChildFirstClassLoader that was set to null before the restart, but the task will not be affected.

!hadoop2.8.5 org.apache.hadoop.security.SaslRpcClient#conf.png|width=466,height=169!

!org.apache.flink.connectors.hive.read.HiveTableInputFormat#createInputSplits.png|width=606,height=261!

Using the 2.10.1 version of hadoop test, an error was thrown after the task failed and restarted, and the data of hive could not be loaded into the cache normally. After the task restarts, the DistributedFileSystem object obtained by calling the org.apache.hadoop.fs.Path#getFileSystem method is the DistributedFileSystem object in FileSystem.Cache. The properties object in the Configuration object of this DistributedFileSystem object contains all configuration parameters. However, the properties object in the Configuration object in the DFSClient#namenode proxy in the DistributedFileSystem object contains only one configuration parameter(The Configuration object will be passed to the org.apache.hadoop.security.SaslRpcClient class eventually use to read the Configuration parameters), the classloader retained in this Configuration object is the ChildFirstClassLoader that was set to null before the restart, but the task have been affected. Due to the loss of the properties configuration parameters, the configuration parameters cannot be obtained when calling the org.apache.hadoop.security.SaslRpcClient#getServerPrincipal method, and the error ""Failed to specify server's Kerberos principal name"" is thrown.

!hadoop2.10.1 org.apache.hadoop.security.SaslRpcClient#conf.png|width=603,height=182!;;;","18/Nov/22 07:46;zhanglu153;1.When is the properties object in the Configuration object of DFSClient#namenode proxy in the DistributedFileSystem object set to null?

When calling the org.apache.flink.connectors.hive.read.HiveTableInputFormat#createInputSplits(int, java.util.List<org.apache.flink.connectors.hive.HiveTablePartition>, org.apache.hadoop.mapred.JobConf) method for the first time before restarting the task, set the properties object in the Configuration object of DFSClient#namenode proxy in the DistributedFileSystem object to null in the following figure:

!Set the properties object in the Configuration object of DFSClient#namenode proxy in the DistributedFileSystem object to null.png|width=608,height=301!

You can see that the org.apache.hadoop.mapred.FileInputFormat#getSplits method is called at the end of the org.apache.flink.connectors.hive.read.HiveTableInputFormat#createInputSplits(int, java.util.List<org.apache.flink.connectors.hive.HiveTablePartition>, org.apache.hadoop.mapred.JobConf) method, and then it will be called all the way to the org.apache.hadoop.mapred.Master#getMasterPrincipal method. In method org.apache.hadoop.mapred.Master#getMasterPrincipal, there is an if else code block. In the logic of the else code block, method org.apache.hadoop.yarn.client.util.YarnClientUtils#getRmPrincipal(org.apache.hadoop.conf.Configuration) will be called, and then it will be called all the way to org.apache.hadoop.yarn.client.util.YarnClientUtils#getYarnConfWithRmHaId. A new YarnConfiguration object will be created here. There is a static code block in the org.apache.hadoop.yarn.conf.YarnConfiguration class. When this static code block calls method org.apache.hadoop.conf.Configuration#addDefaultResource, it will call method org.apache.hadoop.conf.Configuration#reloadConfiguration in the for loop to set the properties object in all Configuration objects to null(Note that the static code block in the YarnConfiguration class is executed only when the method org.apache.hadoop.mapred.FileInputFormat#getSplits is called for the first time, because the YarnConfiguration class has not been loaded by the class loader. Therefore, if the HiveTableInputFormat#createInputSplits method is called again, the static code block in the YarnConfiguration class will not be executed, that is, the property object will not be null again.)

!hadoop2.10.1 org.apache.hadoop.mapred.Master#getMasterPrincipal.png|width=685,height=189!

!hadoop2.10.1 org.apache.hadoop.yarn.client.util.YarnClientUtils#getYarnConfWithRmHaId.png|width=423,height=185!

!hadoop2.10.1 org.apache.hadoop.yarn.conf.YarnConfiguration static code block.png|width=472,height=245!

!org.apache.hadoop.conf.Configuration#addDefaultResource.png|width=366,height=218!;;;","18/Nov/22 08:41;zhanglu153;2.After the properties object in the Configuration object of DFSClient#namenode proxy in the DistributedFileSystem object is set to null, when is a new Properties object created?

!call the org.apache.hadoop.conf.Configuration#getProps method to create the Properties object.png|width=931,height=381!

After the task restarts, call method org.apache.flink.connectors.hive.read.HiveTableInputFormat#createInputSplits(int, java.util.List<org.apache.flink.connectors.hive.HiveTablePartition>, org.apache.hadoop.mapred.JobConf), then call method org.apache.hadoop.fs.FileSystem#exists after getting the filesytem object from FileSystem.Cache, and finally call method org.apache.hadoop.conf.Configuration#getProps. Since properties have been set to null in the reloadConfiguration method, a new Properties object will be created in the getProps method. However, when calling method org.apache.hadoop.conf.Configuration#loadResources, you need to call method org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.SafetyNetWrapperClassLoader#getResources of Flink to obtain resources. Since the ChildFirstClassLoader used has been set to null, Flink will throw an exception, which will result in the inability to reload the configuration properties into the properties object. Therefore, only a Properties object with size 0 has been created in it.

!hadoop2.10.1 org.apache.hadoop.conf.Configuration#getProps.png|width=546,height=303!

The error thrown when calling the getProps method after properties is set to null is shown in the following figure:

!The error thrown when calling the getProps method after properties is set to null.png|width=979,height=298!;;;","18/Nov/22 09:05;zhanglu153;3. Why does the task not throw an exception when using hadoop2.8.5?

In hadoop 2.8.5, method org.apache.hadoop.mapred.Master#getMasterPrincipal does not have an if else logical block, so method org.apache.hadoop.conf.Configuration#reloadConfiguration will not be called to set properties to null. Therefore, there is no problem when calling method org.apache.hadoop.conf.Configuration#getProps again after the task is restarted.

!hadoop2.8.5 org.apache.hadoop.mapred.Master#getMasterPrincipal.png!;;;","18/Nov/22 09:55;zhanglu153;This error is related to the hadoop version and the operation of setting ChildFirstClassLoader to null. The main reason is that the static code block in class org.apache.hadoop.yarn.conf.YarnConfiguration sets the properties object in the Configuration object to null.

The solutions considered are as follows:
 # Set classloader.check-leaked-classloader option is false, but it will cause leakage of the Flink's ChildFirstClassLoader.
 # Set the fs.hdfs.impl.disable.cache parameter to true in the default configuration file of hadoop. It is equivalent to creating a new filesystem file handle every time. For tasks, a new file handle will be created when creating new splits. It needs to be closed in time, otherwise it will lead to the task oom or other situations.
 # Modify method org.apache.flink.connectors.hive.read.HiveTableInputFormat#createInputSplits(int, java.util.List<org.apache.flink.connectors.hive.HiveTablePartition>, org.apache.hadoop.mapred.JobConf) and create a YarnConfiguration object before calling org.apache.hadoop.fs.FileSystem#exists method, as shown below:

{code:java}
public static HiveTableInputSplit[] createInputSplits(
        int minNumSplits, List<HiveTablePartition> partitions, JobConf jobConf)
        throws IOException {
    YarnConfiguration yarnConfiguration = new YarnConfiguration();
    List<HiveTableInputSplit> hiveSplits = new ArrayList<>();
    int splitNum = 0;
    FileSystem fs = null;
    for (HiveTablePartition partition : partitions) {
        StorageDescriptor sd = partition.getStorageDescriptor();
        Path inputPath = new Path(sd.getLocation());
        if (fs == null) {
            fs = inputPath.getFileSystem(jobConf);
        }
        // it's possible a partition exists in metastore but the data has been removed
        if (!fs.exists(inputPath)) {
            continue;
        }
        InputFormat format;
        try {
            format =
                    (InputFormat)
                            Class.forName(
                                            sd.getInputFormat(),
                                            true,
                                            Thread.currentThread().getContextClassLoader())
                                    .newInstance();
        } catch (Exception e) {
            throw new FlinkHiveException(""Unable to instantiate the hadoop input format"", e);
        }
        ReflectionUtils.setConf(format, jobConf);
        jobConf.set(INPUT_DIR, sd.getLocation());
        // TODO: we should consider how to calculate the splits according to minNumSplits in the
        // future.
        org.apache.hadoop.mapred.InputSplit[] splitArray =
                format.getSplits(jobConf, minNumSplits);
        for (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) {
            hiveSplits.add(new HiveTableInputSplit(splitNum++, inputSplit, jobConf, partition));
        }
    } {code}
In this way, the static code block in YarnConfiguration can be executed in advance when the YarnConfiguration class is loaded for the first time. This can avoid setting the properties object in the Configuration object to null when calling the org.apache.hadoop.mapred.FileInputFormat#getSplits method. This method seems to actually solve this problem, but it seems unfriendly, because the YarnConfiguration object is not needed to use in the actual org.apache.flink.connectors.hive.read.HiveTableInputFormat class.;;;","23/Nov/22 15:23;martijnvisser;Will this change if we complete the upgrade to Hadoop 2.10?;;;","24/Nov/22 01:51;zhanglu153;[~martijnvisser] When using hadoop 2.10.1 during testing, an exception will be thrown. Do you have a better solution? This exception also exists when using hadoop 3.;;;","08/Dec/22 12:52;luoyuxia;[~zhanglu153] Really cool analysis. 

> when calling method org.apache.hadoop.conf.Configuration#loadResources, you need to call method org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.SafetyNetWrapperClassLoader#getResources of Flink to obtain resources.

Where can I find such code?

 

TBH, I can't think of a better solution.As a workround, you can use solution 3 which cause hardly regression for perfomance and build it by yourself. But I don't think it'll be accepted by Flink community.

 

 ;;;","08/Dec/22 13:12;zhanglu153;[~luoyuxia] You can read the hadoop source code. In the method org.apache.hadoop.conf.Configuration#loadResources，the method  org.apache.hadoop.conf.Configuration#loadResource will be called. Then call method the org.apache.hadoop.conf.Configuration#getResource. You will see the following code block：
{code:java}
public URL getResource(String name) {
    return this.classLoader.getResource(name);
} {code}
Then call the method org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.SafetyNetWrapperClassLoader#getResources of Flink to obtain resources. Because the classLoader object saved in this Configuration object is the SafetyNetWrapperClassLoader object of Flink.

Solution 3 may not be accepted by the Flink community, and a better solution needs to be discussed.;;;",,,,,,,,,,,,,,,,,,,,,
ES e2e tests are never run,FLINK-30074,13503092,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,chesnay,chesnay,chesnay,17/Nov/22 11:59,17/Nov/22 12:00,04/Jun/24 20:41,17/Nov/22 12:00,,,,,,,,,,,Build System / CI,Connectors / ElasticSearch,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 11:59:46.0,,,,,,,,,,"0|z1c8q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed memory can be wasted if RocksDB memory is fixed-per-slot,FLINK-30073,13503072,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,roman,roman,17/Nov/22 10:30,24/Nov/22 02:54,04/Jun/24 20:41,,1.15.2,1.16.0,1.17.0,,,,,,,,Runtime / Task,,,,0,,,,"When [state.backend.rocksdb.memory.fixed-per-slot|https://github.com/apache/flink/blob/ba4b182955867fedfa9891bf0bf430e92eeab41a/flink-runtime/src/main/java/org/apache/flink/runtime/util/config/memory/ManagedMemoryUtils.java#L75] is set, RocksDB does not use managed memory (this option overrides the 'state.backend.rocksdb.memory.managed').

However, the runtime [doesn't take this into account|https://github.com/apache/flink/blob/ba4b182955867fedfa9891bf0bf430e92eeab41a/flink-runtime/src/main/java/org/apache/flink/runtime/util/config/memory/ManagedMemoryUtils.java#L75] and still reserves the managed memory according to the configured weigths.

cc: [~yunta]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 02:54:44 UTC 2022,,,,,,,,,,"0|z1c8ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 10:31;roman;[~xtsong], can you please confirm that?;;;","17/Nov/22 11:05;xtsong;I'd consider this more a design limitation, rather than a bug.

IIRC, the purpose of this configuration option is for backwards compatibility. The problem was that, rocksdb state backend used an estimation-based approach for managing its memory, which means it cannot guarantee not use more memory than it is expected to use. When running into corner cases, one can manually set rocksdb to use slightly less memory than calculated from weights, while Flink will still reserver as many memory calculated from weights for it.

I'm not sure whether this workaround is still needed. I think people have been discussing about fixing the problem that rocksdb uses more memory than expected, by upgrading the rocksdb version. I'm not aware how that effort ends up. Maybe [~yunta] can share some more details.;;;","17/Nov/22 15:16;roman;According to the ticket (FLINK-14484), the purpose of fixed-per-slot is to cap memory usage by RocksDB (while at the same time having flexible memory distribution among tasks in a slot).
The non-strict limit issue (FLINK-15532) is a separate one and still can not be resolved AFAICT.

I don't think there are any design limitations here.
EmbeddedRocksDBStateBackend could simply return false from useManagedMemory() if fixed-per-slot > 0.

WDYT?;;;","18/Nov/22 02:18;xtsong;I'm no more familiar with the background of FLINK-14484 &FLINK-15532 than you are. So I cannot comment whether returning {{false}} from {{useManagedMemory()}} when {{fixed-per-slot > 0}} is a desirable design.

Assuming the proposed change is a good design, I see a significant behavior change in this. Let's say fixed-per-slot is configured to X, and a Y sized memory calculated from the weights is reserved for rocksdb due to not setting {{useManagedMemory()}} to {{false}}. Assuming currently the job is running well currently, then either Y >= X and some memory is wasted, or Y < X and other not-fully-used memory (framework.off-heap, jvm-overhead, etc.) is making up for the gap. With the proposed change, Y will be shared by other managed memory use cases, while rocksdb still uses X. That means when upgrading to the new version, to keep a job with fixed-per-slot running with the same resources, user needs to increase the framework.off-heap memory by min(X, Y), and either also increase the total memory or reduce the managed memory.

I'm not oppose to this proposal. I don't have enough expertise in the rocksdb state backend to decide whether this is a good design change. Just trying to say that, considering the significant behavior changes, we should be very cautious about this change and apply it only if it's absolutely necessary.

cc [~yuanmei];;;","21/Nov/22 21:28;roman;{quote}to keep a job with fixed-per-slot running with the same resources{quote}
Aren't those resources already reserved? I.e. don't actual memory limits should already include X+Y? (where part of Y is wasted).

In overcommitted environments, where the limits are not strictly enforced, the behavior migth change. But that's I think is acceptable, as those requirements don't guarantee how many instances will run.;;;","24/Nov/22 02:54;yunta;[~roman], the state.backend.rocksdb.memory.fixed-per-slot configuration is used for expert mode, in which user could set the taskmanager.memory.managed.size as zero and control the RocksDB's memory just as what they want. 
In this case, user need to ensure on their own that enough memory is available outside the JVM for RocksDB (you can refer to the 'Expert Mode' description in https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/#memory-management ). This is more like a configuration to keep backward compatibility for the memory usage before flink-1.10.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Cannot assign instance of SerializedLambda to field KeyGroupStreamPartitioner.keySelector,FLINK-30072,13503069,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nkruber,nkruber,17/Nov/22 10:20,01/Dec/22 08:12,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Runtime / Task,,,,0,,,,"In application mode, if the {{usrlib}} directories of the JM and TM differ, e.g. same jars but different names, the job is failing and throws this cryptic exception on the JM:

{code}
2022-11-17 09:55:12,968 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Restarting job.
org.apache.flink.streaming.runtime.tasks.StreamTaskException: Could not instantiate outputs in order.
    at org.apache.flink.streaming.api.graph.StreamConfig.getVertexNonChainedOutputs(StreamConfig.java:537) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.createRecordWriters(StreamTask.java:1600) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.createRecordWriterDelegate(StreamTask.java:1584) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.<init>(StreamTask.java:408) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.<init>(StreamTask.java:362) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.<init>(StreamTask.java:335) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.<init>(StreamTask.java:327) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.<init>(StreamTask.java:317) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.<init>(SourceOperatorStreamTask.java:84) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at jdk.internal.reflect.GeneratedConstructorAccessor38.newInstance(Unknown Source) ~[?:?]
    at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) ~[?:?]
    at java.lang.reflect.Constructor.newInstance(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.taskmanager.Task.loadAndInstantiateInvokable(Task.java:1589) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:714) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner.keySelector of type org.apache.flink.api.java.functions.KeySelector in instance of org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner
    at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(Unknown Source) ~[?:?]
    at java.io.ObjectStreamClass$FieldReflector.checkObjectFieldValueTypes(Unknown Source) ~[?:?]
    at java.io.ObjectStreamClass.checkObjFieldValueTypes(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.defaultCheckFieldValues(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readSerialData(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject0(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.defaultReadFields(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readSerialData(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject0(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject(Unknown Source) ~[?:?]
    at java.util.ArrayList.readObject(Unknown Source) ~[?:?]
    at jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
    at java.io.ObjectStreamClass.invokeReadObject(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readSerialData(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject0(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject(Unknown Source) ~[?:?]
    at java.io.ObjectInputStream.readObject(Unknown Source) ~[?:?]
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    at org.apache.flink.streaming.api.graph.StreamConfig.getVertexNonChainedOutputs(StreamConfig.java:533) ~[flink-dist-1.16.0-ok.0.jar:1.16.0-ok.0]
    ... 15 more
{code}

It seems that our normal exception reporting is not really working (maybe also a problem for more common failure?). The TM logs do contain the actual exception though:

{code}
2022-11-17 10:11:43,551 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - TumblingEventTimeWindows -> Sink: Print to Std. Out (1/1)#3 (cc8e6b0246079230c5ac1bc335c70163_c27dcf7b54ef6bfd6cff02ca8870b681_0_3) switched from INITIALIZING to FAILED with failure cause: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: com.immerok.cloud.examples.WindowAggregation$MetadataEnrichingWindowFunction
ClassLoader info: URL ClassLoader:
    file: 'usrlib/window-agg-2.jar' (missing)
Class not resolvable through given classloader.
    at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:397)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:162)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:60)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:681)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassNotFoundException: com.immerok.cloud.examples.WindowAggregation$MetadataEnrichingWindowFunction
    at java.base/java.net.URLClassLoader.findClass(Unknown Source)
    at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
    at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67)
    at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74)
    at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51)
    at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
    at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Unknown Source)
    at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)
    at java.base/java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
    at java.base/java.io.ObjectInputStream.readClassDesc(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)
    at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543)
    at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:383)
    ... 9 more
{code}

h2. How to reproduce:

# on JM: {{<flink>/usrlib/window-agg-1.jar}}
# on TM: {{<flink>/usrlib/window-agg-2.jar}}

then start the job in application mode",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 10:20:26.0,,,,,,,,,,"0|z1c8l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception at compile time when sequence field does not exist,FLINK-30071,13503066,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,17/Nov/22 10:17,21/Nov/22 11:34,04/Jun/24 20:41,21/Nov/22 11:34,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"!image-2022-11-17-18-16-07-590.png|width=524,height=357!

Let's perform a check at compile time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 10:16;qingyue;image-2022-11-17-18-16-07-590.png;https://issues.apache.org/jira/secure/attachment/13052327/image-2022-11-17-18-16-07-590.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 11:34:23 UTC 2022,,,,,,,,,,"0|z1c8kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 11:34;lzljs3620320;master: 4616e775efd567001704fea0734e5e12e72cfffd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create savepoints without side effects,FLINK-30070,13503060,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pnowojski,pnowojski,17/Nov/22 09:46,29/Nov/22 08:19,04/Jun/24 20:41,,1.14.6,1.15.2,1.16.0,,,,,,,,API / DataStream,Runtime / Checkpointing,,,1,,,,"Side effects are any external state - a state that is stored not in Flink, but in an external system, like for example connectors transactions (KafkaSink, ...).

We shouldn't be relaying on the external systems for storing part of the job's state, especially for any long period of time. The most prominent issue is that Kafka transactions can time out, leading to a data loss if transaction hasn't been committed.

Stop-with-savepoint, currently  guarantee that {{notifyCheckpointCompleted}} call will be issued, so properly implemented operators are guaranteed to committed it's state. However this information is currently not stored in the checkpoint in any way ( FLINK-16419 ). Larger issue is how to deal with savepoints, since there we currently do not have any guarantees that transactions have been committed. 

Some potential solution might be to expand API (like {{CheckpointedFunction}} ), to let the operators/functions know, that they should close/commit/clear/deal with external state differently and use that API during stop-with-savepoint and intermediate savepoints. Note that since Flink 1.15, intermediate savepoints are never committed, so most likely they shouldn't even try to store/pre-commit any external state/transactions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16419,,,FLINK-30068,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 09:46:37.0,,,,,,,,,,"0|z1c8j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expected prune behavior for matches with same priority,FLINK-30069,13503058,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Juntao Hu,Juntao Hu,17/Nov/22 09:35,24/Feb/23 09:15,04/Jun/24 20:41,,1.15.3,1.16.1,,,,,,,,,Library / CEP,,,,0,,,,"When a pattern produces several matches with same priority, is it the expected behavior to keep only the first dequeued one?

E.G.

pattern: A.followedByAny(B).followedBy(C)

seq: a1, b1, b2, b3, c1

aftermatch strategy: skip_to_next

Potential matches (a1, b1, c1) (a1, b2, c1) (a1, b3, c1) reach final state at the same time and have the same priority, but (a1, b1, c1) is the first one in the queue, thus (a1, b2, c1) and (a1, b3, c1) will be dropped by skip_to_next strategy.

If it's by-design, I guess it relies on two things:
 * take must be the first transition for every state, if exists
 * priority queue is first-in-first-out for items with same priority

*UPDATE 17/Feb/23*

Example case is testNotFollowedByWithinAtEndAfterMatch() in [this commit|https://github.com/apache/flink/pull/21920/commits/1b43eab231ea62df1be2a5f3752664eca5b2e15a]. If we change skip strategy to SKIP_TO_NEXT, the expected output is (a1, a2, a3, c1), (a2, a3, c1) and (a3, c1). If we run in JDK11, it works as expected, but after switching to JDK8 this case will fail with (a2, a3, c1) becoming (a2, c1). PriorityQueue.removeAll() is quite suspicous for breaking the order.

*UPDATE 24/Feb/23*

[JDK11|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/19fb8f93c59dfd791f62d41f332db9e306bc1422/src/java.base/share/classes/java/util/PriorityQueue.java#L915] PriorityQueue implements bulk remove, while [JDK8|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/2544d2a351eca1a3d62276f969dd2d95e4a4d2b6/jdk/src/share/classes/java/util/AbstractCollection.java#L370] removes one by one, which result different orders on elements with same priority.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 09:35:15.0,,,,,,,,,,"0|z1c8io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow users to configure what to do with errors while committing transactions during recovery in KafkaSink,FLINK-30068,13503054,13426150,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,pnowojski,pnowojski,17/Nov/22 09:27,23/Mar/23 09:52,04/Jun/24 20:41,,1.15.2,1.16.0,1.17.0,,,,,,,,Connectors / Kafka,,,,2,,,,"Currently it looks like {{KafkaSink}} fails the job on any failures to commit transactions. As [reported by the user|https://lists.apache.org/thread/4f6bb8j6qtvgp888y4dxgj86x3kw2b11], this makes impossible for jobs to recover from older Savepoints.

{noformat}
2022-11-16 10:01:07.168 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Balances aggreagation ETH -> (Filter -> Map -> Save to Kafka realtime ETH: Writer -> Save to Kafka realtime ETH: Committer, Filter -> Map -> Save to Kafka daily ETH: Writer -> Save to Kafka daily ETH: Committer) (4/5) (6d4d91ab8657bba830695b9a011f7db6) switched from INITIALIZING to RUNNING.
2022-11-16 10:01:37.222 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  - Triggering checkpoint 65436 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1668592897201 for job 00000000000000000000000000000000.
2022-11-16 10:01:39.082 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Balances aggreagation ETH -> (Filter -> Map -> Save to Kafka realtime ETH: Writer -> Save to Kafka realtime ETH: Committer, Filter -> Map -> Save to Kafka daily ETH: Writer -> Save to Kafka daily ETH: Committer) (1/5) (cfaca46e7f4dc89629cdcaed5b48c059) switched from RUNNING to FAILED on 10.42.145.181:33297-efc328 @ eth-top-holders-v2-flink-taskmanager-0.eth-top-holders-v2-flink-taskmanager.flink.svc.cluster.local (dataPort=43125).
java.io.IOException: Could not perform checkpoint 65436 for operator Balances aggreagation ETH -> (Filter -> Map -> Save to Kafka realtime ETH: Writer -> Save to Kafka realtime ETH: Committer, Filter -> Map -> Save to Kafka daily ETH: Writer -> Save to Kafka daily ETH: Committer) (1/5)#0.
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1210)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:493)
	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emit(SinkWriterOperator.java:234)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emitCommittables(SinkWriterOperator.java:204)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.prepareSnapshotPreBarrier(SinkWriterOperator.java:166)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.prepareSnapshotPreBarrier(RegularOperatorChain.java:89)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:300)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198)
	... 22 common frames omitted
Caused by: java.lang.IllegalStateException: Failed to commit KafkaCommittable{producerId=6640191, epoch=0, transactionalId=eth_top_holders_daily_v11-0-65435}
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommitRequestImpl.signalFailedWithUnknownReason(CommitRequestImpl.java:77)
	at org.apache.flink.connector.kafka.sink.KafkaCommitter.commit(KafkaCommitter.java:119)
	at org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.commit(CheckpointCommittableManagerImpl.java:127)
	at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmit(CommitterOperator.java:176)
	at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmitCheckpoints(CommitterOperator.java:160)
	at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.processElement(CommitterOperator.java:199)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 35 common frames omitted
Caused by: org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16419,FLINK-30070,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 10:04:41 UTC 2022,,,,,,,,,,"0|z1c8hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 09:34;pnowojski;A temporary workaround might be to drop old {{KafkaSink}}, and replace it with a new one, that has different {{uid}} (recovery would require to enable {{allowedNonRestoredState}} option).;;;","17/Nov/22 10:04;martijnvisser;[~renqs] It's probably worthwhile to pick this up rather quickly. WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelegatingConfiguration#set should return itself,FLINK-30067,13503047,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Nov/22 08:58,17/Nov/22 14:57,04/Jun/24 20:41,17/Nov/22 14:57,,,,,,1.17.0,,,,,API / Core,,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-29993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 14:57:00 UTC 2022,,,,,,,,,,"0|z1c8g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 14:57;chesnay;master: 38c6a4bb8af37f5a05e9d57d9787f0b25334abbf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Kinesis connector code from Flink repo to dedicated AWS repo,FLINK-30066,13503046,13503028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,martijnvisser,martijnvisser,17/Nov/22 08:53,05/Dec/22 12:45,04/Jun/24 20:41,05/Dec/22 12:45,,,,,,aws-connector-4.0.0,,,,,Connectors / Kinesis,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29907,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 12:45:49 UTC 2022,,,,,,,,,,"0|z1c8g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 12:45;dannycranmer;Merged commit [{{aa10d88}}|https://github.com/apache/flink-connector-aws/commit/aa10d88f107dab8264ca6073b6cd64f599b34e93] into apache:main
Merged commit [{{da1ddb0}}|https://github.com/apache/flink-connector-aws/commit/da1ddb0b641d22cd758b49cd6f172d26383c8a6d] into apache:main 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Firehose connector code from Flink repo to dedicated AWS repo,FLINK-30065,13503045,13503028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,martijnvisser,martijnvisser,17/Nov/22 08:52,05/Dec/22 12:45,04/Jun/24 20:41,05/Dec/22 12:45,,,,,,aws-connector-4.0.0,,,,,Connectors / Firehose,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29907,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 12:45:32 UTC 2022,,,,,,,,,,"0|z1c8fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 12:45;dannycranmer;Merged commit [{{aa10d88}}|https://github.com/apache/flink-connector-aws/commit/aa10d88f107dab8264ca6073b6cd64f599b34e93] into apache:main
Merged commit [{{da1ddb0}}|https://github.com/apache/flink-connector-aws/commit/da1ddb0b641d22cd758b49cd6f172d26383c8a6d] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing Hive connector code from Flink repo to dedicated Hive repo,FLINK-30064,13503044,13503028,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,martijnvisser,martijnvisser,17/Nov/22 08:50,08/Dec/23 21:07,04/Jun/24 20:41,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-26603,,FLINK-32286,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33438,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 01:16:05 UTC 2023,,,,,,,,,,"0|z1c8fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 01:16;Sergey Nuyanzin;[~martijnvisser] I submitted a PR after decoupling of hive and planner it looks viable;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create and initialize repository for Hive connector,FLINK-30063,13503043,13503028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:50,23/Nov/22 22:05,04/Jun/24 20:41,23/Nov/22 22:05,,,,,,hive-3.0.0,,,,,Connectors / Hive,,,,0,,,,,,,,,,,,,,,,FLINK-26603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 22:05:54 UTC 2022,,,,,,,,,,"0|z1c8fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 22:05;martijnvisser;Created repository https://github.com/apache/flink-connector-hive
Init commit: 64a95f4d19d453a849ca7aebdc27e3b295677dc5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing HBase connector code from Flink repo to dedicated HBase repo,FLINK-30062,13503042,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,martijnvisser,martijnvisser,17/Nov/22 08:50,12/Dec/22 12:59,04/Jun/24 20:41,02/Dec/22 09:44,,,,,,hbase-3.0.0,,,,,Connectors / HBase,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 09:44:49 UTC 2022,,,,,,,,,,"0|z1c8f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 11:42;ferenc-csaky;[~martijnvisser] can you assign this one to me? Thanks!;;;","22/Nov/22 15:01;martijnvisser;[~ferenc-csaky] The repo is created and I've assigned this issue to you;;;","02/Dec/22 09:44;martijnvisser;Fixed in main:
24ac36d259e0cf394e7a4f457eb3a8422e010af6
d74e518aa000506bccbb5da4604c0814d5840ecb
v3.0:
8a33fbf53eaf8c245647f9b4595ec9c9ea8cc215;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create and initialize repository for HBase connector,FLINK-30061,13503041,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:49,22/Nov/22 15:00,04/Jun/24 20:41,22/Nov/22 15:00,,,,,,hbase-3.0.0,,,,,Connectors / HBase,,,,0,,,,"* Create repository
* Init repo and setup CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 15:00:50 UTC 2022,,,,,,,,,,"0|z1c8ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 15:00;martijnvisser;Repository created at https://github.com/apache/flink-connector-hbase
Init commit: 4faad76941b29e1ac692a5dffc04ccae44657a0c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing JDBC connector code from Flink repo to dedicated JDBC repo,FLINK-30060,13503040,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:49,29/Nov/22 14:49,04/Jun/24 20:41,29/Nov/22 14:49,,,,,,jdbc-3.0.0,,,,,Connectors / JDBC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 14:49:16 UTC 2022,,,,,,,,,,"0|z1c8eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 14:49;martijnvisser;Fixed in master: 9559acf3af9773e6c37c5722d0c7e04bda226c31

All code from {{master}} has been moved to the dedicated JDBC repository;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create and initialize repository for JDBC connector,FLINK-30059,13503039,13503028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:48,23/Nov/22 22:04,04/Jun/24 20:41,23/Nov/22 22:04,,,,,,jdbc-3.0.0,,,,,Connectors / JDBC,,,,0,,,,"* Create repository
* Init repo and setup CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 22:04:52 UTC 2022,,,,,,,,,,"0|z1c8eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 22:04;martijnvisser;Created repository at https://github.com/apache/flink-connector-jdbc
Init commit: 81a73f1fc6835a10804ab3162ab5cd13f6061b0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing Google Cloud PubSub connector code from Flink repo to dedicated Google PubSub repo,FLINK-30058,13503038,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:48,19/Jan/23 12:20,04/Jun/24 20:41,19/Jan/23 12:20,,,,,,gcp-pubsub-3.0.0,,,,,Connectors / Google Cloud PubSub,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 12:20:44 UTC 2023,,,,,,,,,,"0|z1c8e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 15:38;martijnvisser;[~rskraba] Took the liberty to assign this one to you if that's OK. Guide on the externalization can be found at https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development;;;","23/Nov/22 16:02;rskraba;Great!  Thanks – I saw the {{https://github.com/apache/flink-connector-gcp-pubsub}} – I'll swap over the PR to that one!;;;","19/Jan/23 12:20;martijnvisser;Fixed in main, last commit: c546198f971c952bd52340c27f7df38b8c17c364;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create and initialize repository for Google Cloud PubSub connector ,FLINK-30057,13503035,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:42,23/Nov/22 15:37,04/Jun/24 20:41,23/Nov/22 15:37,,,,,,gcp-pubsub-3.0.0,,,,,Connectors / Google Cloud PubSub,,,,0,,,,"* Create repository
* Init repo and setup CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 15:37:20 UTC 2022,,,,,,,,,,"0|z1c8dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 15:37;martijnvisser;Created repo at https://github.com/apache/flink-connector-gcp-pubsub
Init commit: 78795c39e9d879dc18185448bbe6361d9585130f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make polling for metadata no more than specified timeout by using new Consumer#poll(Duration) ,FLINK-30056,13503034,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,17/Nov/22 08:39,25/Nov/22 09:48,04/Jun/24 20:41,25/Nov/22 09:48,1.17.0,,,,,1.17.0,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"New {{Consumer#poll}}
* poll for metadata responses (counts against timeout)
* if no response within timeout, return an empty collection immediately

Also more details are at https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior#KIP266:Fixconsumerindefiniteblockingbehavior-Consumer#poll

The behavior was changed at https://issues.apache.org/jira/browse/KAFKA-5697",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 09:48:14 UTC 2022,,,,,,,,,,"0|z1c8dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:48;chesnay;master: 5e0e0fde6305577192fe8020fcb1fa8ca3bd115e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing Cassandra connector code from Flink repo to dedicated Cassandra repo,FLINK-30055,13503033,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,17/Nov/22 08:39,17/Nov/22 10:36,04/Jun/24 20:41,17/Nov/22 10:36,,,,,,,,,,,Connectors / Cassandra,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29982,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 08:39:07.0,,,,,,,,,,"0|z1c8d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing Pulsar connector code from Flink repo to dedicated Pulsar repo,FLINK-30054,13503032,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:36,29/Nov/22 19:39,04/Jun/24 20:41,29/Nov/22 19:39,,,,,,pulsar-3.0.0,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 19:39:31 UTC 2022,,,,,,,,,,"0|z1c8cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 19:39;martijnvisser;Fixed in main: 5ccbad365dd012733024653309235c3d18469e31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create and initialize repository for Pulsar connector,FLINK-30053,13503031,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:36,23/Nov/22 15:06,04/Jun/24 20:41,23/Nov/22 15:06,,,,,,pulsar-3.0.0,,,,,Connectors / Pulsar,,,,0,,,,"* Create repository
* Init repo and setup CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 15:06:33 UTC 2022,,,,,,,,,,"0|z1c8co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 15:06;martijnvisser;Created repository at https://github.com/apache/flink-connector-pulsar
Init commit: 0d1acdeaa35a8e2b97a1cb1901c2f80c260aa44f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing Kafka connector code from Flink repo to dedicated Kafka repo,FLINK-30052,13503030,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,martijnvisser,martijnvisser,17/Nov/22 08:35,10/Mar/23 07:37,04/Jun/24 20:41,10/Mar/23 07:37,,,,,,kafka-3.0.0,,,,,Connectors / Kafka,,,,0,pull-request-available,,,Instructions guide can be found at https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 07:37:31 UTC 2023,,,,,,,,,,"0|z1c8cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 09:27;mason6345;[~martijnvisser] I started this task over the weekend. There were some issues in porting the Kafka related E2E tests over, using 1.17-SNAPSHOT for the Flink version, since the snapshot artifacts do not exist for flink-sql-client-test [1] and flink-end-to-end-tests-common [2]. [1] is built at build time using the `maven-dependency-plugin`.

 

I suspect that [1] is something that the other connectors will face and can be solved commonly. With regards to flink-to-end-tests-common, should a snapshot be published here: [https://repository.apache.org/content/groups/snapshots/org/apache/flink/]? 

 

Elasticsearch E2E tests fortunately didn't have these dependencies but other connectors do.

 

Also, due to the changes in 1.17 (split level watermark alignment), it isn't straightforward to downgrade the master code to 1.16, so I will need to repeat the externalization instructions for master branch, release-1.16 branch, etc.

 

[1] [https://github.com/apache/flink/blob/bee3e9442fa2749f0b2e21d79949b5410fa422e5/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/pom.xml#L196]

[2] [https://github.com/apache/flink/blob/bee3e9442fa2749f0b2e21d79949b5410fa422e5/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/pom.xml#L43];;;","29/Nov/22 09:44;martijnvisser;[~mason6345] The best course of actions is to first move the code from ""release-1.16"" to the external repository and create a release ""3.0.0"" from that. When that's done, then to move over the code from ""master"" so that can be released as ""3.1.0"" when Flink 1.17 is released.  ;;;","29/Nov/22 09:45;martijnvisser;[~chesnay] WDYT on the issues that Mason mentioned?;;;","29/Nov/22 10:31;chesnay;Just replace the {{flink-end-to-end-test-common}} dependency with {{flink-connector-test-utils}} for flink-sql-client-test.;;;","29/Nov/22 10:34;chesnay;Other tests that rely on flink-end-to-end-tests-common should be re-written.;;;","26/Jan/23 15:41;chesnay;kafka-main:
979bdcfafabae392fa6978fb107c4228d68776b7
2fb6ad544c374c76c0e49c1ef11affbd82fe63d0;;;","10/Mar/23 07:37;martijnvisser;Fixed via https://github.com/apache/flink-connector-kafka/pull/1;;;",,,,,,,,,,,,,,,,,,,,,,,,
Create and initialize repository for Kafka connector,FLINK-30051,13503029,13503028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,17/Nov/22 08:34,22/Nov/22 14:40,04/Jun/24 20:41,22/Nov/22 14:40,,,,,,kafka-3.0.0,,,,,Connectors / Kafka,,,,0,,,,"* Create repository
* Init repo and setup CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 14:40:39 UTC 2022,,,,,,,,,,"0|z1c8c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 14:40;martijnvisser;Created https://github.com/apache/flink-connector-kafka
Init commit: 21111727011674de9d8b6a560861639fb6b806fd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Externalize Flink connectors,FLINK-30050,13503028,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,17/Nov/22 08:33,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,1.20.0,,,,,Connectors / Common,,,,0,,,,"This is an umbrella ticket for all efforts regarding the externalization of connectors. Details (including the links to the mailing list discussions) can be found in https://cwiki.apache.org/confluence/display/FLINK/Connectors. 

Common rules and requirements for connectors that are developed & released separately from Flink can be found at https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development

Feel free to create more subtasks when required to externalize connectors",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 08:33:25.0,,,,,,,,,,"0|z1c8c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsvBulkWriter is unsupported for S3 FileSystem in streaming sink,FLINK-30049,13502994,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lzljs3620320,lzljs3620320,17/Nov/22 02:49,17/Nov/22 02:50,04/Jun/24 20:41,,1.15.2,1.16.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,"{code:java}
Caused by: org.apache.flink.util.SerializedThrowable: Cannot sync state to system like S3. Use persist() to create a persistent recoverable intermediate point.
	at org.apache.flink.core.fs.RefCountedBufferingFileStream.sync(RefCountedBufferingFileStream.java:111) 
	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.sync
	at org.apache.flink.formats.csv.CsvBulkWriter.finish(CsvBulkWriter.java:106) 
	at org.apache.flink.connector.file.table.FileSystemTableSink$ProjectionBulkFactory$1.finish(FileSystemTableSink.java:653) 
	at org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.closeForCommit(BulkPartWriter.java:64) 
{code}

It looks like we should not call `sync` in CsvBulkWriter, we should just use `flush`.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-17 02:49:16.0,,,,,,,,,,"0|z1c84g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapState.remove(UK key) is better to return the old value.,FLINK-30048,13502991,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,xljtswf,xljtswf,17/Nov/22 02:42,21/Nov/22 11:27,04/Jun/24 20:41,21/Nov/22 11:27,,,,,,,,,,,API / DataStream,Runtime / State Backends,,,0,,,,"Hi all:

I found the MapState.remove(UK key) just returns nothing, and the Java Map Interface returns the old value associated with the key.

Consider the flollowing example from the Learn Flink Event-driven Application

[链接标题|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/learn-flink/event_driven/#the-ontimer-method] ,

we want to get the value with the timestamp and then delete the timestamp, with current inplement, we must first call mapState.get(key),then call mapState.remove(key). it will search the key 2 times, I think it is not necessary. If mapState.remove(key) can return the old value, then we can just call mapState.remove(key) and get the old value and save the unnecessary 2nd search in the map.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 10:45:46 UTC 2022,,,,,,,,,,"0|z1c83s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 09:35;wanglijie;I think it makes sense. [~Yanfei Lei] , what do you think?;;;","17/Nov/22 12:04;masteryhx;For RocksDBStateBackend, 'Remove' just add a delete flag to writebuffer, it's very fast.

If we want to return the old value, RocksDBStateBackend has to call get firstly which is not necessary and make 'Remove' slower.


For HashMapStateBackend, I think benefits are limited because they are memory operations.

 

Considering the consistency of interface, I prefer to leave the interface as it is.;;;","18/Nov/22 03:14;wanglijie;Thanks for [~masteryhx] 's detailed explanation. If so, +1 for leaving it as is.;;;","18/Nov/22 10:45;Yanfei Lei;Sorry for the late reply. As [~masteryhx] mentioned, since the data structures of state backends are different from [Java Map|https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java#l796], 'Remove' is not in-place, returning the original value on 'remove' requires additional cost, I'm also +1 for leaving the interface as it is.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
getLastSavepointStatus should return null when there is never savepoint completed or pending,FLINK-30047,13502956,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,claraxiong,claraxiong,claraxiong,16/Nov/22 20:39,17/Nov/22 06:48,04/Jun/24 20:41,17/Nov/22 06:48,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,Current SUCCEEDED is returned in this case but null should be returned instead to distinguish from really success.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29695,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 06:48:59 UTC 2022,,,,,,,,,,"0|z1c7w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 06:48;gyfora;merged to main 215912b08ab0d915be4e84be17f5df97fd4bb6b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot use digest in Helm chart to reference image,FLINK-30046,13502917,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,tshofer,tshofer,16/Nov/22 15:30,07/Dec/22 15:07,04/Jun/24 20:41,07/Dec/22 15:07,kubernetes-operator-1.2.0,kubernetes-operator-1.2.1,kubernetes-operator-1.3.0,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Images can be referenced by tag only.

Referencing images by digest has a number of advantages:
 # Avoid unexpected or undesirable image changes.
 # Increase security and awareness by knowing the specific image running in your environment.

The following document describes a template to handle tags and digests:

[Adding Image Digest References to Your Helm Charts|https://blog.andyserver.com/2021/09/adding-image-digest-references-to-your-helm-charts/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 15:07:36 UTC 2022,,,,,,,,,,"0|z1c7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 15:30;mbalassi;[~tshofer] the link you provided is no longer available. Could you give me access to the guidance in an alternative way please?;;;","30/Nov/22 11:07;tshofer;Hi [~mbalassi], I'm sorry for the situation. I try to contact the webmaster to get a copy.

The guide mainly proposed to use a single value named {{version}} to handle tags and digests transparently. The guide also contained a proposal of the changes in the Helm chart templates. Nothing worldshaking if you're used to create Helm charts (but I'm not).;;;","30/Nov/22 12:03;mbalassi;[~tshofer] that makes sense, thanks. For backwards compatibility reasons I will need to keep the tag configuration option, so I will simply add the digest as an additional option which takes precedence if defined (the tag is ignored in this case).

I would strongly prefer the option you suggest if we implemented this from scratch without prior releases.

I will work on this later this week.;;;","05/Dec/22 16:19;morhidi;[~mbalassi] are we keeping this in 1.3?;;;","06/Dec/22 14:53;mbalassi;Yes [~morhidi] ;;;","07/Dec/22 15:07;morhidi;added to main via 0365416773390ae7a4946ba7180abb9e66b0dd86;;;",,,,,,,,,,,,,,,,,,,,,,,,,
FromClasspathEntryClassInformationProvider too eager to verify MainClass,FLINK-30045,13502881,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,16/Nov/22 11:24,18/Nov/22 07:37,04/Jun/24 20:41,18/Nov/22 07:37,1.16.0,1.17.0,,,,1.17.0,,,,,Client / Job Submission,,,,0,pull-request-available,,,"{{FromClasspathEntryClassInformationProvider}} is attempting to verify (eagerly) whether the given MainClass is on the user classpath. However, it doesn't handle cases where the main class is inside a nested jar.

We actually don't need this check at all since {{PackagedProgram}} is already doing it while attempting to load the main class. Having this once should be enough.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 07:37:23 UTC 2022,,,,,,,,,,"0|z1c7fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 07:37;chesnay;master: 380d8c22d17135fc4164e62d78c769bb69e5aa8a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deduplicate plugin parser loops,FLINK-30044,13502880,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Nov/22 11:19,22/Nov/22 13:51,04/Jun/24 20:41,22/Nov/22 13:51,,,,,,1.17.0,,,,,Build System / CI,,,,0,pull-request-available,,,"The flink-ci-tools parse the output of various modules and each have their own looping logic to that end, that are pretty much identical though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 13:51:40 UTC 2022,,,,,,,,,,"0|z1c7fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 13:51;chesnay;master: 06fcd34eb2646f5e33796ca6bb411728acdfff62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some example sqls in flink table store rescale-bucket doucument are incorrect.,FLINK-30043,13502879,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,StarBoy1005,StarBoy1005,StarBoy1005,16/Nov/22 11:04,17/Nov/22 03:43,04/Jun/24 20:41,17/Nov/22 03:43,1.16.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"[rescale-bucket|https://nightlies.apache.org/flink/flink-table-store-docs-release-0.2/docs/development/rescale-bucket/#use-case]
For example, in this table (in table store catalog),it's columns were defined as the 4 :
""    trade_order_id BIGINT,
    item_id BIGINT,
    item_price DOUBLE,
    dt STRING,""
So when get start “insert overwrite” , probably have no relation with the column “order_status ” in the table ""raw_orders"".
 !image-2022-11-16-18-30-24-261.png! 

What's more, I guess it's better append a DDL example to create the temporary table ""raw_orders"" in this character.
And not all the sqls have a “;” in the end. Maybe could do some little adjustment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/22 10:32;StarBoy1005;image-2022-11-16-18-30-24-261.png;https://issues.apache.org/jira/secure/attachment/13052279/image-2022-11-16-18-30-24-261.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 03:43:47 UTC 2022,,,,,,,,,,"0|z1c7f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 03:43;lzljs3620320;master: c2df467406e7817b1f308cd4a8eb1c5baaa5f294;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create new CI image with Maven 3.8.6,FLINK-30042,13502877,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Nov/22 10:50,18/Nov/22 08:30,04/Jun/24 20:41,18/Nov/22 07:41,,,,,,,,,,,Build System / CI,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 07:41:15 UTC 2022,,,,,,,,,,"0|z1c7eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 07:41;chesnay;https://hub.docker.com/layers/chesnay/flink-ci/java_8_11_17_maven_386
-github.com/zentol/flink-ci-docker/commit/33f472d34b205f829648d0e77fe1cd28554c92f2-
https://github.com/apache/flink-connector-shared-utils/commit/5a2613d842a6cc2dcf4d7107cb74ce1d5e934f10;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup conjars https mirror,FLINK-30041,13502875,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Nov/22 10:48,21/Nov/22 11:40,04/Jun/24 20:41,21/Nov/22 11:40,,,,,,1.17.0,,,,,Build System / CI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 11:37:13 UTC 2022,,,,,,,,,,"0|z1c7e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 13:21;chesnay;master:
9ee56a19f79771b55af685f7179d2563f97e6461
0eee3432b42522d4b78c90b3d2071ea5b46b554f;;;","21/Nov/22 11:34;chesnay;This only works for private builds, not for those on the alibaba machines.;;;","21/Nov/22 11:37;chesnay;We'll need to -setup a {{http://conjars.org/repo/}} mirror in the nexus instances on the CI machines and- setup a mirror in {{alibaba-mirror-settings.xml}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more PyFlink API examples,FLINK-30040,13502870,13500812,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hxbks2ks,hxbks2ks,16/Nov/22 10:02,16/Nov/22 10:03,04/Jun/24 20:41,,1.15.3,1.16.0,,,,,,,,,API / Python,Documentation,,,0,,,,Many APIs of PyFlink currently do not provide corresponding examples,,,,,,,,,,,,,,,FLINK-28957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-16 10:02:11.0,,,,,,,,,,"0|z1c7d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert overwrite show excetion that query and insert schema do not match,FLINK-30039,13502866,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,StarBoy1005,StarBoy1005,16/Nov/22 09:51,19/Mar/23 05:52,04/Jun/24 20:41,19/Mar/23 05:52,1.16.0,,,,,,,,,,Table Store,,,,0,,,,"After change a table's bucket num in table store,I used a insert overwrite sql to reorganize records to new files (I guess form old orc file to the new).
Everytime if select column contains the partition column,then get the next exception: 
 !screenshot-2.png! 
When exclude the partition field,it do perform the overwrite.
I wonder the “EXPR$3: STRING NOT NULL” means what , ",Flink 1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/22 09:54;StarBoy1005;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13052275/screenshot-2.png","16/Nov/22 10:01;StarBoy1005;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13052276/screenshot-3.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 06:18:46 UTC 2022,,,,,,,,,,"0|z1c7c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 10:01;StarBoy1005;Besides, when it was two partitions seems work:
 !screenshot-3.png! ;;;","18/Nov/22 06:18;lzljs3620320;Hi [~StarBoy1005], if you are using static partition inserting, you can not select dt field in your sql.
Example can be found in https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/sql/insert/#examples ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveE2E test is not stable,FLINK-30038,13502832,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,16/Nov/22 06:35,31/Jan/23 04:25,04/Jun/24 20:41,31/Jan/23 04:25,table-store-0.3.0,,,,,table-store-0.4.0,,,,,Table Store,,,,0,pull-request-available,,,"https://github.com/apache/flink-table-store/actions/runs/3476726197/jobs/5812201704

Caused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for log output matching '.*Starting HiveServer2.*'
	at org.testcontainers.containers.wait.strategy.LogMessageWaitStrategy.waitUntilReady(LogMessageWaitStrategy.java:49)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:25:12 UTC 2023,,,,,,,,,,"0|z1c74o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 04:25;lzljs3620320;master: d1f807a4183d357a4282ac831f29252e547a890b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the efficiency of Flink ML Python CI,FLINK-30037,13502829,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,16/Nov/22 06:26,29/Nov/22 07:20,04/Jun/24 20:41,29/Nov/22 07:20,ml-2.1.0,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"It took about thirty minutes to execute Flink ML's python CI[1] for now, which has obviously affected the efficiency of Flink ML development. Thus we need to reduce the total execution time of Flink ML Python CI.

[1] https://github.com/apache/flink-ml/actions/runs/3475256961",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-16 06:26:34.0,,,,,,,,,,"0|z1c740:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Force delete pod when  k8s node is not ready,FLINK-30036,13502807,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kevin_123,kevin_123,16/Nov/22 04:08,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,Deployment / Kubernetes,,,,0,auto-deprioritized-major,pull-request-available,,"When the K8s node is in the NotReady state, the taskmanager pod scheduled on it is always in the terminating state. When the flink cluster has a strict quota, the terminating pod will hold the resources all the time. As a result, the new taskmanager pod cannot apply for resources and cannot be started.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 02:26;kevin_123;image-2022-11-17-10-25-59-945.png;https://issues.apache.org/jira/secure/attachment/13052305/image-2022-11-17-10-25-59-945.png",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:15 UTC 2023,,,,,,,,,,"0|z1c6z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 10:32;wangyang0918;I prefer to believe the terminating pod should not be counted into the quota. Given that when the node turns to ready state, the terminating pod will be cleaned up automatically. I lean towards not to introduce such magic logic in the {{{}FlinkKubeClient#stopPod{}}}.;;;","17/Nov/22 01:43;wangyang0918;After more investigation, it seems that the terminating pods are counted into the used quota. Then I think this ticket is a valid issue. We may need a config option to enable force-delete when the pod might block at terminating(e.g. node not ready).

I have one more concern that node not ready does not always mean the pod will block at terminating status. Force delete will send a SIGKILL to pod and the TM will not have the chance for the clean-up.;;;","17/Nov/22 02:49;kevin_123;Just as [~wangyang0918] said, there are a number of conditions that can cause a pod to remain in a terminating state, so we need to add a config option to determinate the pod should be force-delete or not.

For the concern that node not ready does not always mean the pod will block at terminating status. I lean to think that node not ready will certainly cause the pod block in a terminating status. The node object of k8s api describe the *Ready* of *NodeCondition* as {^}[1]{^}：
{quote}{{True}} if the node is healthy and ready to accept pods, {{False}} if the node is not healthy and is not accepting pods, and {{Unknown}} if the node controller has not heard from the node in the last {{node-monitor-grace-period}} (default is 40 seconds)’
{quote}
When the node is unreachable, pod will be as {^}[2]{^}: 
{quote}A Pod is not deleted automatically when a node is unreachable. The Pods running on an unreachable Node enter the 'Terminating' or 'Unknown' state after a [timeout|https://kubernetes.io/docs/concepts/architecture/nodes/#condition]. Pods may also enter these states when the user attempts graceful deletion of a Pod on an unreachable Node. The only ways in which a Pod in such a state can be removed from the apiserver are as follows:
 * The Node object is deleted (either by you, or by the [Node Controller|https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller]).
 * The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry from the apiserver.
 * Force deletion of the Pod by the user.

The recommended best practice is to use the first or second approach. If a Node is confirmed to be dead (e.g. permanently disconnected from the network, powered down, etc), then delete the Node object. If the Node is suffering from a network partition, then try to resolve this or wait for it to resolve. When the partition heals, the kubelet will complete the deletion of the Pod and free up its name in the apiserver.
{quote}
For flink itself, in the k8s environment, when the taskmanager connection times out, resourcemanager will try to delete the tm pod. If the cause of the timeout is detected, Resourcemanager can forcibly delete the pod to quickly recover the task, because real-time performance is very important to flink.

{*}[1]{*}.Node conditions:[Nodes | Kubernetes|https://kubernetes.io/docs/concepts/architecture/nodes/#condition]

{*}[2].{*}Delete pods:[Force Delete StatefulSet Pods | Kubernetes|https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#delete-pods];;;","02/Dec/22 07:30;kevin_123;TM Pod in terminating status:
!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/44456401/1669954379685-387ff2de-2e29-4b43-9ddf-90ca1b185671.png|width=1429,id=uec269415!
The node which pod on it's conditions are:
!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/44456401/1669953309755-6d6d95b9-bdf3-4c0c-bb55-d13a459c8dbc.png?x-oss-process=image%2Fresize%2Cw_1500%2Climit_0!
!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/44456401/1669953318718-81b4db9a-1bc1-46b7-8f94-792819c4d277.png?x-oss-process=image%2Fresize%2Cw_1500%2Climit_0! We can see when the kubelet can not post node status, the node status is Unknown.
 ;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
./bin/sql-client.sh won't import external jar into the session,FLINK-30035,13502801,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stevenz3wu,stevenz3wu,16/Nov/22 03:43,14/Dec/22 01:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,Table SQL / Client,,,,0,,,,"I used to be able to run the sql-client with iceberg-flink-runtime jar using the `-j,--jar <JAR file>` option (e.g. with 1.15.2). 

{code}
./bin/sql-client.sh embedded --jar iceberg-flink-runtime-1.16-1.1.0.jar
{code}

With 1.16.0, this doesn't work anymore. As a result, I am seeing ClassNotFoundException.

{code}
java.lang.ClassNotFoundException: org.apache.iceberg.hadoop.HadoopCatalog
{code}

I have to put the iceberg-flink-runtime-1.16-1.1.0.jar file inside the `flink/lib` directory to make the jar loaded. This seems like a regression of 1.16.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 01:35:50 UTC 2022,,,,,,,,,,"0|z1c6xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 04:00;luoyuxia;Thanks for reporting. The regression should be reproduced in FLINK-14055. [~lsy] Could you please have a look?;;;","17/Nov/22 03:21;fsk119;Hi. Could you share more exception stack and the steps to reproduce this.;;;","27/Nov/22 20:52;stevenz3wu;[~fsk119] here are the steps to reproduce with 1.16.0. note that jar import works fine for 1.15.2.

* download the [iceberg-flink-runtime jar|https://repository.apache.org/content/repositories/orgapacheiceberg-1114/org/apache/iceberg/iceberg-flink-runtime-1.16/1.1.0/iceberg-flink-runtime-1.16-1.1.0.jar] without putting it into  Flink`lib` dir
* start sql-client with importing external jar
{code}
./bin/sql-client.sh embedded --jar /path/to/iceberg-flink-runtime-1.16-1.1.0.jar
{code}
* run SQL cmd
{code}
CREATE CATALOG hadoop_catalog WITH (
  'type'='iceberg',
  'catalog-type'='hadoop',
  'warehouse'='file:/Users/stevenwu/runtime/hdfs',
  'property-version'='1'
);
{code}

* Then we shall see exception
{code}
[ERROR] Could not execute SQL statement. Reason:
java.lang.NoSuchMethodException: Cannot find constructor for interface org.apache.iceberg.catalog.Catalog
	Missing org.apache.iceberg.hadoop.HadoopCatalog [java.lang.ClassNotFoundException: org.apache.iceberg.hadoop.HadoopCatalog]
{code}

* We have to put the jar inside Flink `lib/` dir for the jar to be loaded. Then the same SQL cmd will execute fine.
{code}
cp /path/to/iceberg-flink-runtime-1.16-1.1.0.jar lib/
./bin/sql-client.sh embedded
{code}


;;;","14/Dec/22 01:35;luoyuxia;May similar issue to FLINK-30318, after FLINK-14055, we won't wrap therad's classload using Thread.currentThread().setContextClassLoader() with the added jars in SQL Client CLI, so if the any class in iceberg-flink-runtime-1.16-1.1.0.jar try to load the class using Thread.currentThread().setContextClassLoader(), it may well get an classload without all the added jars which is different from 1.15.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
class not found exception when using Hive 3.1.3 connector,FLINK-30034,13502795,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,luoyuxia,luoyuxia,16/Nov/22 03:00,29/Jan/23 02:24,04/Jun/24 20:41,29/Jan/23 02:24,,,,,,,,,,,Connectors / Hive,,,,0,,,,"When using Hive 3.1.3 connector, I still found the class not found exceptioin.

We need to include 
{code:java}
<include>org.apache.thrift:libfb303</include> {code}
in pom.xml",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 02:23:00 UTC 2023,,,,,,,,,,"0|z1c6wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 03:17;luoyuxia;[~samrat007]  Could you please help fix again?

Apologize for missing it while reviewing your pr.;;;","29/Jan/23 02:23;luoyuxia;Miss to include it in FLINK-29878;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add primary key data type validation,FLINK-30033,13502792,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,16/Nov/22 02:42,18/Nov/22 04:00,04/Jun/24 20:41,18/Nov/22 04:00,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"Add primary key data type validation, table store can refer to hive in https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 04:00:32 UTC 2022,,,,,,,,,,"0|z1c6vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 04:00;lzljs3620320;master: 03cfe956b63768e6743516e842c4763ecc760c16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IOException during MAX_WATERMARK emission causes message missing,FLINK-30032,13502736,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,functioner,functioner,15/Nov/22 18:08,17/Nov/22 02:17,04/Jun/24 20:41,,1.14.0,,,,,,,,,,,,,,0,,,,"We are doing testing on Flink (version 1.14.0). We launch 1 StandaloneSessionClusterEntrypoint and 2 TaskManagerRunner. Then we run a Flink client which submit a WordCount workload. The code is similar to [https://github.com/apache/flink/blob/release-1.14.0/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/wordcount/WordCount.java], and we only add a Kafka topic output:
{code:java}
    private static DataStreamSink<String> addKafkaSink(
            final DataStream<String> events, final String brokers, final String topic) {
        return events.sinkTo(KafkaSink.<String>builder()
                .setBootstrapServers(brokers)
                .setRecordSerializer(
                        KafkaRecordSerializationSchema.<String>builder()
                                .setValueSerializationSchema(new SimpleStringSchema())
                                .setTopic(topic)
                                .build())
                .build());
    }

    public static void run(final String[] args) throws Exception {
        final String brokers = args[0];
        final String textFilePath = args[1];
        final String kafkaTopic = args[2];
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
        final DataStream<String> text = env.readTextFile(textFilePath);
        final DataStream<Tuple2<String, Integer>> counts =
                text.flatMap(new Tokenizer()).keyBy(value -> value.f0).sum(1);
        addKafkaSink(counts.map(String::valueOf), brokers, kafkaTopic);
        final long nano = System.nanoTime();
        env.execute(""WordCount"");
        FlinkGrayClientMain.reply(""success"", nano);
    }
 {code}
We found that sometimes the Kafka topic fails to receive a few messages. We reproduce the symptom multiple times. We found that the Kafka topic always gets 160~169 messages while the expected number of messages is 170. We also found that the missing messages are always the expected last few messages from the 170 expected messages.

Then we inspect the logs and code.

First, we have an IOException to one of the TaskManagerRunner:
{code:java}
2021-11-02T17:43:41,070 WARN  source.ContinuousFileReaderOperator (ContinuousFileReaderOperator.java:finish(461)) - unable to emit watermark while closing
org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.emitWatermark(ChainingOutput.java:114) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:40) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndEmitWatermark(StreamSourceContexts.java:428) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.emitWatermark(StreamSourceContexts.java:544) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.emitWatermark(StreamSourceContexts.java:113) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.finish(ContinuousFileReaderOperator.java:459) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.finishOperator(StreamOperatorWrapper.java:211) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferFinishOperatorToMailbox$3(StreamOperatorWrapper.java:185) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:97) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndFinishOperator(StreamOperatorWrapper.java:162) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.finish(StreamOperatorWrapper.java:130) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.finishOperators(RegularOperatorChain.java:117) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.endData(StreamTask.java:549) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:508) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) [flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) [flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) [flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]
Caused by: java.lang.RuntimeException: McGray injected exception
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:121) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:40) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:605) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.emitWatermark(ChainingOutput.java:112) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        ... 24 more
Caused by: java.io.IOException {code}
The IOException is from line 104 in RecordWriter#emit:
{code:java}
    protected void emit(T record, int targetSubpartition) throws IOException {
        checkErroneous();

        targetPartition.emitRecord(serializeRecord(serializer, record), targetSubpartition); // line 104

        if (flushAlways) {
            targetPartition.flush(targetSubpartition);
        }
    } {code}
Here, `targetPartition.emitRecord` will finally call some file I/O or memory map I/O, triggering the IOException for some reason.

This exception is caught at `RecordWriterOutput#emitWatermark`:
{code:java}
    @Override
    public void emitWatermark(Watermark mark) {
        if (announcedStatus.isIdle()) {
            return;
        }

        watermarkGauge.setCurrentWatermark(mark.getTimestamp());
        serializationDelegate.setInstance(mark);

        try {
            recordWriter.broadcastEmit(serializationDelegate);
        } catch (Exception e) {
            throw new RuntimeException(e.getMessage(), e);
        }
    } {code}
And then caught at `ChainingOutput#emitWatermark`:
{code:java}
    @Override
    public void emitWatermark(Watermark mark) {
        if (announcedStatus.isIdle()) {
            return;
        }
        try {
            watermarkGauge.setCurrentWatermark(mark.getTimestamp());
            input.processWatermark(mark);
        } catch (Exception e) {
            throw new ExceptionInChainedOperatorException(e);
        }
    } {code}
And finally caught at `ContinuousFileReaderOperator#finish`:
{code:java}
    @Override
    public void finish() throws Exception {
        LOG.debug(""finishing"");
        super.finish();        switch (state) {
            case IDLE:
                switchState(ReaderState.FINISHED);
                break;
            case FINISHED:
                LOG.warn(""operator is already closed, doing nothing"");
                return;
            default:
                switchState(ReaderState.FINISHING);
                while (!state.isTerminal()) {
                    executor.yield();
                }
        }

        try {
            sourceContext.emitWatermark(Watermark.MAX_WATERMARK);
        } catch (Exception e) {
            LOG.warn(""unable to emit watermark while closing"", e);
        }
    } {code}
Here `Watermark.MAX_WATERMARK` is emitted to properly finish the computation.

In Flink (version 1.14.0), the full call stack of all the aforementioned workflow is:
{code:java}
org.apache.flink.runtime.io.network.api.writer.RecordWriter#emit:104
org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter#broadcastEmit:67
org.apache.flink.streaming.runtime.io.RecordWriterOutput#emitWatermark:119
org.apache.flink.streaming.api.operators.CountingOutput#emitWatermark:40
org.apache.flink.streaming.api.operators.AbstractStreamOperator#processWatermark:605
org.apache.flink.streaming.runtime.tasks.ChainingOutput#emitWatermark:112
org.apache.flink.streaming.api.operators.CountingOutput#emitWatermark:40
org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext#processAndEmitWatermark:428
org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext#emitWatermark:544
org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose#emitWatermark:113
org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator#finish:459
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1#runThrowing:50
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper#finishOperator:211
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper#lambda$deferFinishOperatorToMailbox$3:185
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1#runThrowing:50
org.apache.flink.streaming.runtime.tasks.mailbox.Mail#run:90
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl#tryYield:97
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper#quiesceTimeServiceAndFinishOperator:162
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper#finish:130
org.apache.flink.streaming.runtime.tasks.RegularOperatorChain#finishOperators:117
org.apache.flink.streaming.runtime.tasks.StreamTask#endData:549
org.apache.flink.streaming.runtime.tasks.StreamTask#processInput:508
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor#runMailboxLoop:203
org.apache.flink.streaming.runtime.tasks.StreamTask#runMailboxLoop:809
org.apache.flink.streaming.runtime.tasks.StreamTask#invoke:761
org.apache.flink.runtime.taskmanager.Task#runWithSystemExitMonitoring:958
org.apache.flink.runtime.taskmanager.Task#restoreAndInvoke:937
org.apache.flink.runtime.taskmanager.Task#doRun:766
org.apache.flink.runtime.taskmanager.Task#run:575
java.lang.Thread#run:748 {code}
We think the reason for missing a few ending messages in Kafka topic is in `ChannelSelectorRecordWriter#broadcastEmit` (also in the shown call stack):
{code:java}
    @Override
    public void broadcastEmit(T record) throws IOException {
        checkErroneous();

        // Emitting to all channels in a for loop can be better than calling
        // ResultPartitionWriter#broadcastRecord because the broadcastRecord
        // method incurs extra overhead.
        ByteBuffer serializedRecord = serializeRecord(serializer, record);
        for (int channelIndex = 0; channelIndex < numberOfChannels; channelIndex++) {
            serializedRecord.rewind();
            emit(record, channelIndex);   // line 67
        }

        if (flushAlways) {
            flushAll();
        }
    } {code}
Line 67 tries to emit `Watermark.MAX_WATERMARK` (from `ContinuousFileReaderOperator#finish`) to all channels. When the IOException is thrown here, `ContinuousFileReaderOperator#finish` swallows all the exceptions and loop fails to continue running line 67 for the remaining channels. We reproduce the symptom multiple times and we found the number of missing messages is exactly equal to the number of affected channels.

That being said, we suspect the potential IOException at line 67 is not properly handled because the current symptom and logging is not convenient for the user to notice the issue or debug. The user may suddenly get a few ending messages missing. And then the user can only find that there is some IOException when emitting `Watermark.MAX_WATERMARK` somewhere. The users still don’t know why and how a few ending messages are missing.

We would like to propose a fix for this issue. A simple solution is catching the IOException at line 67 and then do some logging and maybe retry to emit. We implemented this solution and found the symptom disappears. However, we also found that this `broadcastEmit` method is called at many places. So, this fix will also affect the other callers, and we are not sure whether this behavior is also proper for those callers.

We are looking for suggestions and feedback. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 02:17:16 UTC 2022,,,,,,,,,,"0|z1c6jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 20:36;martijnvisser;[~functioner] Given that Flink 1.14 isn't supported by the community anymore, can you verify this with preferably Flink 1.16? ;;;","17/Nov/22 02:17;luoyuxia;Just curious about why this fix can solve this problem. Could you please explain a little bit more?

Another question, when you catch the IOException at line 67 and do the logging, will you throw the IOException again?

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink table store run abnormally when shade flink,FLINK-30031,13502718,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,iture123,iture123,15/Nov/22 15:43,19/Mar/23 05:52,04/Jun/24 20:41,19/Mar/23 05:52,1.15.0,table-store-0.2.1,,,,,,,,,Table Store,,,,0,,,,"{color:#172b4d}I try to sink flink-table-store in Apache SeaTunnel, SeaTunnel use the Flink version is 13.6.{color}

{color:#172b4d}To avoid flink conflict,I use maven-shade-plugin plugin to shade flink dependency.{color}

{color:#172b4d}However, runing build jar to write  flink-table-store occur error,throw exception:{color}

 
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/flink/table/store/codegen/CodeGenerator
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:127)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClass(ComponentClassLoader.java:106)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at java.util.Iterator.forEachRemaining(Iterator.java:116)
        at org.apache.flink.table.store.codegen.CodeGenLoader.discover(CodeGenLoader.java:123)
        at org.apache.flink.table.store.codegen.CodeGenUtils.generateRecordComparator(CodeGenUtils.java:65)
        at org.apache.flink.table.store.file.utils.KeyComparatorSupplier.<init>(KeyComparatorSupplier.java:40)
        at org.apache.flink.table.store.file.KeyValueFileStore.<init>(KeyValueFileStore.java:59)
        at org.apache.flink.table.store.table.ChangelogWithKeyFileStoreTable.<init>(ChangelogWithKeyFileStoreTable.java:103)
        at org.apache.flink.table.store.table.FileStoreTableFactory.create(FileStoreTableFactory.java:72)
        at org.apache.flink.table.store.table.FileStoreTableFactory.create(FileStoreTableFactory.java:50)
        at org.example.TestWrite.main(TestWrite.java:24)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.store.codegen.CodeGenerator
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:127)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClass(ComponentClassLoader.java:106)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        ... 27 more
 {code}
 

 
{code:java}
// pom.xml
<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-store-core</artifactId>
        <version>0.2.1</version>
    </dependency>

    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-store-format</artifactId>
        <version>0.2.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-common</artifactId>
        <version>1.15.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-runtime</artifactId>
        <version>1.15.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-base</artifactId>
        <version>1.15.0</version>
    </dependency>

    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-shaded-jackson</artifactId>
        <version>2.12.1-13.0</version>
    </dependency>
     <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-shaded-hadoop-2-uber</artifactId>
        <version>2.7.5-10.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.commons</groupId>
        <artifactId>commons-lang3</artifactId>
        <version>3.3.2</version>
    </dependency>
    <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>1.2.17</version>
    </dependency>

</dependencies>

<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>3.1.1</version>
            <configuration>
                <!-- no create dependency-reduced-pom.xml-->
                <createDependencyReducedPom>false</createDependencyReducedPom>
            </configuration>
            <executions>
                <!-- Run shade goal on package phase -->
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <relocations>
                            <relocation>
                                <pattern>org.apache.flink</pattern>
                                <shadedPattern>shade.org.apache.flink</shadedPattern>
                                <excludes>
                                    <exclude>org.apache.flink.table.store.**</exclude>
                                </excludes>
                            </relocation>
                        </relocations>
                    </configuration>
                </execution>
            </executions>
        </plugin>

    </plugins>
</build>{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30080,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 06:26:37 UTC 2022,,,,,,,,,,"0|z1c6fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 06:26;lzljs3620320;I created FLINK-30080 for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected behavior for overwrite in Hive dialect,FLINK-30030,13502703,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,luoyuxia,luoyuxia,15/Nov/22 13:02,17/Jan/23 01:35,04/Jun/24 20:41,05/Jan/23 01:42,1.15.0,,,,,1.16.0,1.16.1,,,,Connectors / Hive,,,,0,pull-request-available,,,"When overwrite a table in hive dialect, it might not overwrite but to append.

It can reproduce using the following sql:
{code:java}
insert overwrite table `db.tab` values (1, 2) {code}
only happen when the table is encapsulated by backquote and the table contain two splits with ""."".

 

Just find the behavior in hive is also incorrect as reported in HIVE-26741.

Let'us keep to see what's the expected behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 01:42:26 UTC 2023,,,,,,,,,,"0|z1c6c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 09:04;luoyuxia;Update:

found HIVE-16907, in this jira, Hive community decide to throw excpetion for such case.;;;","16/Nov/22 09:35;luoyuxia;Test with other database:

MySQL/MariaDB: will consider `db.tab` as a table named db.tab

PG/SQLServer: Seems  backquote is not allowed.;;;","22/Nov/22 11:32;jingzhang;[~luoyuxia] Thanks for open this issue.
We also received some flink batch user's complain, they said if they run the following query multiple times, they would got wrong result.
{code:java}
// SQL
insert
  overwrite table `db.tb` partition(p_date)
select ** {code};;;","05/Jan/23 01:42;luoyuxia;After some investigation, the expected behavior should consider `db.tab` as a table named db.tab, so in this case, it should throw table not found exception. Based on such conclusion, from Flink 1.16, the behavior is as expected which will also throw exception after FLINK-29337.

So, I will close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Notice/ShadeParser parse version/classifier separately,FLINK-30029,13502675,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Nov/22 09:52,17/Nov/22 18:50,04/Jun/24 20:41,17/Nov/22 18:50,,,,,,1.17.0,,,,,Build System / CI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 18:50:56 UTC 2022,,,,,,,,,,"0|z1c65s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 18:50;chesnay;master: f734fa717a818968ad811e7ccd88f8aa09d50663;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A little fix with document about period in log.changelog-mode description. ,FLINK-30028,13502659,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,StarBoy1005,StarBoy1005,StarBoy1005,15/Nov/22 08:45,18/Nov/22 06:16,04/Jun/24 20:41,18/Nov/22 06:16,1.16.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"One more period in the end of ”log.changelog-mode“description.
[log.changelog-mode|https://nightlies.apache.org/flink/flink-table-store-docs-release-0.2/docs/development/configuration/#coreoptions]",Flink 0.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 06:16:11 UTC 2022,,,,,,,,,,"0|z1c628:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 06:16;lzljs3620320;master: 5e514aa6ee1f44ea39298b85e5f636b1a1704f33;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fields min and max in BinaryTableStats support lazy deserialization,FLINK-30027,13502640,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Nov/22 07:54,17/Nov/22 02:12,04/Jun/24 20:41,17/Nov/22 02:12,table-store-0.3.0,,,,,table-store-0.3.0,,,,,Table Store,,,,0,pull-request-available,,,"Predicate get min and max from BinaryRowData, lazily deserialization",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 02:12:33 UTC 2022,,,,,,,,,,"0|z1c5y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 02:12;lzljs3620320;master: 4d6bc72aeddea0c1bf551a2542ecc990220f8edd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support database table include option in debezium-json,FLINK-30026,13502633,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pensz,pensz,15/Nov/22 07:06,15/Nov/22 07:06,04/Jun/24 20:41,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,"debezium-json also needs options to only read specific databases / tables changelog rows.

Options similar to canal-json : 
{code:java}
debezium-json.database.include
debezium-json.table.include
{code}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-15 07:06:48.0,,,,,,,,,,"0|z1c5wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unified the max display column width for SqlClient and Table APi in both Streaming and Batch execMode,FLINK-30025,13502339,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Nov/22 19:17,04/Oct/23 08:53,04/Jun/24 20:41,04/Oct/23 08:53,1.15.2,1.16.0,,,,1.18.0,,,,,Table SQL / API,,,,0,pull-request-available,,,"FLIP-279 [https://cwiki.apache.org/confluence/display/FLINK/FLIP-279+Unified+the+max+display+column+width+for+SqlClient+and+Table+APi+in+both+Streaming+and+Batch+execMode]

 

Background info:

table.execute().print() can only use the default max column width. When running table API program ""table.execute().print();"", the columns with long string value are truncated to 30 chars. E.g.,:

!https://static.dingtalk.com/media/lALPF6XTM7ZO1FXNASrNBEI_1090_298.png_620x10000q90.jpg?auth_bizType=%27IM%27&bizType=im|width=457,height=125!

I tried set the max width with: tEnv.getConfig.getConfiguration.setInteger(""sql-client.display.max-column-width"", 100); It has no effect.  How can I set the max-width?

Here is the example code:

val env = StreamExecutionEnvironment.getExecutionEnvironment
val tEnv = StreamTableEnvironment.create(env)

tEnv.getConfig.getConfiguration.setInteger(""sql-client.display.max-column-width"", 100)

val orderA = env
  .fromCollection(Seq(Order(1L, ""beer"", 3), Order(1L, ""diaper-{-}{{-}}.diaper{{-}}{-}{-}.diaper{-}{-}{{-}}.diaper{{-}}{-}-."", 4), Order(3L, ""rubber"", 2)))
  .toTable(tEnv)

orderA.execute().print()

 

""sql-client.display.max-column-width"" seems only work in cli: SET 'sql-client.display.max-column-width' = '40';

While using Table API, by default, the DEFAULT_MAX_COLUMN_WIDTH in PrintStyle is used now. It should be configurable. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31403,,,FLINK-30758,FLINK-32906,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 11:55:19 UTC 2023,,,,,,,,,,"0|z1c43c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 20:01;qinjunjerry;[~jingge]Thanks for opening the Jira. 

What about allowing setting the max width here, as well as disabling truncation entirely? e.g., set to '-1' to disable truncation? as what spark does with 

{code:java}
df.show(false)
{code}
;;;","15/Nov/22 21:43;jingge;Afaik, for streaming, there is no way for Flink to get the max width of the column value. Offering ""-1"" might be fine, but it means that internally Flink has to turn ""-1"" into Integer.MAX_VALUE.  It is better to let user be aware of it and use the right number directly.;;;","22/Feb/23 20:00;Sergey Nuyanzin;it looks like the PR is merged.
Shouldn't be the issue moved to resolved?;;;","22/Feb/23 20:09;jingge;[~Sergey Nuyanzin] Thanks for the hint.

merged into master: 91d6fa21de49d979e01c7a093d054bafdd8e2ba9;;;","08/Sep/23 11:55;jingge;merged into master: 3b9fcb5fd1060abd02d761eff767c6e2bead7046;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Replace old kerberos docker image used in secure e2e tests,FLINK-30024,13502164,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,14/Nov/22 13:11,18/Nov/22 10:21,04/Jun/24 20:41,18/Nov/22 08:18,1.17.0,,,,,1.17.0,,,,,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 08:18:44 UTC 2022,,,,,,,,,,"0|z1c30g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 08:18;martijnvisser;Fixed in master: e822cfdbc9f901be74a91185184c3795f443db3f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogStorageMetricsTest#testTotalAttemptsPerUpload failed,FLINK-30023,13502152,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Weijie Guo,Weijie Guo,14/Nov/22 11:56,24/Jan/24 07:58,04/Jun/24 20:41,14/Aug/23 12:29,1.15.3,1.16.0,1.17.0,,,1.18.0,,,,,Runtime / State Backends,,,,0,pull-request-available,stale-major,test-stability,"java.util.concurrent.TimeoutException: Attempt 3 timed out after 20ms

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43021&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9829]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34224,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 12:29:31 UTC 2023,,,,,,,,,,"0|z1c2xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 07:19;Feifan Wang;Hi [~Weijie Guo] , you can rerun the test, and high probability of success. And later I will modify the parameters of this test case to improve stability.;;;","16/Nov/22 08:20;Weijie Guo;[~Feifan Wang] Thank you for reminding me that I ran CI again successfully. ;;;","11/Jan/23 08:53;mapohl;We see a similar test failure in 1.15. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44691&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=25465

In this build, the CompletableFuture provided by the persist function (see 1.15 code in [ChangelogStorageMetricsTest:202|https://github.com/apache/flink/blob/release-1.15/flink-dstl/flink-dstl-dfs/src/test/java/org/apache/flink/changelog/fs/ChangelogStorageMetricsTest.java#L202] doesn't complete. I'm gonna update the ticket. Please provide backports for the fix as well.;;;","11/Jan/23 09:04;mapohl;I updated the affected Version to also include 1.17.0 (because the currently PR is based on {{master}}). But I added 1.16.0 since 1.15.3 is also affected. Feel free to update this if you think it's wrong.;;;","21/Mar/23 20:01;roman;Merged fix for test{*}Total{*}AttemptsPerUpload into *master* ef5b3173c150fbc3e1394cc6add5de21c13b355d.;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 12:29;mapohl;Looks like the backport was missed. But considering that this issue hasn't popped up, anymore, I'm ok with leaving it like that. The fix only made it to master (i.e. 1.18). I'm gonna resolve the issue accordingly.;;;",,,,,,,,,,,,,,,,,,,,,,,,
ChangelogStorageMetricsTest#testTotalAttemptsPerUpload failed,FLINK-30022,13502151,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Weijie Guo,Weijie Guo,14/Nov/22 11:56,14/Nov/22 12:02,04/Jun/24 20:41,14/Nov/22 12:02,,,,,,,,,,,,,,,0,,,,java.util.concurrent.ExecutionException: java.io.IOException: java.util.concurrent.TimeoutException: Attempt 3 timed out after 20ms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-14 11:56:22.0,,,,,,,,,,"0|z1c2xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table store Hive catalog getTableLocation by hms client,FLINK-30021,13502129,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,wukong,wukong,14/Nov/22 09:46,14/Nov/22 10:52,04/Jun/24 20:41,14/Nov/22 10:52,,,,,,,,,,,Table Store,,,,0,,,,"Currently table store catalog load table location use warehouse base path for hive catalog

which was not convenient for multiple warehouse for query client like spark thrift server.

Maybe we should get table location from hive meta client like below:
{code:java}
    @Override
    public Path getTableLocation(ObjectPath tablePath) {
        try {
            Table table = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());
            return new Path(table.getSd().getLocation());
        } catch (TException e) {
            throw new RuntimeException(""Failed to get table location"", e);
        }
    } {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-14 09:46:52.0,,,,,,,,,,"0|z1c2so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use separate Prometheus CollectorRegistries,FLINK-30020,13502123,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Nov/22 09:17,14/Nov/22 18:08,04/Jun/24 20:41,14/Nov/22 17:59,,,,,,1.17.0,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"The PrometheusReporter uses the singleton collector registry, which means that defining multiple prometheus reporters with different metric filters will not behave as expected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 17:59:37 UTC 2022,,,,,,,,,,"0|z1c2rc:",9223372036854775807,"The PrometheusReporters now use a separate CollectorRegistry for each reporter instance instead of the singleton default registry.
This generally shouldn't impact setups, but it may break code that indirectly interacted with the reporter via the singleton instance (e.g., a test trying to assert what metrics are reported).",,,,,,,,,,,,,,,,,,,"14/Nov/22 17:59;chesnay;master: 4f8a11c48832dd7f8f0804e34aeedd7abd806ef6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the unused HiveTableFileInputFormat in hive connector,FLINK-30019,13502121,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,14/Nov/22 09:14,12/Dec/22 02:42,04/Jun/24 20:41,12/Dec/22 02:42,,,,,,1.17.0,,,,,Connectors / Hive,,,,0,pull-request-available,,,"As I see, after https://issues.apache.org/jira/browse/FLINK-19888 the hive connector do not reply on the HiveTableFileInputFormat now, it can be safely removed now",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 02:42:43 UTC 2022,,,,,,,,,,"0|z1c2qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 14:15;aitozi;cc [~luoyuxia], [~lsy]  can you help review this PR ?;;;","12/Dec/22 02:42;jark;Fixed in master: 7bff4eac5a2c04831a8b20efdc258dad2fc4a3fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE error in generated StreamExecCalc,FLINK-30018,13502111,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackwangcs,jackwangcs,14/Nov/22 08:52,06/Jun/23 02:56,04/Jun/24 20:41,06/Jun/23 02:56,1.16.0,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"Hi, I met a NPE exception running Flink SQL. The exception is
{code:java}
rg.apache.flink.runtime.taskmanager.Task                    [] - Join[292] -> Calc[293] -> ConstraintEnforcer[294] (10/48)#0 (e628391c0b38d4d22ae62a181a2d7f22_c9cd1581189658451a8850505c8a0007_9_0) switched from RUNNING to FAILED with failure cause: java.lang.NullPointerException
    at StreamExecCalc$20690.processElement_split881(Unknown Source)
    at StreamExecCalc$20690.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.outputNullPadding(StreamingJoinOperator.java:334)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.processElement(StreamingJoinOperator.java:219)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.processElement1(StreamingJoinOperator.java:124)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.processRecord1(StreamTwoInputProcessorFactory.java:217)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.lambda$create$0(StreamTwoInputProcessorFactory.java:183)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:266)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:750) {code}
`StreamExecCalc$20690.processElement_split881()` function is below:
{code:java}
private final org.apache.flink.table.data.binary.BinaryStringData str$19838 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""N"");

void processElement_split881(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
if (isNull$19828) {
            out.setNullAt(4);
          } else {
            out.setNonPrimitiveValue(4, field$19829);
          }
writer$19903.reset();
writer$19903.writeBoolean(0, ((boolean) false));
if (isNull$19830) {
            writer$19903.setNullAt(1);
          } else {
            writer$19903.writeLong(1, field$19830);
          }
isNull$19831 = isNull$19821 || false;
result$19832 = false;
if (!isNull$19831) {
            
          
          result$19832 = field$19821 == ((long) 44571L);
          
            
          }
result$19834 = -1L;
if (result$19832) {
            
            if (!isNull$19833) {
              result$19834 = field$19833;
            }
            isNull$19834 = isNull$19833;
          } else {
            
            if (!false) {
              result$19834 = ((long) 0L);
            }
            isNull$19834 = false;
          }
if (isNull$19834) {
            writer$19903.setNullAt(2);
          } else {
            writer$19903.writeLong(2, result$19834);
          }
isNull$19851 = false;
if (!isNull$19851) {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$19838).numChars() > 1) {
          result$19852 = ((org.apache.flink.table.data.binary.BinaryStringData) str$19838).substring(0, 1);
          } else {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$19838).numChars() < 1) {
          
          padLength$19853 = 1 - ((org.apache.flink.table.data.binary.BinaryStringData) str$19838).numChars();
          
          padString$19854 = org.apache.flink.table.data.binary.BinaryStringData.blankString(padLength$19853);
          result$19852 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(((org.apache.flink.table.data.binary.BinaryStringData) str$19838), padString$19854);
          } else {
          result$19852 = ((org.apache.flink.table.data.binary.BinaryStringData) str$19838);
          }
          }
          isNull$19851 = result$19852 == null;
          } else {
          result$19852 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          }
isNull$19855 = isNull$19850;
if (!isNull$19855) {
          if (result$19850.numChars() > 1) {
          result$19856 = result$19850.substring(0, 1);
          } else {
          if (result$19850.numChars() < 1) {
          
          padLength$19857 = 1 - result$19850.numChars();
          
          padString$19858 = org.apache.flink.table.data.binary.BinaryStringData.blankString(padLength$19857);
          result$19856 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(result$19850, padString$19858);
          } else {
          result$19856 = result$19850;
          }
          }
          isNull$19855 = result$19856 == null;
          } else {
          result$19856 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          }
result$19859 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
} {code}
 

Could you take a look at this issue? I could not find why NPE is thrown.

 ",,,,,,,,,,,,,,,,,FLINK-30966,,,,,,,FLINK-30559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 08:06:12 UTC 2023,,,,,,,,,,"0|z1c2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 06:51;luoyuxia;[~jackwangcs] Thanks for reporting. Could you please share us the sql so that we can try to reproduce it?;;;","19/May/23 08:06;yunta;I think this problem is the same as FLINK-30559.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove aggregate-scaladoc profile,FLINK-30017,13502109,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Nov/22 08:49,14/Nov/22 10:46,04/Jun/24 20:41,14/Nov/22 10:46,,,,,,1.17.0,,,,,Build System,Documentation,,,0,pull-request-available,,,Empty profile that was required in the past when the documentation generation script lived externally and had to support all versions at once.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 10:46:21 UTC 2022,,,,,,,,,,"0|z1c2o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 10:46;chesnay;master: 278642219ebf4b68a02a3901fe06a7cb006d105b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink 1.16 release notes about updated oshi-core,FLINK-30016,13502101,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,czchen,martijnvisser,martijnvisser,14/Nov/22 08:29,19/Jan/23 10:28,04/Jun/24 20:41,14/Nov/22 13:58,,,,,,1.16.1,1.17.0,,,,Documentation,,,,0,pull-request-available,,,The Flink 1.16 release notes don't mention that system resources reporting now requires a newer version of oshi-core. The required dependency is listed at https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/metrics/#system-resources but it's a newer version compared to https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/ops/metrics/#system-resources,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 13:58:49 UTC 2022,,,,,,,,,,"0|z1c2mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 13:58;martijnvisser;Fixed in
master: 67966e7aa91804940d0dedf3e210d352d1a6035c
release-1.16: 8de5da1c9f67753e76449f3edb466276ca32aba7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks are failing,FLINK-30015,13502093,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Yanfei Lei,martijnvisser,martijnvisser,14/Nov/22 07:57,14/Nov/22 14:00,04/Jun/24 20:41,14/Nov/22 14:00,,,,,,,,,,,Benchmarks,,,,0,test-stability,,,"{code:java}
Build interrupted 1411 of flink-master-benchmarks-regression-check (Open): org.jenkinsci.plugins.workflow.steps.FlowInterruptedException
{code}

Build 1405 until 1411 have all failed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 14:00:13 UTC 2022,,,,,,,,,,"0|z1c2ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 08:03;Yanfei Lei;This was caused by waiting for the available executor.

http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1411/console
{code:java}
Started by timer
Obtained jenkinsfiles/regression-check.jenkinsfile from git https://github.com/apache/flink-benchmarks.git
Running in Durability level: MAX_SURVIVABILITY
[Pipeline] Start of Pipeline
[Pipeline] timestamps
[Pipeline] {
[Pipeline] timeout
08:38:01  Timeout set to expire in 3 hr 0 min
[Pipeline] {
[Pipeline] node
08:38:16  Still waiting to schedule task
08:38:16  Waiting for next available executor on ‘hetzner EX42-NVMe-1245348’
11:38:01  Cancelling nested steps due to timeout
[Pipeline] // node
[Pipeline] }
[Pipeline] // timeout
[Pipeline] slackSend
11:38:01  Slack Send Pipeline step running, values are - baseUrl: <empty>, teamDomain: apache-flink, channel: #flink-dev-benchmarks, color: <empty>, botUser: false, tokenCredentialId: 2d52c4a5-ab95-42f5-b9b7-eb1a1a95b232, notifyCommitters: false, iconEmoji: <empty>, username: <empty>
[Pipeline] }
[Pipeline] // timestamps
[Pipeline] End of Pipeline
Finished: SUCCESS {code}
 
I changed the build trigger to ""H 11 * * *"" to reduce resource competition.

 ;;;","14/Nov/22 14:00;martijnvisser;Looks to have been resolved, thanks [~Yanfei Lei];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the NPE from aggregate util,FLINK-30014,13502091,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,14/Nov/22 07:46,10/Jan/23 04:05,04/Jun/24 20:41,10/Jan/23 04:05,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"The following exception is thrown in Flink ML CI step.
{code:java}
[INFO] Running org.apache.flink.ml.feature.CountVectorizerTest
435Error:  Tests run: 12, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.419 s <<< FAILURE! - in org.apache.flink.ml.feature.CountVectorizerTest
436Error:  testFitAndPredict  Time elapsed: 0.66 s  <<< ERROR!
437java.lang.RuntimeException: Failed to fetch next result
438	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
439	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
440	at org.apache.commons.collections.IteratorUtils.toList(IteratorUtils.java:848)
441	at org.apache.commons.collections.IteratorUtils.toList(IteratorUtils.java:825)
442	at org.apache.flink.ml.feature.CountVectorizerTest.verifyPredictionResult(CountVectorizerTest.java:120)
443	at org.apache.flink.ml.feature.CountVectorizerTest.testFitAndPredict(CountVectorizerTest.java:208)
444	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
445	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
446	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
447	at java.lang.reflect.Method.invoke(Method.java:498)
448	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
449	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
450	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
451	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
452	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
453	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
454	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
455	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
456	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
457	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
458	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
459	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
460	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
461	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
462	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
463	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
464	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
465	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
466	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
467	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
468	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
469	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
470	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
471	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
472	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
473	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
474	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
475	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
476	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
477	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
478	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
479	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
480	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
481	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
482	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)
483	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:109)
484	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
485	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
486	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
487	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
488Caused by: java.io.IOException: Failed to fetch job execution result
489	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
490	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
491	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
492	... 49 more
493Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
494	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
495	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
496	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
497	... 51 more
498Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
499	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
500	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
501	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
502	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
503	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
504	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
505	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
506	... 51 more
507Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
508	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
509	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
510	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
511	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
512	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
513	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
514	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
515	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
516	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
517	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
518	at java.lang.reflect.Method.invoke(Method.java:498)
519	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
520	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
521	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
522	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
523	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
524	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
525	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
526	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
527	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
528	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
529	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
530	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
531	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
532	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
533	at akka.actor.Actor.aroundReceive(Actor.scala:537)
534	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
535	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
536	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
537	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
538	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
539	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
540	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
541	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
542	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
543	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
544	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
545Caused by: java.io.IOException: Could not perform checkpoint 1 for operator aggregate -> *anonymous_datastream_source$658*[743] -> TableToDataSteam (1/1)#0.
546	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1210)
547	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
548	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
549	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
550	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:493)
551	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
552	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
553	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
554	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
555	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
556	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
557	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
558	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
559	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
560	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
561	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
562	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
563	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
564	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
565	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
566	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
567	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
568	at java.lang.Thread.run(Thread.java:750)
569Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator aggregate -> *anonymous_datastream_source$658*[743] -> TableToDataSteam (1/1)#0. Failure reason: Checkpoint was declined.
570	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269)
571	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173)
572	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:348)
573	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:227)
574	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:212)
575	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192)
576	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:647)
577	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:320)
578	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253)
579	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
580	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241)
581	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198)
582	... 22 more
583Caused by: java.lang.NullPointerException: You cannot add null to a ListState.
584	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:76)
585	at org.apache.flink.runtime.state.PartitionableListState.add(PartitionableListState.java:94)
586	at org.apache.flink.ml.common.datastream.DataStreamUtils$AggregateOperator.snapshotState(DataStreamUtils.java:463)
587	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222)
588	... 33 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-14 07:46:20.0,,,,,,,,,,"0|z1c2k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
