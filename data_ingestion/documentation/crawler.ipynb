{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from collections import deque\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, max_depth=2, max_pages=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_pages = max_pages\n",
    "        self.seen = set()\n",
    "        self.pages = []\n",
    "        self.queue = deque()\n",
    "\n",
    "    def add_to_queue(self, url, depth):\n",
    "        self.queue.append({'url': url, 'depth': depth})\n",
    "\n",
    "    def should_continue_crawling(self):\n",
    "        return self.queue and len(self.pages) < self.max_pages\n",
    "\n",
    "    def is_too_deep(self, depth):\n",
    "        return depth > self.max_depth\n",
    "\n",
    "    def is_already_seen(self, url):\n",
    "        return url in self.seen\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for a in soup.find_all('a'):\n",
    "            a.attrs = {}\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "\n",
    "    def extract_urls(self, html, base_url):\n",
    "        urls = list()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        relative_urls = [a.get('href') for a in soup.find_all('a') if a.get('href')]\n",
    "        for relative_url in relative_urls:\n",
    "            if \"zh\" in relative_url.split(\"/\"):\n",
    "                continue\n",
    "            if relative_url.startswith(\"#\"):\n",
    "                continue\n",
    "            if  relative_url.startswith(\"//nightlies.apache.org/flink/flink-docs-release-1.16/\"):\n",
    "                urls.append(urljoin(\"https:\", relative_url))\n",
    "                # logging.error(f\" -- {relative_url}, {urls[-1]}\")\n",
    "            elif relative_url.startswith(\"https://nightlies.apache.org/flink/flink-docs-release-1.16/\"):\n",
    "                urls.append(relative_url)\n",
    "                # logging.error(f\" -- {relative_url}, {urls[-1]}\")\n",
    "            elif \"https://nightlies.apache.org/flink/flink-docs-release-1.16/\" not in relative_url:\n",
    "                #urls.append(urljoin(\"https://nightlies.apache.org/flink/flink-docs-release-1.16/\", relative_url))\n",
    "                logging.error(f\"{base_url} -- {relative_url}\")\n",
    "        return urls\n",
    "\n",
    "    def crawl(self, start_url):\n",
    "        self.add_to_queue(start_url, 0)\n",
    "\n",
    "        while self.should_continue_crawling():\n",
    "            current = self.queue.popleft()\n",
    "            url, depth = current['url'], current['depth']\n",
    "\n",
    "            if self.is_too_deep(depth) or self.is_already_seen(url):\n",
    "                continue\n",
    "\n",
    "            self.seen.add(url)\n",
    "            html = self.fetch_page(url)\n",
    "            # print(f\"Fetched {url}\")\n",
    "            if html:\n",
    "                print(f\"Parsing {url}\")\n",
    "                self.pages.append({'url': url, 'content': self.parse_html(html)})\n",
    "                new_urls = self.extract_urls(html, url)\n",
    "                for new_url in new_urls:\n",
    "                    self.add_to_queue(new_url, depth + 1)\n",
    "\n",
    "        return self.pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(max_depth=5, max_pages=2500)\n",
    "new_pages = crawler.crawl('https://nightlies.apache.org/flink/flink-docs-release-1.16/')\n",
    "for page in new_pages:\n",
    "    print(page['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_pages[0]['content'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list()\n",
    "for page in new_pages:\n",
    "    chunks = splitter.split_text(page['content'])\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={'url': page['url'], 'type':\"document\"}\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TokenTextSplitter(encoding_name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_URL = os.environ['MILVUS_URL']\n",
    "MILVUS_KEY = os.environ['MILVUS_URL']\n",
    "DIMS = 1024\n",
    "EMBEDDING_MODEL = \"embed-english-v3.0\"\n",
    "COHERE_KEY=os.environ['COHERE_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.embeddings import CohereEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(COHERE_KEY)\n",
    "\n",
    "response = co.tokenize(text=new_pages[0]['content'], model=EMBEDDING_MODEL)  # optional\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.split_text(new_pages[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_pages[0]['content'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = CohereEmbeddings(model=EMBEDDING_MODEL, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.zilliz import Zilliz\n",
    "\n",
    "zilliz = Zilliz(\n",
    "    embedding_function = embedding_fn,\n",
    "    collection_name=\"Flink\",\n",
    "    connection_args={\"uri\": MILVUS_URL, \"token\": MILVUS_KEY},\n",
    "    auto_id=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for index, doc in zip(indexes[start:], documents[start:]):\n",
    "    print(index)\n",
    "    time.sleep(1)\n",
    "    zilliz.add_documents([doc], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = zilliz.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"WHat is flink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank(top_n=5, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever.invoke(\"What is flink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_cohere import ChatCohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatCohere(model=\"command-r-plus\", temperature=0.0, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]):\n",
    "    \n",
    "    text = \"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        xml_tag_start = f\"<{doc.metadata['url'].lower()}>\"\n",
    "        xml_tag_end = f\"</{doc.metadata['url'].lower()}>\"\n",
    "        content = doc.page_content\n",
    "        text += f\"{xml_tag_start}\\n{content}\\n{xml_tag_end}\\n\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"List down all the commands used in the flink documenatation along with explanation of the command\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = \"AIzaSyBVI2jAHepUzLwWoK6qwXCOYxD0NFzZIns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GEMINI_KEY, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.Keep the answer concise and to the point. Write down the citation at the end of the answer that you have taken reference from. The citation names are in form of urls, that are provided in the xml tags.\n",
    "Follow below mention format for citation\n",
    "Citation:\n",
    "        (1) Source URL 1\n",
    "        (2) Source URL 2\n",
    "Question: {question} \\nContext: {context} \\nAnswer\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_rag = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | example_prompt\n",
    "    | google_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_rag.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_rag.invoke(\"List down all the commands used in the flink documenatation along with explanation of the command.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
